<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Module Reference - Sonnet Docs</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Module Reference";
    var mkdocs_page_input_path = "sonnet.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Sonnet Docs</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Sonnet</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../INSTALL/">Installing from source</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../contents/">Contents</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Module Reference</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#sonnet-module-reference">sonnet - module reference</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#other-functions-and-classes">Other Functions and Classes</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Sonnet Docs</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Module Reference</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/deepmind/sonnet/edit/master/docs/sonnet.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <!-- This file is machine generated: DO NOT EDIT THIS FILE.
     Edits will be overwritten the next time the file is generated. -->

<!-- common_typos_disable -->

<h1 id="sonnet-module-reference">sonnet - module reference</h1>
<p>This python module contains Neural Network Modules for TensorFlow.</p>
<p>Each module is a Python object which conceptually "owns" any
variables required in that part of the Neural Network. The <code>__call__</code> function
on the object is used to connect that Module into the Graph, and this may be
called repeatedly with sharing automatically taking place.</p>
<p>Everything public should be imported by this top level <code>__init__.py</code> so that the
library can be used as follows:</p>
<pre><code>import sonnet as snt

linear = snt.Linear(...)
</code></pre>

<h2 id="other-functions-and-classes">Other Functions and Classes</h2>
<h3 id="class-actcore"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/pondering_rnn.py?q=class:ACTCore"><code>class ACTCore</code></a><a id="ACTCore" /></h3>
<p>Adaptive computation time core.</p>
<p>Implementation of the model described in "Adaptive Computation Time for
Recurrent Neural Networks" paper, https://arxiv.org/abs/1603.08983.</p>
<p>The <code>ACTCore</code> incorporates the pondering RNN of ACT, with different
computation times for each element in the mini batch. Each pondering step is
performed by the <code>core</code> passed to the constructor of <code>ACTCore</code>.</p>
<p>The output of the <code>ACTCore</code> is made of <code>(act_out, (iteration, remainder)</code>,
where</p>
<ul>
<li><code>iteration</code> counts the number of pondering step in each batch element;</li>
<li><code>remainder</code> is the remainder as defined in the ACT paper;</li>
<li><code>act_out</code> is the weighted average output of all pondering steps (see ACT
  paper for more info).</li>
</ul>
<h4 id="actcore__init__core-output_size-threshold-get_state_for_halting-max_steps0-nameact_core"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/pondering_rnn.py?l=73"><code>ACTCore.__init__(core, output_size, threshold, get_state_for_halting, max_steps=0, name='act_core')</code></a><a id="ACTCore.__init__" /></h4>
<p>Constructor.</p>
<h5 id="args">Args:</h5>
<ul>
<li><code>core</code>: A <code>sonnet.RNNCore</code> object. This should only take a single <code>Tensor</code>
      in input, and output only a single flat <code>Tensor</code>.</li>
<li><code>output_size</code>: An integer. The size of each output in the sequence.</li>
<li><code>threshold</code>: A float between 0 and 1. Probability to reach for ACT to stop
      pondering.</li>
<li><code>get_state_for_halting</code>: A callable that can take the <code>core</code> state and
      return the input to the halting function.</li>
<li><code>max_steps</code>: Integer &gt;= 0, that controls the maximum number of ponder steps.
      If equal to 0, then this disables control.</li>
<li><code>name</code>: A string. The name of this module.</li>
</ul>
<h5 id="raises">Raises:</h5>
<ul>
<li><code>ValueError</code>: if <code>threshold</code> is not between 0 and 1.</li>
<li><code>ValueError</code>: if <code>core</code> has either nested outputs or outputs that are not
      one dimensional.</li>
</ul>
<h4 id="actcore__call__x-prev_state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/pondering_rnn.py?l=166"><code>ACTCore.__call__(x, prev_state)</code></a><a id="ACTCore.__call__" /></h4>
<p>Connects the core to the graph.</p>
<h5 id="args_1">Args:</h5>
<ul>
<li><code>x</code>: Input <code>Tensor</code> of shape <code>(batch_size, input_size)</code>.</li>
<li><code>prev_state</code>: Previous state. This could be a <code>Tensor</code>, or a tuple of
      <code>Tensor</code>s.</li>
</ul>
<h5 id="returns">Returns:</h5>
<p>The tuple <code>(output, state)</code> for this core.</p>
<h5 id="raises_1">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the <code>Tensor</code> <code>x</code> does not have rank 2.</li>
</ul>
<h4 id="actcorebatch_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/pondering_rnn.py?l=122"><code>ACTCore.batch_size</code></a><a id="ACTCore.batch_size" /></h4>
<h4 id="actcoreconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>ACTCore.connected_subgraphs</code></a><a id="ACTCore.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="actcoredefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>ACTCore.defun()</code></a><a id="ACTCore.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="actcoredefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>ACTCore.defun_wrapped</code></a><a id="ACTCore.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="actcoredtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/pondering_rnn.py?l=127"><code>ACTCore.dtype</code></a><a id="ACTCore.dtype" /></h4>
<h4 id="actcoreget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>ACTCore.get_all_variables(collection='trainable_variables')</code></a><a id="ACTCore.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_2">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_1">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_2">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="actcoreget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>ACTCore.get_possible_initializer_keys(cls)</code></a><a id="ACTCore.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_2">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="actcoreget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>ACTCore.get_variables(collection='trainable_variables')</code></a><a id="ACTCore.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_3">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_3">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_3">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="actcoregraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>ACTCore.graph</code></a><a id="ACTCore.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="actcoreinitial_stateargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/pondering_rnn.py?l=110"><code>ACTCore.initial_state(*args, **kwargs)</code></a><a id="ACTCore.initial_state" /></h4>
<h4 id="actcoreis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>ACTCore.is_connected</code></a><a id="ACTCore.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="actcorelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>ACTCore.last_connected_subgraph</code></a><a id="ACTCore.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_4">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_4">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="actcoremodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>ACTCore.module_name</code></a><a id="ACTCore.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="actcorename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>ACTCore.name_scopes</code></a><a id="ACTCore.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="actcorenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>ACTCore.non_trainable_variables</code></a><a id="ACTCore.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_5">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_5">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="actcoreoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/pondering_rnn.py?l=113"><code>ACTCore.output_size</code></a><a id="ACTCore.output_size" /></h4>
<h4 id="actcorescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>ACTCore.scope_name</code></a><a id="ACTCore.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="actcorestate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/pondering_rnn.py?l=118"><code>ACTCore.state_size</code></a><a id="ACTCore.state_size" /></h4>
<h4 id="actcoretrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>ACTCore.trainable_variables</code></a><a id="ACTCore.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_6">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_6">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="actcorevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>ACTCore.variable_scope</code></a><a id="ACTCore.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_7">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_7">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="actcorevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>ACTCore.variables</code></a><a id="ACTCore.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_8">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_8">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="actcorezero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>ACTCore.zero_state(batch_size, dtype)</code></a><a id="ACTCore.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_4">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_9">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-abstractmodule"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?q=class:AbstractModule"><code>class AbstractModule</code></a><a id="AbstractModule" /></h3>
<p>Superclass for Sonnet Modules.</p>
<p>This class defines the functionality that every module should implement,
principally the <code>build</code> method which is wrapped using <code>tf.make_template</code>
and called from <code>__call__</code>. Every time the module is called it will
be connected into the graph but using the same shared set of variables, thanks
to the template.</p>
<p>For this to work correctly, the <code>build</code> implementation in the derived class
must access all variables using <code>tf.get_variable</code>, not <code>tf.Variable</code>. The same
set of variables must be created each time, if this is not the case an Error
will be raised.</p>
<p>Every subclass must call this class' <code>__init__</code> at the start of their
<code>__init__</code>, passing the relevant name. If this step is omitted variable
sharing will not work.</p>
<h4 id="abstractmodule__init___sentinelnone-custom_getternone-namenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=126"><code>AbstractModule.__init__(_sentinel=None, custom_getter=None, name=None)</code></a><a id="AbstractModule.__init__" /></h4>
<p>Performs the initialisation necessary for all AbstractModule instances.</p>
<p>Every subclass of AbstractModule must begin their constructor with a call to
this constructor, i.e.</p>
<p><code>super(MySubModule, self).__init__(custom_getter=custom_getter, name=name)</code>.</p>
<p>If you instantiate sub-modules in <strong>init</strong> you must create them within the
<code>_enter_variable_scope</code> context manager to ensure they are in the module's
variable scope. Alternatively, instantiate sub-modules in <code>_build</code>.</p>
<h5 id="args_5">Args:</h5>
<p>_sentinel: Variable that only carries a non-None value if <code>__init__</code> was
      called without named parameters. If this is the case, a deprecation
      warning is issued in form of a <code>ValueError</code>.</p>
<ul>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the <code>tf.get_variable</code>
    documentation for information about the custom_getter API.</li>
<li><code>name</code>: Name of this module. Used to construct the Templated build function.
      If <code>None</code> the module's class name is used (converted to snake case).</li>
</ul>
<h5 id="raises_9">Raises:</h5>
<ul>
<li><code>TypeError</code>: If <code>name</code> is not a string.</li>
<li><code>TypeError</code>: If a given <code>custom_getter</code> is not callable.</li>
<li><code>ValueError</code>: If <code>__init__</code> was called without named arguments.</li>
</ul>
<h4 id="abstractmodule__call__args-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=294"><code>AbstractModule.__call__(*args, **kwargs)</code></a><a id="AbstractModule.__call__" /></h4>
<p>Add elements to the Graph, computing output Tensors from input Tensors.</p>
<p>Subclasses must implement this method, which will be wrapped in a Template.</p>
<h5 id="args_6">Args:</h5>
<ul>
<li><code>*args</code>: Input Tensors.</li>
<li><code>**kwargs</code>: Additional Python flags controlling connection.</li>
</ul>
<h5 id="returns_10">Returns:</h5>
<p>output Tensor(s).</p>
<h4 id="abstractmoduleconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>AbstractModule.connected_subgraphs</code></a><a id="AbstractModule.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="abstractmoduledefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>AbstractModule.defun()</code></a><a id="AbstractModule.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="abstractmoduledefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>AbstractModule.defun_wrapped</code></a><a id="AbstractModule.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="abstractmoduleget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>AbstractModule.get_all_variables(collection='trainable_variables')</code></a><a id="AbstractModule.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_7">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_11">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_10">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="abstractmoduleget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>AbstractModule.get_possible_initializer_keys(cls)</code></a><a id="AbstractModule.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_12">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="abstractmoduleget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>AbstractModule.get_variables(collection='trainable_variables')</code></a><a id="AbstractModule.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_8">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_13">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_11">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="abstractmodulegraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>AbstractModule.graph</code></a><a id="AbstractModule.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="abstractmoduleis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>AbstractModule.is_connected</code></a><a id="AbstractModule.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="abstractmodulelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>AbstractModule.last_connected_subgraph</code></a><a id="AbstractModule.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_14">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_12">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="abstractmodulemodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>AbstractModule.module_name</code></a><a id="AbstractModule.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="abstractmodulename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>AbstractModule.name_scopes</code></a><a id="AbstractModule.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="abstractmodulenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>AbstractModule.non_trainable_variables</code></a><a id="AbstractModule.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_15">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_13">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="abstractmodulescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>AbstractModule.scope_name</code></a><a id="AbstractModule.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="abstractmoduletrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>AbstractModule.trainable_variables</code></a><a id="AbstractModule.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_16">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_14">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="abstractmodulevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>AbstractModule.variable_scope</code></a><a id="AbstractModule.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_17">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_15">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="abstractmodulevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>AbstractModule.variables</code></a><a id="AbstractModule.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_18">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_16">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-addbias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?q=class:AddBias"><code>class AddBias</code></a><a id="AddBias" /></h3>
<p>AddBias module.</p>
<h4 id="addbias__init__output_shapenone-bias_dimsnone-initializersnone-partitionersnone-regularizersnone-nameadd"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=510"><code>AddBias.__init__(output_shape=None, bias_dims=None, initializers=None, partitioners=None, regularizers=None, name='add')</code></a><a id="AddBias.__init__" /></h4>
<p>Constructs an AddBias module that supports broadcasting.</p>
<h5 id="args_9">Args:</h5>
<ul>
<li><code>output_shape</code>: Output dimensionality. <code>output_shape</code> can be either <code>None</code>,
      a <code>tuple</code>, or a <code>callable</code>. In the latter case, since the function
      invocation is deferred to graph construction time, the user must only
      ensure that <code>output_shape</code> can be called, returning a tuple, when
      build is called. If <code>output_shape</code> is left as <code>None</code>, the size will be
      directly inferred by the input.</li>
<li><code>bias_dims</code>: List of which dimensions to retain from the input shape when
      constructing the bias. The remaining dimensions will get broadcasted
      over (given size of 1), and leading dimensions will be removed
      completely. For example, for an input of [batch_size, dim1_size,
      dim2_size, dim3_size] and <code>bias_dims=[1, 3]</code>, the resulting
      bias will have shape [dim1_size, 1, dim3_size]. The default is to
      retain all dimensions apart from the minibatch dimension. Trying to
      retain the bias shape over the minibatch dimension, e.g.
      <code>bias_dims=[0]</code>, will result in an error at build time. See the
      'Example Usage' section below for more information.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the biases
      (with key 'b'). The default initializer for the bias is a zero
      initializer.</li>
<li><code>partitioners</code>: Optional dict containing a partitioner to partition
      the bias (with key 'b'). As a default, no partitioner is used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers of the biases
    (with key 'b'). As a default, no regularizers are used. A regularizer
    should be a function that takes a single <code>Tensor</code> as an input and
    returns a scalar <code>Tensor</code> output, e.g. the L1 and L2 regularizers in
    <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<p>Example Usage:</p>
<pre><code class="python"># Create a 4D input Tensor.
input = tf.random_normal(
    shape=(batch_size, dim1_size, dim2_size, dim3_size)))

# Create a scalar bias:
scalar_bias = snt.AddBias(bias_dims=[])
scalar_bias_output = scalar_bias(input)
scalar_bias.b.get_shape()  # ()

# Create a bias over all non-minibatch dimensions:
all_bias = snt.AddBias()  # or snt.AddBias(bias_dims=None)
all_bias_output = all_bias(input)
all_bias.b.get_shape()  # (dim1_size, dim2_size, dim3_size)

# Create a bias over the last non-minibatch dimension:
last_bias = snt.AddBias(bias_dims=[-1])
last_bias_output = last_bias(input)
last_bias.b.get_shape()  # (dim3_size)

# Create a bias over the first non-minibatch dimension:
first_bias = snt.AddBias(bias_dims=[1])
first_bias_output = first_bias(input)
first_bias.b.get_shape()  # (dim1_size, 1, 1)

# Subtract and later add the same learned bias:
bias = snt.AddBias()
hidden1 = bias(input, multiplier=-1)
# ...
reconstructed_input = bias(hidden4)

</code></pre>

<h5 id="raises_17">Raises:</h5>
<ul>
<li><code>KeyError</code>: If <code>initializers</code> contains any keys other than 'b'.</li>
<li><code>KeyError</code>: If <code>partitioners</code> contains any keys other than 'b'.</li>
<li><code>KeyError</code>: If <code>regularizers</code> contains any keys other than 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers are not callable.</li>
<li><code>TypeError</code>: If any of the given partitioners are not callable.</li>
<li><code>TypeError</code>: If any of the given regularizers are not callable.</li>
</ul>
<h4 id="addbias__call__inputs-multiplier1"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=603"><code>AddBias.__call__(inputs, multiplier=1)</code></a><a id="AddBias.__call__" /></h4>
<p>Connects the Add module into the graph, with input Tensor <code>inputs</code>.</p>
<h5 id="args_10">Args:</h5>
<ul>
<li><code>inputs</code>: A Tensor of size <code>[batch_size, input_size1, ...]</code>.</li>
<li><code>multiplier</code>: A scalar or Tensor which the bias term is multiplied by
    before adding it to <code>inputs</code>. Anything which works in the expression
    <code>bias * multiplier</code> is acceptable here. This may be useful if you want
    to add a bias in one place and subtract the same bias in another place
    via <code>multiplier=-1</code>.</li>
</ul>
<h5 id="returns_19">Returns:</h5>
<p>A Tensor of size <code>[batch_size, input_size1, ...]</code>.</p>
<h5 id="raises_18">Raises:</h5>
<p>base.IncompatibleShapeError: If the input is not a &gt;= 2D <code>Tensor</code>.
  base.IncompatibleShapeError: If connecting the module into the graph
      any time after the first time, and the inferred size of the input does
      not match previous invocations.
  base.IncompatibleShapeError: If the <code>output_shape</code> has been specified
      but it does not match the input_shape`.
  base.ParentNotBuiltError: If the module is a transposed and the original
      untransposed module has not been built.</p>
<h4 id="addbiasb"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=674"><code>AddBias.b</code></a><a id="AddBias.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_20">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_19">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the
      graph yet, meaning the variables do not exist.</p>
<h4 id="addbiasconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>AddBias.connected_subgraphs</code></a><a id="AddBias.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="addbiasdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>AddBias.defun()</code></a><a id="AddBias.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="addbiasdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>AddBias.defun_wrapped</code></a><a id="AddBias.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="addbiasget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>AddBias.get_all_variables(collection='trainable_variables')</code></a><a id="AddBias.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_11">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_21">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_20">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="addbiasget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>AddBias.get_possible_initializer_keys(cls)</code></a><a id="AddBias.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_22">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="addbiasget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>AddBias.get_variables(collection='trainable_variables')</code></a><a id="AddBias.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_12">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_23">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_21">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="addbiasgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>AddBias.graph</code></a><a id="AddBias.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="addbiasinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=689"><code>AddBias.input_shape</code></a><a id="AddBias.input_shape" /></h4>
<p>Returns shape of input <code>Tensor</code> passed at last call to <code>build</code>.</p>
<h4 id="addbiasis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>AddBias.is_connected</code></a><a id="AddBias.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="addbiaslast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>AddBias.last_connected_subgraph</code></a><a id="AddBias.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_24">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_22">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="addbiasmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>AddBias.module_name</code></a><a id="AddBias.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="addbiasname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>AddBias.name_scopes</code></a><a id="AddBias.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="addbiasnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>AddBias.non_trainable_variables</code></a><a id="AddBias.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_25">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_23">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="addbiasscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>AddBias.scope_name</code></a><a id="AddBias.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="addbiastrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>AddBias.trainable_variables</code></a><a id="AddBias.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_26">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_24">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="addbiastransposenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=696"><code>AddBias.transpose(name=None)</code></a><a id="AddBias.transpose" /></h4>
<p>Returns transposed <code>AddBias</code> module.</p>
<h5 id="args_13">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of transpose module. The default name
      is constructed by appending "_transpose" to <code>self.module_name</code>.</li>
</ul>
<h5 id="returns_27">Returns:</h5>
<p>Transposed <code>AddBias</code> module.</p>
<h4 id="addbiasvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>AddBias.variable_scope</code></a><a id="AddBias.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_28">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_25">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="addbiasvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>AddBias.variables</code></a><a id="AddBias.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_29">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_26">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-affinegridwarper"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?q=class:AffineGridWarper"><code>class AffineGridWarper</code></a><a id="AffineGridWarper" /></h3>
<p>Affine Grid Warper class.</p>
<p>The affine grid warper generates a reference grid of n-dimensional points
and warps it via an affine transormation model determined by an input
parameter Tensor. Some of the transformation parameters can be fixed at
construction time via an <code>AffineWarpConstraints</code> object.</p>
<h4 id="affinegridwarper__init__source_shape-output_shape-constraintsnone-nameaffine_grid_warper"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=159"><code>AffineGridWarper.__init__(source_shape, output_shape, constraints=None, name='affine_grid_warper')</code></a><a id="AffineGridWarper.__init__" /></h4>
<p>Constructs an AffineGridWarper.</p>
<p><code>source_shape</code> and <code>output_shape</code> are used to define the size of the source
and output signal domains, as opposed to the shape of the respective
Tensors. For example, for an image of size <code>width=W</code> and <code>height=H</code>,
<code>{source,output}_shape=[H, W]</code>; for a volume of size <code>width=W</code>, <code>height=H</code>
and <code>depth=D</code>, <code>{source,output}_shape=[H, W, D]</code>.</p>
<h5 id="args_14">Args:</h5>
<ul>
<li><code>source_shape</code>: Iterable of integers determining the size of the source
    signal domain.</li>
<li><code>output_shape</code>: Iterable of integers determining the size of the destination
    resampled signal domain.</li>
<li><code>constraints</code>: Either a double list of shape <code>[N, N+1]</code> defining constraints
    on the entries of a matrix defining an affine transformation in N
    dimensions, or an <code>AffineWarpConstraints</code> object. If the double list is
    passed, a numeric value bakes in a constraint on the corresponding
    entry in the tranformation matrix, whereas <code>None</code> implies that the
    corresponding entry will be specified at run time.</li>
<li><code>name</code>: Name of module.</li>
</ul>
<h5 id="raises_27">Raises:</h5>
<ul>
<li><code>Error</code>: If constraints fully define the affine transformation; or if
    input grid shape and contraints have different dimensionality.</li>
<li><code>TypeError</code>: If output_shape and source_shape are not both iterable.</li>
</ul>
<h4 id="affinegridwarper__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=274"><code>AffineGridWarper.__call__(inputs)</code></a><a id="AffineGridWarper.__call__" /></h4>
<p>Assembles the module network and adds it to the graph.</p>
<p>The internal computation graph is assembled according to the set of
constraints provided at construction time.</p>
<h5 id="args_15">Args:</h5>
<ul>
<li><code>inputs</code>: Tensor containing a batch of transformation parameters.</li>
</ul>
<h5 id="returns_30">Returns:</h5>
<p>A batch of warped grids.</p>
<h5 id="raises_28">Raises:</h5>
<ul>
<li><code>Error</code>: If the input tensor size is not consistent with the constraints
    passed at construction time.</li>
</ul>
<h4 id="affinegridwarperconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>AffineGridWarper.connected_subgraphs</code></a><a id="AffineGridWarper.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="affinegridwarperconstraints"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=357"><code>AffineGridWarper.constraints</code></a><a id="AffineGridWarper.constraints" /></h4>
<h4 id="affinegridwarperdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>AffineGridWarper.defun()</code></a><a id="AffineGridWarper.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="affinegridwarperdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>AffineGridWarper.defun_wrapped</code></a><a id="AffineGridWarper.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="affinegridwarperget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>AffineGridWarper.get_all_variables(collection='trainable_variables')</code></a><a id="AffineGridWarper.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_16">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_31">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_29">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="affinegridwarperget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>AffineGridWarper.get_possible_initializer_keys(cls)</code></a><a id="AffineGridWarper.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_32">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="affinegridwarperget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>AffineGridWarper.get_variables(collection='trainable_variables')</code></a><a id="AffineGridWarper.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_17">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_33">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_30">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="affinegridwarpergraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>AffineGridWarper.graph</code></a><a id="AffineGridWarper.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="affinegridwarperinversenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=361"><code>AffineGridWarper.inverse(name=None)</code></a><a id="AffineGridWarper.inverse" /></h4>
<p>Returns a <code>sonnet</code> module to compute inverse affine transforms.</p>
<p>The function first assembles a network that given the constraints of the
  current AffineGridWarper and a set of input parameters, retrieves the
  coefficients of the corresponding inverse affine transform, then feeds its
  output into a new AffineGridWarper setup to correctly warp the <code>output</code>
  space into the <code>source</code> space.</p>
<h5 id="args_18">Args:</h5>
<ul>
<li><code>name</code>: Name of module implementing the inverse grid transformation.</li>
</ul>
<h5 id="returns_34">Returns:</h5>
<p>A <code>sonnet</code> module performing the inverse affine transform of a reference
  grid of points via an AffineGridWarper module.</p>
<h5 id="raises_31">Raises:</h5>
<p>tf.errors.UnimplementedError: If the function is called on a non 2D
    instance of AffineGridWarper.</p>
<h4 id="affinegridwarperis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>AffineGridWarper.is_connected</code></a><a id="AffineGridWarper.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="affinegridwarperlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>AffineGridWarper.last_connected_subgraph</code></a><a id="AffineGridWarper.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_35">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_32">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="affinegridwarpermodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>AffineGridWarper.module_name</code></a><a id="AffineGridWarper.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="affinegridwarpern_coeff"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=86"><code>AffineGridWarper.n_coeff</code></a><a id="AffineGridWarper.n_coeff" /></h4>
<p>Returns number of coefficients of warping function.</p>
<h4 id="affinegridwarpername_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>AffineGridWarper.name_scopes</code></a><a id="AffineGridWarper.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="affinegridwarpernon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>AffineGridWarper.non_trainable_variables</code></a><a id="AffineGridWarper.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_36">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_33">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="affinegridwarperoutput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=101"><code>AffineGridWarper.output_shape</code></a><a id="AffineGridWarper.output_shape" /></h4>
<p>Returns a tuple containing the shape of the output grid.</p>
<h4 id="affinegridwarperpsi"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=91"><code>AffineGridWarper.psi</code></a><a id="AffineGridWarper.psi" /></h4>
<p>Returns a list of features used to compute the grid warp.</p>
<h4 id="affinegridwarperscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>AffineGridWarper.scope_name</code></a><a id="AffineGridWarper.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="affinegridwarpersource_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=96"><code>AffineGridWarper.source_shape</code></a><a id="AffineGridWarper.source_shape" /></h4>
<p>Returns a tuple containing the shape of the source signal.</p>
<h4 id="affinegridwarpertrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>AffineGridWarper.trainable_variables</code></a><a id="AffineGridWarper.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_37">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_34">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="affinegridwarpervariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>AffineGridWarper.variable_scope</code></a><a id="AffineGridWarper.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_38">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_35">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="affinegridwarpervariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>AffineGridWarper.variables</code></a><a id="AffineGridWarper.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_39">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_36">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-affinewarpconstraints"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?q=class:AffineWarpConstraints"><code>class AffineWarpConstraints</code></a><a id="AffineWarpConstraints" /></h3>
<p>Affine warp contraints class.</p>
<p><code>AffineWarpConstraints</code> allow for very succinct definitions of constraints on
the values of entries in affine transform matrices.</p>
<h4 id="affinewarpconstraints__init__constraintsnone-none-none-none-none-none"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=468"><code>AffineWarpConstraints.__init__(constraints=((None, None, None), (None, None, None)))</code></a><a id="AffineWarpConstraints.__init__" /></h4>
<p>Creates a constraint definition for an affine transformation.</p>
<h5 id="args_19">Args:</h5>
<ul>
<li><code>constraints</code>: A doubly-nested iterable of shape <code>[N, N+1]</code> defining
    constraints on the entries of a matrix that represents an affine
    transformation in <code>N</code> dimensions. A numeric value bakes in a constraint
    on the corresponding entry in the tranformation matrix, whereas <code>None</code>
    implies that the corresponding entry will be specified at run time.</li>
</ul>
<h5 id="raises_37">Raises:</h5>
<ul>
<li><code>TypeError</code>: If <code>constraints</code> is not a nested iterable.</li>
<li><code>ValueError</code>: If the double iterable <code>constraints</code> has inconsistent
    dimensions.</li>
</ul>
<h4 id="affinewarpconstraintscombine_withadditional_constraints"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=538"><code>AffineWarpConstraints.combine_with(additional_constraints)</code></a><a id="AffineWarpConstraints.combine_with" /></h4>
<p>Combines two sets of constraints into a coherent single set.</p>
<h4 id="affinewarpconstraintsconstraints"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=514"><code>AffineWarpConstraints.constraints</code></a><a id="AffineWarpConstraints.constraints" /></h4>
<h4 id="affinewarpconstraintsmask"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=510"><code>AffineWarpConstraints.mask</code></a><a id="AffineWarpConstraints.mask" /></h4>
<h4 id="affinewarpconstraintsno_constraintscls-num_dim2"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=549"><code>AffineWarpConstraints.no_constraints(cls, num_dim=2)</code></a><a id="AffineWarpConstraints.no_constraints" /></h4>
<p>Empty set of constraints for a num_dim-ensional affine transform.</p>
<h4 id="affinewarpconstraintsno_shear_2dcls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=586"><code>AffineWarpConstraints.no_shear_2d(cls)</code></a><a id="AffineWarpConstraints.no_shear_2d" /></h4>
<h4 id="affinewarpconstraintsno_shear_3dcls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=590"><code>AffineWarpConstraints.no_shear_3d(cls)</code></a><a id="AffineWarpConstraints.no_shear_3d" /></h4>
<p>Assigns contraints on shear components of affine transform in 3d.</p>
<h4 id="affinewarpconstraintsnum_dim"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=518"><code>AffineWarpConstraints.num_dim</code></a><a id="AffineWarpConstraints.num_dim" /></h4>
<h4 id="affinewarpconstraintsnum_free_params"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=506"><code>AffineWarpConstraints.num_free_params</code></a><a id="AffineWarpConstraints.num_free_params" /></h4>
<h4 id="affinewarpconstraintsscale_2dcls-xnone-ynone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=567"><code>AffineWarpConstraints.scale_2d(cls, x=None, y=None)</code></a><a id="AffineWarpConstraints.scale_2d" /></h4>
<p>Assigns contraints on scaling components of affine transform in 2d.</p>
<h4 id="affinewarpconstraintsscale_3dcls-xnone-ynone-znone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=573"><code>AffineWarpConstraints.scale_3d(cls, x=None, y=None, z=None)</code></a><a id="AffineWarpConstraints.scale_3d" /></h4>
<p>Assigns contraints on scaling components of affine transform in 3d.</p>
<h4 id="affinewarpconstraintsshear_2dcls-xnone-ynone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=580"><code>AffineWarpConstraints.shear_2d(cls, x=None, y=None)</code></a><a id="AffineWarpConstraints.shear_2d" /></h4>
<p>Assigns contraints on shear components of affine transform in 2d.</p>
<h4 id="affinewarpconstraintstranslation_2dcls-xnone-ynone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=554"><code>AffineWarpConstraints.translation_2d(cls, x=None, y=None)</code></a><a id="AffineWarpConstraints.translation_2d" /></h4>
<p>Assign contraints on translation components of affine transform in 2d.</p>
<h4 id="affinewarpconstraintstranslation_3dcls-xnone-ynone-znone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=560"><code>AffineWarpConstraints.translation_3d(cls, x=None, y=None, z=None)</code></a><a id="AffineWarpConstraints.translation_3d" /></h4>
<p>Assign contraints on translation components of affine transform in 3d.</p>
<h3 id="class-attentiveread"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/attention.py?q=class:AttentiveRead"><code>class AttentiveRead</code></a><a id="AttentiveRead" /></h3>
<p>A module for reading with attention.</p>
<p>This module reads a weighted sum of embeddings from memory, where each
memory slot's weight is based on the logit returned by an attention embedding
module. A mask may be given to ignore some memory slots (e.g. when attending
over variable-length sequences).</p>
<h4 id="attentiveread__init__attention_logit_mod-nameattention"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/attention.py?l=46"><code>AttentiveRead.__init__(attention_logit_mod, name='attention')</code></a><a id="AttentiveRead.__init__" /></h4>
<p>Initialize AttentiveRead module.</p>
<h5 id="args_20">Args:</h5>
<ul>
<li><code>attention_logit_mod</code>: Module that produces logit corresponding to a memory
    slot's compatibility. Must map a [batch_size * memory_size,
    memory_word_size + query_word_size]-shaped Tensor to a
    [batch_size * memory_size, 1] shape Tensor.</li>
<li><code>name</code>: string. Name for module.</li>
</ul>
<h4 id="attentiveread__call__memory-query-memory_masknone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/attention.py?l=66"><code>AttentiveRead.__call__(memory, query, memory_mask=None)</code></a><a id="AttentiveRead.__call__" /></h4>
<p>Perform a differentiable read.</p>
<h5 id="args_21">Args:</h5>
<ul>
<li><code>memory</code>: [batch_size, memory_size, memory_word_size]-shaped Tensor of
    dtype float32. This represents, for each example and memory slot, a
    single embedding to attend over.</li>
<li><code>query</code>: [batch_size, query_word_size]-shaped Tensor of dtype float32.
    Represents, for each example, a single embedding representing a query.</li>
<li><code>memory_mask</code>: None or [batch_size, memory_size]-shaped Tensor of dtype
    bool. An entry of False indicates that a memory slot should not enter
    the resulting weighted sum. If None, all memory is used.</li>
</ul>
<h5 id="returns_40">Returns:</h5>
<p>An AttentionOutput instance containing:</p>
<ul>
<li><code>read</code>: [batch_size, memory_word_size]-shaped Tensor of dtype float32.
      This represents, for each example, a weighted sum of the contents of
      the memory.</li>
<li><code>weights</code>: [batch_size, memory_size]-shaped Tensor of dtype float32. This
      represents, for each example and memory slot, the attention weights
      used to compute the read.</li>
<li><code>weight_logits</code>: [batch_size, memory_size]-shaped Tensor of dtype float32.
      This represents, for each example and memory slot, the logits of the
      attention weights, that is, <code>weights</code> is calculated by taking the
      softmax of the weight logits.</li>
</ul>
<h5 id="raises_38">Raises:</h5>
<ul>
<li><code>UnderspecifiedError</code>: if memory_word_size or query_word_size can not be
    inferred.</li>
<li><code>IncompatibleShapeError</code>: if memory, query, memory_mask, or output of
    attention_logit_mod do not match expected shapes.</li>
</ul>
<h4 id="attentivereadconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>AttentiveRead.connected_subgraphs</code></a><a id="AttentiveRead.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="attentivereaddefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>AttentiveRead.defun()</code></a><a id="AttentiveRead.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="attentivereaddefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>AttentiveRead.defun_wrapped</code></a><a id="AttentiveRead.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="attentivereadget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>AttentiveRead.get_all_variables(collection='trainable_variables')</code></a><a id="AttentiveRead.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_22">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_41">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_39">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="attentivereadget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>AttentiveRead.get_possible_initializer_keys(cls)</code></a><a id="AttentiveRead.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_42">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="attentivereadget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>AttentiveRead.get_variables(collection='trainable_variables')</code></a><a id="AttentiveRead.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_23">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_43">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_40">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="attentivereadgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>AttentiveRead.graph</code></a><a id="AttentiveRead.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="attentivereadis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>AttentiveRead.is_connected</code></a><a id="AttentiveRead.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="attentivereadlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>AttentiveRead.last_connected_subgraph</code></a><a id="AttentiveRead.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_44">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_41">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="attentivereadmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>AttentiveRead.module_name</code></a><a id="AttentiveRead.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="attentivereadname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>AttentiveRead.name_scopes</code></a><a id="AttentiveRead.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="attentivereadnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>AttentiveRead.non_trainable_variables</code></a><a id="AttentiveRead.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_45">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_42">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="attentivereadscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>AttentiveRead.scope_name</code></a><a id="AttentiveRead.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="attentivereadtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>AttentiveRead.trainable_variables</code></a><a id="AttentiveRead.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_46">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_43">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="attentivereadvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>AttentiveRead.variable_scope</code></a><a id="AttentiveRead.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_47">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_44">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="attentivereadvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>AttentiveRead.variables</code></a><a id="AttentiveRead.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_48">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_45">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-batchapply"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?q=class:BatchApply"><code>class BatchApply</code></a><a id="BatchApply" /></h3>
<p>Merges a number of leading dimensions of an input tensor to manipulate it.</p>
<p>Merges a number of leading dimensions of a tensor into a single dimension,
connects the provided module, then splits the leading dimension of the
result to match the input.</p>
<p>Input tensors whose rank is smaller than the number of dimensions to collapse
(e.g. all scalar values, which are tensors of rank 0), are passed unaltered to
the provided module.</p>
<p>This is useful for applying some module to each timestep of a Time x Batch x N
tensor. If a module is hard coded to only support 2D (Batch x N) then the
full 3D Tensor cannot be provided. BatchApply will 'merge' the first two
dimensions of the sequence tensor by reshaping to a (Time * Batch) x N Tensor,
and then the internal module can be applied. The result of that operation is
reshaped such that its first dimensions are split to match the leading
dimensions of the input.</p>
<h4 id="batchapply__init__module_or_op-n_dims2-input_example_index0-namebatch_apply"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=1036"><code>BatchApply.__init__(module_or_op, n_dims=2, input_example_index=0, name='batch_apply')</code></a><a id="BatchApply.__init__" /></h4>
<p>Constructor of the module.</p>
<h5 id="args_24">Args:</h5>
<ul>
<li><code>module_or_op</code>: Module or tensorflow op to apply to an input tensor.</li>
<li><code>n_dims</code>: Number of dimensions to merge before using module on the input
      of BatchApply.</li>
<li><code>input_example_index</code>: Index of input that has same shape for the first
      <code>n_dims</code> dimensions as <code>module_or_op</code> output(s). This is used for
      unflattening the output(s) if static shape inference is not possible.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_46">Raises:</h5>
<ul>
<li><code>TypeError</code>: If n_dims is not an integer.</li>
<li><code>ValueError</code>: If n_dims is not greater than zero.</li>
</ul>
<h4 id="batchapply__call__args-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=1063"><code>BatchApply.__call__(*args, **kwargs)</code></a><a id="BatchApply.__call__" /></h4>
<p>Connects the BatchApply module into the graph.</p>
<h5 id="args_25">Args:</h5>
<ul>
<li><code>*args</code>: a Tensor or a nested list or dictionary of Tensors. The input
      tensors will have their first dimensions merged, then an op or a
      module will be called on the input. The first dimension of the output
      tensor(s) will be split again based on the leading dimensions of the
      first input tensor.</li>
<li><code>**kwargs</code>: Dictionary of named arguments; used in the same way as <code>*args</code>.</li>
</ul>
<h5 id="returns_49">Returns:</h5>
<p>A Tensor or nested list or dictionary of Tensors as a result of applying
  the process above. ("None" return values are also supported.)</p>
<h4 id="batchapplyconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>BatchApply.connected_subgraphs</code></a><a id="BatchApply.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="batchapplydefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>BatchApply.defun()</code></a><a id="BatchApply.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="batchapplydefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>BatchApply.defun_wrapped</code></a><a id="BatchApply.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="batchapplyget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>BatchApply.get_all_variables(collection='trainable_variables')</code></a><a id="BatchApply.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_26">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_50">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_47">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchapplyget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>BatchApply.get_possible_initializer_keys(cls)</code></a><a id="BatchApply.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_51">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="batchapplyget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>BatchApply.get_variables(collection='trainable_variables')</code></a><a id="BatchApply.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_27">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_52">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_48">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchapplygraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>BatchApply.graph</code></a><a id="BatchApply.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="batchapplyis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>BatchApply.is_connected</code></a><a id="BatchApply.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="batchapplylast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>BatchApply.last_connected_subgraph</code></a><a id="BatchApply.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_53">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_49">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchapplymodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>BatchApply.module_name</code></a><a id="BatchApply.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="batchapplyname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>BatchApply.name_scopes</code></a><a id="BatchApply.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="batchapplynon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>BatchApply.non_trainable_variables</code></a><a id="BatchApply.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_54">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_50">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchapplyscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>BatchApply.scope_name</code></a><a id="BatchApply.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="batchapplytrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>BatchApply.trainable_variables</code></a><a id="BatchApply.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_55">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_51">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchapplyvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>BatchApply.variable_scope</code></a><a id="BatchApply.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_56">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_52">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchapplyvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>BatchApply.variables</code></a><a id="BatchApply.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_57">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_53">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-batchflatten"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?q=class:BatchFlatten"><code>class BatchFlatten</code></a><a id="BatchFlatten" /></h3>
<p>Flattens the input Tensor, preserving the batch dimension(s).</p>
<h4 id="batchflatten__init__preserve_dims1-namebatch_flatten"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=880"><code>BatchFlatten.__init__(preserve_dims=1, name='batch_flatten')</code></a><a id="BatchFlatten.__init__" /></h4>
<p>Constructs a BatchFlatten module.</p>
<h5 id="args_28">Args:</h5>
<ul>
<li><code>preserve_dims</code>: Number of leading dimensions that will not be reshaped.
      For example, given an input Tensor with shape <code>[B, H, W, C]</code>:
        * <code>preserve_dims=1</code> will return a Tensor with shape <code>[B, H*W*C]</code>.
        * <code>preserve_dims=2</code> will return a Tensor with
            shape <code>[B, H, W*C]</code>.
        * <code>preserve_dims=3</code> will return the input itself,
            shape <code>[B, H, W, C]</code>.
        * <code>preserve_dims=4</code> will  return a Tensor with
            shape <code>[B, H, W, C, 1]</code>.
        * <code>preserve_dims&gt;=5</code> will throw an error on build.
      The preserved dimensions can be unknown at building time.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h4 id="batchflatten__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=779"><code>BatchFlatten.__call__(inputs)</code></a><a id="BatchFlatten.__call__" /></h4>
<p>Connects the module into the graph, with input Tensor <code>inputs</code>.</p>
<h5 id="args_29">Args:</h5>
<ul>
<li><code>inputs</code>: A Tensor of shape [b_1, b_2, ..., b_preserve_dims,
                             b_preserve_dims+1, ...].</li>
</ul>
<h5 id="returns_58">Returns:</h5>
<p>A Tensor of shape [b_1, b_2, ..., b_preserve_dims,
                     b_reshape_1, b_reshape_2, ...],
    with reshaping defined by the constructor <code>shape</code> parameter.</p>
<h5 id="raises_54">Raises:</h5>
<ul>
<li><code>ValueError</code>: If output shape is incompatible with input shape; or if
      shape array contains non numeric entries; or if shape array contains
      more than 1 wildcard -1; or if the input array contains unknown,
      non-preserved dimensions (except when the unknown dimension is the
      only non-preserved dimension and doesn't actually need reshaping).</li>
</ul>
<h4 id="batchflattenconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>BatchFlatten.connected_subgraphs</code></a><a id="BatchFlatten.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="batchflattendefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>BatchFlatten.defun()</code></a><a id="BatchFlatten.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="batchflattendefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>BatchFlatten.defun_wrapped</code></a><a id="BatchFlatten.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="batchflattenget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>BatchFlatten.get_all_variables(collection='trainable_variables')</code></a><a id="BatchFlatten.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_30">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_59">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_55">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchflattenget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>BatchFlatten.get_possible_initializer_keys(cls)</code></a><a id="BatchFlatten.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_60">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="batchflattenget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>BatchFlatten.get_variables(collection='trainable_variables')</code></a><a id="BatchFlatten.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_31">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_61">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_56">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchflattengraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>BatchFlatten.graph</code></a><a id="BatchFlatten.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="batchflatteninput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=862"><code>BatchFlatten.input_shape</code></a><a id="BatchFlatten.input_shape" /></h4>
<h4 id="batchflattenis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>BatchFlatten.is_connected</code></a><a id="BatchFlatten.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="batchflattenlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>BatchFlatten.last_connected_subgraph</code></a><a id="BatchFlatten.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_62">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_57">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchflattenmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>BatchFlatten.module_name</code></a><a id="BatchFlatten.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="batchflattenname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>BatchFlatten.name_scopes</code></a><a id="BatchFlatten.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="batchflattennon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>BatchFlatten.non_trainable_variables</code></a><a id="BatchFlatten.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_63">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_58">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchflattenscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>BatchFlatten.scope_name</code></a><a id="BatchFlatten.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="batchflattentrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>BatchFlatten.trainable_variables</code></a><a id="BatchFlatten.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_64">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_59">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchflattentransposenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=868"><code>BatchFlatten.transpose(name=None)</code></a><a id="BatchFlatten.transpose" /></h4>
<p>Returns transpose batch reshape.</p>
<h4 id="batchflattenvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>BatchFlatten.variable_scope</code></a><a id="BatchFlatten.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_65">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_60">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchflattenvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>BatchFlatten.variables</code></a><a id="BatchFlatten.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_66">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_61">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-batchnorm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?q=class:BatchNorm"><code>class BatchNorm</code></a><a id="BatchNorm" /></h3>
<p>Batch normalization module, including optional affine transformation.</p>
<p>This module maintains exponential moving averages of the mean and
variance, which can be optionally used to normalize at test time.</p>
<p>At training time, batch statistics (mean, variance) are not shared between
separate connections. The moving averages are shared between separate
connections. At both training and test time, the optional affine
transformation (<code>* gamma + beta</code>) is shared between separate connections.</p>
<p>This is also the case for distributed replica training, where the batch
statistics are not aggregated across replicas, but the moving averages are
shared globally.</p>
<p>When connecting the module to the graph, <code>is_training=True</code> means that</p>
<ul>
<li>Update ops are created to update the moving averages with the current
    batch's statistics.</li>
<li>Features are normalized using the <em>current batch's statistics</em>. The
    <code>test_local_stats</code> setting is ignored. The moving averages are
    <strong>not</strong> used.</li>
</ul>
<p>whereas <code>is_training=False</code> means that</p>
<ul>
<li>Update ops are not created.</li>
<li>Features are normalized using either:<ul>
<li>The test batch statistics if <code>test_local_stats=True</code> (default).</li>
<li>The moving averages if <code>test_local_stats=False</code>.</li>
</ul>
</li>
</ul>
<p>Local batch statistics are used by default at test time, but the moving
averages can be used by specifying a flag when connecting. One often wants
to use local batch statistics at test time to track the progress while the
model is trained as it would ensure that moving average updates do not affect
the training curves. Once the training is finished, it's often advantageous
to use moving average statistics, since it would make evaluation agnostic to
the batch size, and might even lead to small improvements over the local
batch statistics.</p>
<p>You can either update the moving averages automatically by setting
<code>update_ops_collection=None</code> or by running the ops in the given collection,
by default tf.GraphKeys.UPDATE_OPS.</p>
<p>For example, to run the updates automatically:</p>
<pre><code>bn = BatchNorm(update_ops_collection=None)
train_net = bn(train_inputs, is_training=True)
</code></pre>
<p>this does, however, have the effect of blocking the forwards pass of the
network until the update ops have been run and may have a small performance
penalty.</p>
<p>For example, to run the updates manually:</p>
<pre><code>bn = BatchNorm()
train_net = bn(train_inputs, is_training=True)

...

update_ops = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS))
train_op = tf.group(train_op, update_ops)
</code></pre>
<p>Then, whenever <code>train_op</code> is run so also are the moving average update ops.</p>
<p>Some batch normalization caveats:</p>
<ul>
<li>Batch normalization will remove the effect of adding a bias, so e.g.
    <code>use_bias=False</code> should be used for an immediately preceding snt.Linear
    module.</li>
<li>If your data batches aren't i.i.d. then batch normalization can allow your
    network to 'cheat' by using the batch statistics to peek at the rest of
    the batch. This can exhibit itself as a higher test score with
    <code>test_local_stats=True</code> than <code>test_local_stats=False</code>.</li>
</ul>
<h4 id="batchnorm__init__axisnone-offsettrue-scalefalse-decay_rate0999-eps0001-initializersnone-partitionersnone-regularizersnone-update_ops_collectionupdate_ops-fusedfalse-namebatch_norm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=138"><code>BatchNorm.__init__(axis=None, offset=True, scale=False, decay_rate=0.999, eps=0.001, initializers=None, partitioners=None, regularizers=None, update_ops_collection='update_ops', fused=False, name='batch_norm')</code></a><a id="BatchNorm.__init__" /></h4>
<p>Constructs a BatchNorm module.</p>
<p>By default reduces over all input tensor dimensions apart from the final
dimension. This has the effect of treating pixels in 1D/2D/3D images as
additional elements of the minibatch.</p>
<p>If this is not the desired behaviour, the user can specify the tensor
indices to reduce over with <code>axis</code>.</p>
<h5 id="args_32">Args:</h5>
<ul>
<li><code>axis</code>: Optional iterable of indices of dimensions to reduce over. By
    default <code>None</code> and all dimensions except the last are reduced over.</li>
<li><code>offset</code>: Optional boolean to specify whether or not to apply a trained
    component-wise bias after the batch normalization and scaling.</li>
<li><code>scale</code>: Optional boolean to specify whether or not to apply a trained
    component-wise scale after the batch normalization.</li>
<li><code>decay_rate</code>: Decay rate of the exponential moving averages of the mean
    and variance.</li>
<li><code>eps</code>: Small number to avoid dividing by zero when diving by the standard
    deviation.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the weights of
    the affine transform (<code>gamma</code> and <code>beta</code>).</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition the
    weights of the affine transform (<code>gamma</code> and <code>beta</code>).</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the weights of the
    affine transform ('gamma' and 'beta'). As a default, no regularizers are
    used. A regularizer should be a function that takes a single <code>Tensor</code> as
    an input and returns a scalar <code>Tensor</code> output, e.g. the L1 and L2
    regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>update_ops_collection</code>: Name of TensorFlow variable collection to add the
    moving average update ops to. If <code>None</code>, we instead add the update ops
    as control dependencies of the output of the module. This may result in
    some slowdown, as the feed-forward of the network is now blocked. By
    default, <code>tf.GraphKeys.UPDATE_OPS</code>.</li>
<li><code>fused</code>: Use nn.fused_batch_norm if True, nn.batch_normalization otherwise.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_62">Raises:</h5>
<ul>
<li><code>KeyError</code>: If <code>initializers</code> contains any keys other than <code>gamma</code>, <code>beta</code>,
    <code>moving_mean</code> or <code>moving_variance</code>.</li>
<li><code>KeyError</code>: If <code>partitioners</code> or <code>regularizers</code> contains any keys other
    than <code>gamma</code> or <code>beta</code>.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
    are not callable.</li>
</ul>
<h4 id="batchnorm__call__input_batch-is_training-test_local_statstrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=480"><code>BatchNorm.__call__(input_batch, is_training, test_local_stats=True)</code></a><a id="BatchNorm.__call__" /></h4>
<p>Connects the BatchNorm module into the graph.</p>
<h5 id="args_33">Args:</h5>
<ul>
<li><code>input_batch</code>: A Tensor of arbitrary dimension. By default, the final
    dimension is not reduced over when computing the minibatch statistics.</li>
<li><code>is_training</code>: A boolean to indicate if the module should be connected in
    training mode, meaning the moving averages are updated. Can be a Tensor.</li>
<li><code>test_local_stats</code>: A boolean to indicate if local batch statistics should
    be used when <code>is_training=False</code>. If not, moving averages are used.
    By default <code>True</code>. Can be a Tensor.</li>
</ul>
<h5 id="returns_67">Returns:</h5>
<p>A tensor with the same shape as <code>input_batch</code>.</p>
<h5 id="raises_63">Raises:</h5>
<p>base.IncompatibleShapeError: If <code>axis</code> is not valid for the
    input shape or has negative entries.
  base.NotSupportedError: If <code>input_batch</code> has data type of <code>tf.bfloat16</code>.</p>
<h4 id="batchnormbeta"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=581"><code>BatchNorm.beta</code></a><a id="BatchNorm.beta" /></h4>
<h4 id="batchnormconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>BatchNorm.connected_subgraphs</code></a><a id="BatchNorm.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="batchnormdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>BatchNorm.defun()</code></a><a id="BatchNorm.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="batchnormdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>BatchNorm.defun_wrapped</code></a><a id="BatchNorm.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="batchnormgamma"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=591"><code>BatchNorm.gamma</code></a><a id="BatchNorm.gamma" /></h4>
<h4 id="batchnormget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>BatchNorm.get_all_variables(collection='trainable_variables')</code></a><a id="BatchNorm.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_34">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_68">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_64">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>BatchNorm.get_possible_initializer_keys(cls)</code></a><a id="BatchNorm.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_69">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="batchnormget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>BatchNorm.get_variables(collection='trainable_variables')</code></a><a id="BatchNorm.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_35">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_70">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_65">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>BatchNorm.graph</code></a><a id="BatchNorm.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="batchnorminitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=559"><code>BatchNorm.initializers</code></a><a id="BatchNorm.initializers" /></h4>
<h4 id="batchnormis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>BatchNorm.is_connected</code></a><a id="BatchNorm.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="batchnormlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>BatchNorm.last_connected_subgraph</code></a><a id="BatchNorm.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_71">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_66">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>BatchNorm.module_name</code></a><a id="BatchNorm.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="batchnormmoving_mean"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=571"><code>BatchNorm.moving_mean</code></a><a id="BatchNorm.moving_mean" /></h4>
<h4 id="batchnormmoving_variance"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=576"><code>BatchNorm.moving_variance</code></a><a id="BatchNorm.moving_variance" /></h4>
<h4 id="batchnormname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>BatchNorm.name_scopes</code></a><a id="BatchNorm.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="batchnormnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>BatchNorm.non_trainable_variables</code></a><a id="BatchNorm.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_72">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_67">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=563"><code>BatchNorm.partitioners</code></a><a id="BatchNorm.partitioners" /></h4>
<h4 id="batchnormregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=567"><code>BatchNorm.regularizers</code></a><a id="BatchNorm.regularizers" /></h4>
<h4 id="batchnormscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>BatchNorm.scope_name</code></a><a id="BatchNorm.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="batchnormtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>BatchNorm.trainable_variables</code></a><a id="BatchNorm.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_73">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_68">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>BatchNorm.variable_scope</code></a><a id="BatchNorm.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_74">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_69">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>BatchNorm.variables</code></a><a id="BatchNorm.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_75">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_70">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-batchnormlstm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?q=class:BatchNormLSTM"><code>class BatchNormLSTM</code></a><a id="BatchNormLSTM" /></h3>
<p>LSTM recurrent network cell with optional peepholes, batch normalization.</p>
<p>The base implementation is based on: http://arxiv.org/abs/1409.2329. We add
forget_bias (default: 1) to the biases of the forget gate in order to
reduce the scale of forgetting in the beginning of the training.</p>
<h4 id="peep-hole-connections">Peep-hole connections</h4>
<p>Peep-hole connections may optionally be used by specifying a flag in the
constructor. These connections can aid increasing the precision of output
timing, for more details see:</p>
<p>https://research.google.com/pubs/archive/43905.pdf</p>
<h4 id="batch-normalization">Batch normalization</h4>
<p>The batch norm transformation (in training mode) is
  batchnorm(x) = gamma * (x - mean(x)) / stddev(x) + beta,
where gamma is a learnt scaling factor and beta is a learnt offset.</p>
<p>Batch normalization may optionally be used at different places in the LSTM by
specifying flag(s) in the constructor. These are applied when calculating
the gate activations and cell-to-hidden transformation. The set-up is based on</p>
<p>https://arxiv.org/pdf/1603.09025.pdf</p>
<h5 id="batch-normalization-where-to-apply">Batch normalization: where to apply?</h5>
<p>Batch norm can be applied in three different places in the LSTM:</p>
<p>(h) To the W_h h_{t-1} contribution to the gates from the previous hiddens.
  (x) To the W_x x_t contribution to the gates from the current input.
  (c) To the cell value c_t when calculating the output h_t from the cell.</p>
<p>(The notation here is consistent with the Recurrent Batch Normalization
paper). Each of these can be controlled individually, because batch norm is
expensive, and not all are necessary. The paper doesn't mention the relative
effects of these different batch norms; however, experimentation with a
shallow LSTM for the <code>permuted_mnist</code> sequence task suggests that (h) is the
most important and the other two can be left off. For other tasks or deeper
(stacked) LSTMs, other batch norm combinations may be more effective.</p>
<h5 id="batch-normalization-collecting-stats-training-vs-test">Batch normalization: collecting stats (training vs test)</h5>
<p>When switching to testing (see <code>LSTM.with_batch_norm_control</code>), we can use a
mean and stddev learnt from the training data instead of using the statistics
from the test data. (This both increases test accuracy because the statistics
have less variance, and if the test data does not have the same distribution
as the training data then we must use the training statistics to ensure the
effective network does not change when switching to testing anyhow.)</p>
<p>This does however introduces a slight subtlety. The first few time steps of
the RNN tend to have varying statistics (mean and variance) before settling
down to a steady value. Therefore in general, better performance is obtained
by using separate statistics for the first few time steps, and then using the
final set of statistics for all subsequent time steps. This is controlled by
the parameter <code>max_unique_stats</code>. (We can't have an unbounded number of
distinct statistics for both technical reasons and also for the case where
test sequences are longer than anything seen in training.)</p>
<p>You may be fine leaving it at its default value of 1. Small values (like 10)
may achieve better performance on some tasks when testing with cached
statistics.</p>
<p>Attributes:
  state_size: Tuple of <code>tf.TensorShape</code>s indicating the size of state tensors.
  output_size: <code>tf.TensorShape</code> indicating the size of the core output.
  use_peepholes: Boolean indicating whether peephole connections are used.
  use_batch_norm_h: Boolean indicating whether batch norm (h) is enabled.
  use_batch_norm_x: Boolean indicating whether batch norm (x) is enabled.
  use_batch_norm_c: Boolean indicating whether batch norm (c) is enabled.</p>
<h4 id="batchnormlstm__init__hidden_size-forget_bias10-initializersnone-partitionersnone-regularizersnone-use_peepholesfalse-use_batch_norm_htrue-use_batch_norm_xfalse-use_batch_norm_cfalse-max_unique_stats1-hidden_clip_valuenone-cell_clip_valuenone-custom_getternone-namebatch_norm_lstm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=630"><code>BatchNormLSTM.__init__(hidden_size, forget_bias=1.0, initializers=None, partitioners=None, regularizers=None, use_peepholes=False, use_batch_norm_h=True, use_batch_norm_x=False, use_batch_norm_c=False, max_unique_stats=1, hidden_clip_value=None, cell_clip_value=None, custom_getter=None, name='batch_norm_lstm')</code></a><a id="BatchNormLSTM.__init__" /></h4>
<p>Construct <code>BatchNormLSTM</code>.</p>
<h5 id="args_36">Args:</h5>
<ul>
<li><code>hidden_size</code>: (int) Hidden size dimensionality.</li>
<li><code>forget_bias</code>: (float) Bias for the forget activation.</li>
<li><code>initializers</code>: Dict containing ops to initialize the weights.
    This dictionary may contain any of the keys returned by
    <code>BatchNormLSTM.get_possible_initializer_keys</code>.
    The gamma and beta variables control batch normalization values for
    different batch norm transformations inside the cell; see the paper for
    details.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
    the weights and biases. As a default, no partitioners are used. This
    dict may contain any of the keys returned by
    <code>BatchNormLSTM.get_possible_initializer_keys</code>.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the weights and
    biases. As a default, no regularizers are used. This dict may contain
    any of the keys returned by
    <code>BatchNormLSTM.get_possible_initializer_keys</code>.</li>
<li><code>use_peepholes</code>: Boolean that indicates whether peephole connections are
    used.</li>
<li><code>use_batch_norm_h</code>: Boolean that indicates whether to apply batch
    normalization at the previous_hidden -&gt; gates contribution. If you are
    experimenting with batch norm then this may be the most effective to
    use, and is enabled by default.</li>
<li><code>use_batch_norm_x</code>: Boolean that indicates whether to apply batch
    normalization at the input -&gt; gates contribution.</li>
<li><code>use_batch_norm_c</code>: Boolean that indicates whether to apply batch
    normalization at the cell -&gt; output contribution.</li>
<li><code>max_unique_stats</code>: The maximum number of steps to use unique batch norm
    statistics for. (See module description above for more details.)</li>
<li><code>hidden_clip_value</code>: Optional number; if set, then the LSTM hidden state
    vector is clipped by this value.</li>
<li><code>cell_clip_value</code>: Optional number; if set, then the LSTM cell vector is
    clipped by this value.</li>
<li><code>custom_getter</code>: Callable that takes as a first argument the true getter,
    and allows overwriting the internal get_variable method. See the
    <code>tf.get_variable</code> documentation for more details.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_71">Raises:</h5>
<ul>
<li><code>KeyError</code>: if <code>initializers</code> contains any keys not returned by
    <code>BatchNormLSTM.get_possible_initializer_keys</code>.</li>
<li><code>KeyError</code>: if <code>partitioners</code> contains any keys not returned by
    <code>BatchNormLSTM.get_possible_initializer_keys</code>.</li>
<li><code>KeyError</code>: if <code>regularizers</code> contains any keys not returned by
    <code>BatchNormLSTM.get_possible_initializer_keys</code>.</li>
<li><code>ValueError</code>: if a peephole initializer is passed in the initializer list,
    but <code>use_peepholes</code> is False.</li>
<li><code>ValueError</code>: if a batch norm initializer is passed in the initializer list,
    but batch norm is disabled.</li>
<li><code>ValueError</code>: if none of the <code>use_batch_norm_*</code> options are True.</li>
<li><code>ValueError</code>: if <code>max_unique_stats</code> is &lt; 1.</li>
</ul>
<h4 id="batchnormlstm__call__inputs-prev_state-is_trainingnone-test_local_statstrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=816"><code>BatchNormLSTM.__call__(inputs, prev_state, is_training=None, test_local_stats=True)</code></a><a id="BatchNormLSTM.__call__" /></h4>
<p>Connects the LSTM module into the graph.</p>
<p>If this is not the first time the module has been connected to the graph,
the Tensors provided as inputs and state must have the same final
dimension, in order for the existing variables to be the correct size for
their corresponding multiplications. The batch size may differ for each
connection.</p>
<h5 id="args_37">Args:</h5>
<ul>
<li><code>inputs</code>: Tensor of size <code>[batch_size, input_size]</code>.</li>
<li><code>prev_state</code>: Tuple (prev_hidden, prev_cell), or if batch norm is enabled
    and <code>max_unique_stats &gt; 1</code>, then (prev_hidden, prev_cell, time_step).
    Here, prev_hidden and prev_cell are tensors of size
    <code>[batch_size, hidden_size]</code>, and time_step is used to indicate the
    current RNN step.</li>
<li><code>is_training</code>: Boolean indicating whether we are in training mode (as
    opposed to testing mode), passed to the batch norm
    modules. Note to use this you must wrap the cell via the
    <code>with_batch_norm_control</code> function.</li>
<li><code>test_local_stats</code>: Boolean indicating whether to use local batch statistics
    in test mode. See the <code>BatchNorm</code> documentation for more on this.</li>
</ul>
<h5 id="returns_76">Returns:</h5>
<p>A tuple (output, next_state) where 'output' is a Tensor of size
  <code>[batch_size, hidden_size]</code> and 'next_state' is a tuple
  (next_hidden, next_cell) or (next_hidden, next_cell, time_step + 1),
  where next_hidden and next_cell have size <code>[batch_size, hidden_size]</code>.</p>
<h5 id="raises_72">Raises:</h5>
<ul>
<li><code>ValueError</code>: If connecting the module into the graph any time after the
    first time, and the inferred size of the inputs does not match previous
    invocations.</li>
</ul>
<h4 id="batchnormlstmconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>BatchNormLSTM.connected_subgraphs</code></a><a id="BatchNormLSTM.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="batchnormlstmdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>BatchNormLSTM.defun()</code></a><a id="BatchNormLSTM.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="batchnormlstmdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>BatchNormLSTM.defun_wrapped</code></a><a id="BatchNormLSTM.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="batchnormlstmget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>BatchNormLSTM.get_all_variables(collection='trainable_variables')</code></a><a id="BatchNormLSTM.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_38">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_77">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_73">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormlstmget_possible_initializer_keyscls-use_peepholesfalse-use_batch_norm_htrue-use_batch_norm_xfalse-use_batch_norm_cfalse"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=769"><code>BatchNormLSTM.get_possible_initializer_keys(cls, use_peepholes=False, use_batch_norm_h=True, use_batch_norm_x=False, use_batch_norm_c=False)</code></a><a id="BatchNormLSTM.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<h5 id="the-set-of-all-possible-initializer-keys-are">The set of all possible initializer keys are:</h5>
<ul>
<li><code>w_gates</code>: weight for gates</li>
<li><code>b_gates</code>: bias of gates</li>
<li><code>w_f_diag</code>: weight for prev_cell -&gt; forget gate peephole</li>
<li><code>w_i_diag</code>: weight for prev_cell -&gt; input gate peephole</li>
<li><code>w_o_diag</code>: weight for prev_cell -&gt; output gate peephole</li>
<li><code>gamma_h</code>: batch norm scaling for previous_hidden -&gt; gates</li>
<li><code>gamma_x</code>: batch norm scaling for input -&gt; gates</li>
<li><code>gamma_c</code>: batch norm scaling for cell -&gt; output</li>
<li><code>beta_c</code>: batch norm bias for cell -&gt; output</li>
</ul>
<h5 id="args_39">Args:</h5>
<p>cls:The class.</p>
<ul>
<li><code>use_peepholes</code>: Boolean that indicates whether peephole connections are
    used.</li>
<li><code>use_batch_norm_h</code>: Boolean that indicates whether to apply batch
    normalization at the previous_hidden -&gt; gates contribution. If you are
    experimenting with batch norm then this may be the most effective to
    turn on.</li>
<li><code>use_batch_norm_x</code>: Boolean that indicates whether to apply batch
    normalization at the input -&gt; gates contribution.</li>
<li><code>use_batch_norm_c</code>: Boolean that indicates whether to apply batch
    normalization at the cell -&gt; output contribution.</li>
</ul>
<h5 id="returns_78">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
    constructor.</p>
<h4 id="batchnormlstmget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>BatchNormLSTM.get_variables(collection='trainable_variables')</code></a><a id="BatchNormLSTM.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_40">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_79">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_74">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormlstmgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>BatchNormLSTM.graph</code></a><a id="BatchNormLSTM.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="batchnormlstminitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1028"><code>BatchNormLSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None)</code></a><a id="BatchNormLSTM.initial_state" /></h4>
<p>Builds the default start state tensor of zeros.</p>
<h5 id="args_41">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, float or scalar Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.</li>
<li><code>trainable_initializers</code>: An optional pair of initializers for the
      initial hidden state and cell state.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. A
    regularizer should be a function that takes a single <code>Tensor</code> as an
    input and returns a scalar <code>Tensor</code> output, e.g. the L1 and L2
    regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_80">Returns:</h5>
<p>A tensor tuple <code>([batch_size, state_size], [batch_size, state_size], ?)</code>
  filled with zeros, with the third entry present when batch norm is enabled
  with <code>max_unique_stats &gt; 1', with value</code>0` (representing the time step).</p>
<h4 id="batchnormlstmis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>BatchNormLSTM.is_connected</code></a><a id="BatchNormLSTM.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="batchnormlstmlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>BatchNormLSTM.last_connected_subgraph</code></a><a id="BatchNormLSTM.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_81">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_75">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormlstmmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>BatchNormLSTM.module_name</code></a><a id="BatchNormLSTM.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="batchnormlstmname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>BatchNormLSTM.name_scopes</code></a><a id="BatchNormLSTM.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="batchnormlstmnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>BatchNormLSTM.non_trainable_variables</code></a><a id="BatchNormLSTM.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_82">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_76">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormlstmoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1087"><code>BatchNormLSTM.output_size</code></a><a id="BatchNormLSTM.output_size" /></h4>
<p><code>tf.TensorShape</code> indicating the size of the core output.</p>
<h4 id="batchnormlstmscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>BatchNormLSTM.scope_name</code></a><a id="BatchNormLSTM.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="batchnormlstmstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1076"><code>BatchNormLSTM.state_size</code></a><a id="BatchNormLSTM.state_size" /></h4>
<p>Tuple of <code>tf.TensorShape</code>s indicating the size of state tensors.</p>
<h4 id="batchnormlstmtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>BatchNormLSTM.trainable_variables</code></a><a id="BatchNormLSTM.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_83">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_77">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormlstmuse_batch_norm_c"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1107"><code>BatchNormLSTM.use_batch_norm_c</code></a><a id="BatchNormLSTM.use_batch_norm_c" /></h4>
<p>Boolean indicating whether batch norm for cell -&gt; output is enabled.</p>
<h4 id="batchnormlstmuse_batch_norm_h"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1097"><code>BatchNormLSTM.use_batch_norm_h</code></a><a id="BatchNormLSTM.use_batch_norm_h" /></h4>
<p>Boolean indicating whether batch norm for hidden -&gt; gates is enabled.</p>
<h4 id="batchnormlstmuse_batch_norm_x"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1102"><code>BatchNormLSTM.use_batch_norm_x</code></a><a id="BatchNormLSTM.use_batch_norm_x" /></h4>
<p>Boolean indicating whether batch norm for input -&gt; gates is enabled.</p>
<h4 id="batchnormlstmuse_peepholes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1092"><code>BatchNormLSTM.use_peepholes</code></a><a id="BatchNormLSTM.use_peepholes" /></h4>
<p>Boolean indicating whether peephole connections are used.</p>
<h4 id="batchnormlstmvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>BatchNormLSTM.variable_scope</code></a><a id="BatchNormLSTM.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_84">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_78">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormlstmvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>BatchNormLSTM.variables</code></a><a id="BatchNormLSTM.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_85">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_79">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormlstmwith_batch_norm_controlis_training-test_local_statstrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=742"><code>BatchNormLSTM.with_batch_norm_control(is_training, test_local_stats=True)</code></a><a id="BatchNormLSTM.with_batch_norm_control" /></h4>
<p>Wraps this RNNCore with the additional control input to the <code>BatchNorm</code>s.</p>
<p>Example usage:</p>
<p>lstm = snt.BatchNormLSTM(4)
  is_training = tf.placeholder(tf.bool)
  rnn_input = ...
  my_rnn = rnn.rnn(lstm.with_batch_norm_control(is_training), rnn_input)</p>
<h5 id="args_42">Args:</h5>
<ul>
<li><code>is_training</code>: Boolean that indicates whether we are in
    training mode or testing mode. When in training mode, the batch norm
    statistics are taken from the given batch, and moving statistics are
    updated. When in testing mode, the moving statistics are not updated,
    and in addition if <code>test_local_stats</code> is False then the moving
    statistics are used for the batch statistics. See the <code>BatchNorm</code> module
    for more details.</li>
<li><code>test_local_stats</code>: Boolean scalar indicated whether to use local
    batch statistics in test mode.</li>
</ul>
<h5 id="returns_86">Returns:</h5>
<p>snt.RNNCore wrapping this class with the extra input(s) added.</p>
<h4 id="batchnormlstmzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>BatchNormLSTM.zero_state(batch_size, dtype)</code></a><a id="BatchNormLSTM.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_43">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_87">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-batchnormv2"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?q=class:BatchNormV2"><code>class BatchNormV2</code></a><a id="BatchNormV2" /></h3>
<p>Batch normalization module, including optional affine transformation.</p>
<p>This module maintains exponential moving averages of the mean and
variance, which can be optionally used to normalize at test time.</p>
<p>At training time, batch statistics (mean, variance) are not shared between
separate connections. The moving averages are shared between separate
connections. At both training and test time, the optional affine
transformation (<code>* gamma + beta</code>) is shared between separate connections.</p>
<p>This is also the case for distributed replica training, where the batch
statistics are not aggregated across replicas, but the moving averages are
shared globally.</p>
<p>When connecting the module to the graph, <code>is_training=True</code> means that</p>
<ul>
<li>Update ops are created to update the moving averages with the current
    batch's statistics.</li>
<li>Features are normalized using the <em>current batch's statistics</em>. The
    <code>test_local_stats</code> setting is ignored. The moving averages are
    <strong>not</strong> used.</li>
</ul>
<p>whereas <code>is_training=False</code> means that</p>
<ul>
<li>Update ops are not created.</li>
<li>Features are normalized using either:<ul>
<li>The moving averages if <code>test_local_stats=False</code> (default).</li>
<li>The test batch statistics if <code>test_local_stats=True</code>.</li>
</ul>
</li>
</ul>
<p>The moving averages are used by default at test time, but local batch
statistics can be used by specifying a flag when connecting. One often wants
to use local batch statistics at test time to track the progress while the
model is trained as it would ensure that moving average updates do not affect
the training curves. Once the training is finished, it's often advantageous
to use moving average statistics, since it would make evaluation agnostic to
the batch size, and might even lead to small improvements over the local
batch statistics.</p>
<p>The moving averages will be updated automatically by default, but not if
<code>update_ops_collection</code> is provided: in that case they will only be updated
when the ops in that collection are run.</p>
<p>For example, to run the updates automatically:</p>
<pre><code>bn = BatchNormV2()
train_net = bn(train_inputs, is_training=True)
</code></pre>
<p>this does, however, have the effect of blocking the forwards pass of the
network until the update ops have been run and may have a small performance
penalty.</p>
<p>For example, to run the updates manually:</p>
<pre><code>bn = BatchNormV2(update_ops_collection=tf.GraphKeys.UPDATE_OPS)
train_net = bn(train_inputs, is_training=True)

...

update_ops = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS))
train_op = tf.group(train_op, update_ops)
</code></pre>
<p>Then, whenever <code>train_op</code> is run so also are the moving average update ops.</p>
<p>Some batch normalization caveats:</p>
<ul>
<li>Batch normalization will remove the effect of adding a bias, so e.g.
    <code>use_bias=False</code> should be used for an immediately preceding snt.Linear
    module.</li>
<li>If your data batches aren't i.i.d. then batch normalization can allow your
    network to 'cheat' by using the batch statistics to peek at the rest of
    the batch. This can exhibit itself as a higher test score with
    <code>test_local_stats=True</code> than <code>test_local_stats=False</code>.</li>
</ul>
<h4 id="batchnormv2__init__data_formatnone-offsettrue-scalefalse-decay_rate0999-eps0001-initializersnone-partitionersnone-regularizersnone-update_ops_collectionnone-fusedtrue-namebatch_norm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=156"><code>BatchNormV2.__init__(data_format=None, offset=True, scale=False, decay_rate=0.999, eps=0.001, initializers=None, partitioners=None, regularizers=None, update_ops_collection=None, fused=True, name='batch_norm')</code></a><a id="BatchNormV2.__init__" /></h4>
<p>Constructs a BatchNormV2 module.</p>
<p>Reduces over all input tensor dimensions apart from the channel
dimension. This has the effect of treating pixels in 1D/2D/3D images as
additional elements of the minibatch.</p>
<h5 id="args_44">Args:</h5>
<ul>
<li><code>data_format</code>: The data format. Can be "NC", "NWC", "NCW", "NHWC", "NCHW",
    "NDHWC", or "NCDHW". If not provided we assume the channel dimension is
    last.</li>
<li><code>offset</code>: Optional boolean to specify whether or not to apply a trained
    component-wise bias after the batch normalization and scaling.</li>
<li><code>scale</code>: Optional boolean to specify whether or not to apply a trained
    component-wise scale after the batch normalization.</li>
<li><code>decay_rate</code>: Decay rate of the exponential moving averages of the mean
    and variance.</li>
<li><code>eps</code>: Small number to avoid dividing by zero when diving by the standard
    deviation.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the weights of
    the affine transform (<code>gamma</code> and <code>beta</code>).</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition the
    weights of the affine transform (<code>gamma</code> and <code>beta</code>).</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the weights of the
    affine transform ("gamma" and "beta"). As a default, no regularizers are
    used. A regularizer should be a function that takes a single <code>Tensor</code> as
    an input and returns a scalar <code>Tensor</code> output, e.g. the L1 and L2
    regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>update_ops_collection</code>: Optional name of TensorFlow variable collection to
    add the moving average update ops to. If not provided, we instead add
    the update ops as control dependencies of the output of the module. This
    may result in some slowdown, as the feed-forward of the network is now
    blocked.</li>
<li><code>fused</code>: Use nn.fused_batch_norm if True, nn.batch_normalization otherwise.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_80">Raises:</h5>
<ul>
<li><code>KeyError</code>: If <code>initializers</code> contains any keys other than <code>gamma</code>, <code>beta</code>,
    <code>moving_mean</code> or <code>moving_variance</code>.</li>
<li><code>KeyError</code>: If <code>partitioners</code> or <code>regularizers</code> contains any keys other
    than <code>gamma</code> or <code>beta</code>.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
    are not callable.</li>
<li><code>ValueError</code>: If <code>data_format</code> is invalid.</li>
</ul>
<h4 id="batchnormv2__call__input_batch-is_training-test_local_statsfalse"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=505"><code>BatchNormV2.__call__(input_batch, is_training, test_local_stats=False)</code></a><a id="BatchNormV2.__call__" /></h4>
<p>Connects the BatchNormV2 module into the graph.</p>
<h5 id="args_45">Args:</h5>
<ul>
<li><code>input_batch</code>: A Tensor of the same dimension as <code>len(data_format)</code>.</li>
<li><code>is_training</code>: A boolean to indicate if the module should be connected in
    training mode, meaning the moving averages are updated. Can be a Tensor.</li>
<li><code>test_local_stats</code>: A boolean to indicate if local batch statistics should
    be used when <code>is_training=False</code>. If not, moving averages are used.
    By default <code>False</code>. Can be a Tensor.</li>
</ul>
<h5 id="returns_88">Returns:</h5>
<p>A tensor with the same shape as <code>input_batch</code>.</p>
<h5 id="raises_81">Raises:</h5>
<p>base.IncompatibleShapeError: If <code>data_format</code> is not valid for the
    input shape.
  base.NotSupportedError: If <code>input_batch</code> has data type of <code>tf.bfloat16</code>.</p>
<h4 id="batchnormv2beta"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=625"><code>BatchNormV2.beta</code></a><a id="BatchNormV2.beta" /></h4>
<h4 id="batchnormv2connected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>BatchNormV2.connected_subgraphs</code></a><a id="BatchNormV2.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="batchnormv2defun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>BatchNormV2.defun()</code></a><a id="BatchNormV2.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="batchnormv2defun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>BatchNormV2.defun_wrapped</code></a><a id="BatchNormV2.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="batchnormv2gamma"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=635"><code>BatchNormV2.gamma</code></a><a id="BatchNormV2.gamma" /></h4>
<h4 id="batchnormv2get_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>BatchNormV2.get_all_variables(collection='trainable_variables')</code></a><a id="BatchNormV2.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_46">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_89">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_82">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormv2get_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>BatchNormV2.get_possible_initializer_keys(cls)</code></a><a id="BatchNormV2.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_90">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="batchnormv2get_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>BatchNormV2.get_variables(collection='trainable_variables')</code></a><a id="BatchNormV2.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_47">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_91">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_83">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormv2graph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>BatchNormV2.graph</code></a><a id="BatchNormV2.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="batchnormv2initializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=603"><code>BatchNormV2.initializers</code></a><a id="BatchNormV2.initializers" /></h4>
<h4 id="batchnormv2is_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>BatchNormV2.is_connected</code></a><a id="BatchNormV2.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="batchnormv2last_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>BatchNormV2.last_connected_subgraph</code></a><a id="BatchNormV2.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_92">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_84">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormv2module_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>BatchNormV2.module_name</code></a><a id="BatchNormV2.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="batchnormv2moving_mean"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=615"><code>BatchNormV2.moving_mean</code></a><a id="BatchNormV2.moving_mean" /></h4>
<h4 id="batchnormv2moving_variance"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=620"><code>BatchNormV2.moving_variance</code></a><a id="BatchNormV2.moving_variance" /></h4>
<h4 id="batchnormv2name_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>BatchNormV2.name_scopes</code></a><a id="BatchNormV2.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="batchnormv2non_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>BatchNormV2.non_trainable_variables</code></a><a id="BatchNormV2.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_93">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_85">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormv2partitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=607"><code>BatchNormV2.partitioners</code></a><a id="BatchNormV2.partitioners" /></h4>
<h4 id="batchnormv2regularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=611"><code>BatchNormV2.regularizers</code></a><a id="BatchNormV2.regularizers" /></h4>
<h4 id="batchnormv2scope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>BatchNormV2.scope_name</code></a><a id="BatchNormV2.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="batchnormv2trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>BatchNormV2.trainable_variables</code></a><a id="BatchNormV2.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_94">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_86">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormv2variable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>BatchNormV2.variable_scope</code></a><a id="BatchNormV2.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_95">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_87">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchnormv2variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>BatchNormV2.variables</code></a><a id="BatchNormV2.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_96">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_88">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-batchreshape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?q=class:BatchReshape"><code>class BatchReshape</code></a><a id="BatchReshape" /></h3>
<p>Reshapes input Tensor, preserving the batch dimension.</p>
<h4 id="batchreshape__init__shape-preserve_dims1-namebatch_reshape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=719"><code>BatchReshape.__init__(shape, preserve_dims=1, name='batch_reshape')</code></a><a id="BatchReshape.__init__" /></h4>
<p>Constructs a BatchReshape module.</p>
<h5 id="args_48">Args:</h5>
<ul>
<li><code>shape</code>: Shape to reshape the input Tensor to while preserving its
      first <code>preserve_dims</code> dimensions; <code>shape</code> can be either a tuple/list,
      or a callable that returns the actual shape. The callable does not
      need to be ready to return something meaningful at construction time,
      but it will be required to be able to do so when the module is
      connected to the graph. When the special value -1 appears in <code>shape</code>
      the corresponding size is automatically inferred. Note that -1 can
      only appear once in <code>shape</code>. To flatten all non-batch dimensions,
      the snt.BatchFlatten module can also be used.</li>
<li><code>preserve_dims</code>: Number of leading dimensions that will not be reshaped.
      For example, given an input Tensor with shape <code>[B, H, W, C, D]</code>,
      and argument <code>shape</code> equal to <code>(-1, D)</code>:
        * <code>preserve_dims=1</code> will return a Tensor with shape <code>[B, H*W*C, D]</code>.
        * <code>preserve_dims=2</code> will return a Tensor with
            shape <code>[B, H, W*C, D]</code>.
        * <code>preserve_dims=3</code> will return a Tensor with
            shape <code>[B, H, W, C, D]</code>.
        * <code>preserve_dims=4</code> will return a Tensor with
            shape <code>[B, H, W, C, 1, D]</code>.
        * <code>preserve_dims&gt;=5</code> will throw an error on build unless D=1.
      The preserved dimensions can be unknown at building time.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_89">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>preserve_dims &lt;= 0</code>.</li>
</ul>
<h4 id="batchreshape__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=779"><code>BatchReshape.__call__(inputs)</code></a><a id="BatchReshape.__call__" /></h4>
<p>Connects the module into the graph, with input Tensor <code>inputs</code>.</p>
<h5 id="args_49">Args:</h5>
<ul>
<li><code>inputs</code>: A Tensor of shape [b_1, b_2, ..., b_preserve_dims,
                             b_preserve_dims+1, ...].</li>
</ul>
<h5 id="returns_97">Returns:</h5>
<p>A Tensor of shape [b_1, b_2, ..., b_preserve_dims,
                     b_reshape_1, b_reshape_2, ...],
    with reshaping defined by the constructor <code>shape</code> parameter.</p>
<h5 id="raises_90">Raises:</h5>
<ul>
<li><code>ValueError</code>: If output shape is incompatible with input shape; or if
      shape array contains non numeric entries; or if shape array contains
      more than 1 wildcard -1; or if the input array contains unknown,
      non-preserved dimensions (except when the unknown dimension is the
      only non-preserved dimension and doesn't actually need reshaping).</li>
</ul>
<h4 id="batchreshapeconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>BatchReshape.connected_subgraphs</code></a><a id="BatchReshape.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="batchreshapedefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>BatchReshape.defun()</code></a><a id="BatchReshape.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="batchreshapedefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>BatchReshape.defun_wrapped</code></a><a id="BatchReshape.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="batchreshapeget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>BatchReshape.get_all_variables(collection='trainable_variables')</code></a><a id="BatchReshape.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_50">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_98">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_91">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchreshapeget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>BatchReshape.get_possible_initializer_keys(cls)</code></a><a id="BatchReshape.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_99">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="batchreshapeget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>BatchReshape.get_variables(collection='trainable_variables')</code></a><a id="BatchReshape.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_51">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_100">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_92">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchreshapegraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>BatchReshape.graph</code></a><a id="BatchReshape.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="batchreshapeinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=862"><code>BatchReshape.input_shape</code></a><a id="BatchReshape.input_shape" /></h4>
<h4 id="batchreshapeis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>BatchReshape.is_connected</code></a><a id="BatchReshape.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="batchreshapelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>BatchReshape.last_connected_subgraph</code></a><a id="BatchReshape.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_101">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_93">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchreshapemodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>BatchReshape.module_name</code></a><a id="BatchReshape.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="batchreshapename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>BatchReshape.name_scopes</code></a><a id="BatchReshape.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="batchreshapenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>BatchReshape.non_trainable_variables</code></a><a id="BatchReshape.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_102">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_94">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchreshapescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>BatchReshape.scope_name</code></a><a id="BatchReshape.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="batchreshapetrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>BatchReshape.trainable_variables</code></a><a id="BatchReshape.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_103">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_95">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchreshapetransposenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=868"><code>BatchReshape.transpose(name=None)</code></a><a id="BatchReshape.transpose" /></h4>
<p>Returns transpose batch reshape.</p>
<h4 id="batchreshapevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>BatchReshape.variable_scope</code></a><a id="BatchReshape.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_104">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_96">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="batchreshapevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>BatchReshape.variables</code></a><a id="BatchReshape.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_105">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_97">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-bidirectionalrnn"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?q=class:BidirectionalRNN"><code>class BidirectionalRNN</code></a><a id="BidirectionalRNN" /></h3>
<p>Bidirectional RNNCore that processes the sequence forwards and backwards.</p>
<p>Based upon the encoder implementation in: https://arxiv.org/abs/1409.0473</p>
<p>This interface of this module is different than the typical ones found in
the RNNCore family.  The primary difference is that it is pre-conditioned on
the full input sequence in order to produce a full sequence of outputs and
states concatenated along the feature dimension among the forward and
backward cores.</p>
<h4 id="bidirectionalrnn__init__forward_core-backward_core-namebidir_rnn"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=560"><code>BidirectionalRNN.__init__(forward_core, backward_core, name='bidir_rnn')</code></a><a id="BidirectionalRNN.__init__" /></h4>
<p>Construct a Bidirectional RNN core.</p>
<h5 id="args_52">Args:</h5>
<ul>
<li><code>forward_core</code>: callable RNNCore module that computes forward states.</li>
<li><code>backward_core</code>: callable RNNCore module that computes backward states.</li>
<li><code>name</code>: name of the module.</li>
</ul>
<h5 id="raises_98">Raises:</h5>
<ul>
<li><code>ValueError</code>: if not all the modules are recurrent.</li>
</ul>
<h4 id="bidirectionalrnn__call__input_sequence-state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=583"><code>BidirectionalRNN.__call__(input_sequence, state)</code></a><a id="BidirectionalRNN.__call__" /></h4>
<p>Connects the BidirectionalRNN module into the graph.</p>
<h5 id="args_53">Args:</h5>
<ul>
<li><code>input_sequence</code>: tensor (time, batch, [feature_1, ..]). It must be
      time_major.</li>
<li><code>state</code>: tuple of states for the forward and backward cores.</li>
</ul>
<h5 id="returns_106">Returns:</h5>
<p>A dict with forward/backard states and output sequences:</p>
<pre><code>"outputs":{
    "forward": ...,
    "backward": ...},
"state": {
    "forward": ...,
    "backward": ...}
</code></pre>
<h5 id="raises_99">Raises:</h5>
<ul>
<li><code>ValueError</code>: in case time dimension is not statically known.</li>
</ul>
<h4 id="bidirectionalrnnconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>BidirectionalRNN.connected_subgraphs</code></a><a id="BidirectionalRNN.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="bidirectionalrnndefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>BidirectionalRNN.defun()</code></a><a id="BidirectionalRNN.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="bidirectionalrnndefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>BidirectionalRNN.defun_wrapped</code></a><a id="BidirectionalRNN.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="bidirectionalrnnget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>BidirectionalRNN.get_all_variables(collection='trainable_variables')</code></a><a id="BidirectionalRNN.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_54">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_107">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_100">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="bidirectionalrnnget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>BidirectionalRNN.get_possible_initializer_keys(cls)</code></a><a id="BidirectionalRNN.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_108">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="bidirectionalrnnget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>BidirectionalRNN.get_variables(collection='trainable_variables')</code></a><a id="BidirectionalRNN.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_55">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_109">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_101">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="bidirectionalrnngraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>BidirectionalRNN.graph</code></a><a id="BidirectionalRNN.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="bidirectionalrnninitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=651"><code>BidirectionalRNN.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None)</code></a><a id="BidirectionalRNN.initial_state" /></h4>
<p>Builds the default start state for a BidirectionalRNN.</p>
<p>The Bidirectional RNN flattens the states of its forward and backward cores
and concatentates them.</p>
<h5 id="args_56">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, float or scalar Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. A
    regularizer should be a function that takes a single <code>Tensor</code> as an
    input and returns a scalar <code>Tensor</code> output, e.g. the L1 and L2
    regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_110">Returns:</h5>
<p>Tuple of initial states from forward and backward RNNs.</p>
<h4 id="bidirectionalrnnis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>BidirectionalRNN.is_connected</code></a><a id="BidirectionalRNN.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="bidirectionalrnnlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>BidirectionalRNN.last_connected_subgraph</code></a><a id="BidirectionalRNN.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_111">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_102">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="bidirectionalrnnmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>BidirectionalRNN.module_name</code></a><a id="BidirectionalRNN.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="bidirectionalrnnname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>BidirectionalRNN.name_scopes</code></a><a id="BidirectionalRNN.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="bidirectionalrnnnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>BidirectionalRNN.non_trainable_variables</code></a><a id="BidirectionalRNN.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_112">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_103">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="bidirectionalrnnoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=693"><code>BidirectionalRNN.output_size</code></a><a id="BidirectionalRNN.output_size" /></h4>
<p>Flattened output size of cores.</p>
<h4 id="bidirectionalrnnscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>BidirectionalRNN.scope_name</code></a><a id="BidirectionalRNN.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="bidirectionalrnnstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=688"><code>BidirectionalRNN.state_size</code></a><a id="BidirectionalRNN.state_size" /></h4>
<p>Flattened state size of cores.</p>
<h4 id="bidirectionalrnntrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>BidirectionalRNN.trainable_variables</code></a><a id="BidirectionalRNN.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_113">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_104">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="bidirectionalrnnvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>BidirectionalRNN.variable_scope</code></a><a id="BidirectionalRNN.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_114">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_105">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="bidirectionalrnnvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>BidirectionalRNN.variables</code></a><a id="BidirectionalRNN.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_115">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_106">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-causalconv1d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?q=class:CausalConv1D"><code>class CausalConv1D</code></a><a id="CausalConv1D" /></h3>
<p>1D convolution module, including optional bias.</p>
<p>This is deprecated, please use the padding=CAUSAL argument to Conv1D.</p>
<p>This acts as a light wrapper around _ConvND ensuring that the outputs at index
<code>i</code> only depend on indices smaller than <code>i</code> (also known as a causal
convolution). For further details on the theoretical background, refer to:</p>
<p>https://arxiv.org/abs/1610.10099</p>
<h4 id="causalconv1d__init__output_channels-kernel_shape-stride1-rate1-use_biastrue-initializersnone-partitionersnone-regularizersnone-masknone-paddingcausal-data_formatnwc-custom_getternone-namecausal_conv_1d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1561"><code>CausalConv1D.__init__(output_channels, kernel_shape, stride=1, rate=1, use_bias=True, initializers=None, partitioners=None, regularizers=None, mask=None, padding='CAUSAL', data_format='NWC', custom_getter=None, name='causal_conv_1d')</code></a><a id="CausalConv1D.__init__" /></h4>
<p>Constructs a CausalConv1D module.</p>
<p>This is deprecated, please use the padding=CAUSAL argument to Conv1D.</p>
<h5 id="args_57">Args:</h5>
<ul>
<li><code>output_channels</code>: Number of output channels. <code>output_channels</code> can be
      either a number or a callable. In the latter case, since the function
      invocation is deferred to graph construction time, the user must only
      ensure that output_channels can be called, returning an integer,
      when <code>build</code> is called.</li>
<li><code>kernel_shape</code>: Sequence of kernel sizes (of size 1), or integer that is
      used to define kernel size in all dimensions.</li>
<li><code>stride</code>: Sequence of kernel strides (of size 1), or integer that is used to
      define stride in all dimensions.</li>
<li><code>rate</code>: Sequence of dilation rates (of size 1), or integer that is used to
      define dilation rate in all dimensions. 1 corresponds to standard
      convolution, <code>rate &gt; 1</code> corresponds to dilated convolution. Cannot be
      &gt; 1 if any of <code>stride</code> is also &gt; 1.</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      key 'w') or biases (with key 'b'). The default initializer for the
      weights is a truncated normal initializer, which is commonly used
      when the inputs are zero centered (see
      https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for
      the bias is a zero initializer.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
      weights (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
      (with key 'w') and the biases (with key 'b'). As a default, no
      regularizers are used. A regularizer should be a function that takes
      a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output,
      e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>mask</code>: A convertible to a 3D tensor which is multiplied
      component-wise with the weights (Optional).</li>
<li><code>padding</code>: Padding algorithm. Should be <code>snt.CAUSAL</code>.</li>
<li><code>data_format</code>: A string. Specifies whether the channel dimension
      of the input and output is the last dimension (default, NWC), or the
      second dimension (NCW).</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
      custom getters inside the module. If a dictionary, the keys
      correspond to regexes to match variable names. See the
      <code>tf.get_variable</code> documentation for information about the
      custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_107">Raises:</h5>
<p>base.IncompatibleShapeError: If the given kernel shape is not an integer;
      or if the given kernel shape is not a sequence of two integers.
  base.IncompatibleShapeError: If the given stride is not an integer; or if
      the given stride is not a sequence of two integers.
  base.IncompatibleShapeError: If the given rate is not an integer; or if
      the given rate is not a sequence of two integers.
  base.IncompatibleShapeError: If a mask is a TensorFlow Tensor with
      a not fully defined shape.
  base.NotSupportedError: If rate in any dimension and the stride in any
      dimension are simultaneously &gt; 1.</p>
<ul>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
      keys other than 'w' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
      are not callable.</li>
<li><code>TypeError</code>: If mask is given and it is not convertible to a Tensor.</li>
<li><code>ValueError</code>: If the passed-in data_format doesn't have a channel dimension.</li>
<li><code>ValueError</code>: If the given data_format is not a supported format (see
      <code>SUPPORTED_1D_DATA_FORMATS</code>).</li>
</ul>
<h4 id="causalconv1d__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=521"><code>CausalConv1D.__call__(inputs)</code></a><a id="CausalConv1D.__call__" /></h4>
<p>Connects the _ConvND module into the graph, with input Tensor <code>inputs</code>.</p>
<p>If this is not the first time the module has been connected to the graph,
the input Tensor provided here must have the same number of channels, in
order for the existing variables to be the correct size for the
multiplication; the batch size and input spatial dimensions may differ for
each connection.</p>
<h5 id="args_58">Args:</h5>
<ul>
<li><code>inputs</code>: A ND Tensor of the same rank as <code>data_format</code>, and either of types
  <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</li>
</ul>
<h5 id="returns_116">Returns:</h5>
<p>A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ...,
      output_channels].</p>
<h5 id="raises_108">Raises:</h5>
<ul>
<li>
<p><code>ValueError</code>: If connecting the module into the graph any time after the
      first time and the inferred size of the input does not match previous
      invocations.
  base.IncompatibleShapeError: If the input tensor has the wrong number
      of dimensions.
  base.UnderspecifiedError: If the channel dimension of <code>inputs</code> isn't
      defined.
  base.IncompatibleShapeError: If a mask is present and its shape is
      incompatible with the shape of the weights.</p>
</li>
<li>
<p><code>TypeError</code>: If input Tensor dtype is not compatible with either
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</p>
</li>
</ul>
<h4 id="causalconv1db"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=778"><code>CausalConv1D.b</code></a><a id="CausalConv1D.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_117">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_109">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the graph
      yet, meaning the variables do not exist.</p>
<ul>
<li><code>AttributeError</code>: If the module does not use bias.</li>
</ul>
<h4 id="causalconv1dclonenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=840"><code>CausalConv1D.clone(name=None)</code></a><a id="CausalConv1D.clone" /></h4>
<p>Returns a cloned <code>_ConvND</code> module.</p>
<h5 id="args_59">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of cloned module. The default name
    is constructed by appending "_clone" to <code>self.module_name</code>.</li>
</ul>
<h5 id="returns_118">Returns:</h5>
<p>A copy of the current class.</p>
<h4 id="causalconv1dconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>CausalConv1D.connected_subgraphs</code></a><a id="CausalConv1D.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="causalconv1dconv_op_padding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=767"><code>CausalConv1D.conv_op_padding</code></a><a id="CausalConv1D.conv_op_padding" /></h4>
<p>Returns the padding algorithm used for the underlying convolution op.</p>
<h4 id="causalconv1ddata_format"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=821"><code>CausalConv1D.data_format</code></a><a id="CausalConv1D.data_format" /></h4>
<p>Returns the data format.</p>
<h4 id="causalconv1ddefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>CausalConv1D.defun()</code></a><a id="CausalConv1D.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="causalconv1ddefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>CausalConv1D.defun_wrapped</code></a><a id="CausalConv1D.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="causalconv1dget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>CausalConv1D.get_all_variables(collection='trainable_variables')</code></a><a id="CausalConv1D.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_60">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_119">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_110">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="causalconv1dget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=517"><code>CausalConv1D.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="CausalConv1D.get_possible_initializer_keys" /></h4>
<h4 id="causalconv1dget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>CausalConv1D.get_variables(collection='trainable_variables')</code></a><a id="CausalConv1D.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_61">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_120">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_111">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="causalconv1dgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>CausalConv1D.graph</code></a><a id="CausalConv1D.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="causalconv1dhas_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=796"><code>CausalConv1D.has_bias</code></a><a id="CausalConv1D.has_bias" /></h4>
<p>Returns <code>True</code> if bias Variable is present in the module.</p>
<h4 id="causalconv1dinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=801"><code>CausalConv1D.initializers</code></a><a id="CausalConv1D.initializers" /></h4>
<p>Returns the initializers dictionary.</p>
<h4 id="causalconv1dinput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=833"><code>CausalConv1D.input_channels</code></a><a id="CausalConv1D.input_channels" /></h4>
<p>Returns the number of input channels.</p>
<h4 id="causalconv1dinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=827"><code>CausalConv1D.input_shape</code></a><a id="CausalConv1D.input_shape" /></h4>
<p>Returns the input shape.</p>
<h4 id="causalconv1dis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>CausalConv1D.is_connected</code></a><a id="CausalConv1D.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="causalconv1dkernel_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=722"><code>CausalConv1D.kernel_shape</code></a><a id="CausalConv1D.kernel_shape" /></h4>
<p>Returns the kernel shape.</p>
<h4 id="causalconv1dlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>CausalConv1D.last_connected_subgraph</code></a><a id="CausalConv1D.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_121">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_112">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="causalconv1dmask"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=816"><code>CausalConv1D.mask</code></a><a id="CausalConv1D.mask" /></h4>
<p>Returns the mask.</p>
<h4 id="causalconv1dmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>CausalConv1D.module_name</code></a><a id="CausalConv1D.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="causalconv1dname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>CausalConv1D.name_scopes</code></a><a id="CausalConv1D.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="causalconv1dnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>CausalConv1D.non_trainable_variables</code></a><a id="CausalConv1D.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_122">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_113">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="causalconv1doutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=713"><code>CausalConv1D.output_channels</code></a><a id="CausalConv1D.output_channels" /></h4>
<p>Returns the number of output channels.</p>
<h4 id="causalconv1dpadding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=739"><code>CausalConv1D.padding</code></a><a id="CausalConv1D.padding" /></h4>
<p>Returns the padding algorithm used, if this is the same for all dims.</p>
<p>Use <code>.paddings</code> if you want a tuple with the padding algorithm used for each
dimension.</p>
<h5 id="returns_123">Returns:</h5>
<p>The padding algorithm used, if this is the same for all dimensions.</p>
<h5 id="raises_114">Raises:</h5>
<ul>
<li><code>ValueError</code>: If different padding algorithms are used for different
    dimensions.</li>
</ul>
<h4 id="causalconv1dpaddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=762"><code>CausalConv1D.paddings</code></a><a id="CausalConv1D.paddings" /></h4>
<p>Returns a tuple with the padding algorithm used for each dimension.</p>
<h4 id="causalconv1dpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=806"><code>CausalConv1D.partitioners</code></a><a id="CausalConv1D.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="causalconv1drate"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=734"><code>CausalConv1D.rate</code></a><a id="CausalConv1D.rate" /></h4>
<p>Returns the dilation rate.</p>
<h4 id="causalconv1dregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=811"><code>CausalConv1D.regularizers</code></a><a id="CausalConv1D.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="causalconv1dscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>CausalConv1D.scope_name</code></a><a id="CausalConv1D.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="causalconv1dstride"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=727"><code>CausalConv1D.stride</code></a><a id="CausalConv1D.stride" /></h4>
<p>Returns the stride.</p>
<h4 id="causalconv1dtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>CausalConv1D.trainable_variables</code></a><a id="CausalConv1D.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_124">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_115">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="causalconv1dvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>CausalConv1D.variable_scope</code></a><a id="CausalConv1D.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_125">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_116">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="causalconv1dvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>CausalConv1D.variables</code></a><a id="CausalConv1D.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_126">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_117">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="causalconv1dw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=772"><code>CausalConv1D.w</code></a><a id="CausalConv1D.w" /></h4>
<p>Returns the Variable containing the weight matrix.</p>
<h3 id="class-concatlinear"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?q=class:ConcatLinear"><code>class ConcatLinear</code></a><a id="ConcatLinear" /></h3>
<p>Linear transformation of a number of concatenated inputs.</p>
<p>This class ensures that at initialisation, the relative importance of all
inputs are similar even if they have very different sizes. This assumes
that all inputs have roughly the same range of values.</p>
<p>For example, the following code also concatenates a list of inputs and applies
a linear transform:</p>
<pre><code>inp = tf.concat(input_list, axis=-1)
return snt.Linear(output_size)(inp)
</code></pre>

<p>The issue with the above code is that if <code>input_list</code> is made of two Tensors
of very different shapes such as <code>[batch_size, 1]</code> and <code>[batch_size, 128]</code>,
then almost no signal will be received from the first Tensor. This class works
around this problem by using a weight matrix with relatively larger
coefficients for the first Tensor than for the second one.</p>
<h4 id="concatlinear__init__output_size-use_biastrue-initializersnone-partitionersnone-regularizersnone-custom_getternone-nameconcat_linear"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=393"><code>ConcatLinear.__init__(output_size, use_bias=True, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='concat_linear')</code></a><a id="ConcatLinear.__init__" /></h4>
<p>Constructs a ConcatLinear module.</p>
<h5 id="args_62">Args:</h5>
<ul>
<li><code>output_size</code>: Output dimensionality. <code>output_size</code> can be either an integer
      or a callable. In the latter case, since the function invocation is
      deferred to graph construction time, the user must only ensure that
      output_size can be called, returning an integer, when build is called.</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing initializers to initialize the
      weights (with key 'w') or biases (with key 'b'). The default
      initializer for the weights is a truncated normal initializer, which
      is commonly used when the inputs are zero centered (see
      https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for
      the bias is a zero initializer.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
      weights (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the weights
    (with key 'w') and the biases (with key 'b'). As a default, no
    regularizers are used. A regularizer should be a function that takes
    a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output, e.g.
    the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the <code>tf.get_variable</code>
    documentation for information about the custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h4 id="concatlinear__call__inputs_list"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=436"><code>ConcatLinear.__call__(inputs_list)</code></a><a id="ConcatLinear.__call__" /></h4>
<p>Connects the module into the graph.</p>
<p>If this is not the first time the module has been connected to the graph,
the Tensors provided here must have the same final dimensions as when called
the first time, in order for the existing variables to be the correct size
for the multiplication. The batch size may differ for each connection.</p>
<h5 id="args_63">Args:</h5>
<ul>
<li><code>inputs_list</code>: A list of 2D Tensors of rank 2, with leading batch dimension.</li>
</ul>
<h5 id="returns_127">Returns:</h5>
<p>A 2D Tensor of size [batch_size, output_size].</p>
<h4 id="concatlinearconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>ConcatLinear.connected_subgraphs</code></a><a id="ConcatLinear.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="concatlineardefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>ConcatLinear.defun()</code></a><a id="ConcatLinear.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="concatlineardefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>ConcatLinear.defun_wrapped</code></a><a id="ConcatLinear.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="concatlinearget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>ConcatLinear.get_all_variables(collection='trainable_variables')</code></a><a id="ConcatLinear.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_64">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_128">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_118">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="concatlinearget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>ConcatLinear.get_possible_initializer_keys(cls)</code></a><a id="ConcatLinear.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_129">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="concatlinearget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>ConcatLinear.get_variables(collection='trainable_variables')</code></a><a id="ConcatLinear.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_65">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_130">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_119">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="concatlineargraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>ConcatLinear.graph</code></a><a id="ConcatLinear.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="concatlinearis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>ConcatLinear.is_connected</code></a><a id="ConcatLinear.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="concatlinearlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>ConcatLinear.last_connected_subgraph</code></a><a id="ConcatLinear.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_131">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_120">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="concatlinearmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>ConcatLinear.module_name</code></a><a id="ConcatLinear.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="concatlinearname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>ConcatLinear.name_scopes</code></a><a id="ConcatLinear.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="concatlinearnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>ConcatLinear.non_trainable_variables</code></a><a id="ConcatLinear.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_132">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_121">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="concatlinearscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>ConcatLinear.scope_name</code></a><a id="ConcatLinear.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="concatlineartrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>ConcatLinear.trainable_variables</code></a><a id="ConcatLinear.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_133">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_122">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="concatlinearvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>ConcatLinear.variable_scope</code></a><a id="ConcatLinear.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_134">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_123">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="concatlinearvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>ConcatLinear.variables</code></a><a id="ConcatLinear.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_135">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_124">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-conv1d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?q=class:Conv1D"><code>class Conv1D</code></a><a id="Conv1D" /></h3>
<p>1D convolution module, including optional bias.</p>
<p>This acts as a light wrapper around the class <code>_ConvND</code>.</p>
<h4 id="conv1d__init__output_channels-kernel_shape-stride1-rate1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-masknone-data_formatnwc-custom_getternone-nameconv_1d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1287"><code>Conv1D.__init__(output_channels, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, mask=None, data_format='NWC', custom_getter=None, name='conv_1d')</code></a><a id="Conv1D.__init__" /></h4>
<p>Constructs a Conv1D module.</p>
<p>See the following documentation for an explanation of VALID versus SAME
padding modes:
https://www.tensorflow.org/api_guides/python/nn#Convolution</p>
<h5 id="args_66">Args:</h5>
<ul>
<li><code>output_channels</code>: Number of output channels. <code>output_channels</code> can be
      either a number or a callable. In the latter case, since the function
      invocation is deferred to graph construction time, the user must only
      ensure that output_channels can be called, returning an integer,
      when <code>build</code> is called.</li>
<li><code>kernel_shape</code>: Sequence of kernel sizes (of size 1), or integer that is
      used to define kernel size in all dimensions.</li>
<li><code>stride</code>: Sequence of kernel strides (of size 1), or integer that is used to
      define stride in all dimensions.</li>
<li><code>rate</code>: Sequence of dilation rates (of size 1), or integer that is used to
      define dilation rate in all dimensions. 1 corresponds to standard
      convolution, <code>rate &gt; 1</code> corresponds to dilated convolution. Cannot be
      &gt; 1 if any of <code>stride</code> is also &gt; 1.</li>
<li><code>padding</code>: Padding algorithm. Either <code>snt.SAME</code>, <code>snt.VALID</code>, <code>snt.FULL</code>,
      <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code>, or a sequence of these paddings
      of length 1.<ul>
<li>snt.SAME and snt.VALID are explained in the Tensorflow docs at
    https://www.tensorflow.org/api_guides/python/nn#Convolution.</li>
<li>snt.FULL pre- and post-pads with the maximum padding which does not
    result in a convolution over just padded elements.</li>
<li>snt.CAUSAL pre-pads to ensure that each output value only depends on
    input values at the same or preceding indices ("no dependence on the
    future").</li>
<li>snt.REVERSE_CAUSAL post-pads to ensure that each output value only
    depends on input values at the same or <em>greater</em> indices ("no
    dependence on the past").
  If you use the same padding for all dimensions, and it is one of SAME
  or VALID, then this is supported directly by the underlying
  convolution op. In all other cases, the input data will be padded
  using tf.pad before calling the convolution op.</li>
</ul>
</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      key 'w') or biases (with key 'b'). The default initializer for the
      weights is a truncated normal initializer, which is commonly used
      when the inputs are zero centered (see
      https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for
      the bias is a zero initializer.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
      weights (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
      (with key 'w') and the biases (with key 'b'). As a default, no
      regularizers are used. A regularizer should be a function that takes
      a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output,
      e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>mask</code>: A convertible to a 3D tensor which is multiplied
      component-wise with the weights (Optional).</li>
<li><code>data_format</code>: A string. Specifies whether the channel dimension
      of the input and output is the last dimension (default, NWC), or the
      second dimension (NCW).</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
      custom getters inside the module. If a dictionary, the keys
      correspond to regexes to match variable names. See the
      <code>tf.get_variable</code> documentation for information about the
      custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_125">Raises:</h5>
<p>base.IncompatibleShapeError: If the given kernel shape is not an integer;
      or if the given kernel shape is not a sequence of two integers.
  base.IncompatibleShapeError: If the given stride is not an integer; or if
      the given stride is not a sequence of two integers.
  base.IncompatibleShapeError: If the given rate is not an integer; or if
      the given rate is not a sequence of two integers.
  base.IncompatibleShapeError: If a mask is a TensorFlow Tensor with
      a not fully defined shape.
  base.NotSupportedError: If rate in any dimension and the stride in any
      dimension are simultaneously &gt; 1.</p>
<ul>
<li><code>ValueError</code>: If the given padding is not <code>snt.VALID</code> or <code>snt.SAME</code>.</li>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
      keys other than 'w' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
      are not callable.</li>
<li><code>TypeError</code>: If mask is given and it is not convertible to a Tensor.</li>
<li><code>ValueError</code>: If the passed-in data_format doesn't have a channel dimension.</li>
<li><code>ValueError</code>: If the given data_format is not a supported format (see
      <code>SUPPORTED_1D_DATA_FORMATS</code>).</li>
</ul>
<h4 id="conv1d__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=521"><code>Conv1D.__call__(inputs)</code></a><a id="Conv1D.__call__" /></h4>
<p>Connects the _ConvND module into the graph, with input Tensor <code>inputs</code>.</p>
<p>If this is not the first time the module has been connected to the graph,
the input Tensor provided here must have the same number of channels, in
order for the existing variables to be the correct size for the
multiplication; the batch size and input spatial dimensions may differ for
each connection.</p>
<h5 id="args_67">Args:</h5>
<ul>
<li><code>inputs</code>: A ND Tensor of the same rank as <code>data_format</code>, and either of types
  <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</li>
</ul>
<h5 id="returns_136">Returns:</h5>
<p>A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ...,
      output_channels].</p>
<h5 id="raises_126">Raises:</h5>
<ul>
<li>
<p><code>ValueError</code>: If connecting the module into the graph any time after the
      first time and the inferred size of the input does not match previous
      invocations.
  base.IncompatibleShapeError: If the input tensor has the wrong number
      of dimensions.
  base.UnderspecifiedError: If the channel dimension of <code>inputs</code> isn't
      defined.
  base.IncompatibleShapeError: If a mask is present and its shape is
      incompatible with the shape of the weights.</p>
</li>
<li>
<p><code>TypeError</code>: If input Tensor dtype is not compatible with either
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</p>
</li>
</ul>
<h4 id="conv1db"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=778"><code>Conv1D.b</code></a><a id="Conv1D.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_137">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_127">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the graph
      yet, meaning the variables do not exist.</p>
<ul>
<li><code>AttributeError</code>: If the module does not use bias.</li>
</ul>
<h4 id="conv1dclonenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=840"><code>Conv1D.clone(name=None)</code></a><a id="Conv1D.clone" /></h4>
<p>Returns a cloned <code>_ConvND</code> module.</p>
<h5 id="args_68">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of cloned module. The default name
    is constructed by appending "_clone" to <code>self.module_name</code>.</li>
</ul>
<h5 id="returns_138">Returns:</h5>
<p>A copy of the current class.</p>
<h4 id="conv1dconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Conv1D.connected_subgraphs</code></a><a id="Conv1D.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="conv1dconv_op_padding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=767"><code>Conv1D.conv_op_padding</code></a><a id="Conv1D.conv_op_padding" /></h4>
<p>Returns the padding algorithm used for the underlying convolution op.</p>
<h4 id="conv1ddata_format"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=821"><code>Conv1D.data_format</code></a><a id="Conv1D.data_format" /></h4>
<p>Returns the data format.</p>
<h4 id="conv1ddefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Conv1D.defun()</code></a><a id="Conv1D.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="conv1ddefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Conv1D.defun_wrapped</code></a><a id="Conv1D.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="conv1dget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Conv1D.get_all_variables(collection='trainable_variables')</code></a><a id="Conv1D.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_69">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_139">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_128">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=517"><code>Conv1D.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="Conv1D.get_possible_initializer_keys" /></h4>
<h4 id="conv1dget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>Conv1D.get_variables(collection='trainable_variables')</code></a><a id="Conv1D.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_70">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_140">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_129">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Conv1D.graph</code></a><a id="Conv1D.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="conv1dhas_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=796"><code>Conv1D.has_bias</code></a><a id="Conv1D.has_bias" /></h4>
<p>Returns <code>True</code> if bias Variable is present in the module.</p>
<h4 id="conv1dinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=801"><code>Conv1D.initializers</code></a><a id="Conv1D.initializers" /></h4>
<p>Returns the initializers dictionary.</p>
<h4 id="conv1dinput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=833"><code>Conv1D.input_channels</code></a><a id="Conv1D.input_channels" /></h4>
<p>Returns the number of input channels.</p>
<h4 id="conv1dinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=827"><code>Conv1D.input_shape</code></a><a id="Conv1D.input_shape" /></h4>
<p>Returns the input shape.</p>
<h4 id="conv1dis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Conv1D.is_connected</code></a><a id="Conv1D.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="conv1dkernel_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=722"><code>Conv1D.kernel_shape</code></a><a id="Conv1D.kernel_shape" /></h4>
<p>Returns the kernel shape.</p>
<h4 id="conv1dlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Conv1D.last_connected_subgraph</code></a><a id="Conv1D.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_141">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_130">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dmask"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=816"><code>Conv1D.mask</code></a><a id="Conv1D.mask" /></h4>
<p>Returns the mask.</p>
<h4 id="conv1dmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Conv1D.module_name</code></a><a id="Conv1D.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="conv1dname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Conv1D.name_scopes</code></a><a id="Conv1D.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="conv1dnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Conv1D.non_trainable_variables</code></a><a id="Conv1D.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_142">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_131">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1doutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=713"><code>Conv1D.output_channels</code></a><a id="Conv1D.output_channels" /></h4>
<p>Returns the number of output channels.</p>
<h4 id="conv1dpadding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=739"><code>Conv1D.padding</code></a><a id="Conv1D.padding" /></h4>
<p>Returns the padding algorithm used, if this is the same for all dims.</p>
<p>Use <code>.paddings</code> if you want a tuple with the padding algorithm used for each
dimension.</p>
<h5 id="returns_143">Returns:</h5>
<p>The padding algorithm used, if this is the same for all dimensions.</p>
<h5 id="raises_132">Raises:</h5>
<ul>
<li><code>ValueError</code>: If different padding algorithms are used for different
    dimensions.</li>
</ul>
<h4 id="conv1dpaddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=762"><code>Conv1D.paddings</code></a><a id="Conv1D.paddings" /></h4>
<p>Returns a tuple with the padding algorithm used for each dimension.</p>
<h4 id="conv1dpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=806"><code>Conv1D.partitioners</code></a><a id="Conv1D.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="conv1drate"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=734"><code>Conv1D.rate</code></a><a id="Conv1D.rate" /></h4>
<p>Returns the dilation rate.</p>
<h4 id="conv1dregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=811"><code>Conv1D.regularizers</code></a><a id="Conv1D.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="conv1dscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Conv1D.scope_name</code></a><a id="Conv1D.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="conv1dstride"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=727"><code>Conv1D.stride</code></a><a id="Conv1D.stride" /></h4>
<p>Returns the stride.</p>
<h4 id="conv1dtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Conv1D.trainable_variables</code></a><a id="Conv1D.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_144">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_133">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dtransposenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1388"><code>Conv1D.transpose(name=None)</code></a><a id="Conv1D.transpose" /></h4>
<p>Returns matching <code>Conv1DTranspose</code> module.</p>
<h5 id="args_71">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of transpose module. The default name
      is constructed by appending "_transpose" to <code>self.name</code>.</li>
</ul>
<h5 id="returns_145">Returns:</h5>
<p><code>Conv1DTranspose</code> module.</p>
<h5 id="raises_134">Raises:</h5>
<p>base.NotSupportedError: If <code>rate</code> in any dimension &gt; 1.</p>
<h4 id="conv1dvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Conv1D.variable_scope</code></a><a id="Conv1D.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_146">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_135">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Conv1D.variables</code></a><a id="Conv1D.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_147">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_136">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=772"><code>Conv1D.w</code></a><a id="Conv1D.w" /></h4>
<p>Returns the Variable containing the weight matrix.</p>
<h3 id="class-conv1dlstm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?q=class:Conv1DLSTM"><code>class Conv1DLSTM</code></a><a id="Conv1DLSTM" /></h3>
<p>1D convolutional LSTM.</p>
<h4 id="conv1dlstm__init__nameconv_1d_lstm-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1409"><code>Conv1DLSTM.__init__(name='conv_1d_lstm', **kwargs)</code></a><a id="Conv1DLSTM.__init__" /></h4>
<p>Construct Conv1DLSTM. See <code>snt.ConvLSTM</code> for more details.</p>
<h4 id="conv1dlstm__call__inputs-state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1364"><code>Conv1DLSTM.__call__(inputs, state)</code></a><a id="Conv1DLSTM.__call__" /></h4>
<h4 id="conv1dlstmconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Conv1DLSTM.connected_subgraphs</code></a><a id="Conv1DLSTM.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="conv1dlstmconvolutions"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1347"><code>Conv1DLSTM.convolutions</code></a><a id="Conv1DLSTM.convolutions" /></h4>
<h4 id="conv1dlstmdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Conv1DLSTM.defun()</code></a><a id="Conv1DLSTM.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="conv1dlstmdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Conv1DLSTM.defun_wrapped</code></a><a id="Conv1DLSTM.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="conv1dlstmget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Conv1DLSTM.get_all_variables(collection='trainable_variables')</code></a><a id="Conv1DLSTM.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_72">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_148">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_137">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dlstmget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1405"><code>Conv1DLSTM.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="Conv1DLSTM.get_possible_initializer_keys" /></h4>
<h4 id="conv1dlstmget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>Conv1DLSTM.get_variables(collection='trainable_variables')</code></a><a id="Conv1DLSTM.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_73">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_149">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_138">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dlstmgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Conv1DLSTM.graph</code></a><a id="Conv1DLSTM.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="conv1dlstminitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=203"><code>Conv1DLSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)</code></a><a id="Conv1DLSTM.initial_state" /></h4>
<p>Builds the default start state for an RNNCore.</p>
<h5 id="args_74">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, or scalar int32 Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.
    Note that intializers and regularizers will be ignored if
    <code>trainable=False</code>.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. As a
    default, no regularizers are used. A regularizer should be a function
    that takes a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code>
    output, e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_150">Returns:</h5>
<p>A tensor or nested tuple of tensors with same structure and shape as the
  <code>state_size</code> property of the core.</p>
<h5 id="raises_139">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the user passes initializers that are not functions.</li>
<li><code>ValueError</code>: if the user passes regularizers that are not functions.</li>
</ul>
<h4 id="conv1dlstmis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Conv1DLSTM.is_connected</code></a><a id="Conv1DLSTM.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="conv1dlstmlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Conv1DLSTM.last_connected_subgraph</code></a><a id="Conv1DLSTM.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_151">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_140">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dlstmmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Conv1DLSTM.module_name</code></a><a id="Conv1DLSTM.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="conv1dlstmname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Conv1DLSTM.name_scopes</code></a><a id="Conv1DLSTM.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="conv1dlstmnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Conv1DLSTM.non_trainable_variables</code></a><a id="Conv1DLSTM.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_152">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_141">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dlstmoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1358"><code>Conv1DLSTM.output_size</code></a><a id="Conv1DLSTM.output_size" /></h4>
<p><code>tf.TensorShape</code> indicating the size of the core output.</p>
<h4 id="conv1dlstmscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Conv1DLSTM.scope_name</code></a><a id="Conv1DLSTM.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="conv1dlstmstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1351"><code>Conv1DLSTM.state_size</code></a><a id="Conv1DLSTM.state_size" /></h4>
<p>Tuple of <code>tf.TensorShape</code>s indicating the size of state tensors.</p>
<h4 id="conv1dlstmtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Conv1DLSTM.trainable_variables</code></a><a id="Conv1DLSTM.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_153">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_142">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dlstmuse_layer_norm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1396"><code>Conv1DLSTM.use_layer_norm</code></a><a id="Conv1DLSTM.use_layer_norm" /></h4>
<p>Boolean indicating whether layer norm is enabled.</p>
<h4 id="conv1dlstmvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Conv1DLSTM.variable_scope</code></a><a id="Conv1DLSTM.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_154">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_143">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dlstmvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Conv1DLSTM.variables</code></a><a id="Conv1DLSTM.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_155">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_144">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dlstmzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>Conv1DLSTM.zero_state(batch_size, dtype)</code></a><a id="Conv1DLSTM.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_75">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_156">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-conv1dtranspose"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?q=class:Conv1DTranspose"><code>class Conv1DTranspose</code></a><a id="Conv1DTranspose" /></h3>
<p>1D transposed / reverse / up 1D convolution module, including bias.</p>
<p>This performs a 1D transpose convolution by lightly wrapping the TensorFlow op
<code>tf.nn.conv2d_transpose</code>, setting the size of the height dimension of the
image to 1.</p>
<h4 id="conv1dtranspose__init__output_channels-output_shapenone-kernel_shapenone-stride1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatnwc-custom_getternone-nameconv_1d_transpose"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1440"><code>Conv1DTranspose.__init__(output_channels, output_shape=None, kernel_shape=None, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NWC', custom_getter=None, name='conv_1d_transpose')</code></a><a id="Conv1DTranspose.__init__" /></h4>
<p>Constructs a Conv1DTranspose module.</p>
<p>See the following documentation for an explanation of VALID versus SAME
padding modes:
https://www.tensorflow.org/api_guides/python/nn#Convolution</p>
<h5 id="args_76">Args:</h5>
<ul>
<li><code>output_channels</code>: Number of output channels. Can be either a number or a
      callable. In the latter case, since the function invocation is
      deferred to graph construction time, the user must only ensure
      <code>output_channels</code> can be called, returning an integer, when build is
      called.</li>
<li><code>output_shape</code>: Output shape of transpose convolution. Can be either a
      number or a callable. In the latter case, since the function
      invocation is deferred to graph construction time, the user must only
      ensure that <code>output_shape</code> can be called, returning an iterable of
      format <code>(out_length)</code> when build is called. If a None
      value is given, a default shape is automatically calculated (see
      docstring of _default_transpose_size function for more details).</li>
<li><code>kernel_shape</code>: Sequence of kernel sizes (of size 1), or integer that is
      used to define kernel size in all dimensions.</li>
<li><code>stride</code>: Sequence of kernel strides (of size 1), or integer that is used to
      define stride in all dimensions.</li>
<li><code>padding</code>: Padding algorithm, either <code>snt.SAME</code> or <code>snt.VALID</code>.</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      key 'w') or biases (with key 'b').</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
      weights (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
      (with key 'w') and the biases (with key 'b'). As a default, no
      regularizers are used. A regularizer should be a function that takes
      a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output,
      e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>data_format</code>: A string. Specifies whether the channel dimension
      of the input and output is the last dimension (default, NWC), or the
      second dimension (NCW).</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
      custom getters inside the module. If a dictionary, the keys
      correspond to regexes to match variable names. See the
      <code>tf.get_variable</code> documentation for information about the
      custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_145">Raises:</h5>
<p>base.IncompatibleShapeError: If the given kernel shape is not an integer;
      or if the given kernel shape is not a sequence of two integers.
  base.IncompatibleShapeError: If the given stride is not an integer; or if
      the given stride is not a sequence of two or four integers.</p>
<ul>
<li><code>ValueError</code>: If the given padding is not <code>snt.VALID</code> or <code>snt.SAME</code>.</li>
<li><code>ValueError</code>: If the given kernel_shape is <code>None</code>.</li>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
      keys other than 'w' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
      are not callable.</li>
<li><code>ValueError</code>: If the passed-in data_format doesn't have a channel dimension.</li>
<li><code>ValueError</code>: If the given data_format is not a supported format (see
      <code>SUPPORTED_1D_DATA_FORMATS</code>).</li>
</ul>
<h4 id="conv1dtranspose__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=995"><code>Conv1DTranspose.__call__(inputs)</code></a><a id="Conv1DTranspose.__call__" /></h4>
<p>Connects the _ConvNDTranspose module into the graph.</p>
<p>If this is not the first time the module has been connected to the graph,
the input Tensor provided here must have the same final N dimensions, in
order for the existing variables to be the correct size for the
multiplication. The batch size may differ for each connection.</p>
<h5 id="args_77">Args:</h5>
<ul>
<li><code>inputs</code>: A Tensor of shape <code>data_format</code> and of type
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</li>
</ul>
<h5 id="returns_157">Returns:</h5>
<p>A Tensor of shape <code>data_format</code> and of type <code>tf.float16</code>, <code>tf.bfloat16</code>,
     <code>tf.float32</code> or <code>tf.float64</code>.</p>
<h5 id="raises_146">Raises:</h5>
<ul>
<li>
<p><code>ValueError</code>: If connecting the module into the graph any time after the
      first time and the inferred size of the input does not match previous
      invocations.
  base.IncompatibleShapeError: If the input tensor has the wrong number
      of dimensions.
  base.UnderspecifiedError: If the channel dimension of <code>inputs</code> isn't
      defined.
  base.IncompatibleShapeError: If <code>output_shape</code> is an iterable and is not
      in the format <code>(out_height, out_width)</code>.</p>
</li>
<li>
<p><code>TypeError</code>: If input Tensor dtype is not compatible with either
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</p>
</li>
</ul>
<h4 id="conv1dtransposeb"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1230"><code>Conv1DTranspose.b</code></a><a id="Conv1DTranspose.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_158">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_147">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the graph
      yet, meaning the variables do not exist.</p>
<ul>
<li><code>AttributeError</code>: If the module does not use bias.</li>
</ul>
<h4 id="conv1dtransposeconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Conv1DTranspose.connected_subgraphs</code></a><a id="Conv1DTranspose.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="conv1dtransposeconv_op_padding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1219"><code>Conv1DTranspose.conv_op_padding</code></a><a id="Conv1DTranspose.conv_op_padding" /></h4>
<p>Returns the padding algorithm used for the underlying convolution op.</p>
<h4 id="conv1dtransposedefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Conv1DTranspose.defun()</code></a><a id="Conv1DTranspose.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="conv1dtransposedefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Conv1DTranspose.defun_wrapped</code></a><a id="Conv1DTranspose.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="conv1dtransposeget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Conv1DTranspose.get_all_variables(collection='trainable_variables')</code></a><a id="Conv1DTranspose.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_78">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_159">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_148">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dtransposeget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=991"><code>Conv1DTranspose.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="Conv1DTranspose.get_possible_initializer_keys" /></h4>
<h4 id="conv1dtransposeget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>Conv1DTranspose.get_variables(collection='trainable_variables')</code></a><a id="Conv1DTranspose.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_79">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_160">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_149">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dtransposegraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Conv1DTranspose.graph</code></a><a id="Conv1DTranspose.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="conv1dtransposehas_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1248"><code>Conv1DTranspose.has_bias</code></a><a id="Conv1DTranspose.has_bias" /></h4>
<p>Returns <code>True</code> if bias Variable is present in the module.</p>
<h4 id="conv1dtransposeinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1253"><code>Conv1DTranspose.initializers</code></a><a id="Conv1DTranspose.initializers" /></h4>
<p>Returns the initializers dictionary.</p>
<h4 id="conv1dtransposeinput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1274"><code>Conv1DTranspose.input_channels</code></a><a id="Conv1DTranspose.input_channels" /></h4>
<p>Returns the number of input channels.</p>
<h4 id="conv1dtransposeinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1268"><code>Conv1DTranspose.input_shape</code></a><a id="Conv1DTranspose.input_shape" /></h4>
<p>Returns the input shape.</p>
<h4 id="conv1dtransposeis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Conv1DTranspose.is_connected</code></a><a id="Conv1DTranspose.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="conv1dtransposekernel_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1195"><code>Conv1DTranspose.kernel_shape</code></a><a id="Conv1DTranspose.kernel_shape" /></h4>
<p>Returns the kernel shape.</p>
<h4 id="conv1dtransposelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Conv1DTranspose.last_connected_subgraph</code></a><a id="Conv1DTranspose.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_161">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_150">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dtransposemodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Conv1DTranspose.module_name</code></a><a id="Conv1DTranspose.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="conv1dtransposename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Conv1DTranspose.name_scopes</code></a><a id="Conv1DTranspose.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="conv1dtransposenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Conv1DTranspose.non_trainable_variables</code></a><a id="Conv1DTranspose.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_162">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_151">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dtransposeoutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1186"><code>Conv1DTranspose.output_channels</code></a><a id="Conv1DTranspose.output_channels" /></h4>
<p>Returns the number of output channels.</p>
<h4 id="conv1dtransposeoutput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1205"><code>Conv1DTranspose.output_shape</code></a><a id="Conv1DTranspose.output_shape" /></h4>
<p>Returns the output shape.</p>
<h4 id="conv1dtransposepadding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1214"><code>Conv1DTranspose.padding</code></a><a id="Conv1DTranspose.padding" /></h4>
<p>Returns the padding algorithm.</p>
<h4 id="conv1dtransposepartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1258"><code>Conv1DTranspose.partitioners</code></a><a id="Conv1DTranspose.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="conv1dtransposeregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1263"><code>Conv1DTranspose.regularizers</code></a><a id="Conv1DTranspose.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="conv1dtransposescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Conv1DTranspose.scope_name</code></a><a id="Conv1DTranspose.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="conv1dtransposestride"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1200"><code>Conv1DTranspose.stride</code></a><a id="Conv1DTranspose.stride" /></h4>
<p>Returns the stride.</p>
<h4 id="conv1dtransposetrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Conv1DTranspose.trainable_variables</code></a><a id="Conv1DTranspose.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_163">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_152">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dtransposetransposenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1518"><code>Conv1DTranspose.transpose(name=None)</code></a><a id="Conv1DTranspose.transpose" /></h4>
<p>Returns matching <code>Conv1D</code> module.</p>
<h5 id="args_80">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of transpose module. The default name
    is constructed by appending "_transpose" to <code>self.name</code>.</li>
</ul>
<h5 id="returns_164">Returns:</h5>
<p><code>Conv1D</code> module.</p>
<h4 id="conv1dtransposevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Conv1DTranspose.variable_scope</code></a><a id="Conv1DTranspose.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_165">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_153">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dtransposevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Conv1DTranspose.variables</code></a><a id="Conv1DTranspose.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_166">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_154">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv1dtransposew"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1224"><code>Conv1DTranspose.w</code></a><a id="Conv1DTranspose.w" /></h4>
<p>Returns the Variable containing the weight matrix.</p>
<h3 id="class-conv2d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?q=class:Conv2D"><code>class Conv2D</code></a><a id="Conv2D" /></h3>
<p>Spatial convolution and dilated convolution module, including bias.</p>
<p>This acts as a light wrapper around the class <code>_ConvND</code>.</p>
<h4 id="conv2d__init__output_channels-kernel_shape-stride1-rate1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-masknone-data_formatnhwc-custom_getternone-nameconv_2d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1659"><code>Conv2D.__init__(output_channels, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, mask=None, data_format='NHWC', custom_getter=None, name='conv_2d')</code></a><a id="Conv2D.__init__" /></h4>
<p>Constructs a Conv2D module.</p>
<p>See the following documentation for an explanation of VALID versus SAME
padding modes:
https://www.tensorflow.org/api_guides/python/nn#Convolution</p>
<h5 id="args_81">Args:</h5>
<ul>
<li><code>output_channels</code>: Number of output channels. <code>output_channels</code> can be
      either a number or a callable. In the latter case, since the function
      invocation is deferred to graph construction time, the user must only
      ensure that output_channels can be called, returning an integer,
      when <code>build</code> is called.</li>
<li><code>kernel_shape</code>: Sequence of kernel sizes (of size 2), or integer that is
      used to define kernel size in all dimensions.</li>
<li><code>stride</code>: Sequence of kernel strides (of size 2), or integer that is used to
      define stride in all dimensions.</li>
<li><code>rate</code>: Sequence of dilation rates (of size 2), or integer that is used to
      define dilation rate in all dimensions. 1 corresponds to standard 2D
      convolution, <code>rate &gt; 1</code> corresponds to dilated convolution. Cannot be
      &gt; 1 if any of <code>stride</code> is also &gt; 1.</li>
<li><code>padding</code>: Padding algorithm. Either <code>snt.SAME</code>, <code>snt.VALID</code>, <code>snt.FULL</code>,
      <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code>, or a sequence of these paddings
      of length 2.<ul>
<li>snt.SAME and snt.VALID are explained in the Tensorflow docs at
    https://www.tensorflow.org/api_guides/python/nn#Convolution.</li>
<li>snt.FULL pre- and post-pads with the maximum padding which does not
    result in a convolution over just padded elements.</li>
<li>snt.CAUSAL pre-pads to ensure that each output value only depends on
    input values at the same or preceding indices ("no dependence on the
    future").</li>
<li>snt.REVERSE_CAUSAL post-pads to ensure that each output value only
    depends on input values at the same or <em>greater</em> indices ("no
    dependence on the past").
  If you use the same padding for all dimensions, and it is one of SAME
  or VALID, then this is supported directly by the underlying
  convolution op. In all other cases, the input data will be padded
  using tf.pad before calling the convolution op.</li>
</ul>
</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      key 'w') or biases (with key 'b'). The default initializer for the
      weights is a truncated normal initializer, which is commonly used
      when the inputs are zero centered (see
      https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for
      the bias is a zero initializer.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
      weights (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
      (with key 'w') and the biases (with key 'b'). As a default, no
      regularizers are used. A regularizer should be a function that takes
      a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output,
      e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>mask</code>: A convertible to a 4D tensor which is multiplied
      component-wise with the weights (Optional).</li>
<li><code>data_format</code>: A string. Specifies whether the channel dimension
      of the input and output is the last dimension (default, NHWC), or the
      second dimension (NCHW).</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
      custom getters inside the module. If a dictionary, the keys
      correspond to regexes to match variable names. See the
      <code>tf.get_variable</code> documentation for information about the
      custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_155">Raises:</h5>
<p>base.IncompatibleShapeError: If the given kernel shape is not an integer;
      or if the given kernel shape is not a sequence of two integers.
  base.IncompatibleShapeError: If the given stride is not an integer; or if
      the given stride is not a sequence of two integers.
  base.IncompatibleShapeError: If the given rate is not an integer; or if
      the given rate is not a sequence of two integers.
  base.IncompatibleShapeError: If a mask is given and its rank is neither 2
      nor 4, or if it is a TensorFlow Tensor with a not fully defined shape.
  base.NotSupportedError: If rate in any dimension and the stride in any
      dimension are simultaneously &gt; 1.</p>
<ul>
<li><code>ValueError</code>: If the given padding is not <code>snt.VALID</code>, <code>snt.SAME</code>,
      <code>snt.FULL</code>, <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code> or a sequence of these.</li>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
      keys other than 'w' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
      are not callable.</li>
<li><code>TypeError</code>: If mask is given and it is not convertible to a Tensor.</li>
<li><code>ValueError</code>: If the passed-in data_format doesn't have a channel dimension.</li>
<li><code>ValueError</code>: If the given data_format is not a supported format (see
    <code>SUPPORTED_2D_DATA_FORMATS</code>).</li>
</ul>
<h4 id="conv2d__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=521"><code>Conv2D.__call__(inputs)</code></a><a id="Conv2D.__call__" /></h4>
<p>Connects the _ConvND module into the graph, with input Tensor <code>inputs</code>.</p>
<p>If this is not the first time the module has been connected to the graph,
the input Tensor provided here must have the same number of channels, in
order for the existing variables to be the correct size for the
multiplication; the batch size and input spatial dimensions may differ for
each connection.</p>
<h5 id="args_82">Args:</h5>
<ul>
<li><code>inputs</code>: A ND Tensor of the same rank as <code>data_format</code>, and either of types
  <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</li>
</ul>
<h5 id="returns_167">Returns:</h5>
<p>A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ...,
      output_channels].</p>
<h5 id="raises_156">Raises:</h5>
<ul>
<li>
<p><code>ValueError</code>: If connecting the module into the graph any time after the
      first time and the inferred size of the input does not match previous
      invocations.
  base.IncompatibleShapeError: If the input tensor has the wrong number
      of dimensions.
  base.UnderspecifiedError: If the channel dimension of <code>inputs</code> isn't
      defined.
  base.IncompatibleShapeError: If a mask is present and its shape is
      incompatible with the shape of the weights.</p>
</li>
<li>
<p><code>TypeError</code>: If input Tensor dtype is not compatible with either
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</p>
</li>
</ul>
<h4 id="conv2db"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=778"><code>Conv2D.b</code></a><a id="Conv2D.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_168">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_157">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the graph
      yet, meaning the variables do not exist.</p>
<ul>
<li><code>AttributeError</code>: If the module does not use bias.</li>
</ul>
<h4 id="conv2dclonenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=840"><code>Conv2D.clone(name=None)</code></a><a id="Conv2D.clone" /></h4>
<p>Returns a cloned <code>_ConvND</code> module.</p>
<h5 id="args_83">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of cloned module. The default name
    is constructed by appending "_clone" to <code>self.module_name</code>.</li>
</ul>
<h5 id="returns_169">Returns:</h5>
<p>A copy of the current class.</p>
<h4 id="conv2dconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Conv2D.connected_subgraphs</code></a><a id="Conv2D.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="conv2dconv_op_padding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=767"><code>Conv2D.conv_op_padding</code></a><a id="Conv2D.conv_op_padding" /></h4>
<p>Returns the padding algorithm used for the underlying convolution op.</p>
<h4 id="conv2ddata_format"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=821"><code>Conv2D.data_format</code></a><a id="Conv2D.data_format" /></h4>
<p>Returns the data format.</p>
<h4 id="conv2ddefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Conv2D.defun()</code></a><a id="Conv2D.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="conv2ddefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Conv2D.defun_wrapped</code></a><a id="Conv2D.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="conv2dget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Conv2D.get_all_variables(collection='trainable_variables')</code></a><a id="Conv2D.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_84">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_170">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_158">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=517"><code>Conv2D.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="Conv2D.get_possible_initializer_keys" /></h4>
<h4 id="conv2dget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>Conv2D.get_variables(collection='trainable_variables')</code></a><a id="Conv2D.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_85">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_171">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_159">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Conv2D.graph</code></a><a id="Conv2D.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="conv2dhas_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=796"><code>Conv2D.has_bias</code></a><a id="Conv2D.has_bias" /></h4>
<p>Returns <code>True</code> if bias Variable is present in the module.</p>
<h4 id="conv2dinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=801"><code>Conv2D.initializers</code></a><a id="Conv2D.initializers" /></h4>
<p>Returns the initializers dictionary.</p>
<h4 id="conv2dinput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=833"><code>Conv2D.input_channels</code></a><a id="Conv2D.input_channels" /></h4>
<p>Returns the number of input channels.</p>
<h4 id="conv2dinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=827"><code>Conv2D.input_shape</code></a><a id="Conv2D.input_shape" /></h4>
<p>Returns the input shape.</p>
<h4 id="conv2dis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Conv2D.is_connected</code></a><a id="Conv2D.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="conv2dkernel_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=722"><code>Conv2D.kernel_shape</code></a><a id="Conv2D.kernel_shape" /></h4>
<p>Returns the kernel shape.</p>
<h4 id="conv2dlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Conv2D.last_connected_subgraph</code></a><a id="Conv2D.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_172">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_160">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dmask"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=816"><code>Conv2D.mask</code></a><a id="Conv2D.mask" /></h4>
<p>Returns the mask.</p>
<h4 id="conv2dmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Conv2D.module_name</code></a><a id="Conv2D.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="conv2dname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Conv2D.name_scopes</code></a><a id="Conv2D.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="conv2dnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Conv2D.non_trainable_variables</code></a><a id="Conv2D.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_173">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_161">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2doutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=713"><code>Conv2D.output_channels</code></a><a id="Conv2D.output_channels" /></h4>
<p>Returns the number of output channels.</p>
<h4 id="conv2dpadding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=739"><code>Conv2D.padding</code></a><a id="Conv2D.padding" /></h4>
<p>Returns the padding algorithm used, if this is the same for all dims.</p>
<p>Use <code>.paddings</code> if you want a tuple with the padding algorithm used for each
dimension.</p>
<h5 id="returns_174">Returns:</h5>
<p>The padding algorithm used, if this is the same for all dimensions.</p>
<h5 id="raises_162">Raises:</h5>
<ul>
<li><code>ValueError</code>: If different padding algorithms are used for different
    dimensions.</li>
</ul>
<h4 id="conv2dpaddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=762"><code>Conv2D.paddings</code></a><a id="Conv2D.paddings" /></h4>
<p>Returns a tuple with the padding algorithm used for each dimension.</p>
<h4 id="conv2dpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=806"><code>Conv2D.partitioners</code></a><a id="Conv2D.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="conv2drate"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=734"><code>Conv2D.rate</code></a><a id="Conv2D.rate" /></h4>
<p>Returns the dilation rate.</p>
<h4 id="conv2dregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=811"><code>Conv2D.regularizers</code></a><a id="Conv2D.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="conv2dscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Conv2D.scope_name</code></a><a id="Conv2D.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="conv2dstride"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=727"><code>Conv2D.stride</code></a><a id="Conv2D.stride" /></h4>
<p>Returns the stride.</p>
<h4 id="conv2dtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Conv2D.trainable_variables</code></a><a id="Conv2D.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_175">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_163">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dtransposenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1761"><code>Conv2D.transpose(name=None)</code></a><a id="Conv2D.transpose" /></h4>
<p>Returns matching <code>Conv2DTranspose</code> module.</p>
<h5 id="args_86">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of transpose module. The default name
    is constructed by appending "_transpose" to <code>self.name</code>.</li>
</ul>
<h5 id="returns_176">Returns:</h5>
<p><code>Conv2DTranspose</code> module.</p>
<h5 id="raises_164">Raises:</h5>
<p>base.NotSupportedError: If <code>rate</code> in any dimension &gt; 1.</p>
<h4 id="conv2dvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Conv2D.variable_scope</code></a><a id="Conv2D.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_177">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_165">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Conv2D.variables</code></a><a id="Conv2D.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_178">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_166">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=772"><code>Conv2D.w</code></a><a id="Conv2D.w" /></h4>
<p>Returns the Variable containing the weight matrix.</p>
<h3 id="class-conv2dlstm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?q=class:Conv2DLSTM"><code>class Conv2DLSTM</code></a><a id="Conv2DLSTM" /></h3>
<p>2D convolutional LSTM.</p>
<h4 id="conv2dlstm__init__nameconv_2d_lstm-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1423"><code>Conv2DLSTM.__init__(name='conv_2d_lstm', **kwargs)</code></a><a id="Conv2DLSTM.__init__" /></h4>
<p>Construct Conv2DLSTM. See <code>snt.ConvLSTM</code> for more details.</p>
<h4 id="conv2dlstm__call__inputs-state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1364"><code>Conv2DLSTM.__call__(inputs, state)</code></a><a id="Conv2DLSTM.__call__" /></h4>
<h4 id="conv2dlstmconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Conv2DLSTM.connected_subgraphs</code></a><a id="Conv2DLSTM.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="conv2dlstmconvolutions"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1347"><code>Conv2DLSTM.convolutions</code></a><a id="Conv2DLSTM.convolutions" /></h4>
<h4 id="conv2dlstmdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Conv2DLSTM.defun()</code></a><a id="Conv2DLSTM.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="conv2dlstmdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Conv2DLSTM.defun_wrapped</code></a><a id="Conv2DLSTM.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="conv2dlstmget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Conv2DLSTM.get_all_variables(collection='trainable_variables')</code></a><a id="Conv2DLSTM.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_87">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_179">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_167">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dlstmget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1419"><code>Conv2DLSTM.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="Conv2DLSTM.get_possible_initializer_keys" /></h4>
<h4 id="conv2dlstmget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>Conv2DLSTM.get_variables(collection='trainable_variables')</code></a><a id="Conv2DLSTM.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_88">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_180">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_168">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dlstmgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Conv2DLSTM.graph</code></a><a id="Conv2DLSTM.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="conv2dlstminitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=203"><code>Conv2DLSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)</code></a><a id="Conv2DLSTM.initial_state" /></h4>
<p>Builds the default start state for an RNNCore.</p>
<h5 id="args_89">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, or scalar int32 Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.
    Note that intializers and regularizers will be ignored if
    <code>trainable=False</code>.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. As a
    default, no regularizers are used. A regularizer should be a function
    that takes a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code>
    output, e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_181">Returns:</h5>
<p>A tensor or nested tuple of tensors with same structure and shape as the
  <code>state_size</code> property of the core.</p>
<h5 id="raises_169">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the user passes initializers that are not functions.</li>
<li><code>ValueError</code>: if the user passes regularizers that are not functions.</li>
</ul>
<h4 id="conv2dlstmis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Conv2DLSTM.is_connected</code></a><a id="Conv2DLSTM.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="conv2dlstmlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Conv2DLSTM.last_connected_subgraph</code></a><a id="Conv2DLSTM.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_182">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_170">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dlstmmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Conv2DLSTM.module_name</code></a><a id="Conv2DLSTM.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="conv2dlstmname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Conv2DLSTM.name_scopes</code></a><a id="Conv2DLSTM.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="conv2dlstmnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Conv2DLSTM.non_trainable_variables</code></a><a id="Conv2DLSTM.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_183">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_171">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dlstmoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1358"><code>Conv2DLSTM.output_size</code></a><a id="Conv2DLSTM.output_size" /></h4>
<p><code>tf.TensorShape</code> indicating the size of the core output.</p>
<h4 id="conv2dlstmscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Conv2DLSTM.scope_name</code></a><a id="Conv2DLSTM.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="conv2dlstmstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1351"><code>Conv2DLSTM.state_size</code></a><a id="Conv2DLSTM.state_size" /></h4>
<p>Tuple of <code>tf.TensorShape</code>s indicating the size of state tensors.</p>
<h4 id="conv2dlstmtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Conv2DLSTM.trainable_variables</code></a><a id="Conv2DLSTM.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_184">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_172">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dlstmuse_layer_norm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1396"><code>Conv2DLSTM.use_layer_norm</code></a><a id="Conv2DLSTM.use_layer_norm" /></h4>
<p>Boolean indicating whether layer norm is enabled.</p>
<h4 id="conv2dlstmvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Conv2DLSTM.variable_scope</code></a><a id="Conv2DLSTM.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_185">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_173">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dlstmvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Conv2DLSTM.variables</code></a><a id="Conv2DLSTM.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_186">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_174">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dlstmzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>Conv2DLSTM.zero_state(batch_size, dtype)</code></a><a id="Conv2DLSTM.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_90">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_187">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-conv2dtranspose"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?q=class:Conv2DTranspose"><code>class Conv2DTranspose</code></a><a id="Conv2DTranspose" /></h3>
<p>Spatial transposed / reverse / up 2D convolution module, including bias.</p>
<p>This acts as a light wrapper around the TensorFlow op <code>tf.nn.conv2d_transpose</code>
abstracting away variable creation and sharing.</p>
<h4 id="conv2dtranspose__init__output_channels-output_shapenone-kernel_shapenone-stride1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatnhwc-custom_getternone-nameconv_2d_transpose"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1813"><code>Conv2DTranspose.__init__(output_channels, output_shape=None, kernel_shape=None, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NHWC', custom_getter=None, name='conv_2d_transpose')</code></a><a id="Conv2DTranspose.__init__" /></h4>
<p>Constructs a <code>Conv2DTranspose module</code>.</p>
<p>See the following documentation for an explanation of VALID versus SAME
padding modes:
https://www.tensorflow.org/api_guides/python/nn#Convolution</p>
<h5 id="args_91">Args:</h5>
<ul>
<li><code>output_channels</code>: Number of output channels.
      Can be either a number or a callable. In the latter case, since the
      function invocation is deferred to graph construction time, the user
      must only ensure <code>output_channels</code> can be called, returning an
      integer, when build is called.</li>
<li><code>output_shape</code>: Output shape of transpose convolution.
      Can be either an iterable of integers or a callable. In the latter
      case, since the function invocation is deferred to graph construction
      time, the user must only ensure that <code>output_shape</code> can be called,
      returning an iterable of format <code>(out_height, out_width)</code> when <code>build</code>
      is called. Note that <code>output_shape</code> defines the size of output signal
      domain, as opposed to the shape of the output <code>Tensor</code>. If a None
      value is given, a default shape is automatically calculated (see
      docstring of _default_transpose_size function for more details).</li>
<li><code>kernel_shape</code>: Sequence of kernel sizes (of size 2), or integer that is
      used to define kernel size in all dimensions.</li>
<li><code>stride</code>: Sequence of kernel strides (of size 2), or integer that is used to
      define stride in all dimensions.</li>
<li><code>padding</code>: Padding algorithm, either <code>snt.SAME</code> or <code>snt.VALID</code>.</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      key 'w') or biases (with key 'b').</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
      weights (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
      (with key 'w') and the biases (with key 'b'). As a default, no
      regularizers are used. A regularizer should be a function that takes
      a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output,
      e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>data_format</code>: A string. Specifies whether the channel dimension
      of the input and output is the last dimension (default, NHWC), or the
      second dimension ("NCHW").</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
      custom getters inside the module. If a dictionary, the keys
      correspond to regexes to match variable names. See the<code>tf.get_variable</code> documentation for information about the
      custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_175">Raises:</h5>
<p>base.IncompatibleShapeError: If the given kernel shape is neither an
      integer nor a sequence of two integers.
  base.IncompatibleShapeError: If the given stride is neither an integer nor
      a sequence of two or four integers.</p>
<ul>
<li><code>ValueError</code>: If the given padding is not <code>snt.VALID</code> or <code>snt.SAME</code>.</li>
<li><code>ValueError</code>: If the given kernel_shape is <code>None</code>.</li>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
      keys other than 'w' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
      are not callable.</li>
<li><code>ValueError</code>: If the passed-in data_format doesn't have a channel dimension.</li>
<li><code>ValueError</code>: If the given data_format is not a supported format (see
      <code>SUPPORTED_2D_DATA_FORMATS</code>).</li>
</ul>
<h4 id="conv2dtranspose__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=995"><code>Conv2DTranspose.__call__(inputs)</code></a><a id="Conv2DTranspose.__call__" /></h4>
<p>Connects the _ConvNDTranspose module into the graph.</p>
<p>If this is not the first time the module has been connected to the graph,
the input Tensor provided here must have the same final N dimensions, in
order for the existing variables to be the correct size for the
multiplication. The batch size may differ for each connection.</p>
<h5 id="args_92">Args:</h5>
<ul>
<li><code>inputs</code>: A Tensor of shape <code>data_format</code> and of type
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</li>
</ul>
<h5 id="returns_188">Returns:</h5>
<p>A Tensor of shape <code>data_format</code> and of type <code>tf.float16</code>, <code>tf.bfloat16</code>,
     <code>tf.float32</code> or <code>tf.float64</code>.</p>
<h5 id="raises_176">Raises:</h5>
<ul>
<li>
<p><code>ValueError</code>: If connecting the module into the graph any time after the
      first time and the inferred size of the input does not match previous
      invocations.
  base.IncompatibleShapeError: If the input tensor has the wrong number
      of dimensions.
  base.UnderspecifiedError: If the channel dimension of <code>inputs</code> isn't
      defined.
  base.IncompatibleShapeError: If <code>output_shape</code> is an iterable and is not
      in the format <code>(out_height, out_width)</code>.</p>
</li>
<li>
<p><code>TypeError</code>: If input Tensor dtype is not compatible with either
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</p>
</li>
</ul>
<h4 id="conv2dtransposeb"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1230"><code>Conv2DTranspose.b</code></a><a id="Conv2DTranspose.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_189">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_177">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the graph
      yet, meaning the variables do not exist.</p>
<ul>
<li><code>AttributeError</code>: If the module does not use bias.</li>
</ul>
<h4 id="conv2dtransposeconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Conv2DTranspose.connected_subgraphs</code></a><a id="Conv2DTranspose.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="conv2dtransposeconv_op_padding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1219"><code>Conv2DTranspose.conv_op_padding</code></a><a id="Conv2DTranspose.conv_op_padding" /></h4>
<p>Returns the padding algorithm used for the underlying convolution op.</p>
<h4 id="conv2dtransposedefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Conv2DTranspose.defun()</code></a><a id="Conv2DTranspose.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="conv2dtransposedefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Conv2DTranspose.defun_wrapped</code></a><a id="Conv2DTranspose.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="conv2dtransposeget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Conv2DTranspose.get_all_variables(collection='trainable_variables')</code></a><a id="Conv2DTranspose.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_93">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_190">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_178">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dtransposeget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=991"><code>Conv2DTranspose.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="Conv2DTranspose.get_possible_initializer_keys" /></h4>
<h4 id="conv2dtransposeget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>Conv2DTranspose.get_variables(collection='trainable_variables')</code></a><a id="Conv2DTranspose.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_94">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_191">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_179">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dtransposegraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Conv2DTranspose.graph</code></a><a id="Conv2DTranspose.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="conv2dtransposehas_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1248"><code>Conv2DTranspose.has_bias</code></a><a id="Conv2DTranspose.has_bias" /></h4>
<p>Returns <code>True</code> if bias Variable is present in the module.</p>
<h4 id="conv2dtransposeinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1253"><code>Conv2DTranspose.initializers</code></a><a id="Conv2DTranspose.initializers" /></h4>
<p>Returns the initializers dictionary.</p>
<h4 id="conv2dtransposeinput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1274"><code>Conv2DTranspose.input_channels</code></a><a id="Conv2DTranspose.input_channels" /></h4>
<p>Returns the number of input channels.</p>
<h4 id="conv2dtransposeinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1268"><code>Conv2DTranspose.input_shape</code></a><a id="Conv2DTranspose.input_shape" /></h4>
<p>Returns the input shape.</p>
<h4 id="conv2dtransposeis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Conv2DTranspose.is_connected</code></a><a id="Conv2DTranspose.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="conv2dtransposekernel_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1195"><code>Conv2DTranspose.kernel_shape</code></a><a id="Conv2DTranspose.kernel_shape" /></h4>
<p>Returns the kernel shape.</p>
<h4 id="conv2dtransposelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Conv2DTranspose.last_connected_subgraph</code></a><a id="Conv2DTranspose.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_192">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_180">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dtransposemodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Conv2DTranspose.module_name</code></a><a id="Conv2DTranspose.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="conv2dtransposename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Conv2DTranspose.name_scopes</code></a><a id="Conv2DTranspose.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="conv2dtransposenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Conv2DTranspose.non_trainable_variables</code></a><a id="Conv2DTranspose.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_193">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_181">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dtransposeoutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1186"><code>Conv2DTranspose.output_channels</code></a><a id="Conv2DTranspose.output_channels" /></h4>
<p>Returns the number of output channels.</p>
<h4 id="conv2dtransposeoutput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1205"><code>Conv2DTranspose.output_shape</code></a><a id="Conv2DTranspose.output_shape" /></h4>
<p>Returns the output shape.</p>
<h4 id="conv2dtransposepadding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1214"><code>Conv2DTranspose.padding</code></a><a id="Conv2DTranspose.padding" /></h4>
<p>Returns the padding algorithm.</p>
<h4 id="conv2dtransposepartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1258"><code>Conv2DTranspose.partitioners</code></a><a id="Conv2DTranspose.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="conv2dtransposeregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1263"><code>Conv2DTranspose.regularizers</code></a><a id="Conv2DTranspose.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="conv2dtransposescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Conv2DTranspose.scope_name</code></a><a id="Conv2DTranspose.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="conv2dtransposestride"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1200"><code>Conv2DTranspose.stride</code></a><a id="Conv2DTranspose.stride" /></h4>
<p>Returns the stride.</p>
<h4 id="conv2dtransposetrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Conv2DTranspose.trainable_variables</code></a><a id="Conv2DTranspose.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_194">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_182">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dtransposetransposenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1893"><code>Conv2DTranspose.transpose(name=None)</code></a><a id="Conv2DTranspose.transpose" /></h4>
<p>Returns matching <code>Conv2D</code> module.</p>
<h5 id="args_95">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of transpose module. The default name
      is constructed by appending "_transpose" to <code>self.name</code>.</li>
</ul>
<h5 id="returns_195">Returns:</h5>
<p><code>Conv2D</code> module.</p>
<h4 id="conv2dtransposevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Conv2DTranspose.variable_scope</code></a><a id="Conv2DTranspose.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_196">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_183">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dtransposevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Conv2DTranspose.variables</code></a><a id="Conv2DTranspose.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_197">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_184">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv2dtransposew"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1224"><code>Conv2DTranspose.w</code></a><a id="Conv2DTranspose.w" /></h4>
<p>Returns the Variable containing the weight matrix.</p>
<h3 id="class-conv3d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?q=class:Conv3D"><code>class Conv3D</code></a><a id="Conv3D" /></h3>
<p>Volumetric convolution module, including optional bias.</p>
<p>This acts as a light wrapper around the class <code>_ConvND</code>.</p>
<h4 id="conv3d__init__output_channels-kernel_shape-stride1-rate1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-masknone-data_formatndhwc-custom_getternone-nameconv_3d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1930"><code>Conv3D.__init__(output_channels, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, mask=None, data_format='NDHWC', custom_getter=None, name='conv_3d')</code></a><a id="Conv3D.__init__" /></h4>
<p>Constructs a Conv3D module.</p>
<p>See the following documentation for an explanation of VALID versus SAME
padding modes:
https://www.tensorflow.org/api_guides/python/nn#Convolution</p>
<h5 id="args_96">Args:</h5>
<ul>
<li><code>output_channels</code>: Number of output channels. <code>output_channels</code> can be
      either a number or a callable. In the latter case, since the function
      invocation is deferred to graph construction time, the user must only
      ensure that output_channels can be called, returning an integer,
      when <code>build</code> is called.</li>
<li><code>kernel_shape</code>: Sequence of kernel sizes (of size 3), or integer that is
      used to define kernel size in all dimensions.</li>
<li><code>stride</code>: Sequence of kernel strides (of size 3), or integer that is used to
      define stride in all dimensions.</li>
<li><code>rate</code>: Sequence of dilation rates (of size 3), or integer that is used to
      define dilation rate in all dimensions. 1 corresponds to standard 3D
      convolution, <code>rate &gt; 1</code> corresponds to dilated convolution. Cannot be
      &gt; 1 if any of <code>stride</code> is also &gt; 1.</li>
<li><code>padding</code>: Padding algorithm. Either <code>snt.SAME</code>, <code>snt.VALID</code>, <code>snt.FULL</code>,
      <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code>, or a sequence of these paddings
      of length 3.<ul>
<li>snt.SAME and snt.VALID are explained in the Tensorflow docs at
    https://www.tensorflow.org/api_guides/python/nn#Convolution.</li>
<li>snt.FULL pre- and post-pads with the maximum padding which does not
    result in a convolution over just padded elements.</li>
<li>snt.CAUSAL pre-pads to ensure that each output value only depends on
    input values at the same or preceding indices ("no dependence on the
    future").</li>
<li>snt.REVERSE_CAUSAL post-pads to ensure that each output value only
    depends on input values at the same or <em>greater</em> indices ("no
    dependence on the past").
  If you use the same padding for all dimensions, and it is one of SAME
  or VALID, then this is supported directly by the underlying
  convolution op. In all other cases, the input data will be padded
  using tf.pad before calling the convolution op.</li>
</ul>
</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      key 'w') or biases (with key 'b'). The default initializer for the
      weights is a truncated normal initializer, which is commonly used
      when the inputs are zero centered (see
      https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for
      the bias is a zero initializer.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
      weights (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
      (with key 'w') and the biases (with key 'b'). As a default, no
      regularizers are used. A regularizer should be a function that takes
      a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output,
      e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>mask</code>: An object convertible to a 5D tensor which is multiplied
      component-wise with the weights (Optional).</li>
<li><code>data_format</code>: A string. Specifies whether the channel dimension
      of the input and output is the last dimension (default, NDHWC), or
      the second dimension (NCDHW).</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
      custom getters inside the module. If a dictionary, the keys
      correspond to regexes to match variable names. See the
      <code>tf.get_variable</code> documentation for information about the
      custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_185">Raises:</h5>
<p>base.IncompatibleShapeError: If the given kernel shape is not an integer;
      or if the given kernel shape is not a sequence of two integers.
  base.IncompatibleShapeError: If the given stride is not an integer; or if
      the given stride is not a sequence of two or four integers.
  base.IncompatibleShapeError: If the given rate is not an integer; or if
      the given rate is not a sequence of two integers.
  base.NotSupportedError: If rate in any dimension and the stride in any
      dimension are simultaneously &gt; 1.</p>
<ul>
<li><code>ValueError</code>: If the given padding is not <code>snt.VALID</code>, <code>snt.SAME</code>,
      <code>snt.FULL</code>, <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code> or a sequence of these.</li>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
      keys other than 'w' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
      are not callable.</li>
<li><code>ValueError</code>: If the passed-in data_format doesn't have a channel dimension.</li>
<li><code>ValueError</code>: If the given data_format is not a supported format (see
      <code>SUPPORTED_3D_DATA_FORMATS</code>).</li>
</ul>
<h4 id="conv3d__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=521"><code>Conv3D.__call__(inputs)</code></a><a id="Conv3D.__call__" /></h4>
<p>Connects the _ConvND module into the graph, with input Tensor <code>inputs</code>.</p>
<p>If this is not the first time the module has been connected to the graph,
the input Tensor provided here must have the same number of channels, in
order for the existing variables to be the correct size for the
multiplication; the batch size and input spatial dimensions may differ for
each connection.</p>
<h5 id="args_97">Args:</h5>
<ul>
<li><code>inputs</code>: A ND Tensor of the same rank as <code>data_format</code>, and either of types
  <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</li>
</ul>
<h5 id="returns_198">Returns:</h5>
<p>A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ...,
      output_channels].</p>
<h5 id="raises_186">Raises:</h5>
<ul>
<li>
<p><code>ValueError</code>: If connecting the module into the graph any time after the
      first time and the inferred size of the input does not match previous
      invocations.
  base.IncompatibleShapeError: If the input tensor has the wrong number
      of dimensions.
  base.UnderspecifiedError: If the channel dimension of <code>inputs</code> isn't
      defined.
  base.IncompatibleShapeError: If a mask is present and its shape is
      incompatible with the shape of the weights.</p>
</li>
<li>
<p><code>TypeError</code>: If input Tensor dtype is not compatible with either
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</p>
</li>
</ul>
<h4 id="conv3db"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=778"><code>Conv3D.b</code></a><a id="Conv3D.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_199">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_187">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the graph
      yet, meaning the variables do not exist.</p>
<ul>
<li><code>AttributeError</code>: If the module does not use bias.</li>
</ul>
<h4 id="conv3dclonenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=840"><code>Conv3D.clone(name=None)</code></a><a id="Conv3D.clone" /></h4>
<p>Returns a cloned <code>_ConvND</code> module.</p>
<h5 id="args_98">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of cloned module. The default name
    is constructed by appending "_clone" to <code>self.module_name</code>.</li>
</ul>
<h5 id="returns_200">Returns:</h5>
<p>A copy of the current class.</p>
<h4 id="conv3dconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Conv3D.connected_subgraphs</code></a><a id="Conv3D.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="conv3dconv_op_padding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=767"><code>Conv3D.conv_op_padding</code></a><a id="Conv3D.conv_op_padding" /></h4>
<p>Returns the padding algorithm used for the underlying convolution op.</p>
<h4 id="conv3ddata_format"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=821"><code>Conv3D.data_format</code></a><a id="Conv3D.data_format" /></h4>
<p>Returns the data format.</p>
<h4 id="conv3ddefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Conv3D.defun()</code></a><a id="Conv3D.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="conv3ddefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Conv3D.defun_wrapped</code></a><a id="Conv3D.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="conv3dget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Conv3D.get_all_variables(collection='trainable_variables')</code></a><a id="Conv3D.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_99">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_201">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_188">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=517"><code>Conv3D.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="Conv3D.get_possible_initializer_keys" /></h4>
<h4 id="conv3dget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>Conv3D.get_variables(collection='trainable_variables')</code></a><a id="Conv3D.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_100">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_202">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_189">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Conv3D.graph</code></a><a id="Conv3D.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="conv3dhas_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=796"><code>Conv3D.has_bias</code></a><a id="Conv3D.has_bias" /></h4>
<p>Returns <code>True</code> if bias Variable is present in the module.</p>
<h4 id="conv3dinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=801"><code>Conv3D.initializers</code></a><a id="Conv3D.initializers" /></h4>
<p>Returns the initializers dictionary.</p>
<h4 id="conv3dinput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=833"><code>Conv3D.input_channels</code></a><a id="Conv3D.input_channels" /></h4>
<p>Returns the number of input channels.</p>
<h4 id="conv3dinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=827"><code>Conv3D.input_shape</code></a><a id="Conv3D.input_shape" /></h4>
<p>Returns the input shape.</p>
<h4 id="conv3dis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Conv3D.is_connected</code></a><a id="Conv3D.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="conv3dkernel_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=722"><code>Conv3D.kernel_shape</code></a><a id="Conv3D.kernel_shape" /></h4>
<p>Returns the kernel shape.</p>
<h4 id="conv3dlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Conv3D.last_connected_subgraph</code></a><a id="Conv3D.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_203">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_190">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dmask"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=816"><code>Conv3D.mask</code></a><a id="Conv3D.mask" /></h4>
<p>Returns the mask.</p>
<h4 id="conv3dmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Conv3D.module_name</code></a><a id="Conv3D.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="conv3dname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Conv3D.name_scopes</code></a><a id="Conv3D.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="conv3dnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Conv3D.non_trainable_variables</code></a><a id="Conv3D.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_204">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_191">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3doutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=713"><code>Conv3D.output_channels</code></a><a id="Conv3D.output_channels" /></h4>
<p>Returns the number of output channels.</p>
<h4 id="conv3dpadding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=739"><code>Conv3D.padding</code></a><a id="Conv3D.padding" /></h4>
<p>Returns the padding algorithm used, if this is the same for all dims.</p>
<p>Use <code>.paddings</code> if you want a tuple with the padding algorithm used for each
dimension.</p>
<h5 id="returns_205">Returns:</h5>
<p>The padding algorithm used, if this is the same for all dimensions.</p>
<h5 id="raises_192">Raises:</h5>
<ul>
<li><code>ValueError</code>: If different padding algorithms are used for different
    dimensions.</li>
</ul>
<h4 id="conv3dpaddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=762"><code>Conv3D.paddings</code></a><a id="Conv3D.paddings" /></h4>
<p>Returns a tuple with the padding algorithm used for each dimension.</p>
<h4 id="conv3dpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=806"><code>Conv3D.partitioners</code></a><a id="Conv3D.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="conv3drate"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=734"><code>Conv3D.rate</code></a><a id="Conv3D.rate" /></h4>
<p>Returns the dilation rate.</p>
<h4 id="conv3dregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=811"><code>Conv3D.regularizers</code></a><a id="Conv3D.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="conv3dscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Conv3D.scope_name</code></a><a id="Conv3D.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="conv3dstride"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=727"><code>Conv3D.stride</code></a><a id="Conv3D.stride" /></h4>
<p>Returns the stride.</p>
<h4 id="conv3dtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Conv3D.trainable_variables</code></a><a id="Conv3D.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_206">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_193">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dtransposenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2029"><code>Conv3D.transpose(name=None)</code></a><a id="Conv3D.transpose" /></h4>
<p>Returns matching <code>Conv3DTranspose</code> module.</p>
<h5 id="args_101">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of transpose module. The default name
    is constructed by appending "_transpose" to <code>self.name</code>.</li>
</ul>
<h5 id="returns_207">Returns:</h5>
<p><code>Conv3DTranspose</code> module.</p>
<h5 id="raises_194">Raises:</h5>
<p>base.NotSupportedError: If <code>rate</code> in any dimension &gt; 1.</p>
<h4 id="conv3dvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Conv3D.variable_scope</code></a><a id="Conv3D.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_208">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_195">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Conv3D.variables</code></a><a id="Conv3D.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_209">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_196">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=772"><code>Conv3D.w</code></a><a id="Conv3D.w" /></h4>
<p>Returns the Variable containing the weight matrix.</p>
<h3 id="class-conv3dtranspose"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?q=class:Conv3DTranspose"><code>class Conv3DTranspose</code></a><a id="Conv3DTranspose" /></h3>
<p>Volumetric transposed / reverse / up 3D convolution module, including bias.</p>
<p>This acts as a light wrapper around the TensorFlow op <code>tf.nn.conv3d_transpose</code>
abstracting away variable creation and sharing.</p>
<h4 id="conv3dtranspose__init__output_channels-output_shapenone-kernel_shapenone-stride1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatndhwc-custom_getternone-nameconv_3d_transpose"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2080"><code>Conv3DTranspose.__init__(output_channels, output_shape=None, kernel_shape=None, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NDHWC', custom_getter=None, name='conv_3d_transpose')</code></a><a id="Conv3DTranspose.__init__" /></h4>
<p>Constructs a <code>Conv3DTranspose</code> module.</p>
<p>See the following documentation for an explanation of VALID versus SAME
padding modes:
https://www.tensorflow.org/api_guides/python/nn#Convolution</p>
<h5 id="args_102">Args:</h5>
<ul>
<li><code>output_channels</code>: Number of output channels. <code>output_channels</code> can be
      either a number or a callable. In the latter case, since the function
      invocation is deferred to graph construction time, the user must only
      ensure <code>output_channels</code> can be called, returning an integer, when
      <code>build</code> is called.</li>
<li><code>output_shape</code>: Output shape of transpose convolution.
      Can be either an iterable of integers or a callable. In the latter
      case, since the function invocation is deferred to graph construction
      time, the user must only ensure that <code>output_shape</code> can be called,
      returning an iterable of format <code>(out_depth, out_height, out_width)</code>
      when <code>build</code> is called. Note that <code>output_shape</code> defines the size of
      output signal domain, as opposed to the shape of the output <code>Tensor</code>.
      If a None value is given, a default shape is automatically calculated
      (see docstring of _default_transpose_size function for more details).</li>
<li><code>kernel_shape</code>: Sequence of kernel sizes (of size 3), or integer that is
      used to define kernel size in all dimensions.</li>
<li><code>stride</code>: Sequence of kernel strides (of size 3), or integer that is used to
      define stride in all dimensions.</li>
<li><code>padding</code>: Padding algorithm, either <code>snt.SAME</code> or <code>snt.VALID</code>.</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      key 'w') or biases (with key 'b').</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
      weights (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
      (with key 'w') and the biases (with key 'b'). As a default, no
      regularizers are used. A regularizer should be a function that takes
      a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output,
      e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>data_format</code>: A string. Specifies whether the channel dimension
      of the input and output is the last dimension (default, NDHWC), or the
      second dimension (NCDHW).</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
      custom getters inside the module. If a dictionary, the keys
      correspond to regexes to match variable names. See the
      <code>tf.get_variable</code> documentation for information about the
      custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_197">Raises:</h5>
<p>module.IncompatibleShapeError: If the given kernel shape is neither an
      integer nor a sequence of three integers.
  module.IncompatibleShapeError: If the given stride is neither an integer
      nor a sequence of three or five integers.</p>
<ul>
<li><code>ValueError</code>: If the given padding is not <code>snt.VALID</code> or <code>snt.SAME</code>.</li>
<li><code>ValueError</code>: If the given kernel_shape is <code>None</code>.</li>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
      keys other than 'w' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
      are not callable.</li>
<li><code>ValueError</code>: If the passed-in data_format doesn't have a channel dimension.</li>
<li><code>ValueError</code>: If the given data_format is not a supported format (see
      <code>SUPPORTED_3D_DATA_FORMATS</code>).</li>
</ul>
<h4 id="conv3dtranspose__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=995"><code>Conv3DTranspose.__call__(inputs)</code></a><a id="Conv3DTranspose.__call__" /></h4>
<p>Connects the _ConvNDTranspose module into the graph.</p>
<p>If this is not the first time the module has been connected to the graph,
the input Tensor provided here must have the same final N dimensions, in
order for the existing variables to be the correct size for the
multiplication. The batch size may differ for each connection.</p>
<h5 id="args_103">Args:</h5>
<ul>
<li><code>inputs</code>: A Tensor of shape <code>data_format</code> and of type
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</li>
</ul>
<h5 id="returns_210">Returns:</h5>
<p>A Tensor of shape <code>data_format</code> and of type <code>tf.float16</code>, <code>tf.bfloat16</code>,
     <code>tf.float32</code> or <code>tf.float64</code>.</p>
<h5 id="raises_198">Raises:</h5>
<ul>
<li>
<p><code>ValueError</code>: If connecting the module into the graph any time after the
      first time and the inferred size of the input does not match previous
      invocations.
  base.IncompatibleShapeError: If the input tensor has the wrong number
      of dimensions.
  base.UnderspecifiedError: If the channel dimension of <code>inputs</code> isn't
      defined.
  base.IncompatibleShapeError: If <code>output_shape</code> is an iterable and is not
      in the format <code>(out_height, out_width)</code>.</p>
</li>
<li>
<p><code>TypeError</code>: If input Tensor dtype is not compatible with either
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</p>
</li>
</ul>
<h4 id="conv3dtransposeb"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1230"><code>Conv3DTranspose.b</code></a><a id="Conv3DTranspose.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_211">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_199">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the graph
      yet, meaning the variables do not exist.</p>
<ul>
<li><code>AttributeError</code>: If the module does not use bias.</li>
</ul>
<h4 id="conv3dtransposeconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Conv3DTranspose.connected_subgraphs</code></a><a id="Conv3DTranspose.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="conv3dtransposeconv_op_padding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1219"><code>Conv3DTranspose.conv_op_padding</code></a><a id="Conv3DTranspose.conv_op_padding" /></h4>
<p>Returns the padding algorithm used for the underlying convolution op.</p>
<h4 id="conv3dtransposedefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Conv3DTranspose.defun()</code></a><a id="Conv3DTranspose.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="conv3dtransposedefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Conv3DTranspose.defun_wrapped</code></a><a id="Conv3DTranspose.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="conv3dtransposeget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Conv3DTranspose.get_all_variables(collection='trainable_variables')</code></a><a id="Conv3DTranspose.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_104">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_212">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_200">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dtransposeget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=991"><code>Conv3DTranspose.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="Conv3DTranspose.get_possible_initializer_keys" /></h4>
<h4 id="conv3dtransposeget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>Conv3DTranspose.get_variables(collection='trainable_variables')</code></a><a id="Conv3DTranspose.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_105">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_213">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_201">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dtransposegraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Conv3DTranspose.graph</code></a><a id="Conv3DTranspose.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="conv3dtransposehas_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1248"><code>Conv3DTranspose.has_bias</code></a><a id="Conv3DTranspose.has_bias" /></h4>
<p>Returns <code>True</code> if bias Variable is present in the module.</p>
<h4 id="conv3dtransposeinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1253"><code>Conv3DTranspose.initializers</code></a><a id="Conv3DTranspose.initializers" /></h4>
<p>Returns the initializers dictionary.</p>
<h4 id="conv3dtransposeinput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1274"><code>Conv3DTranspose.input_channels</code></a><a id="Conv3DTranspose.input_channels" /></h4>
<p>Returns the number of input channels.</p>
<h4 id="conv3dtransposeinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1268"><code>Conv3DTranspose.input_shape</code></a><a id="Conv3DTranspose.input_shape" /></h4>
<p>Returns the input shape.</p>
<h4 id="conv3dtransposeis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Conv3DTranspose.is_connected</code></a><a id="Conv3DTranspose.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="conv3dtransposekernel_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1195"><code>Conv3DTranspose.kernel_shape</code></a><a id="Conv3DTranspose.kernel_shape" /></h4>
<p>Returns the kernel shape.</p>
<h4 id="conv3dtransposelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Conv3DTranspose.last_connected_subgraph</code></a><a id="Conv3DTranspose.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_214">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_202">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dtransposemodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Conv3DTranspose.module_name</code></a><a id="Conv3DTranspose.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="conv3dtransposename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Conv3DTranspose.name_scopes</code></a><a id="Conv3DTranspose.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="conv3dtransposenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Conv3DTranspose.non_trainable_variables</code></a><a id="Conv3DTranspose.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_215">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_203">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dtransposeoutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1186"><code>Conv3DTranspose.output_channels</code></a><a id="Conv3DTranspose.output_channels" /></h4>
<p>Returns the number of output channels.</p>
<h4 id="conv3dtransposeoutput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1205"><code>Conv3DTranspose.output_shape</code></a><a id="Conv3DTranspose.output_shape" /></h4>
<p>Returns the output shape.</p>
<h4 id="conv3dtransposepadding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1214"><code>Conv3DTranspose.padding</code></a><a id="Conv3DTranspose.padding" /></h4>
<p>Returns the padding algorithm.</p>
<h4 id="conv3dtransposepartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1258"><code>Conv3DTranspose.partitioners</code></a><a id="Conv3DTranspose.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="conv3dtransposeregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1263"><code>Conv3DTranspose.regularizers</code></a><a id="Conv3DTranspose.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="conv3dtransposescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Conv3DTranspose.scope_name</code></a><a id="Conv3DTranspose.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="conv3dtransposestride"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1200"><code>Conv3DTranspose.stride</code></a><a id="Conv3DTranspose.stride" /></h4>
<p>Returns the stride.</p>
<h4 id="conv3dtransposetrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Conv3DTranspose.trainable_variables</code></a><a id="Conv3DTranspose.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_216">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_204">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dtransposetransposenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2160"><code>Conv3DTranspose.transpose(name=None)</code></a><a id="Conv3DTranspose.transpose" /></h4>
<p>Returns transposed Conv3DTranspose module, i.e. a Conv3D module.</p>
<h4 id="conv3dtransposevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Conv3DTranspose.variable_scope</code></a><a id="Conv3DTranspose.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_217">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_205">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dtransposevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Conv3DTranspose.variables</code></a><a id="Conv3DTranspose.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_218">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_206">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="conv3dtransposew"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=1224"><code>Conv3DTranspose.w</code></a><a id="Conv3DTranspose.w" /></h4>
<p>Returns the Variable containing the weight matrix.</p>
<h3 id="class-deeprnn"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?q=class:DeepRNN"><code>class DeepRNN</code></a><a id="DeepRNN" /></h3>
<p>RNN core that passes data through a number of internal modules or ops.</p>
<p>This module is constructed by passing an iterable of externally constructed
modules or ops. The DeepRNN takes <code>(input, prev_state)</code> as input and passes
the input through each internal module in the order they were presented,
using elements from <code>prev_state</code> as necessary for internal recurrent cores.
The output is <code>(output, next_state)</code> in common with other RNN cores.
By default, skip connections from the input to all internal modules and from
each intermediate output to the final output are used.</p>
<p>E.g.:</p>
<pre><code class="python">lstm1 = snt.LSTM(hidden_size=256)
lstm2 = snt.LSTM(hidden_size=256)
deep_rnn = snt.DeepRNN([lstm1, lstm2])
output, next_state = deep_rnn(input, prev_state)
</code></pre>

<p>The computation set up inside the DeepRNN has the same effect as:</p>
<pre><code class="python">prev_state1, prev_state2 = prev_state
lstm1_output, next_state1 = lstm1(input, prev_state1)
lstm2_output, next_state2 = lstm2(
    tf.concat([input, lstm1_output], 1), prev_state2)

next_state = (next_state1, next_state2)
output = tf.concat([lstm1_output, lstm2_output], 1)
</code></pre>

<p>Every internal module receives the preceding module's output and the entire
core's input. The output is created by concatenating each internal module's
output. In the case of internal recurrent elements, corresponding elements
of the state are used such that <code>state[i]</code> is passed to the <code>i</code>'th internal
recurrent element. Note that the state of a <code>DeepRNN</code> is always a tuple, which
will contain the same number of elements as there are internal recurrent
cores. If no internal modules are recurrent, the state of the DeepRNN as a
whole is the empty tuple. Wrapping non-recurrent modules into a DeepRNN can
be useful to produce something API compatible with a "real" recurrent module,
simplifying code that handles the cores.</p>
<p>Without skip connections the previous example would become the following
(note the only difference is the addition of <code>skip_connections=False</code>):</p>
<pre><code class="python"># ... declare other modules as above
deep_rnn = snt.DeepRNN([lin, tanh, lstm], skip_connections=False)
output, next_state = deep_rnn(input, prev_state)
</code></pre>

<p>which is equivalent to:</p>
<pre><code class="python">lin_output = lin(input)
tanh_output = tanh(lin_output)
lstm_output, lstm_next_state = lstm(tanh_output, prev_state[0])

next_state = (lstm_next_state,)
output = lstm_output
</code></pre>

<p>Note: when using skip connections, all the cores should be recurrent.</p>
<h4 id="deeprnn__init__cores-skip_connectionstrue-concat_final_output_if_skiptrue-namedeep_rnn"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=246"><code>DeepRNN.__init__(cores, skip_connections=True, concat_final_output_if_skip=True, name='deep_rnn')</code></a><a id="DeepRNN.__init__" /></h4>
<p>Construct a Deep RNN core.</p>
<h5 id="args_106">Args:</h5>
<ul>
<li><code>cores</code>: iterable of modules or ops.</li>
<li><code>skip_connections</code>: a boolean that indicates whether to use skip
    connections. This means that the input is fed to all the layers, after
    being concatenated on the last dimension with the output of the previous
    layer. The output of the module will be the concatenation of all the
    outputs of the internal modules.</li>
<li><code>concat_final_output_if_skip</code>: A boolean that indicates whether the outputs
    of intermediate layers should be concatenated into the timestep-wise
    output of the core. By default this is True. If this is set to False,
    then the core output is that of the final layer, i.e. that of
    <code>cores[-1]</code>.</li>
<li><code>name</code>: name of the module.</li>
</ul>
<h5 id="raises_207">Raises:</h5>
<ul>
<li><code>ValueError</code>: if <code>cores</code> is not an iterable, or if <code>skip_connections</code> is
      True and not all the modules are recurrent.</li>
</ul>
<h4 id="deeprnn__call__inputs-prev_state-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=311"><code>DeepRNN.__call__(inputs, prev_state, **kwargs)</code></a><a id="DeepRNN.__call__" /></h4>
<p>Connects the DeepRNN module into the graph.</p>
<p>If this is not the first time the module has been connected to the graph,
the Tensors provided as input_ and state must have the same final
dimension, in order for the existing variables to be the correct size for
their corresponding multiplications. The batch size may differ for each
connection.</p>
<h5 id="args_107">Args:</h5>
<ul>
<li><code>inputs</code>: a nested tuple of Tensors of arbitrary dimensionality, with at
    least an initial batch dimension.</li>
<li><code>prev_state</code>: a tuple of <code>prev_state</code>s that corresponds to the state
    of each one of the cores of the <code>DeepCore</code>.</li>
<li><code>**kwargs</code>: optional kwargs to be passed to the <code>_build</code> of all sub-modules.
    E.g. is_training=True. Note all sub-modules must accept the given kwarg.</li>
</ul>
<h5 id="returns_219">Returns:</h5>
<ul>
<li><code>output</code>: a nested tuple of Tensors of arbitrary dimensionality, with at
    least an initial batch dimension.</li>
<li><code>next_state</code>: a tuple of <code>next_state</code>s that corresponds to the updated state
    of each one of the cores of the <code>DeepCore</code>.</li>
</ul>
<h5 id="raises_208">Raises:</h5>
<ul>
<li><code>ValueError</code>: if connecting the module into the graph any time after the
    first time, and the inferred size of the inputs does not match previous
    invocations. This may happen if one connects a module any time after the
    first time that does not have the configuration of skip connections as
    the first time.</li>
</ul>
<h4 id="deeprnnconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>DeepRNN.connected_subgraphs</code></a><a id="DeepRNN.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="deeprnndefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>DeepRNN.defun()</code></a><a id="DeepRNN.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="deeprnndefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>DeepRNN.defun_wrapped</code></a><a id="DeepRNN.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="deeprnnget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>DeepRNN.get_all_variables(collection='trainable_variables')</code></a><a id="DeepRNN.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_108">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_220">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_209">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="deeprnnget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>DeepRNN.get_possible_initializer_keys(cls)</code></a><a id="DeepRNN.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_221">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="deeprnnget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>DeepRNN.get_variables(collection='trainable_variables')</code></a><a id="DeepRNN.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_109">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_222">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_210">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="deeprnngraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>DeepRNN.graph</code></a><a id="DeepRNN.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="deeprnninitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=372"><code>DeepRNN.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None)</code></a><a id="DeepRNN.initial_state" /></h4>
<p>Builds the default start state for a DeepRNN.</p>
<h5 id="args_110">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, float or scalar Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. A
    regularizer should be a function that takes a single <code>Tensor</code> as an
    input and returns a scalar <code>Tensor</code> output, e.g. the L1 and L2
    regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_223">Returns:</h5>
<p>A tensor or nested tuple of tensors with same structure and shape as the
  <code>state_size</code> property of the core.</p>
<h5 id="raises_211">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the number of passed initializers is not the same as the
      number of recurrent cores.</li>
</ul>
<h4 id="deeprnnis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>DeepRNN.is_connected</code></a><a id="DeepRNN.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="deeprnnlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>DeepRNN.last_connected_subgraph</code></a><a id="DeepRNN.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_224">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_212">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="deeprnnmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>DeepRNN.module_name</code></a><a id="DeepRNN.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="deeprnnname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>DeepRNN.name_scopes</code></a><a id="DeepRNN.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="deeprnnnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>DeepRNN.non_trainable_variables</code></a><a id="DeepRNN.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_225">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_213">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="deeprnnoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=436"><code>DeepRNN.output_size</code></a><a id="DeepRNN.output_size" /></h4>
<h4 id="deeprnnscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>DeepRNN.scope_name</code></a><a id="DeepRNN.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="deeprnnstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=428"><code>DeepRNN.state_size</code></a><a id="DeepRNN.state_size" /></h4>
<h4 id="deeprnntrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>DeepRNN.trainable_variables</code></a><a id="DeepRNN.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_226">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_214">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="deeprnnvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>DeepRNN.variable_scope</code></a><a id="DeepRNN.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_227">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_215">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="deeprnnvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>DeepRNN.variables</code></a><a id="DeepRNN.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_228">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_216">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="deeprnnzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>DeepRNN.zero_state(batch_size, dtype)</code></a><a id="DeepRNN.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_111">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_229">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-depthwiseconv2d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?q=class:DepthwiseConv2D"><code>class DepthwiseConv2D</code></a><a id="DepthwiseConv2D" /></h3>
<p>Spatial depthwise 2D convolution module, including bias.</p>
<p>This acts as a light wrapper around the TensorFlow ops
<code>tf.nn.depthwise_conv2d</code>, abstracting away variable creation and sharing.</p>
<h4 id="depthwiseconv2d__init__channel_multiplier-kernel_shape-stride1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatnhwc-custom_getternone-nameconv_2d_depthwise"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2325"><code>DepthwiseConv2D.__init__(channel_multiplier, kernel_shape, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NHWC', custom_getter=None, name='conv_2d_depthwise')</code></a><a id="DepthwiseConv2D.__init__" /></h4>
<p>Constructs a DepthwiseConv2D module.</p>
<p>See the following documentation for an explanation of VALID versus SAME
padding modes:
https://www.tensorflow.org/api_guides/python/nn#Convolution</p>
<h5 id="args_112">Args:</h5>
<ul>
<li><code>channel_multiplier</code>: Number of channels to expand convolution to. Must be
      an integer. Must be &gt; 0. When <code>channel_multiplier</code> is set to 1, apply
      a different filter to each input channel producing one output channel
      per input channel. Numbers larger than 1 cause multiple different
      filters to be applied to each input channel, with their outputs being
      concatenated together, producing <code>channel_multiplier</code> *
      <code>input_channels</code> output channels.</li>
<li><code>kernel_shape</code>: Iterable with 2 elements in the following layout:
      [filter_height, filter_width] or integer that is
      used to define the list in all dimensions.</li>
<li><code>stride</code>: Iterable with 2 or 4 elements of kernel strides, or integer that
      is used to define stride in all dimensions. Layout of list:
      In case of 4 elements: <code>[1, stride_height, stride_widith, 1]</code>
      In case of 2 elements: <code>[stride_height, stride_width]</code>.</li>
<li><code>padding</code>: Padding algorithm. Either <code>snt.SAME</code>, <code>snt.VALID</code>, <code>snt.FULL</code>,
      <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code>, or a sequence of these paddings
      of length 2.<ul>
<li>snt.SAME and snt.VALID are explained in the Tensorflow docs at
    https://www.tensorflow.org/api_guides/python/nn#Convolution.</li>
<li>snt.FULL pre- and post-pads with the maximum padding which does not
    result in a convolution over just padded elements.</li>
<li>snt.CAUSAL pre-pads to ensure that each output value only depends on
    input values at the same or preceding indices ("no dependence on the
    future").</li>
<li>snt.REVERSE_CAUSAL post-pads to ensure that each output value only
    depends on input values at the same or <em>greater</em> indices ("no
    dependence on the past").
  If you use the same padding for all dimensions, and it is one of SAME
  or VALID, then this is supported directly by the underlying
  convolution op. In all other cases, the input data will be padded
  using tf.pad before calling the convolution op.</li>
</ul>
</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      key 'w') or biases (with key 'b').</li>
<li><code>partitioners</code>: Optional dict containing partitioners for the filters
      (with key 'w') and the biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
      (with key 'w') and the biases (with key 'b'). As a default, no
      regularizers are used. A regularizer should be a function that takes
      a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output,
      e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>data_format</code>: A string. Specifies whether the channel dimension
      of the input and output is the last dimension (default, NHWC), or the
      second dimension ("NCHW").</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
      custom getters inside the module. If a dictionary, the keys
      correspond to regexes to match variable names. See the
      <code>tf.get_variable</code> documentation for information about the
      custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_217">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>channel_multiplier</code> isn't of type (<code>numbers.Integral</code> or
      <code>tf.Dimension</code>).</li>
<li><code>ValueError</code>: If <code>channel_multiplier</code> is less than 1.</li>
<li>
<p><code>ValueError</code>: If the given data_format is not a supported format (see
      <code>SUPPORTED_2D_DATA_FORMATS</code>).
  base.IncompatibleShapeError: If the given kernel shape is not an integer;
      or if the given kernel shape is not a sequence of two integers.
  base.IncompatibleShapeError: If the given stride is not an integer; or if
      the given stride is not a sequence of two integers.</p>
</li>
<li>
<p><code>ValueError</code>: If the given padding is not <code>snt.VALID</code>, <code>snt.SAME</code>,
      <code>snt.FULL</code>, <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code> or a sequence of these.</p>
</li>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
      keys other than 'w' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
      are not callable.</li>
<li><code>ValueError</code>: If the passed-in data_format doesn't have a channel dimension.</li>
</ul>
<h4 id="depthwiseconv2d__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=521"><code>DepthwiseConv2D.__call__(inputs)</code></a><a id="DepthwiseConv2D.__call__" /></h4>
<p>Connects the _ConvND module into the graph, with input Tensor <code>inputs</code>.</p>
<p>If this is not the first time the module has been connected to the graph,
the input Tensor provided here must have the same number of channels, in
order for the existing variables to be the correct size for the
multiplication; the batch size and input spatial dimensions may differ for
each connection.</p>
<h5 id="args_113">Args:</h5>
<ul>
<li><code>inputs</code>: A ND Tensor of the same rank as <code>data_format</code>, and either of types
  <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</li>
</ul>
<h5 id="returns_230">Returns:</h5>
<p>A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ...,
      output_channels].</p>
<h5 id="raises_218">Raises:</h5>
<ul>
<li>
<p><code>ValueError</code>: If connecting the module into the graph any time after the
      first time and the inferred size of the input does not match previous
      invocations.
  base.IncompatibleShapeError: If the input tensor has the wrong number
      of dimensions.
  base.UnderspecifiedError: If the channel dimension of <code>inputs</code> isn't
      defined.
  base.IncompatibleShapeError: If a mask is present and its shape is
      incompatible with the shape of the weights.</p>
</li>
<li>
<p><code>TypeError</code>: If input Tensor dtype is not compatible with either
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</p>
</li>
</ul>
<h4 id="depthwiseconv2db"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=778"><code>DepthwiseConv2D.b</code></a><a id="DepthwiseConv2D.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_231">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_219">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the graph
      yet, meaning the variables do not exist.</p>
<ul>
<li><code>AttributeError</code>: If the module does not use bias.</li>
</ul>
<h4 id="depthwiseconv2dchannel_multiplier"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2488"><code>DepthwiseConv2D.channel_multiplier</code></a><a id="DepthwiseConv2D.channel_multiplier" /></h4>
<p>Returns the channel multiplier argument.</p>
<h4 id="depthwiseconv2dclonenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=840"><code>DepthwiseConv2D.clone(name=None)</code></a><a id="DepthwiseConv2D.clone" /></h4>
<p>Returns a cloned <code>_ConvND</code> module.</p>
<h5 id="args_114">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of cloned module. The default name
    is constructed by appending "_clone" to <code>self.module_name</code>.</li>
</ul>
<h5 id="returns_232">Returns:</h5>
<p>A copy of the current class.</p>
<h4 id="depthwiseconv2dconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>DepthwiseConv2D.connected_subgraphs</code></a><a id="DepthwiseConv2D.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="depthwiseconv2dconv_op_padding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=767"><code>DepthwiseConv2D.conv_op_padding</code></a><a id="DepthwiseConv2D.conv_op_padding" /></h4>
<p>Returns the padding algorithm used for the underlying convolution op.</p>
<h4 id="depthwiseconv2ddata_format"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=821"><code>DepthwiseConv2D.data_format</code></a><a id="DepthwiseConv2D.data_format" /></h4>
<p>Returns the data format.</p>
<h4 id="depthwiseconv2ddefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>DepthwiseConv2D.defun()</code></a><a id="DepthwiseConv2D.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="depthwiseconv2ddefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>DepthwiseConv2D.defun_wrapped</code></a><a id="DepthwiseConv2D.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="depthwiseconv2dget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>DepthwiseConv2D.get_all_variables(collection='trainable_variables')</code></a><a id="DepthwiseConv2D.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_115">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_233">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_220">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="depthwiseconv2dget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=517"><code>DepthwiseConv2D.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="DepthwiseConv2D.get_possible_initializer_keys" /></h4>
<h4 id="depthwiseconv2dget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>DepthwiseConv2D.get_variables(collection='trainable_variables')</code></a><a id="DepthwiseConv2D.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_116">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_234">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_221">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="depthwiseconv2dgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>DepthwiseConv2D.graph</code></a><a id="DepthwiseConv2D.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="depthwiseconv2dhas_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=796"><code>DepthwiseConv2D.has_bias</code></a><a id="DepthwiseConv2D.has_bias" /></h4>
<p>Returns <code>True</code> if bias Variable is present in the module.</p>
<h4 id="depthwiseconv2dinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=801"><code>DepthwiseConv2D.initializers</code></a><a id="DepthwiseConv2D.initializers" /></h4>
<p>Returns the initializers dictionary.</p>
<h4 id="depthwiseconv2dinput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=833"><code>DepthwiseConv2D.input_channels</code></a><a id="DepthwiseConv2D.input_channels" /></h4>
<p>Returns the number of input channels.</p>
<h4 id="depthwiseconv2dinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=827"><code>DepthwiseConv2D.input_shape</code></a><a id="DepthwiseConv2D.input_shape" /></h4>
<p>Returns the input shape.</p>
<h4 id="depthwiseconv2dis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>DepthwiseConv2D.is_connected</code></a><a id="DepthwiseConv2D.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="depthwiseconv2dkernel_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=722"><code>DepthwiseConv2D.kernel_shape</code></a><a id="DepthwiseConv2D.kernel_shape" /></h4>
<p>Returns the kernel shape.</p>
<h4 id="depthwiseconv2dlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>DepthwiseConv2D.last_connected_subgraph</code></a><a id="DepthwiseConv2D.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_235">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_222">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="depthwiseconv2dmask"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=816"><code>DepthwiseConv2D.mask</code></a><a id="DepthwiseConv2D.mask" /></h4>
<p>Returns the mask.</p>
<h4 id="depthwiseconv2dmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>DepthwiseConv2D.module_name</code></a><a id="DepthwiseConv2D.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="depthwiseconv2dname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>DepthwiseConv2D.name_scopes</code></a><a id="DepthwiseConv2D.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="depthwiseconv2dnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>DepthwiseConv2D.non_trainable_variables</code></a><a id="DepthwiseConv2D.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_236">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_223">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="depthwiseconv2doutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=713"><code>DepthwiseConv2D.output_channels</code></a><a id="DepthwiseConv2D.output_channels" /></h4>
<p>Returns the number of output channels.</p>
<h4 id="depthwiseconv2dpadding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=739"><code>DepthwiseConv2D.padding</code></a><a id="DepthwiseConv2D.padding" /></h4>
<p>Returns the padding algorithm used, if this is the same for all dims.</p>
<p>Use <code>.paddings</code> if you want a tuple with the padding algorithm used for each
dimension.</p>
<h5 id="returns_237">Returns:</h5>
<p>The padding algorithm used, if this is the same for all dimensions.</p>
<h5 id="raises_224">Raises:</h5>
<ul>
<li><code>ValueError</code>: If different padding algorithms are used for different
    dimensions.</li>
</ul>
<h4 id="depthwiseconv2dpaddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=762"><code>DepthwiseConv2D.paddings</code></a><a id="DepthwiseConv2D.paddings" /></h4>
<p>Returns a tuple with the padding algorithm used for each dimension.</p>
<h4 id="depthwiseconv2dpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=806"><code>DepthwiseConv2D.partitioners</code></a><a id="DepthwiseConv2D.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="depthwiseconv2drate"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=734"><code>DepthwiseConv2D.rate</code></a><a id="DepthwiseConv2D.rate" /></h4>
<p>Returns the dilation rate.</p>
<h4 id="depthwiseconv2dregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=811"><code>DepthwiseConv2D.regularizers</code></a><a id="DepthwiseConv2D.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="depthwiseconv2dscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>DepthwiseConv2D.scope_name</code></a><a id="DepthwiseConv2D.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="depthwiseconv2dstride"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=727"><code>DepthwiseConv2D.stride</code></a><a id="DepthwiseConv2D.stride" /></h4>
<p>Returns the stride.</p>
<h4 id="depthwiseconv2dtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>DepthwiseConv2D.trainable_variables</code></a><a id="DepthwiseConv2D.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_238">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_225">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="depthwiseconv2dvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>DepthwiseConv2D.variable_scope</code></a><a id="DepthwiseConv2D.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_239">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_226">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="depthwiseconv2dvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>DepthwiseConv2D.variables</code></a><a id="DepthwiseConv2D.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_240">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_227">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="depthwiseconv2dw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=772"><code>DepthwiseConv2D.w</code></a><a id="DepthwiseConv2D.w" /></h4>
<p>Returns the Variable containing the weight matrix.</p>
<h3 id="class-differentgrapherror"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base_errors.py?q=class:DifferentGraphError"><code>class DifferentGraphError</code></a><a id="DifferentGraphError" /></h3>
<p>Error raised when trying to connect a Sonnet module to multiple Graphs.</p>
<h3 id="class-embed"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/embed.py?q=class:Embed"><code>class Embed</code></a><a id="Embed" /></h3>
<p>Module for embedding tokens in a low-dimensional space.</p>
<h4 id="embed__init__vocab_sizenone-embed_dimnone-existing_vocabnone-densify_gradientsfalse-initializersnone-partitionersnone-regularizersnone-trainabletrue-custom_getternone-nameembed"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/embed.py?l=53"><code>Embed.__init__(vocab_size=None, embed_dim=None, existing_vocab=None, densify_gradients=False, initializers=None, partitioners=None, regularizers=None, trainable=True, custom_getter=None, name='embed')</code></a><a id="Embed.__init__" /></h4>
<p>Constructs an Embed module.</p>
<h5 id="args_117">Args:</h5>
<ul>
<li><code>vocab_size</code>: int. Number of unique tokens to embed. If not provided, an
    existing vocabulary matrix from which vocab_size can be inferred must
    be provided as existing_vocab.</li>
<li><code>embed_dim</code>: int or None. Number of dimensions to assign to each embedding.
    If not specified, a sensible default is chosen based on <code>vocab_size</code>. If
    an existing vocabulary matrix initializes the module, this should not be
    provided as it will be inferred.</li>
<li><code>existing_vocab</code>: a [vocab_size, embed_dim] vocabulary matrix. Will be
    converted to a tf.float32 tensor. If provided, neither or vocab_size or
    embed_dim should be provided as they are inferred.</li>
<li><code>densify_gradients</code>: if True, we convert the embedding gradient from an
    indexed-slices to a regular tensor before sending it back to the
    parameter server. This avoids excess computation on the parameter
    server. Use this option for moderately sized embeddings, e.g.,
    a vocabulary size on the order of up to thousands. For embeddings larger
    than these, e.g. a vocabulary size on the order of tens or hundreds of
    thousands, set this to False.</li>
<li><code>initializers</code>: Optional dict containing initializers for embeddings (with
    key 'embeddings'). As a default, embeddings are initialized via a
    truncated normal distribution.</li>
<li><code>partitioners</code>: Optional dict containing partitioners for embeddings (with
    key 'embeddings'). As a default, no partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for embeddings (with
    key 'embeddings'). As a default, no regularizers are used. A regularizer
    should be a function that takes a single <code>Tensor</code> as an input and
    returns a scalar <code>Tensor</code> output, e.g. the L1 and L2 regularizers
    in <code>tf.contrib.layers</code>.</li>
<li><code>trainable</code>: if True, the embeddings will be updated during training. If
    False, they are fixed to their initial values. If <code>trainable=False</code> and
    a regularizer is given, the resulting loss stays constant.</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the <code>tf.get_variable</code>
    documentation for information about the custom_getter API.</li>
<li><code>name</code>: string. Name for this module.</li>
</ul>
<h5 id="raises_228">Raises:</h5>
<ul>
<li><code>ValueError</code>: if neither one of vocab_size or existing_vocab is provided, or
    if existing_vocab is provided along with vocab_size, embedding_dim,
    initializers, partitioners or regularizers (as these should
    be inferred).</li>
</ul>
<h4 id="embed__call__ids"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/embed.py?l=139"><code>Embed.__call__(ids)</code></a><a id="Embed.__call__" /></h4>
<p>Lookup embeddings.</p>
<p>Looks up an embedding vector for each value in <code>ids</code>. All ids must be within
[0, vocab_size), else an <code>InvalidArgumentError</code> is raised at runtime.</p>
<h5 id="args_118">Args:</h5>
<ul>
<li><code>ids</code>: Tensor of dtype int64.</li>
</ul>
<h5 id="returns_241">Returns:</h5>
<p>Tensor of tf.shape(ids) + [embedding_dim] and dtype float32.</p>
<h4 id="embedconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Embed.connected_subgraphs</code></a><a id="Embed.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="embeddefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Embed.defun()</code></a><a id="Embed.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="embeddefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Embed.defun_wrapped</code></a><a id="Embed.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="embedembed_dim"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/embed.py?l=189"><code>Embed.embed_dim</code></a><a id="Embed.embed_dim" /></h4>
<p>Size of embedding vectors.</p>
<h4 id="embedembeddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/embed.py?l=194"><code>Embed.embeddings</code></a><a id="Embed.embeddings" /></h4>
<p>Returns the Variable containing embeddings.</p>
<h5 id="returns_242">Returns:</h5>
<p>A 2D Variable containing one embedding vector per row, constructed in the
    most recent <strong>call</strong>.</p>
<h5 id="raises_229">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the
      graph yet, meaning the variables do not exist.</p>
<h4 id="embedget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Embed.get_all_variables(collection='trainable_variables')</code></a><a id="Embed.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_119">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_243">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_230">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="embedget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>Embed.get_possible_initializer_keys(cls)</code></a><a id="Embed.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_244">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="embedget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>Embed.get_variables(collection='trainable_variables')</code></a><a id="Embed.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_120">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_245">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_231">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="embedgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Embed.graph</code></a><a id="Embed.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="embedis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Embed.is_connected</code></a><a id="Embed.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="embedlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Embed.last_connected_subgraph</code></a><a id="Embed.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_246">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_232">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="embedmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Embed.module_name</code></a><a id="Embed.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="embedname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Embed.name_scopes</code></a><a id="Embed.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="embednon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Embed.non_trainable_variables</code></a><a id="Embed.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_247">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_233">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="embedscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Embed.scope_name</code></a><a id="Embed.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="embedtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Embed.trainable_variables</code></a><a id="Embed.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_248">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_234">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="embedvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Embed.variable_scope</code></a><a id="Embed.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_249">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_235">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="embedvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Embed.variables</code></a><a id="Embed.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_250">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_236">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="embedvocab_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/embed.py?l=184"><code>Embed.vocab_size</code></a><a id="Embed.vocab_size" /></h4>
<p>Size of input vocabulary.</p>
<h3 id="class-error"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base_errors.py?q=class:Error"><code>class Error</code></a><a id="Error" /></h3>
<p>Base class for all errors from snt.</p>
<p>This is thrown to indicate a Neural Network specific problem, e.g. wrong
module arity, module is not connected to the graph when it should be,
tried to wire together incompatible modules, etc.</p>
<h3 id="class-flattentrailingdimensions"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?q=class:FlattenTrailingDimensions"><code>class FlattenTrailingDimensions</code></a><a id="FlattenTrailingDimensions" /></h3>
<p>Flattens trailing dimensions of a Tensor.</p>
<h4 id="flattentrailingdimensions__init__dim_from-namebatch_dim_from"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=904"><code>FlattenTrailingDimensions.__init__(dim_from, name='batch_dim_from')</code></a><a id="FlattenTrailingDimensions.__init__" /></h4>
<p>Constructs a FlattenTrailingDimensions module.</p>
<p>For example, given an input Tensor with shape <code>[B, H, W, C]</code>:</p>
<ul>
<li><code>dim_from=1</code> will return a Tensor with shape <code>[B, H*W*C]</code>.</li>
<li><code>dim_from=2</code> will return a Tensor with shape <code>[B, H, W*C]</code>.</li>
<li><code>dim_from=3</code> will return the input itself.</li>
<li><code>dim_from=4</code> will return a Tensor with shape <code>[B, H, W, C, 1]</code>.</li>
<li><code>dim_from&gt;=5</code> will generate a ValueError when building the module.
  The preserved dimensions can be unknown at building time.</li>
</ul>
<p>Equivalent to BatchFlatten(preserve_dims=dim_from, name=name).</p>
<h5 id="args_121">Args:</h5>
<ul>
<li><code>dim_from</code>: All dimensions after and including <code>dim_from</code> will
      be flattened into a single dimension.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_237">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>dim_from &lt;= 0</code>.</li>
</ul>
<h4 id="flattentrailingdimensions__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=779"><code>FlattenTrailingDimensions.__call__(inputs)</code></a><a id="FlattenTrailingDimensions.__call__" /></h4>
<p>Connects the module into the graph, with input Tensor <code>inputs</code>.</p>
<h5 id="args_122">Args:</h5>
<ul>
<li><code>inputs</code>: A Tensor of shape [b_1, b_2, ..., b_preserve_dims,
                             b_preserve_dims+1, ...].</li>
</ul>
<h5 id="returns_251">Returns:</h5>
<p>A Tensor of shape [b_1, b_2, ..., b_preserve_dims,
                     b_reshape_1, b_reshape_2, ...],
    with reshaping defined by the constructor <code>shape</code> parameter.</p>
<h5 id="raises_238">Raises:</h5>
<ul>
<li><code>ValueError</code>: If output shape is incompatible with input shape; or if
      shape array contains non numeric entries; or if shape array contains
      more than 1 wildcard -1; or if the input array contains unknown,
      non-preserved dimensions (except when the unknown dimension is the
      only non-preserved dimension and doesn't actually need reshaping).</li>
</ul>
<h4 id="flattentrailingdimensionsconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>FlattenTrailingDimensions.connected_subgraphs</code></a><a id="FlattenTrailingDimensions.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="flattentrailingdimensionsdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>FlattenTrailingDimensions.defun()</code></a><a id="FlattenTrailingDimensions.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="flattentrailingdimensionsdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>FlattenTrailingDimensions.defun_wrapped</code></a><a id="FlattenTrailingDimensions.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="flattentrailingdimensionsget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>FlattenTrailingDimensions.get_all_variables(collection='trainable_variables')</code></a><a id="FlattenTrailingDimensions.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_123">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_252">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_239">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="flattentrailingdimensionsget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>FlattenTrailingDimensions.get_possible_initializer_keys(cls)</code></a><a id="FlattenTrailingDimensions.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_253">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="flattentrailingdimensionsget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>FlattenTrailingDimensions.get_variables(collection='trainable_variables')</code></a><a id="FlattenTrailingDimensions.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_124">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_254">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_240">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="flattentrailingdimensionsgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>FlattenTrailingDimensions.graph</code></a><a id="FlattenTrailingDimensions.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="flattentrailingdimensionsinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=862"><code>FlattenTrailingDimensions.input_shape</code></a><a id="FlattenTrailingDimensions.input_shape" /></h4>
<h4 id="flattentrailingdimensionsis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>FlattenTrailingDimensions.is_connected</code></a><a id="FlattenTrailingDimensions.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="flattentrailingdimensionslast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>FlattenTrailingDimensions.last_connected_subgraph</code></a><a id="FlattenTrailingDimensions.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_255">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_241">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="flattentrailingdimensionsmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>FlattenTrailingDimensions.module_name</code></a><a id="FlattenTrailingDimensions.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="flattentrailingdimensionsname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>FlattenTrailingDimensions.name_scopes</code></a><a id="FlattenTrailingDimensions.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="flattentrailingdimensionsnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>FlattenTrailingDimensions.non_trainable_variables</code></a><a id="FlattenTrailingDimensions.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_256">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_242">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="flattentrailingdimensionsscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>FlattenTrailingDimensions.scope_name</code></a><a id="FlattenTrailingDimensions.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="flattentrailingdimensionstrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>FlattenTrailingDimensions.trainable_variables</code></a><a id="FlattenTrailingDimensions.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_257">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_243">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="flattentrailingdimensionstransposenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=868"><code>FlattenTrailingDimensions.transpose(name=None)</code></a><a id="FlattenTrailingDimensions.transpose" /></h4>
<p>Returns transpose batch reshape.</p>
<h4 id="flattentrailingdimensionsvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>FlattenTrailingDimensions.variable_scope</code></a><a id="FlattenTrailingDimensions.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_258">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_244">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="flattentrailingdimensionsvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>FlattenTrailingDimensions.variables</code></a><a id="FlattenTrailingDimensions.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_259">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_245">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-gru"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?q=class:GRU"><code>class GRU</code></a><a id="GRU" /></h3>
<p>GRU recurrent network cell.</p>
<p>The implementation is based on: https://arxiv.org/pdf/1412.3555v1.pdf.</p>
<p>Attributes:
  state_size: Integer indicating the size of state tensor.
  output_size: Integer indicating the size of the core output.</p>
<h4 id="gru__init__hidden_size-initializersnone-partitionersnone-regularizersnone-custom_getternone-namegru"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1450"><code>GRU.__init__(hidden_size, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='gru')</code></a><a id="GRU.__init__" /></h4>
<p>Construct GRU.</p>
<h5 id="args_125">Args:</h5>
<ul>
<li><code>hidden_size</code>: (int) Hidden size dimensionality.</li>
<li><code>initializers</code>: Dict containing ops to initialize the weights. This
    dict may contain any of the keys returned by
    <code>GRU.get_possible_initializer_keys</code>.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
    the weights and biases. As a default, no partitioners are used. This
    dict may contain any of the keys returned by
    <code>GRU.get_possible_initializer_keys</code></li>
<li><code>regularizers</code>: Optional dict containing regularizers for the weights and
    biases. As a default, no regularizers are used. This
    dict may contain any of the keys returned by
    <code>GRU.get_possible_initializer_keys</code></li>
<li><code>custom_getter</code>: Callable that takes as a first argument the true getter,
    and allows overwriting the internal get_variable method. See the
    <code>tf.get_variable</code> documentation for more details.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_246">Raises:</h5>
<ul>
<li><code>KeyError</code>: if <code>initializers</code> contains any keys not returned by
    <code>GRU.get_possible_initializer_keys</code>.</li>
<li><code>KeyError</code>: if <code>partitioners</code> contains any keys not returned by
    <code>GRU.get_possible_initializer_keys</code>.</li>
<li><code>KeyError</code>: if <code>regularizers</code> contains any keys not returned by
    <code>GRU.get_possible_initializer_keys</code>.</li>
</ul>
<h4 id="gru__call__inputs-prev_state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1510"><code>GRU.__call__(inputs, prev_state)</code></a><a id="GRU.__call__" /></h4>
<p>Connects the GRU module into the graph.</p>
<p>If this is not the first time the module has been connected to the graph,
the Tensors provided as inputs and state must have the same final
dimension, in order for the existing variables to be the correct size for
their corresponding multiplications. The batch size may differ for each
connection.</p>
<h5 id="args_126">Args:</h5>
<ul>
<li><code>inputs</code>: Tensor of size <code>[batch_size, input_size]</code>.</li>
<li><code>prev_state</code>: Tensor of size <code>[batch_size, hidden_size]</code>.</li>
</ul>
<h5 id="returns_260">Returns:</h5>
<p>A tuple (output, next_state) where <code>output</code> is a Tensor of size
  <code>[batch_size, hidden_size]</code> and <code>next_state</code> is a Tensor of size
  <code>[batch_size, hidden_size]</code>.</p>
<h5 id="raises_247">Raises:</h5>
<ul>
<li><code>ValueError</code>: If connecting the module into the graph any time after the
    first time, and the inferred size of the inputs does not match previous
    invocations.</li>
</ul>
<h4 id="gruconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>GRU.connected_subgraphs</code></a><a id="GRU.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="grudefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>GRU.defun()</code></a><a id="GRU.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="grudefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>GRU.defun_wrapped</code></a><a id="GRU.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="gruget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>GRU.get_all_variables(collection='trainable_variables')</code></a><a id="GRU.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_127">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_261">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_248">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="gruget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1489"><code>GRU.get_possible_initializer_keys(cls)</code></a><a id="GRU.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<h5 id="the-set-of-all-possible-initializer-keys-are_1">The set of all possible initializer keys are:</h5>
<ul>
<li><code>wz</code>: weight for input -&gt; update cell</li>
<li><code>uz</code>: weight for prev_state -&gt; update cell</li>
<li><code>bz</code>: bias for update_cell</li>
<li><code>wr</code>: weight for input -&gt; reset cell</li>
<li><code>ur</code>: weight for prev_state -&gt; reset cell</li>
<li><code>br</code>: bias for reset cell</li>
<li><code>wh</code>: weight for input -&gt; candidate activation</li>
<li><code>uh</code>: weight for prev_state -&gt; candidate activation</li>
<li><code>bh</code>: bias for candidate activation</li>
</ul>
<h5 id="returns_262">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
    constructor.</p>
<h4 id="gruget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>GRU.get_variables(collection='trainable_variables')</code></a><a id="GRU.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_128">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_263">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_249">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="grugraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>GRU.graph</code></a><a id="GRU.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="gruinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=203"><code>GRU.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)</code></a><a id="GRU.initial_state" /></h4>
<p>Builds the default start state for an RNNCore.</p>
<h5 id="args_129">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, or scalar int32 Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.
    Note that intializers and regularizers will be ignored if
    <code>trainable=False</code>.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. As a
    default, no regularizers are used. A regularizer should be a function
    that takes a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code>
    output, e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_264">Returns:</h5>
<p>A tensor or nested tuple of tensors with same structure and shape as the
  <code>state_size</code> property of the core.</p>
<h5 id="raises_250">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the user passes initializers that are not functions.</li>
<li><code>ValueError</code>: if the user passes regularizers that are not functions.</li>
</ul>
<h4 id="gruis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>GRU.is_connected</code></a><a id="GRU.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="grulast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>GRU.last_connected_subgraph</code></a><a id="GRU.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_265">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_251">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="grumodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>GRU.module_name</code></a><a id="GRU.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="gruname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>GRU.name_scopes</code></a><a id="GRU.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="grunon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>GRU.non_trainable_variables</code></a><a id="GRU.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_266">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_252">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="gruoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1590"><code>GRU.output_size</code></a><a id="GRU.output_size" /></h4>
<h4 id="gruscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>GRU.scope_name</code></a><a id="GRU.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="grustate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1586"><code>GRU.state_size</code></a><a id="GRU.state_size" /></h4>
<h4 id="grutrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>GRU.trainable_variables</code></a><a id="GRU.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_267">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_253">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="gruvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>GRU.variable_scope</code></a><a id="GRU.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_268">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_254">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="gruvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>GRU.variables</code></a><a id="GRU.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_269">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_255">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="gruzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>GRU.zero_state(batch_size, dtype)</code></a><a id="GRU.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_130">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_270">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-gridwarper"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?q=class:GridWarper"><code>class GridWarper</code></a><a id="GridWarper" /></h3>
<p>Grid warper interface class.</p>
<p>An object implementing the <code>GridWarper</code> interface generates a reference grid
of feature points at construction time, and warps it via a parametric
transformation model, specified at run time by an input parameter Tensor.
Grid warpers must then implement a <code>create_features</code> function used to generate
the reference grid to be warped in the forward pass (according to a determined
warping model).</p>
<h4 id="gridwarper__init__source_shape-output_shape-num_coeff-name-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=43"><code>GridWarper.__init__(source_shape, output_shape, num_coeff, name, **kwargs)</code></a><a id="GridWarper.__init__" /></h4>
<p>Constructs a GridWarper module and initializes the source grid params.</p>
<p><code>source_shape</code> and <code>output_shape</code> are used to define the size of the source
and output signal domains, as opposed to the shape of the respective
Tensors. For example, for an image of size <code>width=W</code> and <code>height=H</code>,
<code>{source,output}_shape=[H, W]</code>; for a volume of size <code>width=W</code>, <code>height=H</code>
and <code>depth=D</code>, <code>{source,output}_shape=[H, W, D]</code>.</p>
<h5 id="args_131">Args:</h5>
<ul>
<li><code>source_shape</code>: Iterable of integers determining the size of the source
    signal domain.</li>
<li><code>output_shape</code>: Iterable of integers determining the size of the destination
    resampled signal domain.</li>
<li><code>num_coeff</code>: Number of coefficients parametrizing the grid warp.
    For example, a 2D affine transformation will be defined by the 6
    parameters populating the corresponding 2x3 affine matrix.</li>
<li><code>name</code>: Name of Module.</li>
<li><code>**kwargs</code>: Extra kwargs to be forwarded to the <code>create_features</code> function,
    instantiating the source grid parameters.</li>
</ul>
<h5 id="raises_256">Raises:</h5>
<ul>
<li><code>Error</code>: If <code>len(output_shape) &gt; len(source_shape)</code>.</li>
<li><code>TypeError</code>: If <code>output_shape</code> and <code>source_shape</code> are not both iterable.</li>
</ul>
<h4 id="gridwarper__call__args-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=294"><code>GridWarper.__call__(*args, **kwargs)</code></a><a id="GridWarper.__call__" /></h4>
<p>Add elements to the Graph, computing output Tensors from input Tensors.</p>
<p>Subclasses must implement this method, which will be wrapped in a Template.</p>
<h5 id="args_132">Args:</h5>
<ul>
<li><code>*args</code>: Input Tensors.</li>
<li><code>**kwargs</code>: Additional Python flags controlling connection.</li>
</ul>
<h5 id="returns_271">Returns:</h5>
<p>output Tensor(s).</p>
<h4 id="gridwarperconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>GridWarper.connected_subgraphs</code></a><a id="GridWarper.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="gridwarperdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>GridWarper.defun()</code></a><a id="GridWarper.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="gridwarperdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>GridWarper.defun_wrapped</code></a><a id="GridWarper.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="gridwarperget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>GridWarper.get_all_variables(collection='trainable_variables')</code></a><a id="GridWarper.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_133">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_272">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_257">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="gridwarperget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>GridWarper.get_possible_initializer_keys(cls)</code></a><a id="GridWarper.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_273">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="gridwarperget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>GridWarper.get_variables(collection='trainable_variables')</code></a><a id="GridWarper.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_134">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_274">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_258">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="gridwarpergraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>GridWarper.graph</code></a><a id="GridWarper.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="gridwarperis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>GridWarper.is_connected</code></a><a id="GridWarper.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="gridwarperlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>GridWarper.last_connected_subgraph</code></a><a id="GridWarper.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_275">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_259">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="gridwarpermodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>GridWarper.module_name</code></a><a id="GridWarper.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="gridwarpern_coeff"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=86"><code>GridWarper.n_coeff</code></a><a id="GridWarper.n_coeff" /></h4>
<p>Returns number of coefficients of warping function.</p>
<h4 id="gridwarpername_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>GridWarper.name_scopes</code></a><a id="GridWarper.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="gridwarpernon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>GridWarper.non_trainable_variables</code></a><a id="GridWarper.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_276">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_260">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="gridwarperoutput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=101"><code>GridWarper.output_shape</code></a><a id="GridWarper.output_shape" /></h4>
<p>Returns a tuple containing the shape of the output grid.</p>
<h4 id="gridwarperpsi"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=91"><code>GridWarper.psi</code></a><a id="GridWarper.psi" /></h4>
<p>Returns a list of features used to compute the grid warp.</p>
<h4 id="gridwarperscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>GridWarper.scope_name</code></a><a id="GridWarper.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="gridwarpersource_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/spatial_transformer.py?l=96"><code>GridWarper.source_shape</code></a><a id="GridWarper.source_shape" /></h4>
<p>Returns a tuple containing the shape of the source signal.</p>
<h4 id="gridwarpertrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>GridWarper.trainable_variables</code></a><a id="GridWarper.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_277">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_261">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="gridwarpervariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>GridWarper.variable_scope</code></a><a id="GridWarper.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_278">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_262">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="gridwarpervariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>GridWarper.variables</code></a><a id="GridWarper.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_279">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_263">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-highwaycore"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?q=class:HighwayCore"><code>class HighwayCore</code></a><a id="HighwayCore" /></h3>
<p>Recurrent Highway Network cell.</p>
<p>The implementation is based on: https://arxiv.org/pdf/1607.03474v5.pdf
As per the first lines of section 5 of the reference paper, 1 - T is
used instead of a dedicated C gate.</p>
<p>Attributes:
  state_size: Integer indicating the size of state tensor.
  output_size: Integer indicating the size of the core output.</p>
<h4 id="highwaycore__init__hidden_size-num_layers-initializersnone-partitionersnone-regularizersnone-custom_getternone-namehighwaycore"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1613"><code>HighwayCore.__init__(hidden_size, num_layers, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='highwaycore')</code></a><a id="HighwayCore.__init__" /></h4>
<p>Construct a new Recurrent Highway core.</p>
<h5 id="args_135">Args:</h5>
<ul>
<li><code>hidden_size</code>: (int) Hidden size dimensionality.</li>
<li><code>num_layers</code>: (int) Number of highway layers.</li>
<li><code>initializers</code>: Dict containing ops to initialize the weights. This
    dict may contain any of the keys returned by
    <code>HighwayCore.get_possible_initializer_keys</code>.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
    the weights and biases. As a default, no partitioners are used. This
    dict may contain any of the keys returned by
    <code>HighwayCore.get_possible_initializer_keys</code>.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the weights and
    biases. As a default, no regularizers are used. This
    dict may contain any of the keys returned by
    <code>HighwayCore.get_possible_initializer_keys</code>.</li>
<li><code>custom_getter</code>: Callable that takes as a first argument the true getter,
    and allows overwriting the internal get_variable method. See the
    <code>tf.get_variable</code> documentation for more details.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_264">Raises:</h5>
<ul>
<li><code>KeyError</code>: if <code>initializers</code> contains any keys not returned by
    <code>HighwayCore.get_possible_initializer_keys</code>.</li>
<li><code>KeyError</code>: if <code>partitioners</code> contains any keys not returned by
    <code>HighwayCore.get_possible_initializer_keys</code>.</li>
<li><code>KeyError</code>: if <code>regularizers</code> contains any keys not returned by
    <code>HighwayCore.get_possible_initializer_keys</code>.</li>
</ul>
<h4 id="highwaycore__call__inputs-prev_state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1689"><code>HighwayCore.__call__(inputs, prev_state)</code></a><a id="HighwayCore.__call__" /></h4>
<p>Connects the highway core module into the graph.</p>
<h5 id="args_136">Args:</h5>
<ul>
<li><code>inputs</code>: Tensor of size <code>[batch_size, input_size]</code>.</li>
<li><code>prev_state</code>: Tensor of size <code>[batch_size, hidden_size]</code>.</li>
</ul>
<h5 id="returns_280">Returns:</h5>
<p>A tuple (output, next_state) where <code>output</code> is a Tensor of size
  <code>[batch_size, hidden_size]</code> and <code>next_state</code> is a Tensor of size
  <code>[batch_size, hidden_size]</code>.</p>
<h5 id="raises_265">Raises:</h5>
<ul>
<li><code>ValueError</code>: If connecting the module into the graph any time after the
    first time, and the inferred size of the inputs does not match previous
    invocations.</li>
</ul>
<h4 id="highwaycoreconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>HighwayCore.connected_subgraphs</code></a><a id="HighwayCore.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="highwaycoredefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>HighwayCore.defun()</code></a><a id="HighwayCore.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="highwaycoredefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>HighwayCore.defun_wrapped</code></a><a id="HighwayCore.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="highwaycoreget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>HighwayCore.get_all_variables(collection='trainable_variables')</code></a><a id="HighwayCore.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_137">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_281">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_266">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="highwaycoreget_possible_initializer_keyscls-num_layers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1661"><code>HighwayCore.get_possible_initializer_keys(cls, num_layers)</code></a><a id="HighwayCore.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<h5 id="the-set-of-all-possible-initializer-keys-are_2">The set of all possible initializer keys are:</h5>
<ul>
<li><code>wt</code>: weight for input -&gt; T gate</li>
<li><code>wh</code>: weight for input -&gt; H gate</li>
<li><code>wtL</code>: weight for prev state -&gt; T gate for layer L (indexed from 0)</li>
<li><code>whL</code>: weight for prev state -&gt; H gate for layer L (indexed from 0)</li>
<li><code>btL</code>: bias for prev state -&gt; T gate for layer L (indexed from 0)</li>
<li><code>bhL</code>: bias for prev state -&gt; H gate for layer L (indexed from 0)</li>
</ul>
<h5 id="args_138">Args:</h5>
<ul>
<li><code>num_layers</code>: (int) Number of highway layers.</li>
</ul>
<h5 id="returns_282">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
    constructor.</p>
<h4 id="highwaycoreget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>HighwayCore.get_variables(collection='trainable_variables')</code></a><a id="HighwayCore.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_139">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_283">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_267">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="highwaycoregraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>HighwayCore.graph</code></a><a id="HighwayCore.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="highwaycoreinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=203"><code>HighwayCore.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)</code></a><a id="HighwayCore.initial_state" /></h4>
<p>Builds the default start state for an RNNCore.</p>
<h5 id="args_140">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, or scalar int32 Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.
    Note that intializers and regularizers will be ignored if
    <code>trainable=False</code>.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. As a
    default, no regularizers are used. A regularizer should be a function
    that takes a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code>
    output, e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_284">Returns:</h5>
<p>A tensor or nested tuple of tensors with same structure and shape as the
  <code>state_size</code> property of the core.</p>
<h5 id="raises_268">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the user passes initializers that are not functions.</li>
<li><code>ValueError</code>: if the user passes regularizers that are not functions.</li>
</ul>
<h4 id="highwaycoreis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>HighwayCore.is_connected</code></a><a id="HighwayCore.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="highwaycorelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>HighwayCore.last_connected_subgraph</code></a><a id="HighwayCore.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_285">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_269">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="highwaycoremodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>HighwayCore.module_name</code></a><a id="HighwayCore.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="highwaycorename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>HighwayCore.name_scopes</code></a><a id="HighwayCore.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="highwaycorenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>HighwayCore.non_trainable_variables</code></a><a id="HighwayCore.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_286">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_270">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="highwaycoreoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1744"><code>HighwayCore.output_size</code></a><a id="HighwayCore.output_size" /></h4>
<h4 id="highwaycorescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>HighwayCore.scope_name</code></a><a id="HighwayCore.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="highwaycorestate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1740"><code>HighwayCore.state_size</code></a><a id="HighwayCore.state_size" /></h4>
<h4 id="highwaycoretrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>HighwayCore.trainable_variables</code></a><a id="HighwayCore.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_287">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_271">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="highwaycorevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>HighwayCore.variable_scope</code></a><a id="HighwayCore.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_288">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_272">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="highwaycorevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>HighwayCore.variables</code></a><a id="HighwayCore.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_289">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_273">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="highwaycorezero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>HighwayCore.zero_state(batch_size, dtype)</code></a><a id="HighwayCore.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_141">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_290">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-inplaneconv2d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?q=class:InPlaneConv2D"><code>class InPlaneConv2D</code></a><a id="InPlaneConv2D" /></h3>
<p>Applies an in-plane convolution to each channel with tied filter weights.</p>
<p>This acts as a light wrapper around the TensorFlow op
<code>tf.nn.depthwise_conv2d</code>; it differs from the DepthWiseConv2D module in that
it has tied weights (i.e. the same filter) for all the in-out channel pairs.</p>
<h4 id="inplaneconv2d__init__kernel_shape-stride1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatnhwc-custom_getternone-namein_plane_conv2d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2191"><code>InPlaneConv2D.__init__(kernel_shape, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NHWC', custom_getter=None, name='in_plane_conv2d')</code></a><a id="InPlaneConv2D.__init__" /></h4>
<p>Constructs an InPlaneConv2D module.</p>
<p>See the following documentation for an explanation of VALID versus SAME
padding modes:
https://www.tensorflow.org/api_guides/python/nn#Convolution</p>
<h5 id="args_142">Args:</h5>
<ul>
<li><code>kernel_shape</code>: Iterable with 2 elements in the layout [filter_height,
      filter_width]; or integer that is used to define the list in all
      dimensions.</li>
<li><code>stride</code>: Iterable with 2 or 4 elements of kernel strides, or integer that
      is used to define stride in all dimensions.</li>
<li><code>padding</code>: Padding algorithm. Either <code>snt.SAME</code>, <code>snt.VALID</code>, <code>snt.FULL</code>,
      <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code>, or a sequence of these paddings
      of length 2.<ul>
<li>snt.SAME and snt.VALID are explained in the Tensorflow docs at
    https://www.tensorflow.org/api_guides/python/nn#Convolution.</li>
<li>snt.FULL pre- and post-pads with the maximum padding which does not
    result in a convolution over just padded elements.</li>
<li>snt.CAUSAL pre-pads to ensure that each output value only depends on
    input values at the same or preceding indices ("no dependence on the
    future").</li>
<li>snt.REVERSE_CAUSAL post-pads to ensure that each output value only
    depends on input values at the same or <em>greater</em> indices ("no
    dependence on the past").
  If you use the same padding for all dimensions, and it is one of SAME
  or VALID, then this is supported directly by the underlying
  convolution op. In all other cases, the input data will be padded
  using tf.pad before calling the convolution op.</li>
</ul>
</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      key 'w') or biases (with key 'b').</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition the
      filters (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
      (with key 'w') and the biases (with key 'b'). As a default, no
      regularizers are used. A regularizer should be a function that takes
      a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output,
      e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>data_format</code>: A string. Specifies whether the channel dimension
      of the input and output is the last dimension (default, NHWC), or the
      second dimension (NCHW).</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
      custom getters inside the module. If a dictionary, the keys
      correspond to regexes to match variable names. See the
      <code>tf.get_variable</code> documentation for information about the
      custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_274">Raises:</h5>
<ul>
<li>
<p><code>ValueError</code>: If the given data_format is not a supported format (see
      <code>SUPPORTED_2D_DATA_FORMATS</code>).
  base.IncompatibleShapeError: If the given kernel shape is not an integer;
      or if the given kernel shape is not a sequence of two integers.
  base.IncompatibleShapeError: If the given stride is not an integer; or if
      the given stride is not a sequence of two integers.</p>
</li>
<li>
<p><code>ValueError</code>: If the given padding is not <code>snt.VALID</code>, <code>snt.SAME</code>,
      <code>snt.FULL</code>, <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code> or a sequence of these.</p>
</li>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
      keys other than 'w' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
      are not callable.</li>
<li><code>ValueError</code>: If the passed-in data_format doesn't have a channel dimension.</li>
</ul>
<h4 id="inplaneconv2d__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=521"><code>InPlaneConv2D.__call__(inputs)</code></a><a id="InPlaneConv2D.__call__" /></h4>
<p>Connects the _ConvND module into the graph, with input Tensor <code>inputs</code>.</p>
<p>If this is not the first time the module has been connected to the graph,
the input Tensor provided here must have the same number of channels, in
order for the existing variables to be the correct size for the
multiplication; the batch size and input spatial dimensions may differ for
each connection.</p>
<h5 id="args_143">Args:</h5>
<ul>
<li><code>inputs</code>: A ND Tensor of the same rank as <code>data_format</code>, and either of types
  <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</li>
</ul>
<h5 id="returns_291">Returns:</h5>
<p>A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ...,
      output_channels].</p>
<h5 id="raises_275">Raises:</h5>
<ul>
<li>
<p><code>ValueError</code>: If connecting the module into the graph any time after the
      first time and the inferred size of the input does not match previous
      invocations.
  base.IncompatibleShapeError: If the input tensor has the wrong number
      of dimensions.
  base.UnderspecifiedError: If the channel dimension of <code>inputs</code> isn't
      defined.
  base.IncompatibleShapeError: If a mask is present and its shape is
      incompatible with the shape of the weights.</p>
</li>
<li>
<p><code>TypeError</code>: If input Tensor dtype is not compatible with either
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</p>
</li>
</ul>
<h4 id="inplaneconv2db"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=778"><code>InPlaneConv2D.b</code></a><a id="InPlaneConv2D.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_292">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_276">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the graph
      yet, meaning the variables do not exist.</p>
<ul>
<li><code>AttributeError</code>: If the module does not use bias.</li>
</ul>
<h4 id="inplaneconv2dclonenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=840"><code>InPlaneConv2D.clone(name=None)</code></a><a id="InPlaneConv2D.clone" /></h4>
<p>Returns a cloned <code>_ConvND</code> module.</p>
<h5 id="args_144">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of cloned module. The default name
    is constructed by appending "_clone" to <code>self.module_name</code>.</li>
</ul>
<h5 id="returns_293">Returns:</h5>
<p>A copy of the current class.</p>
<h4 id="inplaneconv2dconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>InPlaneConv2D.connected_subgraphs</code></a><a id="InPlaneConv2D.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="inplaneconv2dconv_op_padding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=767"><code>InPlaneConv2D.conv_op_padding</code></a><a id="InPlaneConv2D.conv_op_padding" /></h4>
<p>Returns the padding algorithm used for the underlying convolution op.</p>
<h4 id="inplaneconv2ddata_format"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=821"><code>InPlaneConv2D.data_format</code></a><a id="InPlaneConv2D.data_format" /></h4>
<p>Returns the data format.</p>
<h4 id="inplaneconv2ddefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>InPlaneConv2D.defun()</code></a><a id="InPlaneConv2D.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="inplaneconv2ddefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>InPlaneConv2D.defun_wrapped</code></a><a id="InPlaneConv2D.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="inplaneconv2dget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>InPlaneConv2D.get_all_variables(collection='trainable_variables')</code></a><a id="InPlaneConv2D.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_145">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_294">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_277">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="inplaneconv2dget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=517"><code>InPlaneConv2D.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="InPlaneConv2D.get_possible_initializer_keys" /></h4>
<h4 id="inplaneconv2dget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>InPlaneConv2D.get_variables(collection='trainable_variables')</code></a><a id="InPlaneConv2D.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_146">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_295">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_278">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="inplaneconv2dgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>InPlaneConv2D.graph</code></a><a id="InPlaneConv2D.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="inplaneconv2dhas_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=796"><code>InPlaneConv2D.has_bias</code></a><a id="InPlaneConv2D.has_bias" /></h4>
<p>Returns <code>True</code> if bias Variable is present in the module.</p>
<h4 id="inplaneconv2dinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=801"><code>InPlaneConv2D.initializers</code></a><a id="InPlaneConv2D.initializers" /></h4>
<p>Returns the initializers dictionary.</p>
<h4 id="inplaneconv2dinput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=833"><code>InPlaneConv2D.input_channels</code></a><a id="InPlaneConv2D.input_channels" /></h4>
<p>Returns the number of input channels.</p>
<h4 id="inplaneconv2dinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=827"><code>InPlaneConv2D.input_shape</code></a><a id="InPlaneConv2D.input_shape" /></h4>
<p>Returns the input shape.</p>
<h4 id="inplaneconv2dis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>InPlaneConv2D.is_connected</code></a><a id="InPlaneConv2D.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="inplaneconv2dkernel_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=722"><code>InPlaneConv2D.kernel_shape</code></a><a id="InPlaneConv2D.kernel_shape" /></h4>
<p>Returns the kernel shape.</p>
<h4 id="inplaneconv2dlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>InPlaneConv2D.last_connected_subgraph</code></a><a id="InPlaneConv2D.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_296">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_279">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="inplaneconv2dmask"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=816"><code>InPlaneConv2D.mask</code></a><a id="InPlaneConv2D.mask" /></h4>
<p>Returns the mask.</p>
<h4 id="inplaneconv2dmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>InPlaneConv2D.module_name</code></a><a id="InPlaneConv2D.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="inplaneconv2dname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>InPlaneConv2D.name_scopes</code></a><a id="InPlaneConv2D.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="inplaneconv2dnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>InPlaneConv2D.non_trainable_variables</code></a><a id="InPlaneConv2D.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_297">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_280">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="inplaneconv2doutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=713"><code>InPlaneConv2D.output_channels</code></a><a id="InPlaneConv2D.output_channels" /></h4>
<p>Returns the number of output channels.</p>
<h4 id="inplaneconv2dpadding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=739"><code>InPlaneConv2D.padding</code></a><a id="InPlaneConv2D.padding" /></h4>
<p>Returns the padding algorithm used, if this is the same for all dims.</p>
<p>Use <code>.paddings</code> if you want a tuple with the padding algorithm used for each
dimension.</p>
<h5 id="returns_298">Returns:</h5>
<p>The padding algorithm used, if this is the same for all dimensions.</p>
<h5 id="raises_281">Raises:</h5>
<ul>
<li><code>ValueError</code>: If different padding algorithms are used for different
    dimensions.</li>
</ul>
<h4 id="inplaneconv2dpaddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=762"><code>InPlaneConv2D.paddings</code></a><a id="InPlaneConv2D.paddings" /></h4>
<p>Returns a tuple with the padding algorithm used for each dimension.</p>
<h4 id="inplaneconv2dpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=806"><code>InPlaneConv2D.partitioners</code></a><a id="InPlaneConv2D.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="inplaneconv2drate"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=734"><code>InPlaneConv2D.rate</code></a><a id="InPlaneConv2D.rate" /></h4>
<p>Returns the dilation rate.</p>
<h4 id="inplaneconv2dregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=811"><code>InPlaneConv2D.regularizers</code></a><a id="InPlaneConv2D.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="inplaneconv2dscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>InPlaneConv2D.scope_name</code></a><a id="InPlaneConv2D.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="inplaneconv2dstride"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=727"><code>InPlaneConv2D.stride</code></a><a id="InPlaneConv2D.stride" /></h4>
<p>Returns the stride.</p>
<h4 id="inplaneconv2dtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>InPlaneConv2D.trainable_variables</code></a><a id="InPlaneConv2D.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_299">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_282">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="inplaneconv2dvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>InPlaneConv2D.variable_scope</code></a><a id="InPlaneConv2D.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_300">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_283">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="inplaneconv2dvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>InPlaneConv2D.variables</code></a><a id="InPlaneConv2D.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_301">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_284">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="inplaneconv2dw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=772"><code>InPlaneConv2D.w</code></a><a id="InPlaneConv2D.w" /></h4>
<p>Returns the Variable containing the weight matrix.</p>
<h3 id="class-incompatibleshapeerror"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base_errors.py?q=class:IncompatibleShapeError"><code>class IncompatibleShapeError</code></a><a id="IncompatibleShapeError" /></h3>
<p>Error raised when the shape of the input at build time is incompatible.</p>
<h3 id="class-lstm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?q=class:LSTM"><code>class LSTM</code></a><a id="LSTM" /></h3>
<p>LSTM recurrent network cell with optional peepholes &amp; layer normalization.</p>
<p>The implementation is based on: http://arxiv.org/abs/1409.2329. We add
forget_bias (default: 1) to the biases of the forget gate in order to
reduce the scale of forgetting in the beginning of the training.</p>
<h4 id="layer-normalization">Layer normalization</h4>
<p>This is described in https://arxiv.org/pdf/1607.06450.pdf</p>
<h4 id="peep-hole-connections_1">Peep-hole connections</h4>
<p>Peep-hole connections may optionally be used by specifying a flag in the
constructor. These connections can aid increasing the precision of output
timing, for more details see:</p>
<p>https://research.google.com/pubs/archive/43905.pdf</p>
<h4 id="recurrent-projections">Recurrent projections</h4>
<p>Projection of the recurrent state, to reduce model parameters and speed up
computation. For more details see:</p>
<p>https://arxiv.org/abs/1402.1128</p>
<p>Attributes:
  state_size: Tuple of <code>tf.TensorShape</code>s indicating the size of state tensors.
  output_size: <code>tf.TensorShape</code> indicating the size of the core output.
  use_peepholes: Boolean indicating whether peephole connections are used.</p>
<h4 id="lstm__init__hidden_size-forget_bias10-initializersnone-partitionersnone-regularizersnone-use_peepholesfalse-use_layer_normfalse-hidden_clip_valuenone-projection_sizenone-cell_clip_valuenone-custom_getternone-namelstm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=99"><code>LSTM.__init__(hidden_size, forget_bias=1.0, initializers=None, partitioners=None, regularizers=None, use_peepholes=False, use_layer_norm=False, hidden_clip_value=None, projection_size=None, cell_clip_value=None, custom_getter=None, name='lstm')</code></a><a id="LSTM.__init__" /></h4>
<p>Construct LSTM.</p>
<h5 id="args_147">Args:</h5>
<ul>
<li><code>hidden_size</code>: (int) Hidden size dimensionality.</li>
<li><code>forget_bias</code>: (float) Bias for the forget activation.</li>
<li><code>initializers</code>: Dict containing ops to initialize the weights.
    This dictionary may contain any of the keys returned by
    <code>LSTM.get_possible_initializer_keys</code>.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
    the weights and biases. As a default, no partitioners are used. This
    dict may contain any of the keys returned by
    <code>LSTM.get_possible_initializer_keys</code>.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the weights and
    biases. As a default, no regularizers are used. This dict may contain
    any of the keys returned by
    <code>LSTM.get_possible_initializer_keys</code>.</li>
<li><code>use_peepholes</code>: Boolean that indicates whether peephole connections are
    used.</li>
<li><code>use_layer_norm</code>: Boolean that indicates whether to apply layer
    normalization.</li>
<li><code>hidden_clip_value</code>: Optional number; if set, then the LSTM hidden state
    vector is clipped by this value.</li>
<li><code>projection_size</code>: Optional number; if set, then the LSTM hidden state is
    projected to this size via a learnable projection matrix.</li>
<li><code>cell_clip_value</code>: Optional number; if set, then the LSTM cell vector is
    clipped by this value.</li>
<li><code>custom_getter</code>: Callable that takes as a first argument the true getter,
    and allows overwriting the internal get_variable method. See the
    <code>tf.get_variable</code> documentation for more details.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_285">Raises:</h5>
<ul>
<li><code>KeyError</code>: if <code>initializers</code> contains any keys not returned by
    <code>LSTM.get_possible_initializer_keys</code>.</li>
<li><code>KeyError</code>: if <code>partitioners</code> contains any keys not returned by
    <code>LSTM.get_possible_initializer_keys</code>.</li>
<li><code>KeyError</code>: if <code>regularizers</code> contains any keys not returned by
    <code>LSTM.get_possible_initializer_keys</code>.</li>
<li><code>ValueError</code>: if a peephole initializer is passed in the initializer list,
    but <code>use_peepholes</code> is False.</li>
</ul>
<h4 id="lstm__call__inputs-prev_state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=209"><code>LSTM.__call__(inputs, prev_state)</code></a><a id="LSTM.__call__" /></h4>
<p>Connects the LSTM module into the graph.</p>
<p>If this is not the first time the module has been connected to the graph,
the Tensors provided as inputs and state must have the same final
dimension, in order for the existing variables to be the correct size for
their corresponding multiplications. The batch size may differ for each
connection.</p>
<h5 id="args_148">Args:</h5>
<ul>
<li><code>inputs</code>: Tensor of size <code>[batch_size, input_size]</code>.</li>
<li><code>prev_state</code>: Tuple (prev_hidden, prev_cell).</li>
</ul>
<h5 id="returns_302">Returns:</h5>
<p>A tuple (output, next_state) where 'output' is a Tensor of size
  <code>[batch_size, hidden_size]</code> and 'next_state' is a <code>LSTMState</code> namedtuple
  (next_hidden, next_cell) where <code>next_hidden</code> and <code>next_cell</code> have size
  <code>[batch_size, hidden_size]</code>. If <code>projection_size</code> is specified, then
  <code>next_hidden</code> will have size <code>[batch_size, projection_size]</code>.</p>
<h5 id="raises_286">Raises:</h5>
<ul>
<li><code>ValueError</code>: If connecting the module into the graph any time after the
    first time, and the inferred size of the inputs does not match previous
    invocations.</li>
</ul>
<h4 id="lstmconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>LSTM.connected_subgraphs</code></a><a id="LSTM.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="lstmdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>LSTM.defun()</code></a><a id="LSTM.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="lstmdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>LSTM.defun_wrapped</code></a><a id="LSTM.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="lstmget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>LSTM.get_all_variables(collection='trainable_variables')</code></a><a id="LSTM.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_149">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_303">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_287">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmget_possible_initializer_keyscls-use_peepholesfalse-use_projectionfalse"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=177"><code>LSTM.get_possible_initializer_keys(cls, use_peepholes=False, use_projection=False)</code></a><a id="LSTM.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<h5 id="the-set-of-all-possible-initializer-keys-are_3">The set of all possible initializer keys are:</h5>
<ul>
<li><code>w_gates</code>: weight for gates</li>
<li><code>b_gates</code>: bias of gates</li>
<li><code>w_f_diag</code>: weight for prev_cell -&gt; forget gate peephole</li>
<li><code>w_i_diag</code>: weight for prev_cell -&gt; input gate peephole</li>
<li><code>w_o_diag</code>: weight for prev_cell -&gt; output gate peephole</li>
</ul>
<h5 id="args_150">Args:</h5>
<p>cls:The class.</p>
<ul>
<li><code>use_peepholes</code>: Boolean that indicates whether peephole connections are
    used.</li>
<li><code>use_projection</code>: Boolean that indicates whether a recurrent projection
    layer is used.</li>
</ul>
<h5 id="returns_304">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
    constructor.</p>
<h4 id="lstmget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>LSTM.get_variables(collection='trainable_variables')</code></a><a id="LSTM.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_151">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_305">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_288">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>LSTM.graph</code></a><a id="LSTM.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="lstminitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=203"><code>LSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)</code></a><a id="LSTM.initial_state" /></h4>
<p>Builds the default start state for an RNNCore.</p>
<h5 id="args_152">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, or scalar int32 Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.
    Note that intializers and regularizers will be ignored if
    <code>trainable=False</code>.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. As a
    default, no regularizers are used. A regularizer should be a function
    that takes a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code>
    output, e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_306">Returns:</h5>
<p>A tensor or nested tuple of tensors with same structure and shape as the
  <code>state_size</code> property of the core.</p>
<h5 id="raises_289">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the user passes initializers that are not functions.</li>
<li><code>ValueError</code>: if the user passes regularizers that are not functions.</li>
</ul>
<h4 id="lstmis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>LSTM.is_connected</code></a><a id="LSTM.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="lstmlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>LSTM.last_connected_subgraph</code></a><a id="LSTM.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_307">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_290">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>LSTM.module_name</code></a><a id="LSTM.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="lstmname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>LSTM.name_scopes</code></a><a id="LSTM.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="lstmnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>LSTM.non_trainable_variables</code></a><a id="LSTM.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_308">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_291">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=342"><code>LSTM.output_size</code></a><a id="LSTM.output_size" /></h4>
<p><code>tf.TensorShape</code> indicating the size of the core output.</p>
<h4 id="lstmscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>LSTM.scope_name</code></a><a id="LSTM.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="lstmstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=336"><code>LSTM.state_size</code></a><a id="LSTM.state_size" /></h4>
<p>Tuple of <code>tf.TensorShape</code>s indicating the size of state tensors.</p>
<h4 id="lstmtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>LSTM.trainable_variables</code></a><a id="LSTM.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_309">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_292">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmuse_layer_norm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=352"><code>LSTM.use_layer_norm</code></a><a id="LSTM.use_layer_norm" /></h4>
<p>Boolean indicating whether layer norm is enabled.</p>
<h4 id="lstmuse_peepholes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=347"><code>LSTM.use_peepholes</code></a><a id="LSTM.use_peepholes" /></h4>
<p>Boolean indicating whether peephole connections are used.</p>
<h4 id="lstmvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>LSTM.variable_scope</code></a><a id="LSTM.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_310">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_293">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>LSTM.variables</code></a><a id="LSTM.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_311">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_294">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>LSTM.zero_state(batch_size, dtype)</code></a><a id="LSTM.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_153">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_312">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-lstmblockcell"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?q=class:LSTMBlockCell"><code>class LSTMBlockCell</code></a><a id="LSTMBlockCell" /></h3>
<p>Wraps the TensorFlow LSTMBlockCell as a Sonnet RNNCore.</p>
<h4 id="lstmblockcell__init__num_units-forget_bias10-cell_clipnone-use_peepholefalse-dtypenone-reusenone-namelstm_cell"><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/lstm_ops.py?l=307"><code>LSTMBlockCell.__init__(num_units, forget_bias=1.0, cell_clip=None, use_peephole=False, dtype=None, reuse=None, name='lstm_cell')</code></a><a id="LSTMBlockCell.__init__" /></h4>
<p>Initialize the basic LSTM cell.</p>
<h5 id="args_154">Args:</h5>
<ul>
<li><code>num_units</code>: int, The number of units in the LSTM cell.</li>
<li><code>forget_bias</code>: float, The bias added to forget gates (see above).</li>
<li><code>cell_clip</code>: An optional <code>float</code>. Defaults to <code>-1</code> (no clipping).</li>
<li><code>use_peephole</code>: Whether to use peephole connections or not.</li>
<li><code>dtype</code>: the variable dtype of this layer. Default to tf.float32.</li>
<li><code>reuse</code>: (optional) boolean describing whether to reuse variables in an
    existing scope.  If not <code>True</code>, and the existing scope already has the
    given variables, an error is raised.</li>
<li><code>name</code>: String, the name of the layer. Layers with the same name will
    share weights, but to avoid mistakes we require reuse=True in such
    cases.  By default this is "lstm_cell", for variable-name compatibility
    with <code>tf.compat.v1.nn.rnn_cell.LSTMCell</code>.</li>
</ul>
<p>When restoring from CudnnLSTM-trained checkpoints, must use
  CudnnCompatibleLSTMBlockCell instead.</p>
<h4 id="lstmblockcell__call__inputs-prev_state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=373"><code>LSTMBlockCell.__call__(inputs, prev_state)</code></a><a id="LSTMBlockCell.__call__" /></h4>
<h4 id="lstmblockcellconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>LSTMBlockCell.connected_subgraphs</code></a><a id="LSTMBlockCell.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="lstmblockcelldefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>LSTMBlockCell.defun()</code></a><a id="LSTMBlockCell.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="lstmblockcelldefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>LSTMBlockCell.defun_wrapped</code></a><a id="LSTMBlockCell.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="lstmblockcellget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>LSTMBlockCell.get_all_variables(collection='trainable_variables')</code></a><a id="LSTMBlockCell.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_155">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_313">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_295">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmblockcellget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>LSTMBlockCell.get_possible_initializer_keys(cls)</code></a><a id="LSTMBlockCell.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_314">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="lstmblockcellget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>LSTMBlockCell.get_variables(collection='trainable_variables')</code></a><a id="LSTMBlockCell.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_156">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_315">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_296">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmblockcellgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>LSTMBlockCell.graph</code></a><a id="LSTMBlockCell.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="lstmblockcellinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=203"><code>LSTMBlockCell.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)</code></a><a id="LSTMBlockCell.initial_state" /></h4>
<p>Builds the default start state for an RNNCore.</p>
<h5 id="args_157">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, or scalar int32 Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.
    Note that intializers and regularizers will be ignored if
    <code>trainable=False</code>.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. As a
    default, no regularizers are used. A regularizer should be a function
    that takes a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code>
    output, e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_316">Returns:</h5>
<p>A tensor or nested tuple of tensors with same structure and shape as the
  <code>state_size</code> property of the core.</p>
<h5 id="raises_297">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the user passes initializers that are not functions.</li>
<li><code>ValueError</code>: if the user passes regularizers that are not functions.</li>
</ul>
<h4 id="lstmblockcellis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>LSTMBlockCell.is_connected</code></a><a id="LSTMBlockCell.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="lstmblockcelllast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>LSTMBlockCell.last_connected_subgraph</code></a><a id="LSTMBlockCell.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_317">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_298">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmblockcellmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>LSTMBlockCell.module_name</code></a><a id="LSTMBlockCell.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="lstmblockcellname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>LSTMBlockCell.name_scopes</code></a><a id="LSTMBlockCell.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="lstmblockcellnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>LSTMBlockCell.non_trainable_variables</code></a><a id="LSTMBlockCell.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_318">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_299">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmblockcelloutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=376"><code>LSTMBlockCell.output_size</code></a><a id="LSTMBlockCell.output_size" /></h4>
<h4 id="lstmblockcellscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>LSTMBlockCell.scope_name</code></a><a id="LSTMBlockCell.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="lstmblockcellstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=380"><code>LSTMBlockCell.state_size</code></a><a id="LSTMBlockCell.state_size" /></h4>
<h4 id="lstmblockcelltrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>LSTMBlockCell.trainable_variables</code></a><a id="LSTMBlockCell.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_319">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_300">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmblockcellvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>LSTMBlockCell.variable_scope</code></a><a id="LSTMBlockCell.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_320">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_301">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmblockcellvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>LSTMBlockCell.variables</code></a><a id="LSTMBlockCell.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_321">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_302">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lstmblockcellzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>LSTMBlockCell.zero_state(batch_size, dtype)</code></a><a id="LSTMBlockCell.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_158">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_322">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-lstmstate"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?q=class:LSTMState"><code>class LSTMState</code></a><a id="LSTMState" /></h3>
<p>LSTMState(hidden, cell)</p>
<h4 id="lstmstatecell"><code>LSTMState.cell</code><a id="LSTMState.cell" /></h4>
<p>Alias for field number 1</p>
<h4 id="lstmstatehidden"><code>LSTMState.hidden</code><a id="LSTMState.hidden" /></h4>
<p>Alias for field number 0</p>
<h3 id="class-layernorm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/layer_norm.py?q=class:LayerNorm"><code>class LayerNorm</code></a><a id="LayerNorm" /></h3>
<p>Layer normalization module.</p>
<p>Implementation based on:
https://arxiv.org/abs/1607.06450</p>
<p>This module transforms input x into:</p>
<p>outputs = gamma * (x - mu) / sigma + beta</p>
<p>where mu and sigma are respectively the mean and standard deviation of x.
Gamma and beta are trainable parameters for scaling and shifting respectively.</p>
<p>Since the axes over which normalization is perfomed is configurable, this also
subsumes instance normalization.</p>
<h4 id="layernorm__init__axisnone-offsettrue-scaletrue-eps1e-05-initializersnone-partitionersnone-regularizersnone-namelayer_norm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/layer_norm.py?l=60"><code>LayerNorm.__init__(axis=None, offset=True, scale=True, eps=1e-05, initializers=None, partitioners=None, regularizers=None, name='layer_norm')</code></a><a id="LayerNorm.__init__" /></h4>
<p>Constructs a LayerNorm module.</p>
<h5 id="args_159">Args:</h5>
<ul>
<li><code>axis</code>: Optional dimension or iterable of indices of dimensions to normalize
    and reduce over. By default <code>None</code> and all dimensions except the
    first/batch dimension are reduced over. If the input tensor represents
    an image, summing over all except the batch and channel dimensions (e.g.
    for image format NHWC, axes=[1,2]), then this module corresponds to
    Instance Normalization (https://arxiv.org/abs/1607.08022).</li>
<li><code>offset</code>: Optional boolean to specify whether or not to apply a trained
    component-wise bias after the layer normalization and scaling.</li>
<li><code>scale</code>: Optional boolean to specify whether or not to apply a trained
    component-wise scale after the layer normalization.</li>
<li><code>eps</code>: small epsilon to avoid division by zero variance. Defaults to
    1e-5 as used in the paper.</li>
<li><code>initializers</code>: Dict containing ops to initialize the scale
    (with key 'gamma') and bias (with key 'beta').</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
    the scale (with key 'gamma') and bias (with key 'beta'). As a default,
    no partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the scale (with
    key 'gamma') and bias (with key 'beta').. As a default, no regularizers
    are used.</li>
<li><code>name</code>: name of the module.</li>
</ul>
<h5 id="raises_303">Raises:</h5>
<ul>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain
    any keys other than <code>gamma</code> or <code>beta</code>.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
    are not callable.</li>
</ul>
<h4 id="layernorm__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/layer_norm.py?l=115"><code>LayerNorm.__call__(inputs)</code></a><a id="LayerNorm.__call__" /></h4>
<p>Connects the LayerNorm module into the graph.</p>
<h5 id="args_160">Args:</h5>
<ul>
<li><code>inputs</code>: a Tensor of dimensionality &gt;= 2.</li>
</ul>
<h5 id="returns_323">Returns:</h5>
<ul>
<li><code>normalized</code>: layer normalized outputs with same shape as inputs.</li>
</ul>
<h5 id="raises_304">Raises:</h5>
<p>base.NotSupportedError: If <code>inputs</code> has less than 2 dimensions.</p>
<h4 id="layernormbeta"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/layer_norm.py?l=193"><code>LayerNorm.beta</code></a><a id="LayerNorm.beta" /></h4>
<h4 id="layernormconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>LayerNorm.connected_subgraphs</code></a><a id="LayerNorm.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="layernormdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>LayerNorm.defun()</code></a><a id="LayerNorm.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="layernormdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>LayerNorm.defun_wrapped</code></a><a id="LayerNorm.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="layernormgamma"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/layer_norm.py?l=198"><code>LayerNorm.gamma</code></a><a id="LayerNorm.gamma" /></h4>
<h4 id="layernormget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>LayerNorm.get_all_variables(collection='trainable_variables')</code></a><a id="LayerNorm.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_161">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_324">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_305">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="layernormget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>LayerNorm.get_possible_initializer_keys(cls)</code></a><a id="LayerNorm.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_325">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="layernormget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>LayerNorm.get_variables(collection='trainable_variables')</code></a><a id="LayerNorm.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_162">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_326">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_306">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="layernormgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>LayerNorm.graph</code></a><a id="LayerNorm.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="layernorminitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/layer_norm.py?l=181"><code>LayerNorm.initializers</code></a><a id="LayerNorm.initializers" /></h4>
<h4 id="layernormis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>LayerNorm.is_connected</code></a><a id="LayerNorm.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="layernormlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>LayerNorm.last_connected_subgraph</code></a><a id="LayerNorm.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_327">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_307">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="layernormmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>LayerNorm.module_name</code></a><a id="LayerNorm.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="layernormname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>LayerNorm.name_scopes</code></a><a id="LayerNorm.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="layernormnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>LayerNorm.non_trainable_variables</code></a><a id="LayerNorm.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_328">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_308">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="layernormpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/layer_norm.py?l=185"><code>LayerNorm.partitioners</code></a><a id="LayerNorm.partitioners" /></h4>
<h4 id="layernormregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/layer_norm.py?l=189"><code>LayerNorm.regularizers</code></a><a id="LayerNorm.regularizers" /></h4>
<h4 id="layernormscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>LayerNorm.scope_name</code></a><a id="LayerNorm.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="layernormtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>LayerNorm.trainable_variables</code></a><a id="LayerNorm.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_329">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_309">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="layernormvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>LayerNorm.variable_scope</code></a><a id="LayerNorm.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_330">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_310">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="layernormvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>LayerNorm.variables</code></a><a id="LayerNorm.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_331">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_311">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-linear"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?q=class:Linear"><code>class Linear</code></a><a id="Linear" /></h3>
<p>Linear module, optionally including bias.</p>
<h4 id="linear__init__output_size-use_biastrue-initializersnone-partitionersnone-regularizersnone-custom_getternone-namelinear"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=137"><code>Linear.__init__(output_size, use_bias=True, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='linear')</code></a><a id="Linear.__init__" /></h4>
<p>Constructs a Linear module.</p>
<h5 id="args_163">Args:</h5>
<ul>
<li><code>output_size</code>: Output dimensionality. <code>output_size</code> can be either an integer
      or a callable. In the latter case, since the function invocation is
      deferred to graph construction time, the user must only ensure that
      output_size can be called, returning an integer, when build is called.</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing initializers to initialize the
      weights (with key 'w') or biases (with key 'b'). The default
      initializer for the weights is a truncated normal initializer, which
      is commonly used when the inputs are zero centered (see
      https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for
      the bias is a zero initializer.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
      weights (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the weights
    (with key 'w') and the biases (with key 'b'). As a default, no
    regularizers are used. A regularizer should be a function that takes
    a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output, e.g.
    the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the <code>tf.get_variable</code>
    documentation for information about the custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_312">Raises:</h5>
<ul>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contains any
    keys other than 'w' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
    are not callable.</li>
</ul>
<h4 id="linear__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=197"><code>Linear.__call__(inputs)</code></a><a id="Linear.__call__" /></h4>
<p>Connects the Linear module into the graph, with input Tensor <code>inputs</code>.</p>
<p>If this is not the first time the module has been connected to the graph,
the Tensor provided here must have the same final dimension, in order for
the existing variables to be the correct size for the multiplication. The
batch size may differ for each connection.</p>
<h5 id="args_164">Args:</h5>
<ul>
<li><code>inputs</code>: A 2D Tensor of size [batch_size, input_size].</li>
</ul>
<h5 id="returns_332">Returns:</h5>
<p>A 2D Tensor of size [batch_size, output_size].</p>
<h5 id="raises_313">Raises:</h5>
<p>base.IncompatibleShapeError: If the input is not a 2-D <code>Tensor</code> with
      the size of the second dimension specified.
  base.IncompatibleShapeError: If reconnecting an already connected module
      into the graph, and the shape of the input is not compatible with
      previous inputs.</p>
<h4 id="linearb"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=281"><code>Linear.b</code></a><a id="Linear.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_333">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_314">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the
      graph yet, meaning the variables do not exist.</p>
<ul>
<li><code>AttributeError</code>: If the module does not use bias.</li>
</ul>
<h4 id="linearclonenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=326"><code>Linear.clone(name=None)</code></a><a id="Linear.clone" /></h4>
<p>Returns a cloned <code>Linear</code> module.</p>
<h5 id="args_165">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of cloned module. The default name
      is constructed by appending "_clone" to <code>self.module_name</code>.</li>
</ul>
<h5 id="returns_334">Returns:</h5>
<p>Cloned <code>Linear</code> module.</p>
<h4 id="linearconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Linear.connected_subgraphs</code></a><a id="Linear.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="lineardefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Linear.defun()</code></a><a id="Linear.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="lineardefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Linear.defun_wrapped</code></a><a id="Linear.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="linearget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Linear.get_all_variables(collection='trainable_variables')</code></a><a id="Linear.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_166">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_335">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_315">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="linearget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=193"><code>Linear.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="Linear.get_possible_initializer_keys" /></h4>
<h4 id="linearget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>Linear.get_variables(collection='trainable_variables')</code></a><a id="Linear.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_167">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_336">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_316">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lineargraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Linear.graph</code></a><a id="Linear.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="linearhas_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=306"><code>Linear.has_bias</code></a><a id="Linear.has_bias" /></h4>
<p>Returns <code>True</code> if bias Variable is present in the module.</p>
<h4 id="linearinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=311"><code>Linear.initializers</code></a><a id="Linear.initializers" /></h4>
<p>Returns the initializers dictionary.</p>
<h4 id="linearinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=346"><code>Linear.input_shape</code></a><a id="Linear.input_shape" /></h4>
<p>Returns shape of input <code>Tensor</code> passed at last call to <code>build</code>.</p>
<h4 id="linearis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Linear.is_connected</code></a><a id="Linear.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="linearlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Linear.last_connected_subgraph</code></a><a id="Linear.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_337">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_317">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="linearmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Linear.module_name</code></a><a id="Linear.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="linearname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Linear.name_scopes</code></a><a id="Linear.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="linearnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Linear.non_trainable_variables</code></a><a id="Linear.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_338">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_318">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="linearoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=299"><code>Linear.output_size</code></a><a id="Linear.output_size" /></h4>
<p>Returns the module output size.</p>
<h4 id="linearpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=316"><code>Linear.partitioners</code></a><a id="Linear.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="linearregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=321"><code>Linear.regularizers</code></a><a id="Linear.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="linearscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Linear.scope_name</code></a><a id="Linear.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="lineartrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Linear.trainable_variables</code></a><a id="Linear.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_339">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_319">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="lineartransposenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=353"><code>Linear.transpose(name=None)</code></a><a id="Linear.transpose" /></h4>
<p>Returns transposed <code>Linear</code> module.</p>
<h5 id="args_168">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of transpose module. The default name
      is constructed by appending "_transpose" to <code>self.module_name</code>.</li>
</ul>
<h5 id="returns_340">Returns:</h5>
<p>Transposed <code>Linear</code> module.</p>
<h4 id="linearvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Linear.variable_scope</code></a><a id="Linear.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_341">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_320">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="linearvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Linear.variables</code></a><a id="Linear.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_342">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_321">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="linearw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=267"><code>Linear.w</code></a><a id="Linear.w" /></h4>
<p>Returns the Variable containing the weight matrix.</p>
<h5 id="returns_343">Returns:</h5>
<p>Variable object containing the weights, from the most recent <strong>call</strong>.</p>
<h5 id="raises_322">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the
      graph yet, meaning the variables do not exist.</p>
<h3 id="class-mergedims"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?q=class:MergeDims"><code>class MergeDims</code></a><a id="MergeDims" /></h3>
<p>Merges a tensor or nested list of tensors along a range of dimensions.</p>
<p>Tensors are reshaped by specifying the range of dimensions to merge.
Hence, the reshape can be performed without knowing in advance the rank of
the input tensor.</p>
<p>For example, merging dimensions 1, 2 and 3 together can be performed by
calling:</p>
<pre><code class="python">output = MergeDims(start=1, size=3)(x)
</code></pre>

<p>A nested list of tensors can be merged:</p>
<pre><code class="python">x = [tf.random_uniform(shape=[5, 5]), [tf.random_uniform(shape=[3, 3, 3])]]
output = MergeDims(start=0, size=2)(x)
</code></pre>

<h4 id="mergedims__init__start-size-namemerge_dims"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=1296"><code>MergeDims.__init__(start, size, name='merge_dims')</code></a><a id="MergeDims.__init__" /></h4>
<p>Constructs the MergeDims module.</p>
<h5 id="args_169">Args:</h5>
<ul>
<li><code>start</code>: Start of the range of dimensions to merge.</li>
<li><code>size</code>: Size the range of dimensions to merge.</li>
<li><code>name</code>: The name of the module.</li>
</ul>
<h5 id="raises_323">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>size</code> is not strictly greater than 1.</li>
</ul>
<h4 id="mergedims__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=1354"><code>MergeDims.__call__(inputs)</code></a><a id="MergeDims.__call__" /></h4>
<p>Connects the MergeDims module into the graph.</p>
<h5 id="args_170">Args:</h5>
<ul>
<li><code>inputs</code>: Tensor or a nested list of Tensors to merge. Its rank must be
      greater than or equal to <code>start</code> + <code>size</code>.</li>
</ul>
<h5 id="returns_344">Returns:</h5>
<p>The merged Tensor or a nested list of merged Tensors.</p>
<h5 id="raises_324">Raises:</h5>
<ul>
<li><code>ValueError</code>: If any of the <code>inputs</code> tensors has insufficient rank.</li>
</ul>
<h4 id="mergedimsconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>MergeDims.connected_subgraphs</code></a><a id="MergeDims.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="mergedimsdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>MergeDims.defun()</code></a><a id="MergeDims.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="mergedimsdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>MergeDims.defun_wrapped</code></a><a id="MergeDims.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="mergedimsget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>MergeDims.get_all_variables(collection='trainable_variables')</code></a><a id="MergeDims.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_171">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_345">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_325">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="mergedimsget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>MergeDims.get_possible_initializer_keys(cls)</code></a><a id="MergeDims.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_346">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="mergedimsget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>MergeDims.get_variables(collection='trainable_variables')</code></a><a id="MergeDims.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_172">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_347">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_326">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="mergedimsgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>MergeDims.graph</code></a><a id="MergeDims.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="mergedimsis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>MergeDims.is_connected</code></a><a id="MergeDims.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="mergedimslast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>MergeDims.last_connected_subgraph</code></a><a id="MergeDims.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_348">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_327">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="mergedimsmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>MergeDims.module_name</code></a><a id="MergeDims.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="mergedimsname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>MergeDims.name_scopes</code></a><a id="MergeDims.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="mergedimsnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>MergeDims.non_trainable_variables</code></a><a id="MergeDims.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_349">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_328">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="mergedimsscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>MergeDims.scope_name</code></a><a id="MergeDims.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="mergedimstrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>MergeDims.trainable_variables</code></a><a id="MergeDims.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_350">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_329">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="mergedimsvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>MergeDims.variable_scope</code></a><a id="MergeDims.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_351">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_330">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="mergedimsvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>MergeDims.variables</code></a><a id="MergeDims.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_352">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_331">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-modelrnn"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?q=class:ModelRNN"><code>class ModelRNN</code></a><a id="ModelRNN" /></h3>
<p>RNNCore that ignores input and uses a model to compute its next state.</p>
<h4 id="modelrnn__init__model-namemodel_rnn"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=492"><code>ModelRNN.__init__(model, name='model_rnn')</code></a><a id="ModelRNN.__init__" /></h4>
<p>Construct a Basic RNN core.</p>
<h5 id="args_173">Args:</h5>
<ul>
<li><code>model</code>: callable that computes the next state.</li>
<li><code>name</code>: name of the module.</li>
</ul>
<h5 id="raises_332">Raises:</h5>
<ul>
<li><code>TypeError</code>: if model is not a callable object or if it is an RNNCore.</li>
<li><code>AttributeError</code>: if model does not have an output_size attribute.</li>
</ul>
<h4 id="modelrnn__call__inputs-prev_state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=517"><code>ModelRNN.__call__(inputs, prev_state)</code></a><a id="ModelRNN.__call__" /></h4>
<p>Connects the ModelRNN module into the graph.</p>
<p>If this is not the first time the module has been connected to the graph,
the Tensors provided as input_ and state must have the same final
dimension, in order for the existing variables to be the correct size for
their corresponding multiplications. The batch size may differ for each
connection.</p>
<h5 id="args_174">Args:</h5>
<ul>
<li><code>inputs</code>: Tensor input to the ModelRNN (ignored).</li>
<li><code>prev_state</code>: Tensor of size <code>model.output_size</code>.</li>
</ul>
<h5 id="returns_353">Returns:</h5>
<ul>
<li><code>output</code>: Tensor of size <code>model.output_size</code>.</li>
<li><code>next_state</code>: Tensor of size <code>model.output_size</code>.</li>
</ul>
<h4 id="modelrnnconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>ModelRNN.connected_subgraphs</code></a><a id="ModelRNN.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="modelrnndefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>ModelRNN.defun()</code></a><a id="ModelRNN.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="modelrnndefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>ModelRNN.defun_wrapped</code></a><a id="ModelRNN.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="modelrnnget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>ModelRNN.get_all_variables(collection='trainable_variables')</code></a><a id="ModelRNN.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_175">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_354">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_333">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="modelrnnget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>ModelRNN.get_possible_initializer_keys(cls)</code></a><a id="ModelRNN.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_355">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="modelrnnget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>ModelRNN.get_variables(collection='trainable_variables')</code></a><a id="ModelRNN.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_176">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_356">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_334">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="modelrnngraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>ModelRNN.graph</code></a><a id="ModelRNN.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="modelrnninitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=203"><code>ModelRNN.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)</code></a><a id="ModelRNN.initial_state" /></h4>
<p>Builds the default start state for an RNNCore.</p>
<h5 id="args_177">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, or scalar int32 Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.
    Note that intializers and regularizers will be ignored if
    <code>trainable=False</code>.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. As a
    default, no regularizers are used. A regularizer should be a function
    that takes a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code>
    output, e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_357">Returns:</h5>
<p>A tensor or nested tuple of tensors with same structure and shape as the
  <code>state_size</code> property of the core.</p>
<h5 id="raises_335">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the user passes initializers that are not functions.</li>
<li><code>ValueError</code>: if the user passes regularizers that are not functions.</li>
</ul>
<h4 id="modelrnnis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>ModelRNN.is_connected</code></a><a id="ModelRNN.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="modelrnnlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>ModelRNN.last_connected_subgraph</code></a><a id="ModelRNN.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_358">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_336">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="modelrnnmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>ModelRNN.module_name</code></a><a id="ModelRNN.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="modelrnnname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>ModelRNN.name_scopes</code></a><a id="ModelRNN.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="modelrnnnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>ModelRNN.non_trainable_variables</code></a><a id="ModelRNN.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_359">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_337">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="modelrnnoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=543"><code>ModelRNN.output_size</code></a><a id="ModelRNN.output_size" /></h4>
<h4 id="modelrnnscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>ModelRNN.scope_name</code></a><a id="ModelRNN.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="modelrnnstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=539"><code>ModelRNN.state_size</code></a><a id="ModelRNN.state_size" /></h4>
<h4 id="modelrnntrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>ModelRNN.trainable_variables</code></a><a id="ModelRNN.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_360">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_338">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="modelrnnvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>ModelRNN.variable_scope</code></a><a id="ModelRNN.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_361">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_339">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="modelrnnvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>ModelRNN.variables</code></a><a id="ModelRNN.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_362">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_340">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="modelrnnzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>ModelRNN.zero_state(batch_size, dtype)</code></a><a id="ModelRNN.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_178">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_363">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-module"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?q=class:Module"><code>class Module</code></a><a id="Module" /></h3>
<p>Module wrapping a function provided by the user.</p>
<h4 id="module__init__build-custom_getternone-namenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=764"><code>Module.__init__(build, custom_getter=None, name=None)</code></a><a id="Module.__init__" /></h4>
<p>Constructs a module with a given build function.</p>
<p>The Module class can be used to wrap a function assembling a network into a
module.</p>
<p>For example, the following code implements a simple one-hidden-layer MLP
model by defining a function called make_model and using a Module instance
to wrap it.</p>
<pre><code class="python">def make_model(inputs):
  lin1 = snt.Linear(name=&quot;lin1&quot;, output_size=10)(inputs)
  relu1 = tf.nn.relu(lin1, name=&quot;relu1&quot;)
  lin2 = snt.Linear(name=&quot;lin2&quot;, output_size=20)(relu1)
  return lin2

model = snt.Module(name='simple_mlp', build=make_model)
outputs = model(inputs)
</code></pre>

<p>The <code>partial</code> package from <code>functools</code> can be used to bake configuration
parameters into the function at construction time, as shown in the following
example.</p>
<pre><code class="python">from functools import partial

def make_model(inputs, output_sizes):
  lin1 = snt.Linear(name=&quot;lin1&quot;, output_size=output_sizes[0])(inputs)
  relu1 = tf.nn.relu(lin1, name=&quot;relu1&quot;)
  lin2 = snt.Linear(name=&quot;lin2&quot;, output_size=output_sizes[1])(relu1)
  return lin2

model = snt.Module(name='simple_mlp',
                   build=partial(make_model, output_sizes=[10, 20])
outputs = model(inputs)
</code></pre>

<h5 id="args_179">Args:</h5>
<ul>
<li><code>build</code>: Callable to be invoked when connecting the module to the graph.
      The <code>build</code> function is invoked when the module is called, and its
      role is to specify how to add elements to the Graph, and how to
      compute output Tensors from input Tensors.
      The <code>build</code> function signature can include the following parameters:
        <em>args - Input Tensors.
        </em>*kwargs - Additional Python parameters controlling connection.</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
      custom getters inside the module. If a dictionary, the keys
      correspond to regexes to match variable names. See the
      <code>tf.get_variable</code> documentation for information about the
      custom_getter API.</li>
<li><code>name</code>: Module name. If set to <code>None</code> (the default), the name will be set to
      that of the <code>build</code> callable converted to <code>snake_case</code>. If <code>build</code> has
      no name, the name will be 'module'.</li>
</ul>
<h5 id="raises_341">Raises:</h5>
<ul>
<li><code>TypeError</code>: If build is not callable.</li>
<li><code>TypeError</code>: If a given <code>custom_getter</code> is not callable.</li>
</ul>
<h4 id="module__call__args-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=831"><code>Module.__call__(*args, **kwargs)</code></a><a id="Module.__call__" /></h4>
<p>Forwards call to the passed-in build function.</p>
<h4 id="moduleconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Module.connected_subgraphs</code></a><a id="Module.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="moduledefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Module.defun()</code></a><a id="Module.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="moduledefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Module.defun_wrapped</code></a><a id="Module.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="moduleget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Module.get_all_variables(collection='trainable_variables')</code></a><a id="Module.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_180">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_364">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_342">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="moduleget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>Module.get_possible_initializer_keys(cls)</code></a><a id="Module.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_365">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="moduleget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>Module.get_variables(collection='trainable_variables')</code></a><a id="Module.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_181">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_366">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_343">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="modulegraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Module.graph</code></a><a id="Module.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="moduleis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Module.is_connected</code></a><a id="Module.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="modulelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Module.last_connected_subgraph</code></a><a id="Module.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_367">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_344">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="modulemodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Module.module_name</code></a><a id="Module.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="modulename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Module.name_scopes</code></a><a id="Module.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="modulenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Module.non_trainable_variables</code></a><a id="Module.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_368">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_345">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="modulescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Module.scope_name</code></a><a id="Module.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="moduletrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Module.trainable_variables</code></a><a id="Module.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_369">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_346">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="modulevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Module.variable_scope</code></a><a id="Module.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_370">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_347">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="modulevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Module.variables</code></a><a id="Module.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_371">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_348">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-moduleinfoerror"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base_errors.py?q=class:ModuleInfoError"><code>class ModuleInfoError</code></a><a id="ModuleInfoError" /></h3>
<p>Error raised when Sonnet <code>ModuleInfo</code> cannot be serialized.</p>
<h3 id="class-movingaverage"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/moving_average.py?q=class:MovingAverage"><code>class MovingAverage</code></a><a id="MovingAverage" /></h3>
<p>Calculates a differentiable decaying moving average.</p>
<p>The moving average is kept in a variable that can either be local or global.
The initial moving average value is set to the first value that is received
by the module. The module lets gradients flow through the last element added
to the moving average.</p>
<h4 id="movingaverage__init__decay099-localfalse-namemoving_average"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/moving_average.py?l=38"><code>MovingAverage.__init__(decay=0.99, local=False, name='moving_average')</code></a><a id="MovingAverage.__init__" /></h4>
<p>Constructor.</p>
<h5 id="args_182">Args:</h5>
<ul>
<li><code>decay</code>: float in range [0, 1], decay of the moving average.</li>
<li><code>local</code>: bool, specifies whether the variables are local or not.</li>
<li><code>name</code>: string, name of the Sonnet module. Default is 'moving_average'.</li>
</ul>
<h5 id="raises_349">Raises:</h5>
<ul>
<li><code>ValueError</code>: if decay is not in the valid range [0, 1].</li>
</ul>
<h4 id="movingaverage__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/moving_average.py?l=65"><code>MovingAverage.__call__(inputs)</code></a><a id="MovingAverage.__call__" /></h4>
<p>Returns the moving average of the values that went through <code>inputs</code>.</p>
<h5 id="args_183">Args:</h5>
<ul>
<li><code>inputs</code>: tensor.</li>
</ul>
<h5 id="returns_372">Returns:</h5>
<p>A moving average calculated as <code>(1 - decay) * inputs + decay * average</code>.</p>
<h4 id="movingaverageconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>MovingAverage.connected_subgraphs</code></a><a id="MovingAverage.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="movingaveragedefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>MovingAverage.defun()</code></a><a id="MovingAverage.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="movingaveragedefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>MovingAverage.defun_wrapped</code></a><a id="MovingAverage.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="movingaverageget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>MovingAverage.get_all_variables(collection='trainable_variables')</code></a><a id="MovingAverage.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_184">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_373">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_350">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="movingaverageget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>MovingAverage.get_possible_initializer_keys(cls)</code></a><a id="MovingAverage.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_374">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="movingaverageget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>MovingAverage.get_variables(collection='trainable_variables')</code></a><a id="MovingAverage.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_185">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_375">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_351">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="movingaveragegraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>MovingAverage.graph</code></a><a id="MovingAverage.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="movingaverageis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>MovingAverage.is_connected</code></a><a id="MovingAverage.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="movingaveragelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>MovingAverage.last_connected_subgraph</code></a><a id="MovingAverage.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_376">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_352">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="movingaveragemodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>MovingAverage.module_name</code></a><a id="MovingAverage.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="movingaveragename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>MovingAverage.name_scopes</code></a><a id="MovingAverage.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="movingaveragenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>MovingAverage.non_trainable_variables</code></a><a id="MovingAverage.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_377">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_353">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="movingaveragereset"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/moving_average.py?l=59"><code>MovingAverage.reset()</code></a><a id="MovingAverage.reset" /></h4>
<h4 id="movingaveragescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>MovingAverage.scope_name</code></a><a id="MovingAverage.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="movingaveragetrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>MovingAverage.trainable_variables</code></a><a id="MovingAverage.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_378">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_354">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="movingaveragevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>MovingAverage.variable_scope</code></a><a id="MovingAverage.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_379">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_355">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="movingaveragevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>MovingAverage.variables</code></a><a id="MovingAverage.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_380">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_356">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-notconnectederror"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base_errors.py?q=class:NotConnectedError"><code>class NotConnectedError</code></a><a id="NotConnectedError" /></h3>
<p>Error raised when operating on a module that has not yet been connected.</p>
<p>Some module properties / methods are valid to access before the module has
been connected into the graph, but some are not. This Error is raised when
the user attempts to do anything not valid before connection.</p>
<h3 id="class-notinitializederror"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base_errors.py?q=class:NotInitializedError"><code>class NotInitializedError</code></a><a id="NotInitializedError" /></h3>
<p>Error raised when connecting an uninitialized Sonnet module.</p>
<p>Before they can be connected, all Sonnet modules must call
<code>AbstractModule.__init__</code> (e.g. via a <code>super</code> call).</p>
<h3 id="class-notsupportederror"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base_errors.py?q=class:NotSupportedError"><code>class NotSupportedError</code></a><a id="NotSupportedError" /></h3>
<p>Error raised when something that cannot be supported is requested.</p>
<p>For example a Dilated Convolution module cannot be transposed.</p>
<h3 id="class-optimizationconstraints"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/optimization_constraints.py?q=class:OptimizationConstraints"><code>class OptimizationConstraints</code></a><a id="OptimizationConstraints" /></h3>
<p>Container for optimization constraints.</p>
<p>Users can add to an OptimizationConstraints instance multiple inequality
constraints, either implicitly passing inequality ops, such as
<code>optimization_constraints.add(x &lt; y)</code>, or explicitly specifying the constraint
type, as in <code>optimization_constraints.add_geq(x, y)</code>.
Users can finally add the constraints to the TensorFlow graph calling
<code>optimization_constraints()</code>; when doing so, Lagrange multipliers are
automatically added to the graph, so that users can optimize them alongside
other variables in the graph, using the same optimizer and <code>minimize()</code>.</p>
<p>Example usage:</p>
<pre><code>regularization_loss = model.regularization_loss(data)
reconstruction_error = model.reconstruction_error(data)
avg_reconstruction_error = snt.MovingAverage()(reconstruction_error)
constraints = snt.OptimizationConstraints()
constraints.add(avg_reconstruction_error &lt; reconstruction_threshold)
loss = regularization_loss + constraints()
# The following call actually performs an update step for
# min_{theta} max_{lambda} (
#     regularization_loss(theta) +
#     lambda * (avg_reconstruction_error - reconstruction_threshold))
# where theta are the model parameters and lambda are the Lagrange
# multipliers.
update = optimizer.minimize(loss)
</code></pre>

<h4 id="optimizationconstraints__init__rate10-valid_rangenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/optimization_constraints.py?l=72"><code>OptimizationConstraints.__init__(rate=1.0, valid_range=None)</code></a><a id="OptimizationConstraints.__init__" /></h4>
<p>Instantiates a container for optimization constraints.</p>
<h5 id="args_186">Args:</h5>
<ul>
<li><code>rate</code>: optional float, default 1.0. Default factor for Lagrange multiplier
      gradient scaling. Use there <code>rate</code> argument to scale the gradients of
      the Lagrange multipliers - note that this parameter has no effect when
      using optimisers such as Adam. This parameter can be overridden
      when adding constraints to the container.</li>
<li><code>valid_range</code>: optional tuple of length 2, default None. Default valid range
      for Lagrange multipliers. This parameter can be overridden when adding
      constraints to the container.</li>
</ul>
<h4 id="optimizationconstraintsaddexpression-ratenone-valid_rangenone-initializernone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/optimization_constraints.py?l=100"><code>OptimizationConstraints.add(expression, rate=None, valid_range=None, initializer=None)</code></a><a id="OptimizationConstraints.add" /></h4>
<p>Add inequality constraint whose type depends on analysis of input op.</p>
<h5 id="args_187">Args:</h5>
<ul>
<li><code>expression</code>: op of type <code>Greater</code>, <code>GreaterEqual</code>, <code>Less</code> or <code>LessEqual</code>.
      Note that <code>GreaterEqual</code> and <code>LessEqual</code> are accepted only for
      convenience, and will result in the same behavior as <code>Greater</code> and
      <code>Less</code> respectively.</li>
<li><code>rate</code>: optional float, default None. Factor for Lagrange multiplier
      gradient scaling. Use there <code>rate</code> argument to scale the gradients of
      the Lagrange multipliers - note that this parameter has no effect when
      using optimisers such as Adam. This parameter overrides the defaults
      defined instantiating the container.</li>
<li><code>valid_range</code>: optional tuple of length 2, default None. Default valid
      range for Lagrange multipliers. This parameter overrides the defaults
      defined instantiating the container.</li>
<li><code>initializer</code>: optional tensorflow initializer, array or value to be used
      for the Lagrange multiplier initialization. By default Lagrange
      multiplier will be initialized to 1.0.</li>
</ul>
<h5 id="returns_381">Returns:</h5>
<p>Self.</p>
<h5 id="raises_357">Raises:</h5>
<p><code>TypeError</code>, when input expression op is not one of <code>Greater</code>,
  <code>GreaterEqual</code>, <code>Less</code>, <code>LessEqual</code>.</p>
<h4 id="optimizationconstraintsadd_geqlhs-rhs00-ratenone-valid_rangenone-initializernone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/optimization_constraints.py?l=178"><code>OptimizationConstraints.add_geq(lhs, rhs=0.0, rate=None, valid_range=None, initializer=None)</code></a><a id="OptimizationConstraints.add_geq" /></h4>
<p>Add a 'greater than' inequality constraint.</p>
<h5 id="args_188">Args:</h5>
<ul>
<li><code>lhs</code>: left hand argument of inequality expression.</li>
<li><code>rhs</code>: reft hand argument of inequality expression, defaults to 0.0.</li>
<li><code>rate</code>: optional float, default None. Factor for Lagrange multiplier
      gradient scaling. Use there <code>rate</code> argument to scale the gradients of
      the Lagrange multipliers - note that this parameter has no effect when
      using optimisers such as Adam. This parameter overrides the defaults
      defined instantiating the container.</li>
<li><code>valid_range</code>: optional tuple of length 2, default None. Default valid
      range for Lagrange multipliers. This parameter overrides the defaults
      defined instantiating the container.</li>
<li><code>initializer</code>: optional tensorflow initializer, array or value to be used
      for the Lagrange multiplier initialization. By default Lagrange
      multiplier will be initialized to 1.0.</li>
</ul>
<h5 id="returns_382">Returns:</h5>
<p>Self.</p>
<h4 id="optimizationconstraintsadd_leqlhs-rhs00-ratenone-valid_rangenone-initializernone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/optimization_constraints.py?l=143"><code>OptimizationConstraints.add_leq(lhs, rhs=0.0, rate=None, valid_range=None, initializer=None)</code></a><a id="OptimizationConstraints.add_leq" /></h4>
<p>Add a 'less than' inequality constraint.</p>
<h5 id="args_189">Args:</h5>
<ul>
<li><code>lhs</code>: left hand argument of inequality expression.</li>
<li><code>rhs</code>: reft hand argument of inequality expression, defaults to 0.0.</li>
<li><code>rate</code>: optional float, default None. Factor for Lagrange multiplier
      gradient scaling. Use there <code>rate</code> argument to scale the gradients of
      the Lagrange multipliers - note that this parameter has no effect when
      using optimisers such as Adam. This parameter overrides the defaults
      defined instantiating the container.</li>
<li><code>valid_range</code>: optional tuple of length 2, default None. Default valid
      range for Lagrange multipliers. This parameter overrides the defaults
      defined instantiating the container.</li>
<li><code>initializer</code>: optional tensorflow initializer, array or value to be used
      for the Lagrange multiplier initialization. By default Lagrange
      multiplier will be initialized to 1.0.</li>
</ul>
<h5 id="returns_383">Returns:</h5>
<p>Self.</p>
<h4 id="optimizationconstraintsconstraints"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/optimization_constraints.py?l=92"><code>OptimizationConstraints.constraints</code></a><a id="OptimizationConstraints.constraints" /></h4>
<h4 id="optimizationconstraintslagrange_multipliers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/optimization_constraints.py?l=96"><code>OptimizationConstraints.lagrange_multipliers</code></a><a id="OptimizationConstraints.lagrange_multipliers" /></h4>
<h3 id="class-parentnotbuilterror"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base_errors.py?q=class:ParentNotBuiltError"><code>class ParentNotBuiltError</code></a><a id="ParentNotBuiltError" /></h3>
<p>Error raised when the parent of a module has not been built yet.</p>
<p>For example, when making a transpose of modules that inherit from
<code>module.Transposable</code>, the parent has to be connected to the graph before the
child transpose to ensure that shape inference has already occurred.</p>
<h3 id="class-rnncellwrapper"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?q=class:RNNCellWrapper"><code>class RNNCellWrapper</code></a><a id="RNNCellWrapper" /></h3>
<p>RNN core that delegates to a <code>tf.contrib.rnn.RNNCell</code>.</p>
<h4 id="rnncellwrapper__init__cell_ctor-args-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=355"><code>RNNCellWrapper.__init__(cell_ctor, *args, **kwargs)</code></a><a id="RNNCellWrapper.__init__" /></h4>
<p>Constructs the cell, within this module's variable scope.</p>
<h5 id="args_190">Args:</h5>
<ul>
<li><code>cell_ctor</code>: Callable that instantiates a <code>tf.contrib.rnn.RNNCell</code>.</li>
<li><code>*args</code>: Arguments to pass to <code>cell_ctor</code>.</li>
<li><code>**kwargs</code>: Keyword arguments to pass to <code>cell_ctor</code>.
    If <code>name</code> is provided, it is passed to <code>RNNCore.__init__</code> as well.
    If <code>custom_getter</code> is provided, it is passed to <code>RNNCore.__init__</code>
    but not to <code>cell_ctor</code>.</li>
</ul>
<h4 id="rnncellwrapper__call__inputs-prev_state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=373"><code>RNNCellWrapper.__call__(inputs, prev_state)</code></a><a id="RNNCellWrapper.__call__" /></h4>
<h4 id="rnncellwrapperconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>RNNCellWrapper.connected_subgraphs</code></a><a id="RNNCellWrapper.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="rnncellwrapperdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>RNNCellWrapper.defun()</code></a><a id="RNNCellWrapper.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="rnncellwrapperdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>RNNCellWrapper.defun_wrapped</code></a><a id="RNNCellWrapper.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="rnncellwrapperget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>RNNCellWrapper.get_all_variables(collection='trainable_variables')</code></a><a id="RNNCellWrapper.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_191">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_384">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_358">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncellwrapperget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>RNNCellWrapper.get_possible_initializer_keys(cls)</code></a><a id="RNNCellWrapper.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_385">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="rnncellwrapperget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>RNNCellWrapper.get_variables(collection='trainable_variables')</code></a><a id="RNNCellWrapper.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_192">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_386">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_359">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncellwrappergraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>RNNCellWrapper.graph</code></a><a id="RNNCellWrapper.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="rnncellwrapperinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=203"><code>RNNCellWrapper.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)</code></a><a id="RNNCellWrapper.initial_state" /></h4>
<p>Builds the default start state for an RNNCore.</p>
<h5 id="args_193">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, or scalar int32 Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.
    Note that intializers and regularizers will be ignored if
    <code>trainable=False</code>.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. As a
    default, no regularizers are used. A regularizer should be a function
    that takes a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code>
    output, e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_387">Returns:</h5>
<p>A tensor or nested tuple of tensors with same structure and shape as the
  <code>state_size</code> property of the core.</p>
<h5 id="raises_360">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the user passes initializers that are not functions.</li>
<li><code>ValueError</code>: if the user passes regularizers that are not functions.</li>
</ul>
<h4 id="rnncellwrapperis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>RNNCellWrapper.is_connected</code></a><a id="RNNCellWrapper.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="rnncellwrapperlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>RNNCellWrapper.last_connected_subgraph</code></a><a id="RNNCellWrapper.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_388">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_361">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncellwrappermodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>RNNCellWrapper.module_name</code></a><a id="RNNCellWrapper.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="rnncellwrappername_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>RNNCellWrapper.name_scopes</code></a><a id="RNNCellWrapper.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="rnncellwrappernon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>RNNCellWrapper.non_trainable_variables</code></a><a id="RNNCellWrapper.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_389">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_362">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncellwrapperoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=376"><code>RNNCellWrapper.output_size</code></a><a id="RNNCellWrapper.output_size" /></h4>
<h4 id="rnncellwrapperscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>RNNCellWrapper.scope_name</code></a><a id="RNNCellWrapper.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="rnncellwrapperstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=380"><code>RNNCellWrapper.state_size</code></a><a id="RNNCellWrapper.state_size" /></h4>
<h4 id="rnncellwrappertrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>RNNCellWrapper.trainable_variables</code></a><a id="RNNCellWrapper.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_390">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_363">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncellwrappervariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>RNNCellWrapper.variable_scope</code></a><a id="RNNCellWrapper.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_391">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_364">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncellwrappervariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>RNNCellWrapper.variables</code></a><a id="RNNCellWrapper.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_392">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_365">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncellwrapperzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>RNNCellWrapper.zero_state(batch_size, dtype)</code></a><a id="RNNCellWrapper.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_194">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_393">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-rnncore"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?q=class:RNNCore"><code>class RNNCore</code></a><a id="RNNCore" /></h3>
<p>Superclass for Recurrent Neural Network Cores.</p>
<p>This class defines the basic functionality that every core should implement,
mainly the <code>initial_state</code> method which will return an example of their
initial state.
It also inherits from the interface <code>snt.AbstractModule</code>.</p>
<p>As with any other <code>snt.Module</code> any subclass must implement a <code>_build</code> method
that constructs the graph that corresponds to a core. Such a <code>_build</code> method
should always have the same interface, which is the following:</p>
<pre><code>output, next_state = self._build(input, prev_state)
</code></pre>
<p>where output, next_state, input, and prev_state are arbitrarily nested
tensors. Such structures can be defined according to the following
grammar:</p>
<pre><code>element = tuple(element*) | list(element*) | tf.Tensor
</code></pre>
<p>This class is to be used with tensorflow containers such as <code>rnn</code> in
tensorflow.python.ops.rnn.
These containers only accept inputs which are compatible with the
<code>tf.contrib.rnn.RNNCell</code> API, so that all the RNNCores should expose
<code>state_size</code> and <code>output_size</code> properties.</p>
<h4 id="rnncore__init___sentinelnone-custom_getternone-namenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=126"><code>RNNCore.__init__(_sentinel=None, custom_getter=None, name=None)</code></a><a id="RNNCore.__init__" /></h4>
<p>Performs the initialisation necessary for all AbstractModule instances.</p>
<p>Every subclass of AbstractModule must begin their constructor with a call to
this constructor, i.e.</p>
<p><code>super(MySubModule, self).__init__(custom_getter=custom_getter, name=name)</code>.</p>
<p>If you instantiate sub-modules in <strong>init</strong> you must create them within the
<code>_enter_variable_scope</code> context manager to ensure they are in the module's
variable scope. Alternatively, instantiate sub-modules in <code>_build</code>.</p>
<h5 id="args_195">Args:</h5>
<p>_sentinel: Variable that only carries a non-None value if <code>__init__</code> was
      called without named parameters. If this is the case, a deprecation
      warning is issued in form of a <code>ValueError</code>.</p>
<ul>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the <code>tf.get_variable</code>
    documentation for information about the custom_getter API.</li>
<li><code>name</code>: Name of this module. Used to construct the Templated build function.
      If <code>None</code> the module's class name is used (converted to snake case).</li>
</ul>
<h5 id="raises_366">Raises:</h5>
<ul>
<li><code>TypeError</code>: If <code>name</code> is not a string.</li>
<li><code>TypeError</code>: If a given <code>custom_getter</code> is not callable.</li>
<li><code>ValueError</code>: If <code>__init__</code> was called without named arguments.</li>
</ul>
<h4 id="rnncore__call__args-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=294"><code>RNNCore.__call__(*args, **kwargs)</code></a><a id="RNNCore.__call__" /></h4>
<p>Add elements to the Graph, computing output Tensors from input Tensors.</p>
<p>Subclasses must implement this method, which will be wrapped in a Template.</p>
<h5 id="args_196">Args:</h5>
<ul>
<li><code>*args</code>: Input Tensors.</li>
<li><code>**kwargs</code>: Additional Python flags controlling connection.</li>
</ul>
<h5 id="returns_394">Returns:</h5>
<p>output Tensor(s).</p>
<h4 id="rnncoreconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>RNNCore.connected_subgraphs</code></a><a id="RNNCore.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="rnncoredefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>RNNCore.defun()</code></a><a id="RNNCore.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="rnncoredefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>RNNCore.defun_wrapped</code></a><a id="RNNCore.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="rnncoreget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>RNNCore.get_all_variables(collection='trainable_variables')</code></a><a id="RNNCore.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_197">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_395">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_367">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncoreget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>RNNCore.get_possible_initializer_keys(cls)</code></a><a id="RNNCore.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_396">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="rnncoreget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>RNNCore.get_variables(collection='trainable_variables')</code></a><a id="RNNCore.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_198">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_397">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_368">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncoregraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>RNNCore.graph</code></a><a id="RNNCore.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="rnncoreinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=203"><code>RNNCore.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)</code></a><a id="RNNCore.initial_state" /></h4>
<p>Builds the default start state for an RNNCore.</p>
<h5 id="args_199">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, or scalar int32 Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.
    Note that intializers and regularizers will be ignored if
    <code>trainable=False</code>.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. As a
    default, no regularizers are used. A regularizer should be a function
    that takes a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code>
    output, e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_398">Returns:</h5>
<p>A tensor or nested tuple of tensors with same structure and shape as the
  <code>state_size</code> property of the core.</p>
<h5 id="raises_369">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the user passes initializers that are not functions.</li>
<li><code>ValueError</code>: if the user passes regularizers that are not functions.</li>
</ul>
<h4 id="rnncoreis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>RNNCore.is_connected</code></a><a id="RNNCore.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="rnncorelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>RNNCore.last_connected_subgraph</code></a><a id="RNNCore.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_399">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_370">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncoremodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>RNNCore.module_name</code></a><a id="RNNCore.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="rnncorename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>RNNCore.name_scopes</code></a><a id="RNNCore.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="rnncorenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>RNNCore.non_trainable_variables</code></a><a id="RNNCore.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_400">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_371">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncoreoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=254"><code>RNNCore.output_size</code></a><a id="RNNCore.output_size" /></h4>
<p>Integer or TensorShape: size of outputs produced by this cell.</p>
<h4 id="rnncorescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>RNNCore.scope_name</code></a><a id="RNNCore.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="rnncorestate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=245"><code>RNNCore.state_size</code></a><a id="RNNCore.state_size" /></h4>
<p>size(s) of state(s) used by this cell.</p>
<p>It can be represented by an Integer, a TensorShape or a tuple of Integers
or TensorShapes.</p>
<h4 id="rnncoretrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>RNNCore.trainable_variables</code></a><a id="RNNCore.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_401">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_372">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncorevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>RNNCore.variable_scope</code></a><a id="RNNCore.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_402">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_373">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncorevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>RNNCore.variables</code></a><a id="RNNCore.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_403">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_374">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="rnncorezero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>RNNCore.zero_state(batch_size, dtype)</code></a><a id="RNNCore.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_200">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_404">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-relationalmemory"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/relational_memory.py?q=class:RelationalMemory"><code>class RelationalMemory</code></a><a id="RelationalMemory" /></h3>
<p>Relational Memory Core.</p>
<h4 id="relationalmemory__init__mem_slots-head_size-num_heads1-num_blocks1-forget_bias10-input_bias00-gate_styleunit-attention_mlp_layers2-key_sizenone-namerelational_memory"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/relational_memory.py?l=36"><code>RelationalMemory.__init__(mem_slots, head_size, num_heads=1, num_blocks=1, forget_bias=1.0, input_bias=0.0, gate_style='unit', attention_mlp_layers=2, key_size=None, name='relational_memory')</code></a><a id="RelationalMemory.__init__" /></h4>
<p>Constructs a <code>RelationalMemory</code> object.</p>
<h5 id="args_201">Args:</h5>
<ul>
<li><code>mem_slots</code>: The total number of memory slots to use.</li>
<li><code>head_size</code>: The size of an attention head.</li>
<li><code>num_heads</code>: The number of attention heads to use. Defaults to 1.</li>
<li><code>num_blocks</code>: Number of times to compute attention per time step. Defaults
    to 1.</li>
<li><code>forget_bias</code>: Bias to use for the forget gate, assuming we are using
    some form of gating. Defaults to 1.</li>
<li><code>input_bias</code>: Bias to use for the input gate, assuming we are using
    some form of gating. Defaults to 0.</li>
<li><code>gate_style</code>: Whether to use per-element gating ('unit'),
    per-memory slot gating ('memory'), or no gating at all (None).
    Defaults to <code>unit</code>.</li>
<li><code>attention_mlp_layers</code>: Number of layers to use in the post-attention
    MLP. Defaults to 2.</li>
<li><code>key_size</code>: Size of vector to use for key &amp; query vectors in the attention
    computation. Defaults to None, in which case we use <code>head_size</code>.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_375">Raises:</h5>
<ul>
<li><code>ValueError</code>: gate_style not one of [None, 'memory', 'unit'].</li>
<li><code>ValueError</code>: num_blocks is &lt; 1.</li>
<li><code>ValueError</code>: attention_mlp_layers is &lt; 1.</li>
</ul>
<h4 id="relationalmemory__call__inputs-memory-treat_input_as_matrixfalse"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/relational_memory.py?l=236"><code>RelationalMemory.__call__(inputs, memory, treat_input_as_matrix=False)</code></a><a id="RelationalMemory.__call__" /></h4>
<p>Adds relational memory to the TensorFlow graph.</p>
<h5 id="args_202">Args:</h5>
<ul>
<li><code>inputs</code>: Tensor input.</li>
<li><code>memory</code>: Memory output from the previous time step.</li>
<li><code>treat_input_as_matrix</code>: Optional, whether to treat <code>input</code> as a sequence
    of matrices. Defaulta to False, in which case the input is flattened
    into a vector.</li>
</ul>
<h5 id="returns_405">Returns:</h5>
<ul>
<li><code>output</code>: This time step's output.</li>
<li><code>next_memory</code>: The next version of memory to use.</li>
</ul>
<h4 id="relationalmemoryconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>RelationalMemory.connected_subgraphs</code></a><a id="RelationalMemory.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="relationalmemorydefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>RelationalMemory.defun()</code></a><a id="RelationalMemory.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="relationalmemorydefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>RelationalMemory.defun_wrapped</code></a><a id="RelationalMemory.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="relationalmemoryforget_gate"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/relational_memory.py?l=280"><code>RelationalMemory.forget_gate</code></a><a id="RelationalMemory.forget_gate" /></h4>
<p>Returns the forget gate Tensor.</p>
<h4 id="relationalmemoryget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>RelationalMemory.get_all_variables(collection='trainable_variables')</code></a><a id="RelationalMemory.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_203">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_406">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_376">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="relationalmemoryget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>RelationalMemory.get_possible_initializer_keys(cls)</code></a><a id="RelationalMemory.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_407">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="relationalmemoryget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>RelationalMemory.get_variables(collection='trainable_variables')</code></a><a id="RelationalMemory.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_204">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_408">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_377">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="relationalmemorygraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>RelationalMemory.graph</code></a><a id="RelationalMemory.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="relationalmemoryinitial_statebatch_size-trainablefalse"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/relational_memory.py?l=92"><code>RelationalMemory.initial_state(batch_size, trainable=False)</code></a><a id="RelationalMemory.initial_state" /></h4>
<p>Creates the initial memory.</p>
<p>We should ensure each row of the memory is initialized to be unique,
so initialize the matrix to be the identity. We then pad or truncate
as necessary so that init_state is of size
(batch_size, self._mem_slots, self._mem_size).</p>
<h5 id="args_205">Args:</h5>
<ul>
<li><code>batch_size</code>: The size of the batch.</li>
<li><code>trainable</code>: Whether the initial state is trainable. This is always True.</li>
</ul>
<h5 id="returns_409">Returns:</h5>
<ul>
<li><code>init_state</code>: A truncated or padded matrix of size
    (batch_size, self._mem_slots, self._mem_size).</li>
</ul>
<h4 id="relationalmemoryinput_gate"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/relational_memory.py?l=274"><code>RelationalMemory.input_gate</code></a><a id="RelationalMemory.input_gate" /></h4>
<p>Returns the input gate Tensor.</p>
<h4 id="relationalmemoryis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>RelationalMemory.is_connected</code></a><a id="RelationalMemory.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="relationalmemorylast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>RelationalMemory.last_connected_subgraph</code></a><a id="RelationalMemory.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_410">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_378">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="relationalmemorymodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>RelationalMemory.module_name</code></a><a id="RelationalMemory.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="relationalmemoryname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>RelationalMemory.name_scopes</code></a><a id="RelationalMemory.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="relationalmemorynon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>RelationalMemory.non_trainable_variables</code></a><a id="RelationalMemory.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_411">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_379">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="relationalmemoryoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/relational_memory.py?l=167"><code>RelationalMemory.output_size</code></a><a id="RelationalMemory.output_size" /></h4>
<h4 id="relationalmemoryscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>RelationalMemory.scope_name</code></a><a id="RelationalMemory.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="relationalmemorystate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/relational_memory.py?l=163"><code>RelationalMemory.state_size</code></a><a id="RelationalMemory.state_size" /></h4>
<h4 id="relationalmemorytrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>RelationalMemory.trainable_variables</code></a><a id="RelationalMemory.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_412">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_380">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="relationalmemoryvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>RelationalMemory.variable_scope</code></a><a id="RelationalMemory.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_413">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_381">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="relationalmemoryvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>RelationalMemory.variables</code></a><a id="RelationalMemory.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_414">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_382">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="relationalmemoryzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>RelationalMemory.zero_state(batch_size, dtype)</code></a><a id="RelationalMemory.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_206">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_415">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-residual"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?q=class:Residual"><code>class Residual</code></a><a id="Residual" /></h3>
<p>Adds a residual connection to a base module.</p>
<p>This module wraps a module M, where if M with traditionally output M(X),
Residual(M)(x) = M(x) + x.</p>
<h4 id="residual__init__base_module-nameresidual"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=36"><code>Residual.__init__(base_module, name='residual')</code></a><a id="Residual.__init__" /></h4>
<h4 id="residual__call__inputs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=41"><code>Residual.__call__(inputs, **kwargs)</code></a><a id="Residual.__call__" /></h4>
<h4 id="residualconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Residual.connected_subgraphs</code></a><a id="Residual.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="residualdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Residual.defun()</code></a><a id="Residual.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="residualdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Residual.defun_wrapped</code></a><a id="Residual.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="residualget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Residual.get_all_variables(collection='trainable_variables')</code></a><a id="Residual.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_207">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_416">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_383">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>Residual.get_possible_initializer_keys(cls)</code></a><a id="Residual.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_417">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="residualget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>Residual.get_variables(collection='trainable_variables')</code></a><a id="Residual.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_208">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_418">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_384">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Residual.graph</code></a><a id="Residual.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="residualis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Residual.is_connected</code></a><a id="Residual.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="residuallast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Residual.last_connected_subgraph</code></a><a id="Residual.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_419">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_385">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Residual.module_name</code></a><a id="Residual.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="residualname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Residual.name_scopes</code></a><a id="Residual.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="residualnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Residual.non_trainable_variables</code></a><a id="Residual.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_420">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_386">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Residual.scope_name</code></a><a id="Residual.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="residualtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Residual.trainable_variables</code></a><a id="Residual.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_421">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_387">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Residual.variable_scope</code></a><a id="Residual.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_422">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_388">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Residual.variables</code></a><a id="Residual.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_423">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_389">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-residualcore"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?q=class:ResidualCore"><code>class ResidualCore</code></a><a id="ResidualCore" /></h3>
<p>Adds a residual connection to a base RNN core.</p>
<p>This module wraps a module M, where if M with traditionally output M(X),
Residual(M)(x) = M(x) + x.</p>
<h4 id="residualcore__init__base_core-nameresidual_core"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=54"><code>ResidualCore.__init__(base_core, name='residual_core')</code></a><a id="ResidualCore.__init__" /></h4>
<h4 id="residualcore__call__inputs-prev_state-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=58"><code>ResidualCore.__call__(inputs, prev_state, **kwargs)</code></a><a id="ResidualCore.__call__" /></h4>
<h4 id="residualcoreconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>ResidualCore.connected_subgraphs</code></a><a id="ResidualCore.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="residualcoredefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>ResidualCore.defun()</code></a><a id="ResidualCore.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="residualcoredefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>ResidualCore.defun_wrapped</code></a><a id="ResidualCore.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="residualcoreget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>ResidualCore.get_all_variables(collection='trainable_variables')</code></a><a id="ResidualCore.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_209">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_424">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_390">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualcoreget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>ResidualCore.get_possible_initializer_keys(cls)</code></a><a id="ResidualCore.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_425">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="residualcoreget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>ResidualCore.get_variables(collection='trainable_variables')</code></a><a id="ResidualCore.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_210">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_426">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_391">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualcoregraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>ResidualCore.graph</code></a><a id="ResidualCore.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="residualcoreinitial_stateargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=71"><code>ResidualCore.initial_state(*args, **kwargs)</code></a><a id="ResidualCore.initial_state" /></h4>
<h4 id="residualcoreis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>ResidualCore.is_connected</code></a><a id="ResidualCore.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="residualcorelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>ResidualCore.last_connected_subgraph</code></a><a id="ResidualCore.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_427">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_392">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualcoremodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>ResidualCore.module_name</code></a><a id="ResidualCore.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="residualcorename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>ResidualCore.name_scopes</code></a><a id="ResidualCore.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="residualcorenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>ResidualCore.non_trainable_variables</code></a><a id="ResidualCore.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_428">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_393">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualcoreoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=63"><code>ResidualCore.output_size</code></a><a id="ResidualCore.output_size" /></h4>
<h4 id="residualcorescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>ResidualCore.scope_name</code></a><a id="ResidualCore.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="residualcorestate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=67"><code>ResidualCore.state_size</code></a><a id="ResidualCore.state_size" /></h4>
<h4 id="residualcoretrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>ResidualCore.trainable_variables</code></a><a id="ResidualCore.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_429">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_394">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualcorevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>ResidualCore.variable_scope</code></a><a id="ResidualCore.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_430">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_395">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualcorevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>ResidualCore.variables</code></a><a id="ResidualCore.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_431">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_396">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="residualcorezero_stateargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=74"><code>ResidualCore.zero_state(*args, **kwargs)</code></a><a id="ResidualCore.zero_state" /></h4>
<h3 id="class-selectinput"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?q=class:SelectInput"><code>class SelectInput</code></a><a id="SelectInput" /></h3>
<p>Returns a subset of its inputs in an arbitrarily nested configuration.</p>
<p>This module can be used for multiple purposes.</p>
<p>The basic usage is to select a tensor or a subset of tensors:</p>
<pre><code>output = snt.SelectInput(idx=0, name='select')(input0, input1)
==&gt; input0

output = snt.SelectInput(idx=[0, 2], name='select')(input0, input1, input2)
==&gt; (input0, input2)
</code></pre>

<p>Another usage is to change the orders of the input tensors:</p>
<pre><code>output = snt.SelectInput(idx=[1, 0], name='select')(input0, input1)
==&gt; (input1, input0)
</code></pre>

<p>Another usage is to duplicate an input:</p>
<pre><code>output = snt.SelectInput(idx=[0, 0], name='select')(input0)
==&gt; (input0, input0)
</code></pre>

<p>Another usage is to add arbitrary nesting:</p>
<pre><code>output = snt.SelectInput(
    idx=[0, [1, [2]]], name='select')(input0, input1, input2)
==&gt; (input0, (input1, (input2,)))
</code></pre>

<h4 id="selectinput__init__idx-nameselect_input"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=1413"><code>SelectInput.__init__(idx, name='select_input')</code></a><a id="SelectInput.__init__" /></h4>
<p>Module constructor.</p>
<h5 id="args_211">Args:</h5>
<ul>
<li><code>idx</code>: Indexes of the tensors to select. If <code>idx</code> is an integer, then
      a <code>Tensor</code> is returned. If <code>idx</code> is a (nested) list/tuple, then a
      (nested) tuple of <code>Tensor</code> is returned.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_397">Raises:</h5>
<ul>
<li><code>TypeError</code>: If <code>idx</code> is not an list, tuple or integer.</li>
</ul>
<h4 id="selectinput__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=1447"><code>SelectInput.__call__(*inputs)</code></a><a id="SelectInput.__call__" /></h4>
<p>Connects the module into the graph.</p>
<h5 id="args_212">Args:</h5>
<ul>
<li><code>*inputs</code>: <code>Tensor</code> variables to select.</li>
</ul>
<h5 id="returns_432">Returns:</h5>
<p>Subset of <code>inputs</code> in an arbitrarily nested configuration.</p>
<h5 id="raises_398">Raises:</h5>
<ul>
<li><code>ValueError</code>: If any entry of <code>idx</code> is out of bounds with respect to the
      size of <code>inputs</code>.</li>
</ul>
<h4 id="selectinputconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>SelectInput.connected_subgraphs</code></a><a id="SelectInput.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="selectinputdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>SelectInput.defun()</code></a><a id="SelectInput.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="selectinputdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>SelectInput.defun_wrapped</code></a><a id="SelectInput.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="selectinputget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>SelectInput.get_all_variables(collection='trainable_variables')</code></a><a id="SelectInput.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_213">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_433">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_399">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="selectinputget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>SelectInput.get_possible_initializer_keys(cls)</code></a><a id="SelectInput.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_434">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="selectinputget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>SelectInput.get_variables(collection='trainable_variables')</code></a><a id="SelectInput.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_214">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_435">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_400">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="selectinputgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>SelectInput.graph</code></a><a id="SelectInput.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="selectinputis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>SelectInput.is_connected</code></a><a id="SelectInput.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="selectinputlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>SelectInput.last_connected_subgraph</code></a><a id="SelectInput.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_436">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_401">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="selectinputmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>SelectInput.module_name</code></a><a id="SelectInput.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="selectinputname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>SelectInput.name_scopes</code></a><a id="SelectInput.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="selectinputnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>SelectInput.non_trainable_variables</code></a><a id="SelectInput.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_437">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_402">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="selectinputscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>SelectInput.scope_name</code></a><a id="SelectInput.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="selectinputtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>SelectInput.trainable_variables</code></a><a id="SelectInput.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_438">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_403">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="selectinputvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>SelectInput.variable_scope</code></a><a id="SelectInput.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_439">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_404">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="selectinputvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>SelectInput.variables</code></a><a id="SelectInput.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_440">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_405">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-separableconv1d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?q=class:SeparableConv1D"><code>class SeparableConv1D</code></a><a id="SeparableConv1D" /></h3>
<p>Performs an in-plane convolution to each channel independently.</p>
<p>This acts as a light wrapper around the TensorFlow op
<code>tf.nn.separable_conv2d</code>, abstracting away variable creation and sharing.</p>
<h4 id="separableconv1d__init__output_channels-channel_multiplier-kernel_shape-stride1-rate1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatnwc-custom_getternone-nameseparable_conv1d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2728"><code>SeparableConv1D.__init__(output_channels, channel_multiplier, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NWC', custom_getter=None, name='separable_conv1d')</code></a><a id="SeparableConv1D.__init__" /></h4>
<p>Constructs a SeparableConv1D module.</p>
<p>See the following documentation for an explanation of VALID versus SAME
padding modes:
https://www.tensorflow.org/api_guides/python/nn#Convolution</p>
<h5 id="args_215">Args:</h5>
<ul>
<li><code>output_channels</code>: Number of output channels. Must be an integer.</li>
<li><code>channel_multiplier</code>: Number of channels to expand pointwise (depthwise)
      convolution to. Must be an integer. Must be &gt; 0.
      When <code>channel_multiplier</code> is set to 1, applies a different filter to
      each input channel. Numbers larger than 1 cause the filter to be
      applied to <code>channel_multiplier</code> input channels. Outputs are
      concatenated together.</li>
<li><code>kernel_shape</code>: List with 2 elements in the following layout:
      [filter_height, filter_width] or integer that is
      used to define the list in all dimensions.</li>
<li><code>stride</code>: List with 4 elements of kernel strides, or integer that is used to
      define stride in all dimensions. Layout of list:
      [1, stride_y, stride_x, 1].</li>
<li><code>rate</code>: Sequence of dilation rates (of size 1), or integer that is used to
      define dilation rate in all dimensions. 1 corresponds to standard 1D
      convolution, <code>rate &gt; 1</code> corresponds to dilated convolution. Cannot be
      &gt; 1 if any of <code>stride</code> is also &gt; 1.</li>
<li><code>padding</code>: Padding algorithm. Either <code>snt.SAME</code>, <code>snt.VALID</code>, <code>snt.FULL</code>,
      <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code>, or a sequence of these paddings
      of length 1.<ul>
<li>snt.SAME and snt.VALID are explained in the Tensorflow docs at
    https://www.tensorflow.org/api_guides/python/nn#Convolution.</li>
<li>snt.FULL pre- and post-pads with the maximum padding which does not
    result in a convolution over just padded elements.</li>
<li>snt.CAUSAL pre-pads to ensure that each output value only depends on
    input values at the same or preceding indices ("no dependence on the
    future").</li>
<li>snt.REVERSE_CAUSAL post-pads to ensure that each output value only
    depends on input values at the same or <em>greater</em> indices ("no
    dependence on the past").
  If you use the same padding for all dimensions, and it is one of SAME
  or VALID, then this is supported directly by the underlying
  convolution op. In all other cases, the input data will be padded
  using tf.pad before calling the convolution op.</li>
</ul>
</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      keys 'w_dw' for depthwise and 'w_pw' for pointwise) or biases
      (with key 'b').</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition the
      filters (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
      (with keys 'w_dw' for depthwise and 'w_pw' for pointwise) and the
      biases (with key 'b'). As a default, no regularizers are used.
      A regularizer should be a function that takes a single <code>Tensor</code> as an
      input and returns a scalar <code>Tensor</code> output, e.g. the L1 and L2
      regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>data_format</code>: A string. Specifies whether the channel dimension
      of the input and output is the last dimension (default, NWC), or the
      second dimension ("NCW").</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
      custom getters inside the module. If a dictionary, the keys
      correspond to regexes to match variable names. See the
      <code>tf.get_variable</code> documentation for information about the
      custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_406">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>channel_multiplier</code> isn't of type (<code>numbers.Integral</code> or
      <code>tf.Dimension</code>).</li>
<li><code>ValueError</code>: If <code>channel_multiplier</code> is less than 1.</li>
<li>
<p><code>ValueError</code>: If the given data_format is not a supported format (see
      <code>SUPPORTED_1D_DATA_FORMATS</code>).
  base.IncompatibleShapeError: If the given kernel shape is not an integer;
      or if the given kernel shape is not a sequence of one integer.
  base.IncompatibleShapeError: If the given stride is not an integer; or if
      the given stride is not a sequence of two integers.
  base.IncompatibleShapeError: If the given rate is not an integer; or if
      the given rate is not a sequence of two integers.
  base.IncompatibleShapeError: If a mask is a TensorFlow Tensor with
      a not fully defined shape.
  base.NotSupportedError: If rate in any dimension and the stride in any
      dimension are simultaneously &gt; 1.</p>
</li>
<li>
<p><code>ValueError</code>: If the given padding is not <code>snt.VALID</code>, <code>snt.SAME</code>,
      <code>snt.FULL</code>, <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code> or a sequence of these.</p>
</li>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
      keys other than 'w_dw', 'w_pw' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
      are not callable.</li>
<li><code>TypeError</code>: If mask is given and it is not convertible to a Tensor.</li>
<li><code>ValueError</code>: If the passed-in data_format doesn't have a channel dimension.</li>
</ul>
<h4 id="separableconv1d__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=521"><code>SeparableConv1D.__call__(inputs)</code></a><a id="SeparableConv1D.__call__" /></h4>
<p>Connects the _ConvND module into the graph, with input Tensor <code>inputs</code>.</p>
<p>If this is not the first time the module has been connected to the graph,
the input Tensor provided here must have the same number of channels, in
order for the existing variables to be the correct size for the
multiplication; the batch size and input spatial dimensions may differ for
each connection.</p>
<h5 id="args_216">Args:</h5>
<ul>
<li><code>inputs</code>: A ND Tensor of the same rank as <code>data_format</code>, and either of types
  <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</li>
</ul>
<h5 id="returns_441">Returns:</h5>
<p>A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ...,
      output_channels].</p>
<h5 id="raises_407">Raises:</h5>
<ul>
<li>
<p><code>ValueError</code>: If connecting the module into the graph any time after the
      first time and the inferred size of the input does not match previous
      invocations.
  base.IncompatibleShapeError: If the input tensor has the wrong number
      of dimensions.
  base.UnderspecifiedError: If the channel dimension of <code>inputs</code> isn't
      defined.
  base.IncompatibleShapeError: If a mask is present and its shape is
      incompatible with the shape of the weights.</p>
</li>
<li>
<p><code>TypeError</code>: If input Tensor dtype is not compatible with either
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</p>
</li>
</ul>
<h4 id="separableconv1db"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=778"><code>SeparableConv1D.b</code></a><a id="SeparableConv1D.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_442">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_408">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the graph
      yet, meaning the variables do not exist.</p>
<ul>
<li><code>AttributeError</code>: If the module does not use bias.</li>
</ul>
<h4 id="separableconv1dchannel_multiplier"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2943"><code>SeparableConv1D.channel_multiplier</code></a><a id="SeparableConv1D.channel_multiplier" /></h4>
<p>Returns the channel multiplier argument.</p>
<h4 id="separableconv1dclonenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=840"><code>SeparableConv1D.clone(name=None)</code></a><a id="SeparableConv1D.clone" /></h4>
<p>Returns a cloned <code>_ConvND</code> module.</p>
<h5 id="args_217">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of cloned module. The default name
    is constructed by appending "_clone" to <code>self.module_name</code>.</li>
</ul>
<h5 id="returns_443">Returns:</h5>
<p>A copy of the current class.</p>
<h4 id="separableconv1dconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>SeparableConv1D.connected_subgraphs</code></a><a id="SeparableConv1D.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="separableconv1dconv_op_padding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=767"><code>SeparableConv1D.conv_op_padding</code></a><a id="SeparableConv1D.conv_op_padding" /></h4>
<p>Returns the padding algorithm used for the underlying convolution op.</p>
<h4 id="separableconv1ddata_format"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=821"><code>SeparableConv1D.data_format</code></a><a id="SeparableConv1D.data_format" /></h4>
<p>Returns the data format.</p>
<h4 id="separableconv1ddefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>SeparableConv1D.defun()</code></a><a id="SeparableConv1D.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="separableconv1ddefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>SeparableConv1D.defun_wrapped</code></a><a id="SeparableConv1D.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="separableconv1dget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>SeparableConv1D.get_all_variables(collection='trainable_variables')</code></a><a id="SeparableConv1D.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_218">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_444">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_409">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv1dget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2854"><code>SeparableConv1D.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="SeparableConv1D.get_possible_initializer_keys" /></h4>
<h4 id="separableconv1dget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>SeparableConv1D.get_variables(collection='trainable_variables')</code></a><a id="SeparableConv1D.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_219">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_445">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_410">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv1dgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>SeparableConv1D.graph</code></a><a id="SeparableConv1D.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="separableconv1dhas_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=796"><code>SeparableConv1D.has_bias</code></a><a id="SeparableConv1D.has_bias" /></h4>
<p>Returns <code>True</code> if bias Variable is present in the module.</p>
<h4 id="separableconv1dinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=801"><code>SeparableConv1D.initializers</code></a><a id="SeparableConv1D.initializers" /></h4>
<p>Returns the initializers dictionary.</p>
<h4 id="separableconv1dinput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=833"><code>SeparableConv1D.input_channels</code></a><a id="SeparableConv1D.input_channels" /></h4>
<p>Returns the number of input channels.</p>
<h4 id="separableconv1dinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=827"><code>SeparableConv1D.input_shape</code></a><a id="SeparableConv1D.input_shape" /></h4>
<p>Returns the input shape.</p>
<h4 id="separableconv1dis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>SeparableConv1D.is_connected</code></a><a id="SeparableConv1D.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="separableconv1dkernel_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=722"><code>SeparableConv1D.kernel_shape</code></a><a id="SeparableConv1D.kernel_shape" /></h4>
<p>Returns the kernel shape.</p>
<h4 id="separableconv1dlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>SeparableConv1D.last_connected_subgraph</code></a><a id="SeparableConv1D.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_446">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_411">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv1dmask"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=816"><code>SeparableConv1D.mask</code></a><a id="SeparableConv1D.mask" /></h4>
<p>Returns the mask.</p>
<h4 id="separableconv1dmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>SeparableConv1D.module_name</code></a><a id="SeparableConv1D.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="separableconv1dname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>SeparableConv1D.name_scopes</code></a><a id="SeparableConv1D.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="separableconv1dnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>SeparableConv1D.non_trainable_variables</code></a><a id="SeparableConv1D.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_447">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_412">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv1doutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=713"><code>SeparableConv1D.output_channels</code></a><a id="SeparableConv1D.output_channels" /></h4>
<p>Returns the number of output channels.</p>
<h4 id="separableconv1dpadding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=739"><code>SeparableConv1D.padding</code></a><a id="SeparableConv1D.padding" /></h4>
<p>Returns the padding algorithm used, if this is the same for all dims.</p>
<p>Use <code>.paddings</code> if you want a tuple with the padding algorithm used for each
dimension.</p>
<h5 id="returns_448">Returns:</h5>
<p>The padding algorithm used, if this is the same for all dimensions.</p>
<h5 id="raises_413">Raises:</h5>
<ul>
<li><code>ValueError</code>: If different padding algorithms are used for different
    dimensions.</li>
</ul>
<h4 id="separableconv1dpaddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=762"><code>SeparableConv1D.paddings</code></a><a id="SeparableConv1D.paddings" /></h4>
<p>Returns a tuple with the padding algorithm used for each dimension.</p>
<h4 id="separableconv1dpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=806"><code>SeparableConv1D.partitioners</code></a><a id="SeparableConv1D.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="separableconv1drate"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=734"><code>SeparableConv1D.rate</code></a><a id="SeparableConv1D.rate" /></h4>
<p>Returns the dilation rate.</p>
<h4 id="separableconv1dregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=811"><code>SeparableConv1D.regularizers</code></a><a id="SeparableConv1D.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="separableconv1dscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>SeparableConv1D.scope_name</code></a><a id="SeparableConv1D.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="separableconv1dstride"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=727"><code>SeparableConv1D.stride</code></a><a id="SeparableConv1D.stride" /></h4>
<p>Returns the stride.</p>
<h4 id="separableconv1dtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>SeparableConv1D.trainable_variables</code></a><a id="SeparableConv1D.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_449">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_414">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv1dvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>SeparableConv1D.variable_scope</code></a><a id="SeparableConv1D.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_450">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_415">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv1dvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>SeparableConv1D.variables</code></a><a id="SeparableConv1D.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_451">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_416">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv1dw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=772"><code>SeparableConv1D.w</code></a><a id="SeparableConv1D.w" /></h4>
<p>Returns the Variable containing the weight matrix.</p>
<h4 id="separableconv1dw_dw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2948"><code>SeparableConv1D.w_dw</code></a><a id="SeparableConv1D.w_dw" /></h4>
<p>Returns the Variable containing the depthwise weight matrix.</p>
<h4 id="separableconv1dw_pw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2954"><code>SeparableConv1D.w_pw</code></a><a id="SeparableConv1D.w_pw" /></h4>
<p>Returns the Variable containing the pointwise weight matrix.</p>
<h3 id="class-separableconv2d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?q=class:SeparableConv2D"><code>class SeparableConv2D</code></a><a id="SeparableConv2D" /></h3>
<p>Performs an in-plane convolution to each channel independently.</p>
<p>This acts as a light wrapper around the TensorFlow op
<code>tf.nn.separable_conv2d</code>, abstracting away variable creation and sharing.</p>
<h4 id="separableconv2d__init__output_channels-channel_multiplier-kernel_shape-stride1-rate1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatnhwc-custom_getternone-nameseparable_conv2d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2501"><code>SeparableConv2D.__init__(output_channels, channel_multiplier, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NHWC', custom_getter=None, name='separable_conv2d')</code></a><a id="SeparableConv2D.__init__" /></h4>
<p>Constructs a SeparableConv2D module.</p>
<p>See the following documentation for an explanation of VALID versus SAME
padding modes:
https://www.tensorflow.org/api_guides/python/nn#Convolution</p>
<h5 id="args_220">Args:</h5>
<ul>
<li><code>output_channels</code>: Number of output channels. Must be an integer.</li>
<li><code>channel_multiplier</code>: Number of channels to expand pointwise (depthwise)
      convolution to. Must be an integer. Must be &gt; 0.
      When <code>channel_multiplier</code> is set to 1, applies a different filter to
      each input channel. Numbers larger than 1 cause the filter to be
      applied to <code>channel_multiplier</code> input channels. Outputs are
      concatenated together.</li>
<li><code>kernel_shape</code>: List with 2 elements in the following layout:
      [filter_height, filter_width] or integer that is
      used to define the list in all dimensions.</li>
<li><code>stride</code>: List with 4 elements of kernel strides, or integer that is used to
      define stride in all dimensions. Layout of list:
      [1, stride_y, stride_x, 1].</li>
<li><code>rate</code>: Sequence of dilation rates (of size 2), or integer that is used to
      define dilation rate in all dimensions. 1 corresponds to standard 2D
      convolution, <code>rate &gt; 1</code> corresponds to dilated convolution. Cannot be
      &gt; 1 if any of <code>stride</code> is also &gt; 1.</li>
<li><code>padding</code>: Padding algorithm. Either <code>snt.SAME</code>, <code>snt.VALID</code>, <code>snt.FULL</code>,
      <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code>, or a sequence of these paddings
      of length 2.<ul>
<li>snt.SAME and snt.VALID are explained in the Tensorflow docs at
    https://www.tensorflow.org/api_guides/python/nn#Convolution.</li>
<li>snt.FULL pre- and post-pads with the maximum padding which does not
    result in a convolution over just padded elements.</li>
<li>snt.CAUSAL pre-pads to ensure that each output value only depends on
    input values at the same or preceding indices ("no dependence on the
    future").</li>
<li>snt.REVERSE_CAUSAL post-pads to ensure that each output value only
    depends on input values at the same or <em>greater</em> indices ("no
    dependence on the past").
  If you use the same padding for all dimensions, and it is one of SAME
  or VALID, then this is supported directly by the underlying
  convolution op. In all other cases, the input data will be padded
  using tf.pad before calling the convolution op.</li>
</ul>
</li>
<li><code>use_bias</code>: Whether to include bias parameters. Default <code>True</code>.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      keys 'w_dw' for depthwise and 'w_pw' for pointwise) or biases
      (with key 'b').</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition the
      filters (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
      (with keys 'w_dw' for depthwise and 'w_pw' for pointwise) and the
      biases (with key 'b'). As a default, no regularizers are used.
      A regularizer should be a function that takes a single <code>Tensor</code> as an
      input and returns a scalar <code>Tensor</code> output, e.g. the L1 and L2
      regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>data_format</code>: A string. Specifies whether the channel dimension
      of the input and output is the last dimension (default, NHWC), or the
      second dimension ("NCHW").</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
      custom getters inside the module. If a dictionary, the keys
      correspond to regexes to match variable names. See the
      <code>tf.get_variable</code> documentation for information about the
      custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_417">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>channel_multiplier</code> isn't of type (<code>numbers.Integral</code> or
      <code>tf.Dimension</code>).</li>
<li><code>ValueError</code>: If <code>channel_multiplier</code> is less than 1.</li>
<li>
<p><code>ValueError</code>: If the given data_format is not a supported format (see
      <code>SUPPORTED_2D_DATA_FORMATS</code>).
  base.IncompatibleShapeError: If the given kernel shape is not an integer;
      or if the given kernel shape is not a sequence of two integers.
  base.IncompatibleShapeError: If the given stride is not an integer; or if
      the given stride is not a sequence of two integers.
  base.IncompatibleShapeError: If the given rate is not an integer; or if
      the given rate is not a sequence of two integers.
  base.IncompatibleShapeError: If a mask is a TensorFlow Tensor with
      a not fully defined shape.
  base.NotSupportedError: If rate in any dimension and the stride in any
      dimension are simultaneously &gt; 1.</p>
</li>
<li>
<p><code>ValueError</code>: If the given padding is not <code>snt.VALID</code>, <code>snt.SAME</code>,
      <code>snt.FULL</code>, <code>snt.CAUSAL</code>, <code>snt.REVERSE_CAUSAL</code> or a sequence of these.</p>
</li>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
      keys other than 'w_dw', 'w_pw' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
      are not callable.</li>
<li><code>TypeError</code>: If mask is given and it is not convertible to a Tensor.</li>
<li><code>ValueError</code>: If the passed-in data_format doesn't have a channel dimension.</li>
</ul>
<h4 id="separableconv2d__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=521"><code>SeparableConv2D.__call__(inputs)</code></a><a id="SeparableConv2D.__call__" /></h4>
<p>Connects the _ConvND module into the graph, with input Tensor <code>inputs</code>.</p>
<p>If this is not the first time the module has been connected to the graph,
the input Tensor provided here must have the same number of channels, in
order for the existing variables to be the correct size for the
multiplication; the batch size and input spatial dimensions may differ for
each connection.</p>
<h5 id="args_221">Args:</h5>
<ul>
<li><code>inputs</code>: A ND Tensor of the same rank as <code>data_format</code>, and either of types
  <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</li>
</ul>
<h5 id="returns_452">Returns:</h5>
<p>A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ...,
      output_channels].</p>
<h5 id="raises_418">Raises:</h5>
<ul>
<li>
<p><code>ValueError</code>: If connecting the module into the graph any time after the
      first time and the inferred size of the input does not match previous
      invocations.
  base.IncompatibleShapeError: If the input tensor has the wrong number
      of dimensions.
  base.UnderspecifiedError: If the channel dimension of <code>inputs</code> isn't
      defined.
  base.IncompatibleShapeError: If a mask is present and its shape is
      incompatible with the shape of the weights.</p>
</li>
<li>
<p><code>TypeError</code>: If input Tensor dtype is not compatible with either
      <code>tf.float16</code>, <code>tf.bfloat16</code>, <code>tf.float32</code> or <code>tf.float64</code>.</p>
</li>
</ul>
<h4 id="separableconv2db"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=778"><code>SeparableConv2D.b</code></a><a id="SeparableConv2D.b" /></h4>
<p>Returns the Variable containing the bias.</p>
<h5 id="returns_453">Returns:</h5>
<p>Variable object containing the bias, from the most recent <strong>call</strong>.</p>
<h5 id="raises_419">Raises:</h5>
<p>base.NotConnectedError: If the module has not been connected to the graph
      yet, meaning the variables do not exist.</p>
<ul>
<li><code>AttributeError</code>: If the module does not use bias.</li>
</ul>
<h4 id="separableconv2dchannel_multiplier"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2703"><code>SeparableConv2D.channel_multiplier</code></a><a id="SeparableConv2D.channel_multiplier" /></h4>
<p>Returns the channel multiplier argument.</p>
<h4 id="separableconv2dclonenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=840"><code>SeparableConv2D.clone(name=None)</code></a><a id="SeparableConv2D.clone" /></h4>
<p>Returns a cloned <code>_ConvND</code> module.</p>
<h5 id="args_222">Args:</h5>
<ul>
<li><code>name</code>: Optional string assigning name of cloned module. The default name
    is constructed by appending "_clone" to <code>self.module_name</code>.</li>
</ul>
<h5 id="returns_454">Returns:</h5>
<p>A copy of the current class.</p>
<h4 id="separableconv2dconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>SeparableConv2D.connected_subgraphs</code></a><a id="SeparableConv2D.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="separableconv2dconv_op_padding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=767"><code>SeparableConv2D.conv_op_padding</code></a><a id="SeparableConv2D.conv_op_padding" /></h4>
<p>Returns the padding algorithm used for the underlying convolution op.</p>
<h4 id="separableconv2ddata_format"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=821"><code>SeparableConv2D.data_format</code></a><a id="SeparableConv2D.data_format" /></h4>
<p>Returns the data format.</p>
<h4 id="separableconv2ddefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>SeparableConv2D.defun()</code></a><a id="SeparableConv2D.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="separableconv2ddefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>SeparableConv2D.defun_wrapped</code></a><a id="SeparableConv2D.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="separableconv2dget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>SeparableConv2D.get_all_variables(collection='trainable_variables')</code></a><a id="SeparableConv2D.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_223">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_455">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_420">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv2dget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2628"><code>SeparableConv2D.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="SeparableConv2D.get_possible_initializer_keys" /></h4>
<h4 id="separableconv2dget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>SeparableConv2D.get_variables(collection='trainable_variables')</code></a><a id="SeparableConv2D.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_224">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_456">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_421">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv2dgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>SeparableConv2D.graph</code></a><a id="SeparableConv2D.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="separableconv2dhas_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=796"><code>SeparableConv2D.has_bias</code></a><a id="SeparableConv2D.has_bias" /></h4>
<p>Returns <code>True</code> if bias Variable is present in the module.</p>
<h4 id="separableconv2dinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=801"><code>SeparableConv2D.initializers</code></a><a id="SeparableConv2D.initializers" /></h4>
<p>Returns the initializers dictionary.</p>
<h4 id="separableconv2dinput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=833"><code>SeparableConv2D.input_channels</code></a><a id="SeparableConv2D.input_channels" /></h4>
<p>Returns the number of input channels.</p>
<h4 id="separableconv2dinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=827"><code>SeparableConv2D.input_shape</code></a><a id="SeparableConv2D.input_shape" /></h4>
<p>Returns the input shape.</p>
<h4 id="separableconv2dis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>SeparableConv2D.is_connected</code></a><a id="SeparableConv2D.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="separableconv2dkernel_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=722"><code>SeparableConv2D.kernel_shape</code></a><a id="SeparableConv2D.kernel_shape" /></h4>
<p>Returns the kernel shape.</p>
<h4 id="separableconv2dlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>SeparableConv2D.last_connected_subgraph</code></a><a id="SeparableConv2D.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_457">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_422">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv2dmask"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=816"><code>SeparableConv2D.mask</code></a><a id="SeparableConv2D.mask" /></h4>
<p>Returns the mask.</p>
<h4 id="separableconv2dmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>SeparableConv2D.module_name</code></a><a id="SeparableConv2D.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="separableconv2dname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>SeparableConv2D.name_scopes</code></a><a id="SeparableConv2D.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="separableconv2dnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>SeparableConv2D.non_trainable_variables</code></a><a id="SeparableConv2D.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_458">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_423">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv2doutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=713"><code>SeparableConv2D.output_channels</code></a><a id="SeparableConv2D.output_channels" /></h4>
<p>Returns the number of output channels.</p>
<h4 id="separableconv2dpadding"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=739"><code>SeparableConv2D.padding</code></a><a id="SeparableConv2D.padding" /></h4>
<p>Returns the padding algorithm used, if this is the same for all dims.</p>
<p>Use <code>.paddings</code> if you want a tuple with the padding algorithm used for each
dimension.</p>
<h5 id="returns_459">Returns:</h5>
<p>The padding algorithm used, if this is the same for all dimensions.</p>
<h5 id="raises_424">Raises:</h5>
<ul>
<li><code>ValueError</code>: If different padding algorithms are used for different
    dimensions.</li>
</ul>
<h4 id="separableconv2dpaddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=762"><code>SeparableConv2D.paddings</code></a><a id="SeparableConv2D.paddings" /></h4>
<p>Returns a tuple with the padding algorithm used for each dimension.</p>
<h4 id="separableconv2dpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=806"><code>SeparableConv2D.partitioners</code></a><a id="SeparableConv2D.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="separableconv2drate"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=734"><code>SeparableConv2D.rate</code></a><a id="SeparableConv2D.rate" /></h4>
<p>Returns the dilation rate.</p>
<h4 id="separableconv2dregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=811"><code>SeparableConv2D.regularizers</code></a><a id="SeparableConv2D.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="separableconv2dscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>SeparableConv2D.scope_name</code></a><a id="SeparableConv2D.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="separableconv2dstride"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=727"><code>SeparableConv2D.stride</code></a><a id="SeparableConv2D.stride" /></h4>
<p>Returns the stride.</p>
<h4 id="separableconv2dtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>SeparableConv2D.trainable_variables</code></a><a id="SeparableConv2D.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_460">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_425">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv2dvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>SeparableConv2D.variable_scope</code></a><a id="SeparableConv2D.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_461">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_426">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv2dvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>SeparableConv2D.variables</code></a><a id="SeparableConv2D.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_462">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_427">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="separableconv2dw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=772"><code>SeparableConv2D.w</code></a><a id="SeparableConv2D.w" /></h4>
<p>Returns the Variable containing the weight matrix.</p>
<h4 id="separableconv2dw_dw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2708"><code>SeparableConv2D.w_dw</code></a><a id="SeparableConv2D.w_dw" /></h4>
<p>Returns the Variable containing the depthwise weight matrix.</p>
<h4 id="separableconv2dw_pw"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=2714"><code>SeparableConv2D.w_pw</code></a><a id="SeparableConv2D.w_pw" /></h4>
<p>Returns the Variable containing the pointwise weight matrix.</p>
<h3 id="class-sequential"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/sequential.py?q=class:Sequential"><code>class Sequential</code></a><a id="Sequential" /></h3>
<p>Builds a module out of a sequence of callables.</p>
<p>Note that <code>Sequential</code> is limited in the range of possible architectures
it can handle. This is a deliberate design decision; <code>Sequential</code> is only
meant to be used for the simple case of fusing together modules/ops where
the input of a particular module/op is the output of the previous one. Another
restriction is that it is not possible to have extra arguments in the <code>_build</code>
method that are passed to the constituents of the module - for example,
if there is a <code>BatchNorm</code> module in <code>Sequential</code> and the user wishes to switch
the <code>is_training</code> flag. If this is the desired use case, the recommended
solution is to use <code>snt.Module</code> to wrap a custom function, as shown in the
following example:</p>
<p>https://github.com/deepmind/sonnet/blob/master/sonnet/examples/module_with_build_args.py</p>
<h4 id="sequential__init__layers-namesequential"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/sequential.py?l=47"><code>Sequential.__init__(layers, name='sequential')</code></a><a id="Sequential.__init__" /></h4>
<p>Constructs a Sequential module.</p>
<p>This feeds the output of each layer into the next and returns the output
of the final layer.</p>
<p>If a layer returns a tuple, it is assumed that this must be unpacked into
the argument list of the next layer. If it is not a tuple, it is simply
passed through to the next layer unchanged.</p>
<h5 id="args_225">Args:</h5>
<ul>
<li><code>layers</code>: Iterable of callables to stack together, which can be modules
      or ops.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_428">Raises:</h5>
<ul>
<li><code>TypeError</code>: If <code>layers</code> is None or contains any non-callable items.</li>
</ul>
<h4 id="sequential__call__args"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/sequential.py?l=79"><code>Sequential.__call__(*args)</code></a><a id="Sequential.__call__" /></h4>
<p>Connects the Sequential module into the graph.</p>
<h5 id="args_226">Args:</h5>
<ul>
<li><code>*args</code>: A tuple of inputs, to be unpacked as the arguments to the first
      layer.</li>
</ul>
<h5 id="returns_463">Returns:</h5>
<p>The output value of the last layer.</p>
<h4 id="sequentialconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>Sequential.connected_subgraphs</code></a><a id="Sequential.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="sequentialdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>Sequential.defun()</code></a><a id="Sequential.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="sequentialdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>Sequential.defun_wrapped</code></a><a id="Sequential.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="sequentialget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>Sequential.get_all_variables(collection='trainable_variables')</code></a><a id="Sequential.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_227">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_464">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_429">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="sequentialget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>Sequential.get_possible_initializer_keys(cls)</code></a><a id="Sequential.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_465">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="sequentialget_variablesargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/sequential.py?l=113"><code>Sequential.get_variables(*args, **kwargs)</code></a><a id="Sequential.get_variables" /></h4>
<p>Provide a warning that get_variables on Sequential always returns ().</p>
<h4 id="sequentialgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>Sequential.graph</code></a><a id="Sequential.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="sequentialis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>Sequential.is_connected</code></a><a id="Sequential.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="sequentiallast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>Sequential.last_connected_subgraph</code></a><a id="Sequential.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_466">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_430">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="sequentiallayers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/sequential.py?l=109"><code>Sequential.layers</code></a><a id="Sequential.layers" /></h4>
<h4 id="sequentialmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>Sequential.module_name</code></a><a id="Sequential.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="sequentialname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>Sequential.name_scopes</code></a><a id="Sequential.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="sequentialnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>Sequential.non_trainable_variables</code></a><a id="Sequential.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_467">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_431">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="sequentialscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>Sequential.scope_name</code></a><a id="Sequential.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="sequentialtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>Sequential.trainable_variables</code></a><a id="Sequential.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_468">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_432">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="sequentialvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>Sequential.variable_scope</code></a><a id="Sequential.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_469">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_433">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="sequentialvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>Sequential.variables</code></a><a id="Sequential.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_470">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_434">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-skipconnectioncore"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?q=class:SkipConnectionCore"><code>class SkipConnectionCore</code></a><a id="SkipConnectionCore" /></h3>
<p>Adds a skip connection to the base RNN core.</p>
<p>The output of the wrapped core is the concatenation of the output of the base
core with its input. The state of the wrapped core is the state of the base
core.</p>
<h4 id="skipconnectioncore__init__base_core-input_shapenone-nameskip_connection_core"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=86"><code>SkipConnectionCore.__init__(base_core, input_shape=None, name='skip_connection_core')</code></a><a id="SkipConnectionCore.__init__" /></h4>
<p>Construct a SkipConnectionCore.</p>
<h5 id="args_228">Args:</h5>
<ul>
<li><code>base_core</code>: Base RNNCore to wrap.</li>
<li><code>input_shape</code>: Shape of the input as tuple, excluding the batch size.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h4 id="skipconnectioncore__call__inputs-prev_state-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=98"><code>SkipConnectionCore.__call__(inputs, prev_state, **kwargs)</code></a><a id="SkipConnectionCore.__call__" /></h4>
<h4 id="skipconnectioncoreconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>SkipConnectionCore.connected_subgraphs</code></a><a id="SkipConnectionCore.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="skipconnectioncoredefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>SkipConnectionCore.defun()</code></a><a id="SkipConnectionCore.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="skipconnectioncoredefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>SkipConnectionCore.defun_wrapped</code></a><a id="SkipConnectionCore.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="skipconnectioncoreget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>SkipConnectionCore.get_all_variables(collection='trainable_variables')</code></a><a id="SkipConnectionCore.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_229">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_471">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_435">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="skipconnectioncoreget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>SkipConnectionCore.get_possible_initializer_keys(cls)</code></a><a id="SkipConnectionCore.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_472">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="skipconnectioncoreget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>SkipConnectionCore.get_variables(collection='trainable_variables')</code></a><a id="SkipConnectionCore.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_230">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_473">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_436">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="skipconnectioncoregraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>SkipConnectionCore.graph</code></a><a id="SkipConnectionCore.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="skipconnectioncoreinitial_stateargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=126"><code>SkipConnectionCore.initial_state(*args, **kwargs)</code></a><a id="SkipConnectionCore.initial_state" /></h4>
<h4 id="skipconnectioncoreis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>SkipConnectionCore.is_connected</code></a><a id="SkipConnectionCore.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="skipconnectioncorelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>SkipConnectionCore.last_connected_subgraph</code></a><a id="SkipConnectionCore.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_474">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_437">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="skipconnectioncoremodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>SkipConnectionCore.module_name</code></a><a id="SkipConnectionCore.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="skipconnectioncorename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>SkipConnectionCore.name_scopes</code></a><a id="SkipConnectionCore.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="skipconnectioncorenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>SkipConnectionCore.non_trainable_variables</code></a><a id="SkipConnectionCore.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_475">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_438">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="skipconnectioncoreoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=108"><code>SkipConnectionCore.output_size</code></a><a id="SkipConnectionCore.output_size" /></h4>
<h4 id="skipconnectioncorescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>SkipConnectionCore.scope_name</code></a><a id="SkipConnectionCore.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="skipconnectioncorestate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=122"><code>SkipConnectionCore.state_size</code></a><a id="SkipConnectionCore.state_size" /></h4>
<h4 id="skipconnectioncoretrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>SkipConnectionCore.trainable_variables</code></a><a id="SkipConnectionCore.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_476">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_439">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="skipconnectioncorevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>SkipConnectionCore.variable_scope</code></a><a id="SkipConnectionCore.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_477">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_440">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="skipconnectioncorevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>SkipConnectionCore.variables</code></a><a id="SkipConnectionCore.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_478">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_441">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="skipconnectioncorezero_stateargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/residual.py?l=129"><code>SkipConnectionCore.zero_state(*args, **kwargs)</code></a><a id="SkipConnectionCore.zero_state" /></h4>
<h3 id="class-slicebydim"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?q=class:SliceByDim"><code>class SliceByDim</code></a><a id="SliceByDim" /></h3>
<p>Slices a tensor along specific dimensions.</p>
<p>The user can slice a tensor by specifying only the list of dimensions that
they want to slice, together with the lists of integers containing the
beginning indices of the slicing, and the size of the slices. Hence, with
<code>SliceByDim</code> slicing can be performed without knowing in advance the rank of
the input tensor.</p>
<p>Tensorflow also offers a built-in op performing slicing, <code>tf.slice</code>. However,
<code>tf.slice</code> requires all the slicing dimensions to be specified, using
wildcards when no slicing is required. For example, with <code>tf.slice</code>, slicing
half a 5D tensor along dimension <code>1</code> would be:</p>
<pre><code class="python">output = tf.slice(inputs,
                  begin=[0, 0, 0, 0, 0],
                  size=[-1, inputs.get_shape()[1].value//2, -1, -1, -1])
</code></pre>

<p>The same operation using <code>SliceByDim</code> would be:</p>
<pre><code class="python">output = SliceByDim(dims=[1], begin=[0], size=[x.get_shape()[1].value//2])(x)
</code></pre>

<p><code>SliceByDim</code> can be used to specify multiple slicing dimensions, for example:</p>
<pre><code class="python">output = SliceByDim(dims=[1, 3], begin=[0, 0], size=[12, 24])(x)
</code></pre>

<h4 id="slicebydim__init__dims-begin-size-nameslice_by_dim"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=1136"><code>SliceByDim.__init__(dims, begin, size, name='slice_by_dim')</code></a><a id="SliceByDim.__init__" /></h4>
<p>Constructs the <code>SliceByDim</code> module.</p>
<h5 id="args_231">Args:</h5>
<ul>
<li><code>dims</code>: The dimensions to slice along, as a list of unique integers.
      Negative integers index from the final dimension backwards, as in
      python arrays.</li>
<li><code>begin</code>: The beginning indices of the slicing, as a list of integers. Must
      be the same length as the <code>dims</code> list.</li>
<li><code>size</code>: The size of the slices, as a list of integers. Must be the same
      length as the <code>dims</code> list.</li>
<li><code>name</code>: The name of the module.</li>
</ul>
<h5 id="raises_442">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>dims</code> has non-unique integers, or if the size of <code>begin</code>
      is different from the size of <code>dims</code>, or if the size of <code>size</code> is
      different from the size of <code>dims</code>.</li>
</ul>
<h4 id="slicebydim__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=1167"><code>SliceByDim.__call__(inputs)</code></a><a id="SliceByDim.__call__" /></h4>
<p>Connects the SliceByDim module into the graph.</p>
<h5 id="args_232">Args:</h5>
<ul>
<li><code>inputs</code>: <code>Tensor</code> to slice. Its rank must be greater than the maximum
      dimension specified in <code>dims</code> (plus one as python is 0 indexed).</li>
</ul>
<h5 id="returns_479">Returns:</h5>
<p>The sliced tensor.</p>
<h5 id="raises_443">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>inputs</code> tensor has insufficient rank.</li>
</ul>
<h4 id="slicebydimconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>SliceByDim.connected_subgraphs</code></a><a id="SliceByDim.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="slicebydimdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>SliceByDim.defun()</code></a><a id="SliceByDim.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="slicebydimdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>SliceByDim.defun_wrapped</code></a><a id="SliceByDim.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="slicebydimget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>SliceByDim.get_all_variables(collection='trainable_variables')</code></a><a id="SliceByDim.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_233">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_480">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_444">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="slicebydimget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>SliceByDim.get_possible_initializer_keys(cls)</code></a><a id="SliceByDim.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_481">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="slicebydimget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>SliceByDim.get_variables(collection='trainable_variables')</code></a><a id="SliceByDim.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_234">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_482">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_445">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="slicebydimgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>SliceByDim.graph</code></a><a id="SliceByDim.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="slicebydimis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>SliceByDim.is_connected</code></a><a id="SliceByDim.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="slicebydimlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>SliceByDim.last_connected_subgraph</code></a><a id="SliceByDim.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_483">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_446">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="slicebydimmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>SliceByDim.module_name</code></a><a id="SliceByDim.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="slicebydimname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>SliceByDim.name_scopes</code></a><a id="SliceByDim.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="slicebydimnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>SliceByDim.non_trainable_variables</code></a><a id="SliceByDim.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_484">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_447">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="slicebydimscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>SliceByDim.scope_name</code></a><a id="SliceByDim.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="slicebydimtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>SliceByDim.trainable_variables</code></a><a id="SliceByDim.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_485">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_448">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="slicebydimvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>SliceByDim.variable_scope</code></a><a id="SliceByDim.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_486">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_449">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="slicebydimvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>SliceByDim.variables</code></a><a id="SliceByDim.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_487">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_450">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-tilebydim"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?q=class:TileByDim"><code>class TileByDim</code></a><a id="TileByDim" /></h3>
<p>Tile a tensor along specific dimensions.</p>
<p>The user can tile a tensor by specifying only the list of dimensions that
they want to tile, together with the lists of integers containing the
multiples of the tiling. Hence, with <code>TileByDim</code> tiling can be performed
without knowing in advance the rank of the input tensor.</p>
<p>Tensorflow also offers a built-in op performing tiling, <code>tf.tile</code>. However,
<code>tf.tile</code> requires all the tiling dimensions to be specified, using <code>1</code>
when no tiling is required. For example, with tf.tiling, tiling a 5D
tensor along dimension <code>1</code>, by <code>2</code> would be:</p>
<pre><code class="python">output = tf.tile(inputs, multiples=[1, 2, 1, 1, 1])
</code></pre>

<p>The same operation using <code>TileByDim</code> would be:</p>
<pre><code class="python">output = TileByDim(dims=[1], multiples=[2])(x)
</code></pre>

<p><code>TileByDim</code> can be used to specify multiple tiling dimensions, for example:</p>
<pre><code class="python">output = TileByDim(dims=[1, 3], multiples=[2, 4])(x)
</code></pre>

<h4 id="tilebydim__init__dims-multiples-nametile_by_dim"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=1230"><code>TileByDim.__init__(dims, multiples, name='tile_by_dim')</code></a><a id="TileByDim.__init__" /></h4>
<p>Constructs the <code>TileByDim</code> module.</p>
<h5 id="args_235">Args:</h5>
<ul>
<li><code>dims</code>: The dimensions to tile along, as a list of unique integers.</li>
<li><code>multiples</code>: The multiple of the tiling, as a list of integers. Must
      be the same length as the <code>dims</code> list.</li>
<li><code>name</code>: The name of the module.</li>
</ul>
<h5 id="raises_451">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>dims</code> has non-unique integers, or if the size of
      <code>multiples</code> is different from the size of <code>dims</code>.</li>
</ul>
<h4 id="tilebydim__call__inputs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=1252"><code>TileByDim.__call__(inputs)</code></a><a id="TileByDim.__call__" /></h4>
<p>Connects the <code>TileByDim</code> module into the graph.</p>
<h5 id="args_236">Args:</h5>
<ul>
<li><code>inputs</code>: <code>Tensor</code> to tile.</li>
</ul>
<h5 id="returns_488">Returns:</h5>
<p>The tiled tensor.</p>
<h4 id="tilebydimconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>TileByDim.connected_subgraphs</code></a><a id="TileByDim.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="tilebydimdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>TileByDim.defun()</code></a><a id="TileByDim.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="tilebydimdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>TileByDim.defun_wrapped</code></a><a id="TileByDim.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="tilebydimget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>TileByDim.get_all_variables(collection='trainable_variables')</code></a><a id="TileByDim.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_237">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_489">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_452">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="tilebydimget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>TileByDim.get_possible_initializer_keys(cls)</code></a><a id="TileByDim.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_490">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="tilebydimget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>TileByDim.get_variables(collection='trainable_variables')</code></a><a id="TileByDim.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_238">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_491">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_453">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="tilebydimgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>TileByDim.graph</code></a><a id="TileByDim.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="tilebydimis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>TileByDim.is_connected</code></a><a id="TileByDim.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="tilebydimlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>TileByDim.last_connected_subgraph</code></a><a id="TileByDim.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_492">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_454">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="tilebydimmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>TileByDim.module_name</code></a><a id="TileByDim.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="tilebydimname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>TileByDim.name_scopes</code></a><a id="TileByDim.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="tilebydimnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>TileByDim.non_trainable_variables</code></a><a id="TileByDim.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_493">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_455">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="tilebydimscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>TileByDim.scope_name</code></a><a id="TileByDim.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="tilebydimtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>TileByDim.trainable_variables</code></a><a id="TileByDim.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_494">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_456">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="tilebydimvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>TileByDim.variable_scope</code></a><a id="TileByDim.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_495">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_457">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="tilebydimvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>TileByDim.variables</code></a><a id="TileByDim.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_496">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_458">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-trainableinitialstate"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?q=class:TrainableInitialState"><code>class TrainableInitialState</code></a><a id="TrainableInitialState" /></h3>
<p>Helper Module that creates a learnable initial state for an RNNCore.</p>
<p>This class receives an example (possibly nested) initial state of an RNNCore,
and returns a state that has the same shape, structure, and values, but is
trainable. Additionally, the user may specify a boolean mask that
indicates which parts of the initial state should be trainable.</p>
<p>This allows users to train an unrolled RNNCore with a learnable initial state
in the following way:</p>
<pre><code>core = ... # Any RNNCore module object.
initial_state = core.initial_state(batch_size, dtype)
trainable_initial_state = snt.TrainableInitialState(initial_state)()
output, final_state = tf.nn.dynamic_rnn(
    core, input_sequence, initial_state=trainable_initial_state)
</code></pre>
<h4 id="trainableinitialstate__init__initial_state-masknone-nametrainable_initial_state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=298"><code>TrainableInitialState.__init__(initial_state, mask=None, name='trainable_initial_state')</code></a><a id="TrainableInitialState.__init__" /></h4>
<p>Constructs the Module that introduces a trainable state in the graph.</p>
<p>It receives an initial state that will be used as the initial values for the
trainable variables that the module contains, and optionally a mask that
indicates the parts of the initial state that should be learnable.</p>
<h5 id="args_239">Args:</h5>
<ul>
<li><code>initial_state</code>: tensor or arbitrarily nested iterables of tensors.</li>
<li><code>mask</code>: optional boolean mask. It should have the same nested structure as
   the given initial_state.</li>
<li><code>name</code>: module name.</li>
</ul>
<h5 id="raises_459">Raises:</h5>
<ul>
<li><code>TypeError</code>: if mask is not a list of booleans or None.</li>
</ul>
<h4 id="trainableinitialstate__call__"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=331"><code>TrainableInitialState.__call__()</code></a><a id="TrainableInitialState.__call__" /></h4>
<p>Connects the module to the graph.</p>
<h5 id="returns_497">Returns:</h5>
<p>The learnable state, which has the same type, structure and shape as
    the <code>initial_state</code> passed to the constructor.</p>
<h4 id="trainableinitialstateconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>TrainableInitialState.connected_subgraphs</code></a><a id="TrainableInitialState.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="trainableinitialstatedefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>TrainableInitialState.defun()</code></a><a id="TrainableInitialState.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="trainableinitialstatedefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>TrainableInitialState.defun_wrapped</code></a><a id="TrainableInitialState.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="trainableinitialstateget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>TrainableInitialState.get_all_variables(collection='trainable_variables')</code></a><a id="TrainableInitialState.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_240">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_498">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_460">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainableinitialstateget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>TrainableInitialState.get_possible_initializer_keys(cls)</code></a><a id="TrainableInitialState.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_499">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="trainableinitialstateget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>TrainableInitialState.get_variables(collection='trainable_variables')</code></a><a id="TrainableInitialState.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_241">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_500">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_461">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainableinitialstategraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>TrainableInitialState.graph</code></a><a id="TrainableInitialState.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="trainableinitialstateis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>TrainableInitialState.is_connected</code></a><a id="TrainableInitialState.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="trainableinitialstatelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>TrainableInitialState.last_connected_subgraph</code></a><a id="TrainableInitialState.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_501">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_462">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainableinitialstatemodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>TrainableInitialState.module_name</code></a><a id="TrainableInitialState.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="trainableinitialstatename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>TrainableInitialState.name_scopes</code></a><a id="TrainableInitialState.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="trainableinitialstatenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>TrainableInitialState.non_trainable_variables</code></a><a id="TrainableInitialState.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_502">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_463">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainableinitialstatescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>TrainableInitialState.scope_name</code></a><a id="TrainableInitialState.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="trainableinitialstatetrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>TrainableInitialState.trainable_variables</code></a><a id="TrainableInitialState.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_503">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_464">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainableinitialstatevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>TrainableInitialState.variable_scope</code></a><a id="TrainableInitialState.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_504">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_465">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainableinitialstatevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>TrainableInitialState.variables</code></a><a id="TrainableInitialState.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_505">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_466">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-trainablevariable"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?q=class:TrainableVariable"><code>class TrainableVariable</code></a><a id="TrainableVariable" /></h3>
<p>Provides learnable parameter Tensor.</p>
<h4 id="trainablevariable__init__shape-dtypetffloat32-initializersnone-partitionersnone-regularizersnone-custom_getternone-nametrainable_variable"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=937"><code>TrainableVariable.__init__(shape, dtype=tf.float32, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='trainable_variable')</code></a><a id="TrainableVariable.__init__" /></h4>
<p>Constructs a TrainableVariable module.</p>
<h5 id="args_242">Args:</h5>
<ul>
<li><code>shape</code>: Tensor shape.</li>
<li><code>dtype</code>: Tensor data type.</li>
<li><code>initializers</code>: Optional dictionary containing ops to initialize the weight
      Tensor, with key 'w'.</li>
<li><code>partitioners</code>: Optional dict containing a partitioner to partition
      the weight (with key 'w'). As a default, no partitioner is used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the weights
    (with key 'w'). As a default, no regularizers are used. A regularizer
    should be a function that takes a single <code>Tensor</code> as an input and
    returns a scalar <code>Tensor</code> output, e.g. the L1 and L2 regularizers in
    <code>tf.contrib.layers</code>.</li>
<li><code>custom_getter</code>: Optional callable or dictionary of callables to use as
    custom_getter for the module.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_467">Raises:</h5>
<ul>
<li><code>KeyError</code>: If <code>initializers</code> contains any keys other than 'w'.</li>
<li><code>KeyError</code>: If <code>partitioners</code> contains any keys other than 'w'.</li>
<li><code>KeyError</code>: If <code>regularizers</code> contains any keys other than 'w'.</li>
<li><code>TypeError</code>: If any of the given initializers are not callable.</li>
<li><code>TypeError</code>: If any of the given partitioners are not callable.</li>
<li><code>TypeError</code>: If any of the given regularizers are not callable.</li>
</ul>
<h4 id="trainablevariable__call__"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=983"><code>TrainableVariable.__call__()</code></a><a id="TrainableVariable.__call__" /></h4>
<p>Connects the TrainableTensor module into the graph.</p>
<h5 id="returns_506">Returns:</h5>
<p>A Tensor of shape as determined in the constructor.</p>
<h4 id="trainablevariableconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>TrainableVariable.connected_subgraphs</code></a><a id="TrainableVariable.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="trainablevariabledefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>TrainableVariable.defun()</code></a><a id="TrainableVariable.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="trainablevariabledefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>TrainableVariable.defun_wrapped</code></a><a id="TrainableVariable.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="trainablevariableget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>TrainableVariable.get_all_variables(collection='trainable_variables')</code></a><a id="TrainableVariable.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_243">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_507">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_468">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainablevariableget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>TrainableVariable.get_possible_initializer_keys(cls)</code></a><a id="TrainableVariable.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_508">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="trainablevariableget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>TrainableVariable.get_variables(collection='trainable_variables')</code></a><a id="TrainableVariable.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_244">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_509">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_469">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainablevariablegraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>TrainableVariable.graph</code></a><a id="TrainableVariable.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="trainablevariableis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>TrainableVariable.is_connected</code></a><a id="TrainableVariable.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="trainablevariablelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>TrainableVariable.last_connected_subgraph</code></a><a id="TrainableVariable.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_510">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_470">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainablevariablemodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>TrainableVariable.module_name</code></a><a id="TrainableVariable.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="trainablevariablename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>TrainableVariable.name_scopes</code></a><a id="TrainableVariable.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="trainablevariablenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>TrainableVariable.non_trainable_variables</code></a><a id="TrainableVariable.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_511">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_471">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainablevariablescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>TrainableVariable.scope_name</code></a><a id="TrainableVariable.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="trainablevariabletrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>TrainableVariable.trainable_variables</code></a><a id="TrainableVariable.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_512">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_472">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainablevariablevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>TrainableVariable.variable_scope</code></a><a id="TrainableVariable.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_513">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_473">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainablevariablevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>TrainableVariable.variables</code></a><a id="TrainableVariable.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_514">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_474">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="trainablevariablew"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=1001"><code>TrainableVariable.w</code></a><a id="TrainableVariable.w" /></h4>
<p>Returns the Variable containing the weights Tensor.</p>
<h5 id="returns_515">Returns:</h5>
<p>Variable object containing the weights, from the most recent <strong>call</strong>.</p>
<h5 id="raises_475">Raises:</h5>
<p>base.Error: If the module has not been connected to the graph yet,
      meaning the variables do not exist.</p>
<h3 id="class-transposable"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?q=class:Transposable"><code>class Transposable</code></a><a id="Transposable" /></h3>
<p>Transposable module interface.</p>
<p>The Transposable interface requires that transposable modules implement
a method called <code>transpose</code>, returning a module that is the transposed
version of the one the method is called on.
Calling the method twice should return a module with the same specifications
as the original module.</p>
<p>When implementing a transposable module, special care is required to make
sure that parameters needed to instantiate the module are provided as
functions whose invocation is deferred to graph construction time.</p>
<p>For example, in Linear we might want to call:</p>
<pre><code class="python">linear = snt.Linear(name=&quot;linear&quot;, output_size=output_size)
linear_transpose = linear.transpose()
</code></pre>

<p>where the output_size for linear_transpose is not known yet, as linear is
not yet connected to the graph: output_size is passed to linear_transpose's
constructor as a lambda returning linear.input_size. The lambda will return
the correct value once linear is given an input.
Notice that linear_transpose's output_size value does not need to be defined
until the module is connected to the graph.</p>
<h4 id="transposableinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=756"><code>Transposable.input_shape()</code></a><a id="Transposable.input_shape" /></h4>
<p>Returns shape of input <code>Tensor</code> passed at last call to <code>build</code>.</p>
<h4 id="transposabletransposenamenone-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=744"><code>Transposable.transpose(name=None, **kwargs)</code></a><a id="Transposable.transpose" /></h4>
<p>Builds and returns transposed version of module.</p>
<h5 id="args_245">Args:</h5>
<ul>
<li><code>name</code>: Name of the transposed module.</li>
<li><code>**kwargs</code>: Additional Python flags controlling transposition.</li>
</ul>
<h5 id="returns_516">Returns:</h5>
<p>Transposed version of the module.</p>
<h3 id="class-underspecifiederror"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base_errors.py?q=class:UnderspecifiedError"><code>class UnderspecifiedError</code></a><a id="UnderspecifiedError" /></h3>
<p>Error raised when too little information is available.</p>
<p>This does not typically mean the user is trying to do something that doesn't
work (in which case <code>IncompatibleShapeError</code> should be used), just that
some more information needs to be provided in order to build the Graph.</p>
<h3 id="class-vanillarnn"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?q=class:VanillaRNN"><code>class VanillaRNN</code></a><a id="VanillaRNN" /></h3>
<p>Basic fully connected vanilla RNN core.</p>
<h4 id="vanillarnn__init__hidden_size-activationtanh-initializersnone-partitionersnone-regularizersnone-namevanilla_rnn"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=71"><code>VanillaRNN.__init__(hidden_size, activation=tanh, initializers=None, partitioners=None, regularizers=None, name='vanilla_rnn')</code></a><a id="VanillaRNN.__init__" /></h4>
<p>Construct a Basic RNN core.</p>
<h5 id="args_246">Args:</h5>
<ul>
<li><code>hidden_size</code>: hidden size dimensionality.</li>
<li><code>activation</code>: activation function to use.</li>
<li><code>initializers</code>: optional dict containing ops to initialize the weights. This
    dictionary may contain the keys 'in_to_hidden' and/or
    'hidden_to_hidden'.</li>
<li><code>partitioners</code>: optional dict containing ops to partition the weights. This
    dictionary may contain the keys 'in_to_hidden' and/or
    'hidden_to_hidden'.</li>
<li><code>regularizers</code>: optional dict containing ops to regularize the weights. This
    dictionary may contain the keys 'in_to_hidden' and/or
    'hidden_to_hidden'.</li>
<li><code>name</code>: name of the module.</li>
</ul>
<h5 id="raises_476">Raises:</h5>
<ul>
<li><code>KeyError</code>: if <code>initializers</code> contains any keys other than 'in_to_hidden' or
    'hidden_to_hidden'.</li>
<li><code>KeyError</code>: if <code>partitioners</code> contains any keys other than 'in_to_hidden' or
    'hidden_to_hidden'.</li>
<li><code>KeyError</code>: if <code>regularizers</code> contains any keys other than 'in_to_hidden' or
    'hidden_to_hidden'.</li>
<li><code>TypeError</code>: If any of the given initializers are not callable.</li>
<li><code>TypeError</code>: If any of the given partitioners are not callable.</li>
<li><code>TypeError</code>: If any of the given regularizers are not callable.</li>
</ul>
<h4 id="vanillarnn__call__input_-prev_state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=110"><code>VanillaRNN.__call__(input_, prev_state)</code></a><a id="VanillaRNN.__call__" /></h4>
<p>Connects the VanillaRNN module into the graph.</p>
<p>If this is not the first time the module has been connected to the graph,
the Tensors provided as input_ and state must have the same final
dimension, in order for the existing variables to be the correct size for
their corresponding multiplications. The batch size may differ for each
connection.</p>
<h5 id="args_247">Args:</h5>
<ul>
<li><code>input_</code>: a 2D Tensor of size [batch_size, input_size].</li>
<li><code>prev_state</code>: a 2D Tensor of size [batch_size, hidden_size].</li>
</ul>
<h5 id="returns_517">Returns:</h5>
<ul>
<li><code>output</code>: a 2D Tensor of size [batch_size, hidden_size].</li>
<li><code>next_state</code>: a Tensor of size [batch_size, hidden_size].</li>
</ul>
<h5 id="raises_477">Raises:</h5>
<ul>
<li><code>ValueError</code>: if connecting the module into the graph any time after the
    first time, and the inferred size of the inputs does not match previous
    invocations.</li>
</ul>
<h4 id="vanillarnnconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>VanillaRNN.connected_subgraphs</code></a><a id="VanillaRNN.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="vanillarnndefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>VanillaRNN.defun()</code></a><a id="VanillaRNN.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="vanillarnndefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>VanillaRNN.defun_wrapped</code></a><a id="VanillaRNN.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="vanillarnnget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>VanillaRNN.get_all_variables(collection='trainable_variables')</code></a><a id="VanillaRNN.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_248">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_518">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_478">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="vanillarnnget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>VanillaRNN.get_possible_initializer_keys(cls)</code></a><a id="VanillaRNN.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_519">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="vanillarnnget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>VanillaRNN.get_variables(collection='trainable_variables')</code></a><a id="VanillaRNN.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_249">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_520">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_479">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="vanillarnngraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>VanillaRNN.graph</code></a><a id="VanillaRNN.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="vanillarnnhidden_to_hidden_linear"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=156"><code>VanillaRNN.hidden_to_hidden_linear</code></a><a id="VanillaRNN.hidden_to_hidden_linear" /></h4>
<h4 id="vanillarnnhidden_to_hidden_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=166"><code>VanillaRNN.hidden_to_hidden_variables</code></a><a id="VanillaRNN.hidden_to_hidden_variables" /></h4>
<h4 id="vanillarnnin_to_hidden_linear"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=151"><code>VanillaRNN.in_to_hidden_linear</code></a><a id="VanillaRNN.in_to_hidden_linear" /></h4>
<h4 id="vanillarnnin_to_hidden_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=161"><code>VanillaRNN.in_to_hidden_variables</code></a><a id="VanillaRNN.in_to_hidden_variables" /></h4>
<h4 id="vanillarnninitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=203"><code>VanillaRNN.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)</code></a><a id="VanillaRNN.initial_state" /></h4>
<p>Builds the default start state for an RNNCore.</p>
<h5 id="args_250">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, or scalar int32 Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.
    Note that intializers and regularizers will be ignored if
    <code>trainable=False</code>.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. As a
    default, no regularizers are used. A regularizer should be a function
    that takes a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code>
    output, e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_521">Returns:</h5>
<p>A tensor or nested tuple of tensors with same structure and shape as the
  <code>state_size</code> property of the core.</p>
<h5 id="raises_480">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the user passes initializers that are not functions.</li>
<li><code>ValueError</code>: if the user passes regularizers that are not functions.</li>
</ul>
<h4 id="vanillarnnis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>VanillaRNN.is_connected</code></a><a id="VanillaRNN.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="vanillarnnlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>VanillaRNN.last_connected_subgraph</code></a><a id="VanillaRNN.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_522">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_481">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="vanillarnnmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>VanillaRNN.module_name</code></a><a id="VanillaRNN.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="vanillarnnname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>VanillaRNN.name_scopes</code></a><a id="VanillaRNN.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="vanillarnnnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>VanillaRNN.non_trainable_variables</code></a><a id="VanillaRNN.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_523">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_482">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="vanillarnnoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=175"><code>VanillaRNN.output_size</code></a><a id="VanillaRNN.output_size" /></h4>
<h4 id="vanillarnnscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>VanillaRNN.scope_name</code></a><a id="VanillaRNN.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="vanillarnnstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic_rnn.py?l=171"><code>VanillaRNN.state_size</code></a><a id="VanillaRNN.state_size" /></h4>
<h4 id="vanillarnntrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>VanillaRNN.trainable_variables</code></a><a id="VanillaRNN.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_524">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_483">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="vanillarnnvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>VanillaRNN.variable_scope</code></a><a id="VanillaRNN.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_525">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_484">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="vanillarnnvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>VanillaRNN.variables</code></a><a id="VanillaRNN.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_526">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_485">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="vanillarnnzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>VanillaRNN.zero_state(batch_size, dtype)</code></a><a id="VanillaRNN.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_251">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_527">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="check_initializersinitializers-keys"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=142"><code>check_initializers(initializers, keys)</code></a><a id="check_initializers" /></h3>
<p>Checks the given initializers.</p>
<p>This checks that <code>initializers</code> is a dictionary that only contains keys in
<code>keys</code>, and furthermore the entries in <code>initializers</code> are functions or
further dictionaries (the latter used, for example, in passing initializers
to modules inside modules) that must satisfy the same constraints.</p>
<h5 id="args_252">Args:</h5>
<ul>
<li><code>initializers</code>: Dictionary of initializers (allowing nested dictionaries) or
    None.</li>
<li><code>keys</code>: Iterable of valid keys for <code>initializers</code>.</li>
</ul>
<h5 id="returns_528">Returns:</h5>
<p>Copy of checked dictionary of initializers. If <code>initializers=None</code>, an empty
  dictionary will be returned.</p>
<h5 id="raises_486">Raises:</h5>
<ul>
<li><code>KeyError</code>: If an initializer is provided for a key not in <code>keys</code>.</li>
<li><code>TypeError</code>: If a provided initializer is not a callable function, or
    <code>initializers</code> is not a Mapping.</li>
</ul>
<h3 id="check_partitionerspartitioners-keys"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=183"><code>check_partitioners(partitioners, keys)</code></a><a id="check_partitioners" /></h3>
<p>Checks the given partitioners.</p>
<p>This checks that <code>partitioners</code> is a dictionary that only contains keys in
<code>keys</code>, and furthermore the entries in <code>partitioners</code> are functions or
further dictionaries (the latter used, for example, in passing partitioners
to modules inside modules) that must satisfy the same constraints.</p>
<h5 id="args_253">Args:</h5>
<ul>
<li><code>partitioners</code>: Dictionary of partitioners (allowing nested dictionaries) or
      None.</li>
<li><code>keys</code>: Iterable of valid keys for <code>partitioners</code>.</li>
</ul>
<h5 id="returns_529">Returns:</h5>
<p>Checked dictionary of partitioners. If <code>partitioners=None</code>, an empty
  dictionary will be returned.</p>
<h5 id="raises_487">Raises:</h5>
<ul>
<li><code>KeyError</code>: If an partitioner is provided for a key not in <code>keys</code>.</li>
<li><code>TypeError</code>: If a provided partitioner is not a callable function, or
    <code>partitioners</code> is not a Mapping.</li>
</ul>
<h3 id="check_regularizersregularizers-keys"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=224"><code>check_regularizers(regularizers, keys)</code></a><a id="check_regularizers" /></h3>
<p>Checks the given regularizers.</p>
<p>This checks that <code>regularizers</code> is a dictionary that only contains keys in
<code>keys</code>, and furthermore the entries in <code>regularizers</code> are functions or
further dictionaries (the latter used, for example, in passing regularizers
to modules inside modules) that must satisfy the same constraints.</p>
<h5 id="args_254">Args:</h5>
<ul>
<li><code>regularizers</code>: Dictionary of regularizers (allowing nested dictionaries) or
    None.</li>
<li><code>keys</code>: Iterable of valid keys for <code>regularizers</code>.</li>
</ul>
<h5 id="returns_530">Returns:</h5>
<p>Copy of checked dictionary of regularizers. If <code>regularizers=None</code>, an empty
  dictionary will be returned.</p>
<h5 id="raises_488">Raises:</h5>
<ul>
<li><code>KeyError</code>: If an regularizers is provided for a key not in <code>keys</code>.</li>
<li><code>TypeError</code>: If a provided regularizer is not a callable function, or
    <code>regularizers</code> is not a Mapping.</li>
</ul>
<h3 id="clip_gradientnet-clip_value_min-clip_value_max-namenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/clip_gradient.py?l=62"><code>clip_gradient(net, clip_value_min, clip_value_max, name=None)</code></a><a id="clip_gradient" /></h3>
<p>Clips respective gradients of a given tensor.</p>
<p>Acts as identity for the forward pass, but clips gradient tensor element-wise
by value during the backward pass. Any gradient values less than
<code>clip_value_min</code> or greater than <code>clip_values_max</code> are set to the respective
limit values.</p>
<h5 id="args_255">Args:</h5>
<ul>
<li><code>net</code>: A <code>tf.Tensor</code>.</li>
<li><code>clip_value_min</code>: A 0-D Tensor or scalar. The minimum value to clip by.</li>
<li><code>clip_value_max</code>: A 0-D Tensor or scalar. The maximum value to clip by.</li>
<li><code>name</code>: A name for the operation (optional, default 'clip_gradient').</li>
</ul>
<h5 id="returns_531">Returns:</h5>
<p>A <code>tf.Tensor</code> with the same type as the input tensor.</p>
<h5 id="raises_489">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>net</code> dtype is non-float.</li>
</ul>
<h3 id="count_variables_by_typevariablesnone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=632"><code>count_variables_by_type(variables=None)</code></a><a id="count_variables_by_type" /></h3>
<p>Returns a dict mapping dtypes to number of variables and scalars.</p>
<h5 id="args_256">Args:</h5>
<ul>
<li><code>variables</code>: iterable of <code>tf.Variable</code>s, or None. If None is passed, then all
    global and local variables in the current graph are used.</li>
</ul>
<h5 id="returns_532">Returns:</h5>
<p>A dict mapping tf.dtype keys to a dict containing the keys 'num_scalars' and
    'num_variables'.</p>
<h3 id="custom_getter_routercustom_getter_map-name_fn"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=302"><code>custom_getter_router(custom_getter_map, name_fn)</code></a><a id="custom_getter_router" /></h3>
<p>Creates a custom getter than matches requests to dict of custom getters.</p>
<p>Custom getters are callables which implement the
[custom getter API]
(https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/get_variable).</p>
<p>The returned custom getter dispatches calls based on pattern matching the
name of the requested variable to the keys of custom_getter_map. For example,</p>
<pre><code>{
  ".*/w": snt.custom_getters.stop_gradient,
}
</code></pre>
<p>will match all variables named with the suffix "/w". The <code>name_fn</code> is
provided to allow processing of the name, such as stripping off a scope prefix
before matching.</p>
<h5 id="args_257">Args:</h5>
<ul>
<li><code>custom_getter_map</code>: Mapping of regular expressions to custom getter
    functions.</li>
<li><code>name_fn</code>: Callable to map variable name through before matching to regular
    expressions. This might, for example, strip off a scope prefix.</li>
</ul>
<h5 id="returns_533">Returns:</h5>
<p>A custom getter.</p>
<h5 id="raises_490">Raises:</h5>
<ul>
<li><code>TypeError</code>: If an entry in <code>custom_getter_map</code> is not a callable function.</li>
</ul>
<h3 id="deprecation_warningdeprecation_message"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=971"><code>deprecation_warning(deprecation_message)</code></a><a id="deprecation_warning" /></h3>
<p>Log a warning message the user is using deprecated functionality.</p>
<h3 id="format_variable_mapvariable_map-join_linestrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=552"><code>format_variable_map(variable_map, join_lines=True)</code></a><a id="format_variable_map" /></h3>
<p>Takes a key-to-variable map and formats it as a table.</p>
<h3 id="format_variablesvariables-join_linestrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=536"><code>format_variables(variables, join_lines=True)</code></a><a id="format_variables" /></h3>
<p>Takes a collection of variables and formats it as a table.</p>
<h3 id="get_lagrange_multipliershape-rate10-initializer10-maximizetrue-valid_rangenone-namelagrange_multiplier"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/optimization_constraints.py?l=227"><code>get_lagrange_multiplier(shape=(), rate=1.0, initializer=1.0, maximize=True, valid_range=None, name='lagrange_multiplier')</code></a><a id="get_lagrange_multiplier" /></h3>
<p>Lagrange multiplier factory.</p>
<p>This factory returns ops that help setting up constrained optimization
problems in Tensorflow. Given a constraint function op (either scalar or
vectorial), use this function to instantiate a Lagrange multiplier op, then
dot product the two and add them to the loss that is being optimized over.
There is no need to instantiate a second optimizer to solve the minmax
problem, as the Lagrange Multiplier op is setup to manipulate its own
gradients so that a single optmizer can be used to update all the variables
correctly.</p>
<h5 id="args_258">Args:</h5>
<ul>
<li><code>shape</code>: Lagrange multipliers can be used with both scalar and vector
      constraint functions; when using vector constraints use the shape kwarg
      to pass in shape information and instantiate variables of the correct
      shape.</li>
<li><code>rate</code>: Scalar used to scale the magnitude of gradients of the Lagrange
      multipliers, defaulting to 1e-2. Using the default value will make the
      Lagrange multipliers updates slower compared to the ones for the model's
      parameters.</li>
<li><code>initializer</code>: Initializer for the Lagrange multipliers. Note that
      when using inequality constraints the initial value of the multiplier
      will be transformed via the parametrization function.</li>
<li><code>maximize</code>: Boolean, True if we want to maximize the loss w.r.t. the Lagrange
      multipliers, False otherwise.</li>
<li><code>valid_range</code>: tuple, or list. of values used to clip the value of the
      (possibly reparametrized) Lagrange multipliers.</li>
<li><code>name</code>: Name of the Lagrange multiplier op.</li>
</ul>
<h5 id="returns_534">Returns:</h5>
<p>An op to be inserted in the graph, by multipling it with a constraint op
      and adding the resulting op to a loss. The Lagrange multiplier
      gradients are modified to that by calling minimize on the loss the
      optimizer will actually minimize w.r.t. to the model's parameters and
      maximize w.r.t. the Lagrande multipliers, hence enforcing the
      constraints.</p>
<h5 id="raises_491">Raises:</h5>
<ul>
<li><code>ValueError</code>: If the Lagrange multiplier is set to enforce an equality
      constraint and a parametrization function is also provided.</li>
</ul>
<h3 id="get_normalized_variable_mapscope_or_module-collectionvariables-contextnone-group_sliced_variablestrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=373"><code>get_normalized_variable_map(scope_or_module, collection='variables', context=None, group_sliced_variables=True)</code></a><a id="get_normalized_variable_map" /></h3>
<p>Builds map of <code>tf.Variable</code>s in scope or module with normalized names.</p>
<p>The names of the variables are normalized to remove the scope prefix.</p>
<h5 id="args_259">Args:</h5>
<ul>
<li><code>scope_or_module</code>: Scope or module to build map from.</li>
<li><code>collection</code>: Collection to restrict query to. By default this is
      <code>tf.Graphkeys.GLOBAL_VARIABLES</code>, which includes non-trainable variables
      such as moving averages.</li>
<li><code>context</code>: Scope or module, identical to or parent of <code>scope</code>. If given, this
      will be used as the stripped prefix. By default <code>None</code>, which means
      <code>context=scope</code>.</li>
<li><code>group_sliced_variables</code>: Boolean, if set to True, sliced variables are
     grouped together in the returned map; if set to False, each partition of
     a sliced variable is a separate (key, value) pair.</li>
</ul>
<h5 id="returns_535">Returns:</h5>
<p>Dictionary mapping normalized variable name to <code>tf.Variable</code>, or a list
      of <code>tf.Variables</code> if the variable is a sliced (partitioned) variable.</p>
<h5 id="raises_492">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>context</code> is given but is not a proper prefix of <code>scope</code>.</li>
</ul>
<h3 id="get_saverscope-collectionsvariables-contextnone-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=430"><code>get_saver(scope, collections=('variables',), context=None, **kwargs)</code></a><a id="get_saver" /></h3>
<p>Builds a <code>tf.train.Saver</code> for the scope or module, with normalized names.</p>
<p>The names of the variables are normalized to remove the scope prefix.
This allows the same variables to be restored into another similar scope or
module using a complementary <code>tf.train.Saver</code> object.</p>
<h5 id="args_260">Args:</h5>
<ul>
<li><code>scope</code>: Scope or module. Variables within will be saved or restored.</li>
<li><code>collections</code>: Sequence of collections of variables to restrict
      <code>tf.train.Saver</code> to. By default this is <code>tf.GraphKeys.GLOBAL_VARIABLES</code>
      which includes moving averages variables as well as trainable variables.</li>
<li><code>context</code>: Scope or module, identical to or parent of <code>scope</code>. If given, this
      will be used as the stripped prefix.</li>
<li><code>**kwargs</code>: Extra keyword arguments to pass to tf.train.Saver.</li>
</ul>
<h5 id="returns_536">Returns:</h5>
<pre><code>A `tf.train.Saver` object for Variables in the scope or module.
</code></pre>
<h3 id="get_variables_in_modulemodule-collectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=86"><code>get_variables_in_module(module, collection='trainable_variables')</code></a><a id="get_variables_in_module" /></h3>
<p>Returns tuple of <code>tf.Variable</code>s declared inside an <code>snt.Module</code>.</p>
<p>Note that this operates by searching the variable scope a module contains,
and so does not know about any modules which were constructed elsewhere but
used inside this module.</p>
<h5 id="args_261">Args:</h5>
<ul>
<li><code>module</code>: <code>snt.Module</code> instance to query the scope of.</li>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_537">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_493">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="get_variables_in_scopescope-collectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=63"><code>get_variables_in_scope(scope, collection='trainable_variables')</code></a><a id="get_variables_in_scope" /></h3>
<p>Returns a tuple <code>tf.Variable</code>s in a scope for a given collection.</p>
<h5 id="args_262">Args:</h5>
<ul>
<li><code>scope</code>: <code>tf.VariableScope</code> or string to retrieve variables from.</li>
<li><code>collection</code>: Collection to restrict query to. By default this is
      <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
      variables such as moving averages.</li>
</ul>
<h5 id="returns_538">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h3 id="has_variable_scopeobj"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=458"><code>has_variable_scope(obj)</code></a><a id="has_variable_scope" /></h3>
<p>Determines whether the given object has a variable scope.</p>
<h3 id="highway_core_with_recurrent_dropouthidden_size-num_layers-keep_prob05-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1749"><code>highway_core_with_recurrent_dropout(hidden_size, num_layers, keep_prob=0.5, **kwargs)</code></a><a id="highway_core_with_recurrent_dropout" /></h3>
<p>Highway core with recurrent dropout.</p>
<h5 id="args_263">Args:</h5>
<ul>
<li><code>hidden_size</code>: (int) Hidden size dimensionality.</li>
<li><code>num_layers</code>: (int) Number of highway layers.</li>
<li><code>keep_prob</code>: the probability to keep an entry when applying dropout.</li>
<li><code>**kwargs</code>: Extra keyword arguments to pass to the highway core.</li>
</ul>
<h5 id="returns_539">Returns:</h5>
<p>A tuple (train_core, test_core) where train_core is a higway core with
  recurrent dropout enabled to be used for training and test_core is the
  same highway core without recurrent dropout.</p>
<h3 id="log_variablesvariablesnone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=567"><code>log_variables(variables=None)</code></a><a id="log_variables" /></h3>
<p>Logs variable information.</p>
<p>This function logs the name, shape, type, collections, and device for either
all variables or a given iterable of variables. In the "Device" columns,
the nature of the variable (legacy or resource (for ResourceVariables)) is
also specified in parenthesis.</p>
<h5 id="args_264">Args:</h5>
<ul>
<li><code>variables</code>: iterable of variables; if not provided, then all variables
      (in the default graph) are logged.</li>
</ul>
<h3 id="lstm_with_recurrent_dropouthidden_size-keep_prob05-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=433"><code>lstm_with_recurrent_dropout(hidden_size, keep_prob=0.5, **kwargs)</code></a><a id="lstm_with_recurrent_dropout" /></h3>
<p>LSTM with recurrent dropout.</p>
<h5 id="args_265">Args:</h5>
<ul>
<li><code>hidden_size</code>: the LSTM hidden size.</li>
<li><code>keep_prob</code>: the probability to keep an entry when applying dropout.</li>
<li><code>**kwargs</code>: Extra keyword arguments to pass to the LSTM.</li>
</ul>
<h5 id="returns_540">Returns:</h5>
<p>A tuple (train_lstm, test_lstm) where train_lstm is an LSTM with
  recurrent dropout enabled to be used for training and test_lstm
  is the same LSTM without recurrent dropout.</p>
<h3 id="lstm_with_zoneouthidden_size-keep_prob_c05-keep_prob_h095-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=519"><code>lstm_with_zoneout(hidden_size, keep_prob_c=0.5, keep_prob_h=0.95, **kwargs)</code></a><a id="lstm_with_zoneout" /></h3>
<p>LSTM with recurrent dropout.</p>
<h5 id="args_266">Args:</h5>
<ul>
<li><code>hidden_size</code>: the LSTM hidden size.</li>
<li><code>keep_prob_c</code>: the probability to use the new value of the cell state rather
    than freezing it.</li>
<li><code>keep_prob_h</code>: the probability to use the new value of the hidden state
    rather than freezing it.</li>
<li><code>**kwargs</code>: Extra keyword arguments to pass to the LSTM.</li>
</ul>
<h5 id="returns_541">Returns:</h5>
<p>A tuple (train_lstm, test_lstm) where train_lstm is an LSTM with
  recurrent dropout enabled to be used for training and test_lstm
  is the same LSTM without zoneout.</p>
<h3 id="merge_leading_dimsarray_or_tensor-n_dims2"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=38"><code>merge_leading_dims(array_or_tensor, n_dims=2)</code></a><a id="merge_leading_dims" /></h3>
<p>Merge the first dimensions of a tensor.</p>
<h5 id="args_267">Args:</h5>
<ul>
<li><code>array_or_tensor</code>: Tensor to have its first dimensions merged. Can also
      be an array or numerical value, which will be converted to a tensor
      for batch application, if needed.</li>
<li><code>n_dims</code>: Number of dimensions to merge.</li>
</ul>
<h5 id="returns_542">Returns:</h5>
<p>Either the input value converted to a Tensor, with the requested dimensions
  merged, or the unmodified input value if the input has less than <code>n_dims</code>
  dimensions.</p>
<h5 id="raises_494">Raises:</h5>
<ul>
<li><code>ValueError</code>: If the rank of <code>array_or_tensor</code> is not well-defined.</li>
</ul>
<h3 id="observe_connectionsobserver"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=82"><code>observe_connections(observer)</code></a><a id="observe_connections" /></h3>
<p>Notifies the observer whenever any Sonnet module is connected to the graph.</p>
<p>If a module contains nested modules, the observer is notified once for each
nested module, followed by the containing module.</p>
<p>For example:</p>
<pre><code class="python">def logging_observer(connected_subgraph):
  logging.info(connected_subgraph.module.module_name)

with snt.observe_connections(logging_observer):
  output = imagenet_module(input_tensor)
</code></pre>

<h5 id="args_268">Args:</h5>
<ul>
<li><code>observer</code>: Callable accepting a single argument. Will be called with a
  <code>ConnectedSubGraph</code> each time a module is connected to the graph.</li>
</ul>
<h5 id="yields">Yields:</h5>
<ul>
<li><code>None</code>: just yields control to the inner context.</li>
</ul>
<h3 id="parse_string_to_constructorctor_string"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=986"><code>parse_string_to_constructor(ctor_string)</code></a><a id="parse_string_to_constructor" /></h3>
<p>Returns a callable which corresponds to the constructor string.</p>
<p>Various modules (eg, ConvNet2D) take constructor arguments which are
callables, indicating a submodule to build. These can be passed as
actual constructors, eg <code>snt.LayerNorm</code>, however that makes the config
for that module not trivially serializable. This function tries to map
a string representation to the underlying callable, allowing configs to
remain serializable where necessary.</p>
<h5 id="args_269">Args:</h5>
<ul>
<li><code>ctor_string</code>: string representing some module in Sonnet. If the string is
    provided with no dots, we assume it is a member of Sonnet available at
    top level, i.e. "LayerNorm" maps to <code>snt.LayerNorm</code>.</li>
</ul>
<h5 id="raises_495">Raises:</h5>
<ul>
<li><code>ValueError</code>: if no matching constructor can be found.</li>
</ul>
<h5 id="returns_543">Returns:</h5>
<p>Callable constructor which corresponds to <code>ctor_string</code>.</p>
<h3 id="remove_unsupported_kwargsmodule_or_fn-all_kwargs_dict"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=1104"><code>remove_unsupported_kwargs(module_or_fn, all_kwargs_dict)</code></a><a id="remove_unsupported_kwargs" /></h3>
<p>Removes any kwargs not supported by <code>module_or_fn</code> from <code>all_kwargs_dict</code>.</p>
<p>A new dict is return with shallow copies of keys &amp; values from
<code>all_kwargs_dict</code>, as long as the key is accepted by module_or_fn. The
returned dict can then be used to connect <code>module_or_fn</code> (along with some
other inputs, ie non-keyword arguments, in general).</p>
<p><code>snt.supports_kwargs</code> is used to tell whether a given kwarg is supported. Note
that this method may give false negatives, which would lead to extraneous
removals in the result of this function. Please read the docstring for
<code>snt.supports_kwargs</code> for details, and manually inspect the results from this
function if in doubt.</p>
<h5 id="args_270">Args:</h5>
<ul>
<li><code>module_or_fn</code>: some callable which can be interrogated by
    <code>snt.supports_kwargs</code>. Generally a Sonnet module or a method (wrapped in
    <code>@reuse_variables</code>) of a Sonnet module.</li>
<li><code>all_kwargs_dict</code>: a dict containing strings as keys, or None.</li>
</ul>
<h5 id="raises_496">Raises:</h5>
<ul>
<li><code>ValueError</code>: if <code>all_kwargs_dict</code> is not a dict.</li>
</ul>
<h5 id="returns_544">Returns:</h5>
<p>A dict containing some subset of the keys and values in <code>all_kwargs_dict</code>.
  This subset may be empty. If <code>all_kwargs_dict</code> is None, this will be an
  empty dict.</p>
<h3 id="reuse_variablesmethod"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=662"><code>reuse_variables(method)</code></a><a id="reuse_variables" /></h3>
<p>Wraps an arbitrary method so it does variable sharing.</p>
<p>This decorator creates variables the first time it calls <code>method</code>, and reuses
them for subsequent calls. The object that calls <code>method</code> provides a
<code>tf.VariableScope</code>, either as a <code>variable_scope</code> attribute or as the return
value of an <code>_enter_variable_scope()</code> method.</p>
<p>The first time the wrapped method is invoked, it enters the caller's
<code>tf.VariableScope</code> with <code>reuse=False</code>. On all subsequent calls it enters the
same variable scope with <code>reuse=True</code>.</p>
<p>Variables are created in the context of the <code>tf.VariableScope</code> provided by the
caller object. Ops are created with an additional <code>tf.name_scope()</code>, which
adds a scope for the wrapped method name. For example:</p>
<pre><code class="python">class MyClass(object):

  def __init__(self, name):
    with tf.variable_scope(None, default_name=name) as variable_scope:
      self.variable_scope = variable_scope

  @snt.reuse_variables
  def add_x(self, tensor):
    x = tf.get_variable(&quot;x&quot;, shape=tensor.get_shape())
    return tensor + x

module = MyClass(&quot;my_module_name&quot;)
input_tensor = tf.zeros(shape=(5,))

# This creates the variable &quot;my_module_name/x&quot;
# and op &quot;my_module_name/add_x/add&quot;
output = module.add_x(input_tensor)
</code></pre>

<p>For performance when executing eagerly it may be desirable to additionally
annotate these methods using <code>defun</code>, such that they are encapsulated as
graph functions. This is not recommended if your method returns a variable
since the output of <code>defun</code> would be an op that returned the variable's value
when evaluated (rather than the variable instance).</p>
<pre><code class="python">class FooModule(snt.AbstractModule):
  def _build(self, inputs):
    return complex_math(inputs)

  @tfe.defun
  @snt.reuse_variables
  def more_complex_stuff(self, inputs):
    return more_complex_math(inputs)
</code></pre>

<h5 id="args_271">Args:</h5>
<ul>
<li><code>method</code>: The method to wrap.</li>
</ul>
<h5 id="returns_545">Returns:</h5>
<p>The wrapped method.</p>
<h3 id="scale_gradientnet-scale-namescale_gradient"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/scale_gradient.py?l=64"><code>scale_gradient(net, scale, name='scale_gradient')</code></a><a id="scale_gradient" /></h3>
<p>Scales gradients for the backwards pass.</p>
<p>This might be used to, for example, allow one part of a model to learn at a
lower rate than the rest.</p>
<p>WARNING: Think carefully about how your optimizer works. If, for example, you
use rmsprop, the gradient is always rescaled (with some additional epsilon)
towards unity. This means <code>scale_gradient</code> won't have the effect of
lowering the learning rate.</p>
<p>If <code>scale</code> is <code>0.0</code>, this op reduces to <code>tf.stop_gradient</code>. If <code>scale</code>
is <code>1.0</code>, this op reduces to <code>tf.identity</code>.</p>
<h5 id="args_272">Args:</h5>
<ul>
<li><code>net</code>: A <code>tf.Tensor</code> or in eager mode a callable that produces a <code>tf.Tensor</code>.</li>
<li><code>scale</code>: The scale factor for the gradient on the backwards pass.</li>
<li><code>name</code>: A name for the operation (optional).</li>
</ul>
<h5 id="returns_546">Returns:</h5>
<p>In graph mode returns a <code>tf.Tensor</code> with the same type as the input tensor.
  In eager mode returns a callable wrapping <code>net</code> whose gradients are scaled.</p>
<h5 id="raises_497">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>net</code> dtype is non-float and <code>scale</code> is not zero or one.</li>
</ul>
<h3 id="split_leading_dimtensor-inputs-n_dims2"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=92"><code>split_leading_dim(tensor, inputs, n_dims=2)</code></a><a id="split_leading_dim" /></h3>
<p>Split the first dimension of a tensor.</p>
<h5 id="args_273">Args:</h5>
<ul>
<li><code>tensor</code>: Tensor to have its first dimension split.</li>
<li><code>inputs</code>: Original reference input to look the dimensions of.</li>
<li><code>n_dims</code>: Number of dimensions to split.</li>
</ul>
<h5 id="returns_547">Returns:</h5>
<p>The input tensor, with its first dimension split.</p>
<h3 id="summarize_variablesvariablesnone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=597"><code>summarize_variables(variables=None)</code></a><a id="summarize_variables" /></h3>
<p>Logs a summary of variable information.</p>
<p>This function groups Variables by dtype and prints out the number of Variables
and the total number of scalar values for each datatype, as well as the total
memory consumed.</p>
<p>For Variables of type tf.string, the memory usage cannot be accurately
calculated from the Graph as the memory requirements change based on what
strings are actually stored, which can only be determined inside a session.
In this case, the amount of memory used to stored the pointers to the strings
is logged, along with a warning.</p>
<h5 id="args_274">Args:</h5>
<ul>
<li><code>variables</code>: iterable of variables; if not provided, then all variables
    (in the default graph) are summarized.</li>
</ul>
<h3 id="supports_kwargsmodule_or_fn-kwargs_list"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=1041"><code>supports_kwargs(module_or_fn, kwargs_list)</code></a><a id="supports_kwargs" /></h3>
<p>Determines whether the provided callable supports all the kwargs.</p>
<p>This is useful when you have a module that might or might not support a
kwarg such as <code>is_training</code>. Rather than calling the module and catching the
error, risking the potential modification of underlying state, this function
introspects the module to see what kwargs are actually supported, using
the python <code>inspect</code> module.</p>
<p>Note that many TF functions do not export a valid argspec object, rather they
have a generic <em>args, </em>*kwargs signature due to various layers of wrapping
(deprecation decorators, etc). In those circumstances we return
MAYBE_SUPPORTED, and users will have to use another method to tell whether
the kwargs are supported (e.g. by just calling the function).</p>
<h5 id="args_275">Args:</h5>
<ul>
<li><code>module_or_fn</code>: some callable, generally an object or a method of some object.
    If an object is provided, we check wither <code>module_or_fn.__call__</code> supports
    the provided kwargs, which for a Sonnet module will automatically check
    the signature of _build. If <code>module_or_fn</code> is a function/method, then
    we check its signature directly, so non-Sonnet functions can be used.</li>
<li><code>kwargs_list</code>: string or iterable of strings of keyword arg names to test for.
    If an empty iterable is provided this function will always return True.</li>
</ul>
<h5 id="raises_498">Raises:</h5>
<ul>
<li><code>ValueError</code>: if a non-string is provided in <code>kwargs_list</code>.</li>
</ul>
<h5 id="returns_548">Returns:</h5>
<p>a string, one of 'supported', 'not_supported' or 'maybe_supported'.</p>
<h3 id="trainable_initial_statebatch_size-state_size-dtype-initializersnone-regularizersnone-namenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=92"><code>trainable_initial_state(batch_size, state_size, dtype, initializers=None, regularizers=None, name=None)</code></a><a id="trainable_initial_state" /></h3>
<p>Creates an initial state consisting of trainable variables.</p>
<p>The trainable variables are created with the same shapes as the elements of
<code>state_size</code> and are tiled to produce an initial state.</p>
<h5 id="args_276">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, or scalar int32 Tensor representing the batch size.</li>
<li><code>state_size</code>: A <code>TensorShape</code> or nested tuple of <code>TensorShape</code>s to use for the
      shape of the trainable variables.</li>
<li><code>dtype</code>: The data type used to create the variables and thus initial state.</li>
<li><code>initializers</code>: An optional container of the same structure as <code>state_size</code>
      containing initializers for the variables.</li>
<li><code>regularizers</code>: An optional container of the same structure as <code>state_size</code>
      containing regularizers for the variables.</li>
<li><code>name</code>: optional string used to prefix the initial state variable names.</li>
</ul>
<h5 id="returns_549">Returns:</h5>
<p>A <code>Tensor</code> or nested tuple of <code>Tensor</code>s with the same size and structure
  as <code>state_size</code>, where each <code>Tensor</code> is a tiled trainable <code>Variable</code>.</p>
<h5 id="raises_499">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the user passes initializers that are not functions.</li>
<li><code>ValueError</code>: if the user passes regularizers that are not functions.</li>
</ul>
<h3 id="variable_map_itemsvariable_map"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=473"><code>variable_map_items(variable_map)</code></a><a id="variable_map_items" /></h3>
<p>Yields an iterator over (string, variable) pairs in the variable map.</p>
<p>In general, variable maps map variable names to either a <code>tf.Variable</code>, or
list of <code>tf.Variable</code>s (in case of sliced variables).</p>
<h5 id="args_277">Args:</h5>
<ul>
<li><code>variable_map</code>: dict, variable map over which to iterate.</li>
</ul>
<h5 id="yields_1">Yields:</h5>
<p>(string, tf.Variable) pairs.</p>
<h3 id="class-custom_getterscontext"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/context.py?q=class:Context"><code>class custom_getters.Context</code></a><a id="custom_getters.Context" /></h3>
<p>Contextually switching a custom getter on.</p>
<p>Example usage, using <code>snt.custom_getters.stop_gradient</code> with <code>Context</code> to
selectively disable gradients flowing to variables for particular connections
of the module:</p>
<pre><code class="python">  custom_getter = snt.custom_getters.Context(snt.custom_getters.stop_gradient)
  lin = snt.Linear(10, custom_getter=custom_getter)

  lin(net1)  # custom getter not used, gradients on
  with custom_getter:
    lin(net2)  # custom getter used, gradients off
</code></pre>

<p>Warning: If the custom getter affects the way the variable is created, then
switching it on or off after the variable has been created will have no
effect. For example, it is not possible to selectively switch off
trainability using <code>custom_getters.non_trainable</code>, since this is a
creation-time attribute. It is however possible to selectively switch
off gradients using <code>custom_getters.stop_gradient</code>, since
this applies an operation to the variable.</p>
<h4 id="custom_getterscontext__init__getter-verbosefalse"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/context.py?l=49"><code>custom_getters.Context.__init__(getter, verbose=False)</code></a><a id="custom_getters.Context.__init__" /></h4>
<p>Initializes a contextual switch for a custom getter.</p>
<h5 id="args_278">Args:</h5>
<ul>
<li><code>getter</code>: The custom getter which we may want to switch on.</li>
<li><code>verbose</code>: Log out every time a variable is fetched, and whether or not
    <code>getter</code> is used.</li>
</ul>
<h5 id="returns_550">Returns:</h5>
<p>A custom getter which can also be used as a context manager.
  Entering the context enables the custom getter.</p>
<h3 id="custom_gettersnon_trainablegetter-args-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/non_trainable.py?l=22"><code>custom_getters.non_trainable(getter, *args, **kwargs)</code></a><a id="custom_getters.non_trainable" /></h3>
<p>Custom getter which makes a variable non-trainable.</p>
<p>Usage like:</p>
<p>with tf.variable_scope("", custom_getter=snt.custom_getters.non_trainable):
      net = snt.Linear(num_hidden)(net)</p>
<p>or, using the <code>custom_getter</code> constructor argument,</p>
<p>linear = snt.Linear(num_hidden,
                      custom_getter=snt.custom_getters.non_trainable)
  net = linear(net)</p>
<p>will result in the variables inside the linear having <code>trainable=False</code>, i.e.
won't be added to tf.trainable_variables() and thus won't be optimized.</p>
<p>Warning: If <code>reuse=True</code> and the variable has previously been created in
the same graph with <code>trainable=True</code>, this custom getter will do
nothing. Similarly if the variable is reused after being created by this
custom getter it will still be non-trainable, even if <code>trainable=True</code>.</p>
<p>When used with a Sonnet module, the module must be constructed inside the
variable scope with the custom getter. Just building the module inside said
variable scope will not use the custom getter.</p>
<h5 id="args_279">Args:</h5>
<ul>
<li><code>getter</code>: The true getter to call.</li>
<li><code>*args</code>: Arguments, in the same format as tf.get_variable.</li>
<li><code>**kwargs</code>: Keyword arguments, in the same format as tf.get_variable.</li>
</ul>
<h5 id="returns_551">Returns:</h5>
<p>The return value of <code>getter(*args, **kwargs)</code> except with <code>trainable=False</code>
  enforced.</p>
<h3 id="custom_gettersoverride_argskwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/override_args.py?l=24"><code>custom_getters.override_args(**kwargs)</code></a><a id="custom_getters.override_args" /></h3>
<p>Creates a custom getter that applies specified named arguments.</p>
<h5 id="args_280">Args:</h5>
<ul>
<li><code>**kwargs</code>: Overriding arguments for the custom getter to use in preference
    the named arguments it's called with.</li>
</ul>
<h5 id="returns_552">Returns:</h5>
<p>Custom getter.</p>
<h3 id="custom_gettersoverride_default_argskwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/override_args.py?l=55"><code>custom_getters.override_default_args(**kwargs)</code></a><a id="custom_getters.override_default_args" /></h3>
<p>Creates a custom getter that applies specified named arguments.</p>
<p>The returned custom getter treats the specified named arguments as revised
defaults, and does not override any non-<code>None</code> argument values supplied by
the original get_variable call (or by a nested scope's custom getter).</p>
<h5 id="args_281">Args:</h5>
<ul>
<li><code>**kwargs</code>: Overriding arguments for the custom getter to use in preference
    the named arguments it's called with.</li>
</ul>
<h5 id="returns_553">Returns:</h5>
<p>Custom getter.</p>
<h3 id="custom_gettersrestore_initializerfilename-name_fnnone-collectionvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/restore_initializer.py?l=26"><code>custom_getters.restore_initializer(filename, name_fn=None, collection='variables')</code></a><a id="custom_getters.restore_initializer" /></h3>
<p>Custom getter to restore all variables with <code>snt.restore_initializer</code>.</p>
<h5 id="args_282">Args:</h5>
<ul>
<li><code>filename</code>: The filename of the checkpoint.</li>
<li><code>name_fn</code>: A function which can map the name of the variable requested. This
    allows restoring variables with values having different names in the
    checkpoint.</li>
<li><code>collection</code>: Only set the restore initializer for variables in this
    collection. If <code>None</code>, it will attempt to restore all variables. By
    default <code>tf.GraphKeys.GLOBAL_VARIABLES</code>.</li>
</ul>
<h5 id="returns_554">Returns:</h5>
<p>A restore_initializer custom getter, which is a function taking arguments
  (getter, name, <em>args, </em>*kwargs).</p>
<h3 id="custom_gettersstop_gradientgetter-args-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/stop_gradient.py?l=24"><code>custom_getters.stop_gradient(getter, *args, **kwargs)</code></a><a id="custom_getters.stop_gradient" /></h3>
<p>Custom getter which prevents variables being optimized.</p>
<p>Usage like:</p>
<p>with tf.variable_scope("", custom_getter=snt.custom_getters.stop_gradient):
      net = snt.Linear(num_hidden)(net)</p>
<p>or, using the <code>custom_getter</code> constructor argument,</p>
<p>linear = snt.Linear(num_hidden,
                      custom_getter=snt.custom_getters.stop_gradient)
  net = linear(net)</p>
<p>will result in the gradient with respect to the variables in the linear
module being <code>None</code>. By default, the variables will still be in the trainable
variables collection.</p>
<p>When used with a Sonnet module, the module must be constructed inside the
variable scope with the custom getter. Just building the module inside said
variable scope will not use the custom getter.</p>
<h5 id="args_283">Args:</h5>
<ul>
<li><code>getter</code>: The true getter to call.</li>
<li><code>*args</code>: Arguments, in the same format as tf.get_variable.</li>
<li><code>**kwargs</code>: Keyword arguments, in the same format as tf.get_variable.</li>
</ul>
<h5 id="returns_555">Returns:</h5>
<p>The return value of <code>getter(*args, **kwargs)</code> with a tf.stop_gradient.</p>
<h3 id="class-custom_gettersbayes_by_backpropestimatormodes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/bayes_by_backprop.py?q=class:EstimatorModes"><code>class custom_getters.bayes_by_backprop.EstimatorModes</code></a><a id="custom_getters.bayes_by_backprop.EstimatorModes" /></h3>
<h3 id="class-custom_gettersbayes_by_backprop_variablemetadata"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/bayes_by_backprop.py?q=class:VariableMetadata"><code>class custom_getters.bayes_by_backprop._VariableMetadata</code></a><a id="custom_getters.bayes_by_backprop._VariableMetadata" /></h3>
<p>VariableMetadata(raw_variable_name, raw_variable_shape, scope_name, posterior, posterior_estimate, prior, kl_cost, prior_vars, posterior_vars)</p>
<h4 id="custom_gettersbayes_by_backprop_variablemetadatakl_cost"><code>custom_getters.bayes_by_backprop._VariableMetadata.kl_cost</code><a id="custom_getters.bayes_by_backprop._VariableMetadata.kl_cost" /></h4>
<p>Alias for field number 6</p>
<h4 id="custom_gettersbayes_by_backprop_variablemetadataposterior"><code>custom_getters.bayes_by_backprop._VariableMetadata.posterior</code><a id="custom_getters.bayes_by_backprop._VariableMetadata.posterior" /></h4>
<p>Alias for field number 3</p>
<h4 id="custom_gettersbayes_by_backprop_variablemetadataposterior_estimate"><code>custom_getters.bayes_by_backprop._VariableMetadata.posterior_estimate</code><a id="custom_getters.bayes_by_backprop._VariableMetadata.posterior_estimate" /></h4>
<p>Alias for field number 4</p>
<h4 id="custom_gettersbayes_by_backprop_variablemetadataposterior_vars"><code>custom_getters.bayes_by_backprop._VariableMetadata.posterior_vars</code><a id="custom_getters.bayes_by_backprop._VariableMetadata.posterior_vars" /></h4>
<p>Alias for field number 8</p>
<h4 id="custom_gettersbayes_by_backprop_variablemetadataprior"><code>custom_getters.bayes_by_backprop._VariableMetadata.prior</code><a id="custom_getters.bayes_by_backprop._VariableMetadata.prior" /></h4>
<p>Alias for field number 5</p>
<h4 id="custom_gettersbayes_by_backprop_variablemetadataprior_vars"><code>custom_getters.bayes_by_backprop._VariableMetadata.prior_vars</code><a id="custom_getters.bayes_by_backprop._VariableMetadata.prior_vars" /></h4>
<p>Alias for field number 7</p>
<h4 id="custom_gettersbayes_by_backprop_variablemetadataraw_variable_name"><code>custom_getters.bayes_by_backprop._VariableMetadata.raw_variable_name</code><a id="custom_getters.bayes_by_backprop._VariableMetadata.raw_variable_name" /></h4>
<p>Alias for field number 0</p>
<h4 id="custom_gettersbayes_by_backprop_variablemetadataraw_variable_shape"><code>custom_getters.bayes_by_backprop._VariableMetadata.raw_variable_shape</code><a id="custom_getters.bayes_by_backprop._VariableMetadata.raw_variable_shape" /></h4>
<p>Alias for field number 1</p>
<h4 id="custom_gettersbayes_by_backprop_variablemetadatascope_name"><code>custom_getters.bayes_by_backprop._VariableMetadata.scope_name</code><a id="custom_getters.bayes_by_backprop._VariableMetadata.scope_name" /></h4>
<p>Alias for field number 2</p>
<h3 id="custom_gettersbayes_by_backpropadaptive_gaussian_prior_buildergetter-name-args-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/bayes_by_backprop.py?l=218"><code>custom_getters.bayes_by_backprop.adaptive_gaussian_prior_builder(getter, name, *args, **kwargs)</code></a><a id="custom_getters.bayes_by_backprop.adaptive_gaussian_prior_builder" /></h3>
<p>A pre-canned builder for adaptive scalar gaussian prior distributions.</p>
<p>Given a true <code>getter</code> function and arguments forwarded from <code>tf.get_variable</code>,
return a distribution object for a scalar-valued adaptive gaussian prior
which will be broadcast over a variable of the requisite shape. This prior's
parameters (e.g <code>loc</code> and <code>scale</code> for a gaussian) will consist of a single
learned scalar for the entire <code>tf.Variable</code> for which it serves as the prior,
regardless of that <code>tf.Variable</code>'s shape.</p>
<h5 id="args_284">Args:</h5>
<ul>
<li><code>getter</code>: The <code>getter</code> passed to a <code>custom_getter</code>. Please see the
    documentation for <code>tf.get_variable</code>.</li>
<li><code>name</code>: The <code>name</code> argument passed to <code>tf.get_variable</code>.</li>
<li><code>*args</code>: See positional arguments passed to <code>tf.get_variable</code>.</li>
<li><code>**kwargs</code>: See keyword arguments passed to <code>tf.get_variable</code>.</li>
</ul>
<h5 id="returns_556">Returns:</h5>
<p>An instance of <code>tfp.distributions.Normal</code> representing the prior
  distribution over the variable in question.</p>
<h3 id="custom_gettersbayes_by_backpropanalytic_kl_builderposterior-prior-sample"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/bayes_by_backprop.py?l=257"><code>custom_getters.bayes_by_backprop.analytic_kl_builder(posterior, prior, sample)</code></a><a id="custom_getters.bayes_by_backprop.analytic_kl_builder" /></h3>
<p>A pre-canned builder for the analytic kl divergence.</p>
<h3 id="custom_gettersbayes_by_backpropbayes_by_backprop_getterposterior_builderdiagonal_gaussian_posterior_builder-prior_builderfixed_gaussian_prior_builder-kl_builderstochastic_kl_builder-sampling_mode_tensornone-fresh_noise_per_connectiontrue-keep_control_dependenciesfalse"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/bayes_by_backprop.py?l=263"><code>custom_getters.bayes_by_backprop.bayes_by_backprop_getter(posterior_builder=diagonal_gaussian_posterior_builder, prior_builder=fixed_gaussian_prior_builder, kl_builder=stochastic_kl_builder, sampling_mode_tensor=None, fresh_noise_per_connection=True, keep_control_dependencies=False)</code></a><a id="custom_getters.bayes_by_backprop.bayes_by_backprop_getter" /></h3>
<p>Creates a custom getter which does Bayes by Backprop.</p>
<p>Please see <code>tf.get_variable</code> for general documentation on custom getters.</p>
<p>All arguments are optional. If nothing is configued, then a diagonal gaussian
posterior will be used, and a fixed N(0, 0.01) prior will be used. Please
see the default <code>posterior_builder</code> and <code>prior_builder</code> for a more detailed
understanding of the default settings.</p>
<h5 id="args_285">Args:</h5>
<ul>
<li>
<p><code>posterior_builder</code>: A builder function which constructs an instance of
    <code>tfp.distributions.Distribution</code> which shall serve as the posterior over
    the <code>tf.Variable</code> of interest. The builder receives the <code>getter</code> and the
    arguments forwarded from <code>tf.get_variable</code>. Suppose one wrote</p>
<p><code>tf.get_variable(
    'weights', shape=(3,), initializer=tf.zeros_initializer,
    dtype=tf.float32)</code></p>
<p>then the <code>posterior_builder</code> argument would receive the <code>name</code>, <code>shape</code>,
<code>initializer</code>, and <code>dtype</code> arguments passed above. The builder must return
a <code>tfp.distributions.Distribution</code> object.</p>
<p>Please see the <code>tf.get_variable</code> for documentation on <code>custom_getter</code> and
<code>getter</code>, and see <code>bbb.diagonal_gaussian_posterior_builder</code>
(the default) for an example of using this builder API.</p>
</li>
<li>
<p><code>prior_builder</code>: A builder function which constructs an instance of
    <code>tfp.distributions.Distribution</code> which shall serve as the prior over the
    <code>tf.Variable</code> of interest. Identical API to <code>posterior_builder</code>. See
    <code>bbb.fixed_gaussian_prior_builder</code> (the default) for an example.</p>
</li>
<li><code>kl_builder</code>: A builder function which receives the posterior distribution,
    prior distribution, and a sample from the posterior. It returns a
    scalar-shaped <code>tf.Tensor</code> representing the total KL cost for the
    <code>tf.Variable</code> in question. See <code>bbb.stochastic_kl_builder</code> (default) and
    <code>bbb.analytic_kl_builder</code> for examples.</li>
<li><code>sampling_mode_tensor</code>: A <code>tf.Tensor</code> which determines how an estimate from
    the posterior is produced. It must be scalar-shaped and have a <code>dtype</code> of
    <code>tf.string</code>. Valid values for this tensor are <code>bbb.EstimatorModes.sample</code>
    (which is the default), <code>bbb.EstimatorModes.mean</code>, and
    <code>bbb.EstimatorModes.last_sample</code>. <code>bbb.EstimatorModes.sample</code> is
    appropriate for training, and <code>bbb.EstimatorModes.mean</code> can be used
    at test time.</li>
<li><code>fresh_noise_per_connection</code>: A boolean. Indicates that each time a stochastic
    variable is retrieved with this custom getter, new sampling noise should
    be used. This is <code>True</code> by default. If this argument is set to <code>False</code>,
    then the same noise is used for each connection. Note that this does not
    apply to connections within a <code>tf.while_loop</code>; the same sampling noise
    is always used in different iterations of a <code>tf.while_loop</code> within one
    <code>session.run()</code> call. See the unit tests for details.</li>
<li>
<p><code>keep_control_dependencies</code>: A boolean. This argument should only be
    used by advanced users. Indicates that each time a stochastic variable is
    retrieved in the loop body of a <code>tf.while_loop</code> construct, new sampling
    noise should be used.
    The default behavior is <code>False</code>, so that RNNs use the same weights at each
    recurrent time step. This is done by removing the creation of the Variable
    from any existing control flow contexts. Notably, the Variables will be
    created outside the context of any tf.while_loop, making them fetchable.
    When this argument is <code>True</code>, any Variables used in the loop body of a
    <code>tf.while_loop</code> will be non-fetchable. If the KL cost needs to be
    evaluated, the Variable must <em>first</em> be used <em>outside</em> the loop body. This
    op using the Variable simply needs to be placed on the graph to get a
    stochastic estimate of the KL; it doesn't need to ever be used. Example:</p>
<p>```
def loop_body(i):
  logits = sonnet_module(queue)
  i = i + 1</p>
<p>with tf.variable_scope('bbb', custom_getter=bbb.bayes_by_backprop_getter(
    fresh_noise_per_connection=True,
    keep_control_dependencies=True)):
  unused_op = sonnet_module(queue)  # Adds KL estimate to bbb Collection
  final_i = tf.while_loop(lambda i: i &lt; 5, loop_body, tf.constant(0.))
```</p>
<p>Here when we add <code>unused_op</code> to the graph, we also add a number of tensors
associated with the particular stochastic variable, including its
contribution to the KL cost, to a graph-level registry. These are
organized in a per-stochastic-variable data structure and be accessed with
<code>bbb.get_variable_metadata()</code>. Without this line, these Tensors would
instead be added the first time the Variable is used in the while_loop,
which would make them non-fetchable.</p>
<p>In all cases, the KL cost is only added once per Variable, which is the
correct behavior, since if a variable is used multiple times in a model,
the KL cost should remain unaffected.</p>
</li>
</ul>
<h5 id="returns_557">Returns:</h5>
<p>A <code>custom_getter</code> function which implements Bayes by Backprop.</p>
<h3 id="custom_gettersbayes_by_backpropdiagonal_gaussian_posterior_buildergetter-name-shapenone-args-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/bayes_by_backprop.py?l=148"><code>custom_getters.bayes_by_backprop.diagonal_gaussian_posterior_builder(getter, name, shape=None, *args, **kwargs)</code></a><a id="custom_getters.bayes_by_backprop.diagonal_gaussian_posterior_builder" /></h3>
<p>A pre-canned builder for diagonal gaussian posterior distributions.</p>
<p>Given a true <code>getter</code> function and arguments forwarded from <code>tf.get_variable</code>,
return a distribution object for a diagonal posterior over a variable of the
requisite shape.</p>
<h5 id="args_286">Args:</h5>
<ul>
<li><code>getter</code>: The <code>getter</code> passed to a <code>custom_getter</code>. Please see the
    documentation for <code>tf.get_variable</code>.</li>
<li><code>name</code>: The <code>name</code> argument passed to <code>tf.get_variable</code>.</li>
<li><code>shape</code>: The <code>shape</code> argument passed to <code>tf.get_variable</code>.</li>
<li><code>*args</code>: See positional arguments passed to <code>tf.get_variable</code>.</li>
<li><code>**kwargs</code>: See keyword arguments passed to <code>tf.get_variable</code>.</li>
</ul>
<h5 id="returns_558">Returns:</h5>
<p>An instance of <code>tfp.distributions.Normal</code> representing the posterior
  distribution over the variable in question.</p>
<h3 id="custom_gettersbayes_by_backpropfixed_gaussian_prior_buildergetter-name-dtypenone-args-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/bayes_by_backprop.py?l=188"><code>custom_getters.bayes_by_backprop.fixed_gaussian_prior_builder(getter, name, dtype=None, *args, **kwargs)</code></a><a id="custom_getters.bayes_by_backprop.fixed_gaussian_prior_builder" /></h3>
<p>A pre-canned builder for fixed gaussian prior distributions.</p>
<p>Given a true <code>getter</code> function and arguments forwarded from <code>tf.get_variable</code>,
return a distribution object for a scalar-valued fixed gaussian prior which
will be broadcast over a variable of the requisite shape.</p>
<h5 id="args_287">Args:</h5>
<ul>
<li><code>getter</code>: The <code>getter</code> passed to a <code>custom_getter</code>. Please see the
    documentation for <code>tf.get_variable</code>.</li>
<li><code>name</code>: The <code>name</code> argument passed to <code>tf.get_variable</code>.</li>
<li><code>dtype</code>: The <code>dtype</code> argument passed to <code>tf.get_variable</code>.</li>
<li><code>*args</code>: See positional arguments passed to <code>tf.get_variable</code>.</li>
<li><code>**kwargs</code>: See keyword arguments passed to <code>tf.get_variable</code>.</li>
</ul>
<h5 id="returns_559">Returns:</h5>
<p>An instance of <code>tfp.distributions.Normal</code> representing the prior
  distribution over the variable in question.</p>
<h3 id="custom_gettersbayes_by_backpropget_total_kl_costnametotal_kl_cost-filter_by_name_substringnone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/bayes_by_backprop.py?l=513"><code>custom_getters.bayes_by_backprop.get_total_kl_cost(name='total_kl_cost', filter_by_name_substring=None)</code></a><a id="custom_getters.bayes_by_backprop.get_total_kl_cost" /></h3>
<p>Get the total cost for all (or a subset of) the stochastic variables.</p>
<h5 id="args_288">Args:</h5>
<ul>
<li><code>name</code>: A name for the tensor representing the total kl cost.</li>
<li><code>filter_by_name_substring</code>: A string used to filter which variables count
    toward the total KL cost. By default, this argument is <code>None</code>, and all
    variables trained using Bayes by Backprop are included. If this argument
    is provided, the variables whose KL costs are summed will be all those
    whose name contains <code>filter_by_name_substring</code>. An example use of this
    would be to select all variables within a particular scope.</li>
</ul>
<h5 id="returns_560">Returns:</h5>
<p>A tensor representing the total KL cost in the ELBO loss.</p>
<h3 id="custom_gettersbayes_by_backpropget_variable_metadatascope_name_substringnone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/bayes_by_backprop.py?l=535"><code>custom_getters.bayes_by_backprop.get_variable_metadata(scope_name_substring=None)</code></a><a id="custom_getters.bayes_by_backprop.get_variable_metadata" /></h3>
<h3 id="custom_gettersbayes_by_backpropinverse_softplusy"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/bayes_by_backprop.py?l=114"><code>custom_getters.bayes_by_backprop.inverse_softplus(y)</code></a><a id="custom_getters.bayes_by_backprop.inverse_softplus" /></h3>
<p>The inverse of the softplus function.</p>
<p>Computes the <em>inverse</em> of softplus, a function which maps an
unconstrained real number to the positive reals, e.g. to squash an
unconstrained neural network activation to parameterize a variance.</p>
<h5 id="args_289">Args:</h5>
<ul>
<li><code>y</code>: A positive number.</li>
</ul>
<h5 id="returns_561">Returns:</h5>
<p>The number <code>x</code> such that softplus(x) = y.</p>
<h3 id="custom_gettersbayes_by_backpropscale_variable_initializerdesired_scale"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/bayes_by_backprop.py?l=129"><code>custom_getters.bayes_by_backprop.scale_variable_initializer(desired_scale)</code></a><a id="custom_getters.bayes_by_backprop.scale_variable_initializer" /></h3>
<h3 id="custom_gettersbayes_by_backpropstochastic_kl_builderposterior-prior-sample"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/custom_getters/bayes_by_backprop.py?l=250"><code>custom_getters.bayes_by_backprop.stochastic_kl_builder(posterior, prior, sample)</code></a><a id="custom_getters.bayes_by_backprop.stochastic_kl_builder" /></h3>
<p>A pre-canned builder for a ubiquitous stochastic KL estimator.</p>
<h3 id="nestassert_same_structureargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=35"><code>nest.assert_same_structure(*args, **kwargs)</code></a><a id="nest.assert_same_structure" /></h3>
<h3 id="nestassert_shallow_structureargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=35"><code>nest.assert_shallow_structure(*args, **kwargs)</code></a><a id="nest.assert_shallow_structure" /></h3>
<h3 id="nestflattenargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=35"><code>nest.flatten(*args, **kwargs)</code></a><a id="nest.flatten" /></h3>
<h3 id="nestflatten_dict_itemsargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=35"><code>nest.flatten_dict_items(*args, **kwargs)</code></a><a id="nest.flatten_dict_items" /></h3>
<h3 id="nestflatten_iterableargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=35"><code>nest.flatten_iterable(*args, **kwargs)</code></a><a id="nest.flatten_iterable" /></h3>
<h3 id="nestflatten_up_toargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=35"><code>nest.flatten_up_to(*args, **kwargs)</code></a><a id="nest.flatten_up_to" /></h3>
<h3 id="nestis_iterableargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=35"><code>nest.is_iterable(*args, **kwargs)</code></a><a id="nest.is_iterable" /></h3>
<h3 id="nestis_sequenceargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=35"><code>nest.is_sequence(*args, **kwargs)</code></a><a id="nest.is_sequence" /></h3>
<h3 id="nestmapargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=35"><code>nest.map(*args, **kwargs)</code></a><a id="nest.map" /></h3>
<h3 id="nestmap_up_toargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=35"><code>nest.map_up_to(*args, **kwargs)</code></a><a id="nest.map_up_to" /></h3>
<h3 id="nestpack_iterable_asargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=35"><code>nest.pack_iterable_as(*args, **kwargs)</code></a><a id="nest.pack_iterable_as" /></h3>
<h3 id="nestpack_sequence_asargs-kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=35"><code>nest.pack_sequence_as(*args, **kwargs)</code></a><a id="nest.pack_sequence_as" /></h3>
<h3 id="nestwith_deprecation_warningfn-extra_message"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/ops/nest.py?l=33"><code>nest.with_deprecation_warning(fn, extra_message='')</code></a><a id="nest.with_deprecation_warning" /></h3>
<p>Wraps the function and prints a warn-once (per <code>extra_message</code>) warning.</p>
<h3 id="class-netsalexnet"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?q=class:AlexNet"><code>class nets.AlexNet</code></a><a id="nets.AlexNet" /></h3>
<p>Implementation of AlexNet with full and mini versions.</p>
<p>Based on:
  'ImageNet Classification with Deep Convolutional Neural Networks'
  Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, NIPS 2012
  http://papers.nips.cc/paper/4824-imagenet-classification-w</p>
<h4 id="netsalexnet__init__mode-use_batch_normfalse-batch_norm_confignone-initializersnone-partitionersnone-regularizersnone-bn_on_fc_layerstrue-custom_getternone-namealex_net"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=61"><code>nets.AlexNet.__init__(mode, use_batch_norm=False, batch_norm_config=None, initializers=None, partitioners=None, regularizers=None, bn_on_fc_layers=True, custom_getter=None, name='alex_net')</code></a><a id="nets.AlexNet.__init__" /></h4>
<p>Constructs AlexNet.</p>
<h5 id="args_290">Args:</h5>
<ul>
<li><code>mode</code>: Construction mode of network: <code>AlexNet.FULL</code> or <code>AlexNet.MINI</code>.</li>
<li><code>use_batch_norm</code>: Whether to use batch normalization between the output of
      a layer and the activation function.</li>
<li><code>batch_norm_config</code>: Optional mapping of additional configuration for the
      <code>snt.BatchNorm</code> modules.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      key 'w') or biases (with key 'b'). The default initializers are
      truncated normal initializers, which are commonly used when the inputs
      are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf).</li>
<li><code>partitioners</code>: Optional dict containing partitioners for the filters
    (with key 'w') and the biases (with key 'b'). As a default, no
    partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
    (with key 'w') and the biases (with key 'b'). As a default, no
    regularizers are used. A regularizer should be a function that takes
    a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output, e.g.
    the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>bn_on_fc_layers</code>: If <code>use_batch_norm</code> is True, add batch normalization to
    the fully-connected layers. This is deprecated.</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the <code>tf.get_variable</code>
    documentation for information about the custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_500">Raises:</h5>
<p>base.Error: If the given <code>mode</code> is not one of <code>AlexNet.FULL</code>,
    or <code>AlexNet.MINI</code>.</p>
<ul>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contains any
    keys other than 'w' or 'b'.</li>
</ul>
<h4 id="netsalexnet__call__inputs-keep_probnone-is_trainingnone-test_local_statstrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=183"><code>nets.AlexNet.__call__(inputs, keep_prob=None, is_training=None, test_local_stats=True)</code></a><a id="nets.AlexNet.__call__" /></h4>
<p>Connects the AlexNet module into the graph.</p>
<p>The is_training flag only controls the batch norm settings, if <code>False</code> it
does not force no dropout by overriding any input <code>keep_prob</code>. To avoid any
confusion this may cause, if <code>is_training=False</code> and <code>keep_prob</code> would cause
dropout to be applied, an error is thrown.</p>
<h5 id="args_291">Args:</h5>
<ul>
<li><code>inputs</code>: A Tensor of size [batch_size, input_height, input_width,
    input_channels], representing a batch of input images.</li>
<li><code>keep_prob</code>: A scalar Tensor representing the dropout keep probability.
    When <code>is_training=False</code> this must be None or 1 to give no dropout.</li>
<li><code>is_training</code>: Boolean to indicate if we are currently training. Must be
      specified if batch normalization or dropout is used.</li>
<li><code>test_local_stats</code>: Boolean to indicate to <code>snt.BatchNorm</code> if batch
    normalization should  use local batch statistics at test time.
    By default <code>True</code>.</li>
</ul>
<h5 id="returns_562">Returns:</h5>
<p>A Tensor of size [batch_size, output_size], where <code>output_size</code> depends
  on the mode the network was constructed in.</p>
<h5 id="raises_501">Raises:</h5>
<p>base.IncompatibleShapeError: If any of the input image dimensions
    (input_height, input_width) are too small for the given network mode.</p>
<ul>
<li><code>ValueError</code>: If <code>keep_prob</code> is not None or 1 when <code>is_training=False</code>.</li>
<li><code>ValueError</code>: If <code>is_training</code> is not explicitly specified when using
    batch normalization.</li>
</ul>
<h4 id="netsalexnetconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>nets.AlexNet.connected_subgraphs</code></a><a id="nets.AlexNet.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="netsalexnetconv_modules"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=319"><code>nets.AlexNet.conv_modules</code></a><a id="nets.AlexNet.conv_modules" /></h4>
<p>Returns list containing convolutional modules of network.</p>
<h5 id="returns_563">Returns:</h5>
<p>A list containing the Conv2D modules.</p>
<h4 id="netsalexnetdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>nets.AlexNet.defun()</code></a><a id="nets.AlexNet.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="netsalexnetdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>nets.AlexNet.defun_wrapped</code></a><a id="nets.AlexNet.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="netsalexnetget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>nets.AlexNet.get_all_variables(collection='trainable_variables')</code></a><a id="nets.AlexNet.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_292">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_564">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_502">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>nets.AlexNet.get_possible_initializer_keys(cls)</code></a><a id="nets.AlexNet.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_565">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="netsalexnetget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>nets.AlexNet.get_variables(collection='trainable_variables')</code></a><a id="nets.AlexNet.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_293">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_566">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_503">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>nets.AlexNet.graph</code></a><a id="nets.AlexNet.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="netsalexnetinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=295"><code>nets.AlexNet.initializers</code></a><a id="nets.AlexNet.initializers" /></h4>
<h4 id="netsalexnetis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>nets.AlexNet.is_connected</code></a><a id="nets.AlexNet.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="netsalexnetlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>nets.AlexNet.last_connected_subgraph</code></a><a id="nets.AlexNet.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_567">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_504">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetlinear_modules"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=330"><code>nets.AlexNet.linear_modules</code></a><a id="nets.AlexNet.linear_modules" /></h4>
<p>Returns list containing linear modules of network.</p>
<h5 id="returns_568">Returns:</h5>
<p>A list containing the Linear modules.</p>
<h4 id="netsalexnetmin_input_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=307"><code>nets.AlexNet.min_input_size</code></a><a id="nets.AlexNet.min_input_size" /></h4>
<p>Returns integer specifying the minimum width and height for the input.</p>
<p>Note that the input can be non-square, but both the width and height must
be &gt;= this number in size.</p>
<h5 id="returns_569">Returns:</h5>
<p>The minimum size as an integer.</p>
<h4 id="netsalexnetmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>nets.AlexNet.module_name</code></a><a id="nets.AlexNet.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="netsalexnetname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>nets.AlexNet.name_scopes</code></a><a id="nets.AlexNet.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="netsalexnetnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>nets.AlexNet.non_trainable_variables</code></a><a id="nets.AlexNet.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_570">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_505">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=299"><code>nets.AlexNet.partitioners</code></a><a id="nets.AlexNet.partitioners" /></h4>
<h4 id="netsalexnetregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=303"><code>nets.AlexNet.regularizers</code></a><a id="nets.AlexNet.regularizers" /></h4>
<h4 id="netsalexnetscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>nets.AlexNet.scope_name</code></a><a id="nets.AlexNet.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="netsalexnettrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>nets.AlexNet.trainable_variables</code></a><a id="nets.AlexNet.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_571">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_506">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>nets.AlexNet.variable_scope</code></a><a id="nets.AlexNet.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_572">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_507">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>nets.AlexNet.variables</code></a><a id="nets.AlexNet.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_573">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_508">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-netsalexnetfull"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?q=class:AlexNetFull"><code>class nets.AlexNetFull</code></a><a id="nets.AlexNetFull" /></h3>
<p>AlexNet constructed in the 'FULL' mode.</p>
<h4 id="netsalexnetfull__init__use_batch_normfalse-batch_norm_confignone-initializersnone-partitionersnone-regularizersnone-custom_getternone-namealex_net_full"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=345"><code>nets.AlexNetFull.__init__(use_batch_norm=False, batch_norm_config=None, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='alex_net_full')</code></a><a id="nets.AlexNetFull.__init__" /></h4>
<p>Constructs AlexNet.</p>
<h5 id="args_294">Args:</h5>
<ul>
<li><code>use_batch_norm</code>: Whether to use batch normalization between the output of
      a layer and the activation function.</li>
<li><code>batch_norm_config</code>: Optional mapping of additional configuration for the
      <code>snt.BatchNorm</code> modules.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      key 'w') or biases (with key 'b'). The default initializers are
      truncated normal initializers, which are commonly used when the inputs
      are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf).</li>
<li><code>partitioners</code>: Optional dict containing partitioners for the filters
    (with key 'w') and the biases (with key 'b'). As a default, no
    partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
    (with key 'w') and the biases (with key 'b'). As a default, no
    regularizers are used. A regularizer should be a function that takes
    a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output, e.g.
    the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the <code>tf.get_variable</code>
    documentation for information about the custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_509">Raises:</h5>
<ul>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contains any
    keys other than 'w' or 'b'.</li>
</ul>
<h4 id="netsalexnetfull__call__inputs-keep_probnone-is_trainingnone-test_local_statstrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=183"><code>nets.AlexNetFull.__call__(inputs, keep_prob=None, is_training=None, test_local_stats=True)</code></a><a id="nets.AlexNetFull.__call__" /></h4>
<p>Connects the AlexNet module into the graph.</p>
<p>The is_training flag only controls the batch norm settings, if <code>False</code> it
does not force no dropout by overriding any input <code>keep_prob</code>. To avoid any
confusion this may cause, if <code>is_training=False</code> and <code>keep_prob</code> would cause
dropout to be applied, an error is thrown.</p>
<h5 id="args_295">Args:</h5>
<ul>
<li><code>inputs</code>: A Tensor of size [batch_size, input_height, input_width,
    input_channels], representing a batch of input images.</li>
<li><code>keep_prob</code>: A scalar Tensor representing the dropout keep probability.
    When <code>is_training=False</code> this must be None or 1 to give no dropout.</li>
<li><code>is_training</code>: Boolean to indicate if we are currently training. Must be
      specified if batch normalization or dropout is used.</li>
<li><code>test_local_stats</code>: Boolean to indicate to <code>snt.BatchNorm</code> if batch
    normalization should  use local batch statistics at test time.
    By default <code>True</code>.</li>
</ul>
<h5 id="returns_574">Returns:</h5>
<p>A Tensor of size [batch_size, output_size], where <code>output_size</code> depends
  on the mode the network was constructed in.</p>
<h5 id="raises_510">Raises:</h5>
<p>base.IncompatibleShapeError: If any of the input image dimensions
    (input_height, input_width) are too small for the given network mode.</p>
<ul>
<li><code>ValueError</code>: If <code>keep_prob</code> is not None or 1 when <code>is_training=False</code>.</li>
<li><code>ValueError</code>: If <code>is_training</code> is not explicitly specified when using
    batch normalization.</li>
</ul>
<h4 id="netsalexnetfullconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>nets.AlexNetFull.connected_subgraphs</code></a><a id="nets.AlexNetFull.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="netsalexnetfullconv_modules"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=319"><code>nets.AlexNetFull.conv_modules</code></a><a id="nets.AlexNetFull.conv_modules" /></h4>
<p>Returns list containing convolutional modules of network.</p>
<h5 id="returns_575">Returns:</h5>
<p>A list containing the Conv2D modules.</p>
<h4 id="netsalexnetfulldefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>nets.AlexNetFull.defun()</code></a><a id="nets.AlexNetFull.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="netsalexnetfulldefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>nets.AlexNetFull.defun_wrapped</code></a><a id="nets.AlexNetFull.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="netsalexnetfullget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>nets.AlexNetFull.get_all_variables(collection='trainable_variables')</code></a><a id="nets.AlexNetFull.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_296">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_576">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_511">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetfullget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>nets.AlexNetFull.get_possible_initializer_keys(cls)</code></a><a id="nets.AlexNetFull.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_577">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="netsalexnetfullget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>nets.AlexNetFull.get_variables(collection='trainable_variables')</code></a><a id="nets.AlexNetFull.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_297">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_578">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_512">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetfullgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>nets.AlexNetFull.graph</code></a><a id="nets.AlexNetFull.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="netsalexnetfullinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=295"><code>nets.AlexNetFull.initializers</code></a><a id="nets.AlexNetFull.initializers" /></h4>
<h4 id="netsalexnetfullis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>nets.AlexNetFull.is_connected</code></a><a id="nets.AlexNetFull.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="netsalexnetfulllast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>nets.AlexNetFull.last_connected_subgraph</code></a><a id="nets.AlexNetFull.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_579">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_513">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetfulllinear_modules"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=330"><code>nets.AlexNetFull.linear_modules</code></a><a id="nets.AlexNetFull.linear_modules" /></h4>
<p>Returns list containing linear modules of network.</p>
<h5 id="returns_580">Returns:</h5>
<p>A list containing the Linear modules.</p>
<h4 id="netsalexnetfullmin_input_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=307"><code>nets.AlexNetFull.min_input_size</code></a><a id="nets.AlexNetFull.min_input_size" /></h4>
<p>Returns integer specifying the minimum width and height for the input.</p>
<p>Note that the input can be non-square, but both the width and height must
be &gt;= this number in size.</p>
<h5 id="returns_581">Returns:</h5>
<p>The minimum size as an integer.</p>
<h4 id="netsalexnetfullmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>nets.AlexNetFull.module_name</code></a><a id="nets.AlexNetFull.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="netsalexnetfullname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>nets.AlexNetFull.name_scopes</code></a><a id="nets.AlexNetFull.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="netsalexnetfullnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>nets.AlexNetFull.non_trainable_variables</code></a><a id="nets.AlexNetFull.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_582">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_514">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetfullpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=299"><code>nets.AlexNetFull.partitioners</code></a><a id="nets.AlexNetFull.partitioners" /></h4>
<h4 id="netsalexnetfullregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=303"><code>nets.AlexNetFull.regularizers</code></a><a id="nets.AlexNetFull.regularizers" /></h4>
<h4 id="netsalexnetfullscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>nets.AlexNetFull.scope_name</code></a><a id="nets.AlexNetFull.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="netsalexnetfulltrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>nets.AlexNetFull.trainable_variables</code></a><a id="nets.AlexNetFull.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_583">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_515">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetfullvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>nets.AlexNetFull.variable_scope</code></a><a id="nets.AlexNetFull.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_584">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_516">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetfullvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>nets.AlexNetFull.variables</code></a><a id="nets.AlexNetFull.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_585">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_517">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-netsalexnetmini"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?q=class:AlexNetMini"><code>class nets.AlexNetMini</code></a><a id="nets.AlexNetMini" /></h3>
<p>AlexNet constructed in the 'MINI' mode.</p>
<h4 id="netsalexnetmini__init__use_batch_normfalse-batch_norm_confignone-initializersnone-partitionersnone-regularizersnone-custom_getternone-namealex_net_mini"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=397"><code>nets.AlexNetMini.__init__(use_batch_norm=False, batch_norm_config=None, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='alex_net_mini')</code></a><a id="nets.AlexNetMini.__init__" /></h4>
<p>Constructs AlexNet.</p>
<h5 id="args_298">Args:</h5>
<ul>
<li><code>use_batch_norm</code>: Whether to use batch normalization between the output of
      a layer and the activation function.</li>
<li><code>batch_norm_config</code>: Optional mapping of additional configuration for the
      <code>snt.BatchNorm</code> modules.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters (with
      key 'w') or biases (with key 'b'). The default initializers are
      truncated normal initializers, which are commonly used when the inputs
      are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf).</li>
<li><code>partitioners</code>: Optional dict containing partitioners for the filters
    (with key 'w') and the biases (with key 'b'). As a default, no
    partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters
    (with key 'w') and the biases (with key 'b'). As a default, no
    regularizers are used. A regularizer should be a function that takes
    a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output, e.g.
    the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the <code>tf.get_variable</code>
    documentation for information about the custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_518">Raises:</h5>
<ul>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contains any
    keys other than 'w' or 'b'.</li>
</ul>
<h4 id="netsalexnetmini__call__inputs-keep_probnone-is_trainingnone-test_local_statstrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=183"><code>nets.AlexNetMini.__call__(inputs, keep_prob=None, is_training=None, test_local_stats=True)</code></a><a id="nets.AlexNetMini.__call__" /></h4>
<p>Connects the AlexNet module into the graph.</p>
<p>The is_training flag only controls the batch norm settings, if <code>False</code> it
does not force no dropout by overriding any input <code>keep_prob</code>. To avoid any
confusion this may cause, if <code>is_training=False</code> and <code>keep_prob</code> would cause
dropout to be applied, an error is thrown.</p>
<h5 id="args_299">Args:</h5>
<ul>
<li><code>inputs</code>: A Tensor of size [batch_size, input_height, input_width,
    input_channels], representing a batch of input images.</li>
<li><code>keep_prob</code>: A scalar Tensor representing the dropout keep probability.
    When <code>is_training=False</code> this must be None or 1 to give no dropout.</li>
<li><code>is_training</code>: Boolean to indicate if we are currently training. Must be
      specified if batch normalization or dropout is used.</li>
<li><code>test_local_stats</code>: Boolean to indicate to <code>snt.BatchNorm</code> if batch
    normalization should  use local batch statistics at test time.
    By default <code>True</code>.</li>
</ul>
<h5 id="returns_586">Returns:</h5>
<p>A Tensor of size [batch_size, output_size], where <code>output_size</code> depends
  on the mode the network was constructed in.</p>
<h5 id="raises_519">Raises:</h5>
<p>base.IncompatibleShapeError: If any of the input image dimensions
    (input_height, input_width) are too small for the given network mode.</p>
<ul>
<li><code>ValueError</code>: If <code>keep_prob</code> is not None or 1 when <code>is_training=False</code>.</li>
<li><code>ValueError</code>: If <code>is_training</code> is not explicitly specified when using
    batch normalization.</li>
</ul>
<h4 id="netsalexnetminiconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>nets.AlexNetMini.connected_subgraphs</code></a><a id="nets.AlexNetMini.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="netsalexnetminiconv_modules"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=319"><code>nets.AlexNetMini.conv_modules</code></a><a id="nets.AlexNetMini.conv_modules" /></h4>
<p>Returns list containing convolutional modules of network.</p>
<h5 id="returns_587">Returns:</h5>
<p>A list containing the Conv2D modules.</p>
<h4 id="netsalexnetminidefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>nets.AlexNetMini.defun()</code></a><a id="nets.AlexNetMini.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="netsalexnetminidefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>nets.AlexNetMini.defun_wrapped</code></a><a id="nets.AlexNetMini.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="netsalexnetminiget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>nets.AlexNetMini.get_all_variables(collection='trainable_variables')</code></a><a id="nets.AlexNetMini.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_300">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_588">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_520">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetminiget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>nets.AlexNetMini.get_possible_initializer_keys(cls)</code></a><a id="nets.AlexNetMini.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_589">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="netsalexnetminiget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>nets.AlexNetMini.get_variables(collection='trainable_variables')</code></a><a id="nets.AlexNetMini.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_301">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_590">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_521">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetminigraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>nets.AlexNetMini.graph</code></a><a id="nets.AlexNetMini.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="netsalexnetminiinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=295"><code>nets.AlexNetMini.initializers</code></a><a id="nets.AlexNetMini.initializers" /></h4>
<h4 id="netsalexnetminiis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>nets.AlexNetMini.is_connected</code></a><a id="nets.AlexNetMini.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="netsalexnetminilast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>nets.AlexNetMini.last_connected_subgraph</code></a><a id="nets.AlexNetMini.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_591">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_522">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetminilinear_modules"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=330"><code>nets.AlexNetMini.linear_modules</code></a><a id="nets.AlexNetMini.linear_modules" /></h4>
<p>Returns list containing linear modules of network.</p>
<h5 id="returns_592">Returns:</h5>
<p>A list containing the Linear modules.</p>
<h4 id="netsalexnetminimin_input_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=307"><code>nets.AlexNetMini.min_input_size</code></a><a id="nets.AlexNetMini.min_input_size" /></h4>
<p>Returns integer specifying the minimum width and height for the input.</p>
<p>Note that the input can be non-square, but both the width and height must
be &gt;= this number in size.</p>
<h5 id="returns_593">Returns:</h5>
<p>The minimum size as an integer.</p>
<h4 id="netsalexnetminimodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>nets.AlexNetMini.module_name</code></a><a id="nets.AlexNetMini.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="netsalexnetmininame_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>nets.AlexNetMini.name_scopes</code></a><a id="nets.AlexNetMini.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="netsalexnetmininon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>nets.AlexNetMini.non_trainable_variables</code></a><a id="nets.AlexNetMini.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_594">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_523">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetminipartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=299"><code>nets.AlexNetMini.partitioners</code></a><a id="nets.AlexNetMini.partitioners" /></h4>
<h4 id="netsalexnetminiregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/alexnet.py?l=303"><code>nets.AlexNetMini.regularizers</code></a><a id="nets.AlexNetMini.regularizers" /></h4>
<h4 id="netsalexnetminiscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>nets.AlexNetMini.scope_name</code></a><a id="nets.AlexNetMini.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="netsalexnetminitrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>nets.AlexNetMini.trainable_variables</code></a><a id="nets.AlexNetMini.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_595">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_524">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetminivariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>nets.AlexNetMini.variable_scope</code></a><a id="nets.AlexNetMini.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_596">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_525">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsalexnetminivariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>nets.AlexNetMini.variables</code></a><a id="nets.AlexNetMini.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_597">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_526">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-netsconvnet2d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?q=class:ConvNet2D"><code>class nets.ConvNet2D</code></a><a id="nets.ConvNet2D" /></h3>
<p>A 2D Convolutional Network module.</p>
<h4 id="netsconvnet2d__init__output_channels-kernel_shapes-strides-paddings-rates1-activationrelu-activate_finalfalse-normalization_ctornone-normalization_kwargsnone-normalize_finalnone-initializersnone-partitionersnone-regularizersnone-use_batch_normnone-use_biastrue-batch_norm_confignone-data_formatnhwc-custom_getternone-nameconv_net_2d"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=51"><code>nets.ConvNet2D.__init__(output_channels, kernel_shapes, strides, paddings, rates=(1,), activation=relu, activate_final=False, normalization_ctor=None, normalization_kwargs=None, normalize_final=None, initializers=None, partitioners=None, regularizers=None, use_batch_norm=None, use_bias=True, batch_norm_config=None, data_format='NHWC', custom_getter=None, name='conv_net_2d')</code></a><a id="nets.ConvNet2D.__init__" /></h4>
<p>Constructs a <code>ConvNet2D</code> module.</p>
<p>By default, neither batch normalization nor activation are applied to the
output of the final layer.</p>
<h5 id="args_302">Args:</h5>
<ul>
<li><code>output_channels</code>: Iterable of output channels, as defined in
    <code>conv.Conv2D</code>. Output channels can be defined either as number or via a
    callable. In the latter case, since the function invocation is deferred
    to graph construction time, the user must only ensure that entries can
    be called when build is called. Each entry in the iterable defines
    properties in the corresponding convolutional layer.</li>
<li><code>kernel_shapes</code>: Iterable of kernel sizes as defined in <code>conv.Conv2D</code>; if
    the list contains one element only, the same kernel shape is used in
    each layer of the network.</li>
<li><code>strides</code>: Iterable of kernel strides as defined in <code>conv.Conv2D</code>; if the
    list contains one element only, the same stride is used in each layer of
    the network.</li>
<li><code>paddings</code>: Iterable of padding options as defined in <code>conv.Conv2D</code>. Each
    can be <code>snt.SAME</code>, <code>snt.VALID</code>, <code>snt.FULL</code>, <code>snt.CAUSAL</code>,
    <code>snt.REVERSE_CAUSAL</code> or a pair of these to use for height and width.
    If the Iterable contains one element only, the same padding is used in
    each layer of the network.</li>
<li><code>rates</code>: Iterable of dilation rates as defined in <code>conv.Conv2D</code>; if the
    list contains one element only, the same rate is used in each layer of
    the network.</li>
<li><code>activation</code>: An activation op.</li>
<li><code>activate_final</code>: Boolean determining if the activation and batch
    normalization, if turned on, are applied to the final layer.</li>
<li><code>normalization_ctor</code>: Constructor to return a callable which will perform
    normalization at each layer. Defaults to None / no normalization.
    Examples of what could go here: <code>snt.BatchNormV2</code>, <code>snt.LayerNorm</code>. If
    a string is provided, importlib is used to convert the string to a
    callable, so either <code>snt.LayerNorm</code> or <code>"snt.LayerNorm"</code> can be
    provided.</li>
<li><code>normalization_kwargs</code>: kwargs to be provided to <code>normalization_ctor</code> when
    it is called.</li>
<li><code>normalize_final</code>: Whether to apply normalization after the final conv
    layer. Default is to take the value of activate_final.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters of
    the whole network (with key 'w') or biases (with key 'b').</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
      weights (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters of the
    whole network (with key 'w') or biases (with key 'b'). As a default, no
    regularizers are used. A regularizer should be a function that takes a
    single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output, e.g.
    the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>use_batch_norm</code>: Boolean determining if batch normalization is applied
    after convolution. Deprecated, use <code>normalization_ctor</code> instead.</li>
<li><code>use_bias</code>: Boolean or iterable of booleans determining whether to include
    bias parameters in the convolutional layers. Default <code>True</code>.</li>
<li><code>batch_norm_config</code>: Optional mapping of additional configuration for the
    <code>snt.BatchNorm</code> modules. Deprecated, use <code>normalization_kwargs</code> instead.</li>
<li><code>data_format</code>: A string, one of "NCHW" or "NHWC". Specifies whether the
    channel dimension of the input and output is the last dimension
    (default, "NHWC"), or the second dimension ("NCHW").</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the
    <code>tf.get_variable</code> documentation for information about the
    custom_getter API. Note that this <code>custom_getter</code> will not be passed
    to the <code>transpose</code> method. If you want to use a custom getter with
    the transposed of this convolutional network, you should provide one
    to the <code>transpose</code> method instead.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_527">Raises:</h5>
<ul>
<li><code>TypeError</code>: If <code>output_channels</code> is not iterable; or if <code>kernel_shapes</code> is
    not iterable; or <code>strides</code> is not iterable; or <code>paddings</code> is not
    iterable; or if <code>activation</code> is not callable.</li>
<li><code>ValueError</code>: If <code>output_channels</code> is empty; or if <code>kernel_shapes</code> has not
    length 1 or <code>len(output_channels)</code>; or if <code>strides</code> has not
    length 1 or <code>len(output_channels)</code>; or if <code>paddings</code> has not
    length 1 or <code>len(output_channels)</code>; or if <code>rates</code> has not
    length 1 or <code>len(output_channels)</code>; or if the given data_format is not a
    supported format ("NHWC" or "NCHW"); or if <code>normalization_ctor</code> is
    provided but cannot be mapped to a callable.</li>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
    keys other than 'w' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
    are not callable.</li>
</ul>
<h4 id="netsconvnet2d__call__inputs-normalization_build_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=311"><code>nets.ConvNet2D.__call__(inputs, **normalization_build_kwargs)</code></a><a id="nets.ConvNet2D.__call__" /></h4>
<p>Assembles the <code>ConvNet2D</code> and connects it to the graph.</p>
<h5 id="args_303">Args:</h5>
<ul>
<li><code>inputs</code>: A 4D Tensor of shape <code>[batch_size, input_height, input_width,
    input_channels]</code>.</li>
<li><code>**normalization_build_kwargs</code>: kwargs passed to the normalization module
    at _build time.</li>
</ul>
<h5 id="returns_598">Returns:</h5>
<p>A 4D Tensor of shape <code>[batch_size, output_height, output_width,
    output_channels[-1]]</code>.</p>
<h5 id="raises_528">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>is_training</code> is not explicitly specified when using
    batch normalization.</li>
</ul>
<h4 id="netsconvnet2dactivate_final"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=434"><code>nets.ConvNet2D.activate_final</code></a><a id="nets.ConvNet2D.activate_final" /></h4>
<h4 id="netsconvnet2dactivation"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=430"><code>nets.ConvNet2D.activation</code></a><a id="nets.ConvNet2D.activation" /></h4>
<h4 id="netsconvnet2dbatch_norm_config"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=411"><code>nets.ConvNet2D.batch_norm_config</code></a><a id="nets.ConvNet2D.batch_norm_config" /></h4>
<h4 id="netsconvnet2dconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>nets.ConvNet2D.connected_subgraphs</code></a><a id="nets.ConvNet2D.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="netsconvnet2ddefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>nets.ConvNet2D.defun()</code></a><a id="nets.ConvNet2D.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="netsconvnet2ddefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>nets.ConvNet2D.defun_wrapped</code></a><a id="nets.ConvNet2D.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="netsconvnet2dget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>nets.ConvNet2D.get_all_variables(collection='trainable_variables')</code></a><a id="nets.ConvNet2D.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_304">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_599">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_529">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsconvnet2dget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>nets.ConvNet2D.get_possible_initializer_keys(cls)</code></a><a id="nets.ConvNet2D.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_600">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="netsconvnet2dget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>nets.ConvNet2D.get_variables(collection='trainable_variables')</code></a><a id="nets.ConvNet2D.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_305">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_601">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_530">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsconvnet2dgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>nets.ConvNet2D.graph</code></a><a id="nets.ConvNet2D.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="netsconvnet2dinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=368"><code>nets.ConvNet2D.initializers</code></a><a id="nets.ConvNet2D.initializers" /></h4>
<h4 id="netsconvnet2dinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=439"><code>nets.ConvNet2D.input_shape</code></a><a id="nets.ConvNet2D.input_shape" /></h4>
<p>Returns shape of input <code>Tensor</code> passed at last call to <code>build</code>.</p>
<h4 id="netsconvnet2dis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>nets.ConvNet2D.is_connected</code></a><a id="nets.ConvNet2D.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="netsconvnet2dkernel_shapes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=392"><code>nets.ConvNet2D.kernel_shapes</code></a><a id="nets.ConvNet2D.kernel_shapes" /></h4>
<h4 id="netsconvnet2dlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>nets.ConvNet2D.last_connected_subgraph</code></a><a id="nets.ConvNet2D.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_602">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_531">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsconvnet2dlayers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=363"><code>nets.ConvNet2D.layers</code></a><a id="nets.ConvNet2D.layers" /></h4>
<p>Returns a tuple containing the convolutional layers of the network.</p>
<h4 id="netsconvnet2dmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>nets.ConvNet2D.module_name</code></a><a id="nets.ConvNet2D.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="netsconvnet2dname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>nets.ConvNet2D.name_scopes</code></a><a id="nets.ConvNet2D.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="netsconvnet2dnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>nets.ConvNet2D.non_trainable_variables</code></a><a id="nets.ConvNet2D.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_603">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_532">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsconvnet2dnormalization_ctor"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=418"><code>nets.ConvNet2D.normalization_ctor</code></a><a id="nets.ConvNet2D.normalization_ctor" /></h4>
<h4 id="netsconvnet2dnormalization_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=422"><code>nets.ConvNet2D.normalization_kwargs</code></a><a id="nets.ConvNet2D.normalization_kwargs" /></h4>
<h4 id="netsconvnet2dnormalize_final"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=426"><code>nets.ConvNet2D.normalize_final</code></a><a id="nets.ConvNet2D.normalize_final" /></h4>
<h4 id="netsconvnet2doutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=396"><code>nets.ConvNet2D.output_channels</code></a><a id="nets.ConvNet2D.output_channels" /></h4>
<h4 id="netsconvnet2dpaddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=384"><code>nets.ConvNet2D.paddings</code></a><a id="nets.ConvNet2D.paddings" /></h4>
<h4 id="netsconvnet2dpartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=372"><code>nets.ConvNet2D.partitioners</code></a><a id="nets.ConvNet2D.partitioners" /></h4>
<h4 id="netsconvnet2drates"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=388"><code>nets.ConvNet2D.rates</code></a><a id="nets.ConvNet2D.rates" /></h4>
<h4 id="netsconvnet2dregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=376"><code>nets.ConvNet2D.regularizers</code></a><a id="nets.ConvNet2D.regularizers" /></h4>
<h4 id="netsconvnet2dscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>nets.ConvNet2D.scope_name</code></a><a id="nets.ConvNet2D.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="netsconvnet2dstrides"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=380"><code>nets.ConvNet2D.strides</code></a><a id="nets.ConvNet2D.strides" /></h4>
<h4 id="netsconvnet2dtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>nets.ConvNet2D.trainable_variables</code></a><a id="nets.ConvNet2D.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_604">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_533">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsconvnet2dtransposenamenone-output_channelsnone-kernel_shapesnone-stridesnone-paddingsnone-activationnone-activate_finalnone-normalization_ctornone-normalization_kwargsnone-normalize_finalnone-initializersnone-partitionersnone-regularizersnone-use_batch_normnone-use_biasnone-batch_norm_confignone-data_formatnone-custom_getternone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=588"><code>nets.ConvNet2D.transpose(name=None, output_channels=None, kernel_shapes=None, strides=None, paddings=None, activation=None, activate_final=None, normalization_ctor=None, normalization_kwargs=None, normalize_final=None, initializers=None, partitioners=None, regularizers=None, use_batch_norm=None, use_bias=None, batch_norm_config=None, data_format=None, custom_getter=None)</code></a><a id="nets.ConvNet2D.transpose" /></h4>
<p>Returns transposed version of this network.</p>
<h5 id="args_306">Args:</h5>
<ul>
<li><code>name</code>: Optional string specifying the name of the transposed module. The
    default name is constructed by appending "_transpose"
    to <code>self.module_name</code>.</li>
<li><code>output_channels</code>: Optional iterable of numbers of output channels.</li>
<li><code>kernel_shapes</code>: Optional iterable of kernel sizes. The default value is
    constructed by reversing <code>self.kernel_shapes</code>.</li>
<li><code>strides</code>: Optional iterable of kernel strides. The default value is
    constructed by reversing <code>self.strides</code>.</li>
<li><code>paddings</code>: Optional iterable of padding options, either <code>snt.SAME</code> or
    <code>snt.VALID</code>; The default value is constructed by reversing
    <code>self.paddings</code>.</li>
<li><code>activation</code>: Optional activation op. Default value is <code>self.activation</code>.</li>
<li><code>activate_final</code>: Optional boolean determining if the activation and batch
    normalization, if turned on, are applied to the final layer.</li>
<li><code>normalization_ctor</code>: Constructor to return a callable which will perform
    normalization at each layer. Defaults to None / no normalization.
    Examples of what could go here: <code>snt.BatchNormV2</code>, <code>snt.LayerNorm</code>. If
    a string is provided, importlib is used to convert the string to a
    callable, so either <code>snt.LayerNorm</code> or <code>"snt.LayerNorm"</code> can be
    provided.</li>
<li><code>normalization_kwargs</code>: kwargs to be provided to <code>normalization_ctor</code> when
    it is called.</li>
<li><code>normalize_final</code>: Whether to apply normalization after the final conv
    layer. Default is to take the value of activate_final.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters of
    the whole network (with key 'w') or biases (with key 'b'). The default
    value is <code>self.initializers</code>.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
    weights (with key 'w') or biases (with key 'b'). The default value is
    <code>self.partitioners</code>.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters of the
    whole network (with key 'w') or biases (with key 'b'). The default is
    <code>self.regularizers</code>.</li>
<li><code>use_batch_norm</code>: Optional boolean determining if batch normalization is
    applied after convolution. The default value is <code>self.use_batch_norm</code>.</li>
<li><code>use_bias</code>: Optional boolean or iterable of booleans determining whether to
    include bias parameters in the convolutional layers. Default
    is constructed by reversing <code>self.use_bias</code>.</li>
<li><code>batch_norm_config</code>: Optional mapping of additional configuration for the
    <code>snt.BatchNorm</code> modules. Default is <code>self.batch_norm_config</code>.</li>
<li><code>data_format</code>: Optional string, one of "NCHW" or "NHWC". Specifies whether
    the channel dimension of the input and output is the last dimension.
    Default is <code>self._data_format</code>.</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the
    <code>tf.get_variable</code> documentation for information about the
    custom_getter API.</li>
</ul>
<h5 id="returns_605">Returns:</h5>
<p>Matching <code>ConvNet2DTranspose</code> module.</p>
<h5 id="raises_534">Raises:</h5>
<ul>
<li><code>ValueError</code>: If output_channels is specified and its length does not match
    the number of layers.</li>
<li><code>ValueError</code>: If the given data_format is not a supported format ("NHWC" or
    "NCHW").</li>
<li><code>NotImplementedError</code>: If the convolutions are dilated.</li>
</ul>
<h4 id="netsconvnet2duse_batch_norm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=404"><code>nets.ConvNet2D.use_batch_norm</code></a><a id="nets.ConvNet2D.use_batch_norm" /></h4>
<h4 id="netsconvnet2duse_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=400"><code>nets.ConvNet2D.use_bias</code></a><a id="nets.ConvNet2D.use_bias" /></h4>
<h4 id="netsconvnet2dvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>nets.ConvNet2D.variable_scope</code></a><a id="nets.ConvNet2D.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_606">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_535">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsconvnet2dvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>nets.ConvNet2D.variables</code></a><a id="nets.ConvNet2D.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_607">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_536">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-netsconvnet2dtranspose"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?q=class:ConvNet2DTranspose"><code>class nets.ConvNet2DTranspose</code></a><a id="nets.ConvNet2DTranspose" /></h3>
<p>A 2D Transpose-Convolutional Network module.</p>
<h4 id="netsconvnet2dtranspose__init__output_channels-output_shapes-kernel_shapes-strides-paddings-activationrelu-activate_finalfalse-normalization_ctornone-normalization_kwargsnone-normalize_finalnone-initializersnone-partitionersnone-regularizersnone-use_batch_normfalse-use_biastrue-batch_norm_confignone-data_formatnhwc-custom_getternone-nameconv_net_2d_transpose"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=718"><code>nets.ConvNet2DTranspose.__init__(output_channels, output_shapes, kernel_shapes, strides, paddings, activation=relu, activate_final=False, normalization_ctor=None, normalization_kwargs=None, normalize_final=None, initializers=None, partitioners=None, regularizers=None, use_batch_norm=False, use_bias=True, batch_norm_config=None, data_format='NHWC', custom_getter=None, name='conv_net_2d_transpose')</code></a><a id="nets.ConvNet2DTranspose.__init__" /></h4>
<p>Constructs a <code>ConvNetTranspose2D</code> module.</p>
<p><code>output_{shapes,channels}</code> can be defined either as iterable of
{iterables,integers} or via a callable. In the latter case, since the
function invocation is deferred to graph construction time, the user
must only ensure that entries can be called returning meaningful values when
build is called. Each entry in the iterable defines properties in the
corresponding convolutional layer.</p>
<p>By default, neither batch normalization nor activation are applied to the
output of the final layer.</p>
<h5 id="args_307">Args:</h5>
<ul>
<li><code>output_channels</code>: Iterable of numbers of output channels.</li>
<li><code>output_shapes</code>: Iterable of output shapes as defined in
    <code>conv.conv2DTranpose</code>; if the iterable contains one element only, the
    same shape is used in each layer of the network.</li>
<li><code>kernel_shapes</code>: Iterable of kernel sizes as defined in <code>conv.Conv2D</code>; if
    the list contains one element only, the same kernel shape is used in
    each layer of the network.</li>
<li><code>strides</code>: Iterable of kernel strides as defined in <code>conv.Conv2D</code>; if the
    list contains one element only, the same stride is used in each layer of
    the network.</li>
<li><code>paddings</code>: Iterable of padding options, either <code>snt.SAME</code> or
    <code>snt.VALID</code>; if the Iterable contains one element only, the same padding
    is used in each layer of the network.</li>
<li><code>activation</code>: An activation op.</li>
<li><code>activate_final</code>: Boolean determining if the activation and batch
    normalization, if turned on, are applied to the final layer.</li>
<li><code>normalization_ctor</code>: Constructor to return a callable which will perform
    normalization at each layer. Defaults to None / no normalization.
    Examples of what could go here: <code>snt.BatchNormV2</code>, <code>snt.LayerNorm</code>. If
    a string is provided, importlib is used to convert the string to a
    callable, so either <code>snt.LayerNorm</code> or <code>"snt.LayerNorm"</code> can be
    provided.</li>
<li><code>normalization_kwargs</code>: kwargs to be provided to <code>normalization_ctor</code> when
    it is called.</li>
<li><code>normalize_final</code>: Whether to apply normalization after the final conv
    layer. Default is to take the value of activate_final.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters of
    the whole network (with key 'w') or biases (with key 'b').</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
      weights (with key 'w') or biases (with key 'b'). As a default, no
      partitioners are used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters of the
    whole network (with key 'w') or biases (with key 'b'). As a default, no
    regularizers are used. A regularizer should be a function that takes a
    single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output, e.g.
    the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>use_batch_norm</code>: Boolean determining if batch normalization is applied
    after convolution.</li>
<li><code>use_bias</code>: Boolean or iterable of booleans determining whether to include
    bias parameters in the convolutional layers. Default <code>True</code>.</li>
<li><code>batch_norm_config</code>: Optional mapping of additional configuration for the
    <code>snt.BatchNorm</code> modules.</li>
<li><code>data_format</code>: A string, one of "NCHW" or "NHWC". Specifies whether the
    channel dimension of the input and output is the last dimension
    (default, "NHWC"), or the second dimension ("NCHW").</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the
    <code>tf.get_variable</code> documentation for information about the
    custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_537">Raises:</h5>
<ul>
<li><code>TypeError</code>: If <code>output_channels</code> is not iterable; or if <code>output_shapes</code>
    is not iterable; or if <code>kernel_shapes</code> is not iterable; or if <code>strides</code>
    is not iterable; or if <code>paddings</code> is not iterable; or if <code>activation</code> is
    not callable.</li>
<li><code>ValueError</code>: If <code>output_channels</code> is empty; or if <code>kernel_shapes</code> has not
    length 1 or <code>len(output_channels)</code>; or if <code>strides</code> has not
    length 1 or <code>len(output_channels)</code>; or if <code>paddings</code> has not
    length 1 or <code>len(output_channels)</code>.</li>
<li><code>ValueError</code>: If the given data_format is not a supported format ("NHWC" or
    "NCHW").</li>
<li><code>ValueError</code>: If <code>normalization_ctor</code> is provided but cannot be converted
    to a callable.</li>
<li><code>KeyError</code>: If <code>initializers</code>, <code>partitioners</code> or <code>regularizers</code> contain any
    keys other than 'w' or 'b'.</li>
<li><code>TypeError</code>: If any of the given initializers, partitioners or regularizers
    are not callable.</li>
</ul>
<h4 id="netsconvnet2dtranspose__call__inputs-normalization_build_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=311"><code>nets.ConvNet2DTranspose.__call__(inputs, **normalization_build_kwargs)</code></a><a id="nets.ConvNet2DTranspose.__call__" /></h4>
<p>Assembles the <code>ConvNet2D</code> and connects it to the graph.</p>
<h5 id="args_308">Args:</h5>
<ul>
<li><code>inputs</code>: A 4D Tensor of shape <code>[batch_size, input_height, input_width,
    input_channels]</code>.</li>
<li><code>**normalization_build_kwargs</code>: kwargs passed to the normalization module
    at _build time.</li>
</ul>
<h5 id="returns_608">Returns:</h5>
<p>A 4D Tensor of shape <code>[batch_size, output_height, output_width,
    output_channels[-1]]</code>.</p>
<h5 id="raises_538">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>is_training</code> is not explicitly specified when using
    batch normalization.</li>
</ul>
<h4 id="netsconvnet2dtransposeactivate_final"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=434"><code>nets.ConvNet2DTranspose.activate_final</code></a><a id="nets.ConvNet2DTranspose.activate_final" /></h4>
<h4 id="netsconvnet2dtransposeactivation"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=430"><code>nets.ConvNet2DTranspose.activation</code></a><a id="nets.ConvNet2DTranspose.activation" /></h4>
<h4 id="netsconvnet2dtransposebatch_norm_config"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=411"><code>nets.ConvNet2DTranspose.batch_norm_config</code></a><a id="nets.ConvNet2DTranspose.batch_norm_config" /></h4>
<h4 id="netsconvnet2dtransposeconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>nets.ConvNet2DTranspose.connected_subgraphs</code></a><a id="nets.ConvNet2DTranspose.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="netsconvnet2dtransposedefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>nets.ConvNet2DTranspose.defun()</code></a><a id="nets.ConvNet2DTranspose.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="netsconvnet2dtransposedefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>nets.ConvNet2DTranspose.defun_wrapped</code></a><a id="nets.ConvNet2DTranspose.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="netsconvnet2dtransposeget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>nets.ConvNet2DTranspose.get_all_variables(collection='trainable_variables')</code></a><a id="nets.ConvNet2DTranspose.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_309">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_609">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_539">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsconvnet2dtransposeget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>nets.ConvNet2DTranspose.get_possible_initializer_keys(cls)</code></a><a id="nets.ConvNet2DTranspose.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_610">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="netsconvnet2dtransposeget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>nets.ConvNet2DTranspose.get_variables(collection='trainable_variables')</code></a><a id="nets.ConvNet2DTranspose.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_310">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_611">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_540">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsconvnet2dtransposegraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>nets.ConvNet2DTranspose.graph</code></a><a id="nets.ConvNet2DTranspose.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="netsconvnet2dtransposeinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=368"><code>nets.ConvNet2DTranspose.initializers</code></a><a id="nets.ConvNet2DTranspose.initializers" /></h4>
<h4 id="netsconvnet2dtransposeinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=439"><code>nets.ConvNet2DTranspose.input_shape</code></a><a id="nets.ConvNet2DTranspose.input_shape" /></h4>
<p>Returns shape of input <code>Tensor</code> passed at last call to <code>build</code>.</p>
<h4 id="netsconvnet2dtransposeis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>nets.ConvNet2DTranspose.is_connected</code></a><a id="nets.ConvNet2DTranspose.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="netsconvnet2dtransposekernel_shapes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=392"><code>nets.ConvNet2DTranspose.kernel_shapes</code></a><a id="nets.ConvNet2DTranspose.kernel_shapes" /></h4>
<h4 id="netsconvnet2dtransposelast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>nets.ConvNet2DTranspose.last_connected_subgraph</code></a><a id="nets.ConvNet2DTranspose.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_612">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_541">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsconvnet2dtransposelayers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=363"><code>nets.ConvNet2DTranspose.layers</code></a><a id="nets.ConvNet2DTranspose.layers" /></h4>
<p>Returns a tuple containing the convolutional layers of the network.</p>
<h4 id="netsconvnet2dtransposemodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>nets.ConvNet2DTranspose.module_name</code></a><a id="nets.ConvNet2DTranspose.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="netsconvnet2dtransposename_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>nets.ConvNet2DTranspose.name_scopes</code></a><a id="nets.ConvNet2DTranspose.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="netsconvnet2dtransposenon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>nets.ConvNet2DTranspose.non_trainable_variables</code></a><a id="nets.ConvNet2DTranspose.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_613">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_542">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsconvnet2dtransposenormalization_ctor"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=418"><code>nets.ConvNet2DTranspose.normalization_ctor</code></a><a id="nets.ConvNet2DTranspose.normalization_ctor" /></h4>
<h4 id="netsconvnet2dtransposenormalization_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=422"><code>nets.ConvNet2DTranspose.normalization_kwargs</code></a><a id="nets.ConvNet2DTranspose.normalization_kwargs" /></h4>
<h4 id="netsconvnet2dtransposenormalize_final"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=426"><code>nets.ConvNet2DTranspose.normalize_final</code></a><a id="nets.ConvNet2DTranspose.normalize_final" /></h4>
<h4 id="netsconvnet2dtransposeoutput_channels"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=396"><code>nets.ConvNet2DTranspose.output_channels</code></a><a id="nets.ConvNet2DTranspose.output_channels" /></h4>
<h4 id="netsconvnet2dtransposeoutput_shapes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=875"><code>nets.ConvNet2DTranspose.output_shapes</code></a><a id="nets.ConvNet2DTranspose.output_shapes" /></h4>
<h4 id="netsconvnet2dtransposepaddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=384"><code>nets.ConvNet2DTranspose.paddings</code></a><a id="nets.ConvNet2DTranspose.paddings" /></h4>
<h4 id="netsconvnet2dtransposepartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=372"><code>nets.ConvNet2DTranspose.partitioners</code></a><a id="nets.ConvNet2DTranspose.partitioners" /></h4>
<h4 id="netsconvnet2dtransposerates"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=388"><code>nets.ConvNet2DTranspose.rates</code></a><a id="nets.ConvNet2DTranspose.rates" /></h4>
<h4 id="netsconvnet2dtransposeregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=376"><code>nets.ConvNet2DTranspose.regularizers</code></a><a id="nets.ConvNet2DTranspose.regularizers" /></h4>
<h4 id="netsconvnet2dtransposescope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>nets.ConvNet2DTranspose.scope_name</code></a><a id="nets.ConvNet2DTranspose.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="netsconvnet2dtransposestrides"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=380"><code>nets.ConvNet2DTranspose.strides</code></a><a id="nets.ConvNet2DTranspose.strides" /></h4>
<h4 id="netsconvnet2dtransposetrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>nets.ConvNet2DTranspose.trainable_variables</code></a><a id="nets.ConvNet2DTranspose.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_614">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_543">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsconvnet2dtransposetransposenamenone-output_channelsnone-kernel_shapesnone-stridesnone-paddingsnone-activationnone-activate_finalnone-normalization_ctornone-normalization_kwargsnone-normalize_finalnone-initializersnone-partitionersnone-regularizersnone-use_batch_normnone-use_biasnone-batch_norm_confignone-data_formatnone-custom_getternone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=880"><code>nets.ConvNet2DTranspose.transpose(name=None, output_channels=None, kernel_shapes=None, strides=None, paddings=None, activation=None, activate_final=None, normalization_ctor=None, normalization_kwargs=None, normalize_final=None, initializers=None, partitioners=None, regularizers=None, use_batch_norm=None, use_bias=None, batch_norm_config=None, data_format=None, custom_getter=None)</code></a><a id="nets.ConvNet2DTranspose.transpose" /></h4>
<p>Returns transposed version of this network.</p>
<h5 id="args_311">Args:</h5>
<ul>
<li><code>name</code>: Optional string specifying the name of the transposed module. The
    default name is constructed by appending "_transpose"
    to <code>self.module_name</code>.</li>
<li><code>output_channels</code>: Optional iterable of numbers of output channels.</li>
<li><code>kernel_shapes</code>: Optional iterable of kernel sizes. The default value is
    constructed by reversing <code>self.kernel_shapes</code>.</li>
<li><code>strides</code>: Optional iterable of kernel strides. The default value is
    constructed by reversing <code>self.strides</code>.</li>
<li><code>paddings</code>: Optional iterable of padding options, either <code>snt.SAME</code> or
    <code>snt.VALID</code>; The default value is constructed by reversing
    <code>self.paddings</code>.</li>
<li><code>activation</code>: Optional activation op. Default value is <code>self.activation</code>.</li>
<li><code>activate_final</code>: Optional boolean determining if the activation and batch
    normalization, if turned on, are applied to the final layer.</li>
<li><code>normalization_ctor</code>: Constructor to return a callable which will perform
    normalization at each layer. Defaults to None / no normalization.
    Examples of what could go here: <code>snt.BatchNormV2</code>, <code>snt.LayerNorm</code>. If
    a string is provided, importlib is used to convert the string to a
    callable, so either <code>snt.LayerNorm</code> or <code>"snt.LayerNorm"</code> can be
    provided.</li>
<li><code>normalization_kwargs</code>: kwargs to be provided to <code>normalization_ctor</code> when
    it is called.</li>
<li><code>normalize_final</code>: Whether to apply normalization after the final conv
    layer. Default is to take the value of activate_final.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the filters of
    the whole network (with key 'w') or biases (with key 'b'). The default
    value is <code>self.initializers</code>.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
    weights (with key 'w') or biases (with key 'b'). The default value is
    <code>self.partitioners</code>.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the filters of the
    whole network (with key 'w') or biases (with key 'b'). The default is
    <code>self.regularizers</code>.</li>
<li><code>use_batch_norm</code>: Optional boolean determining if batch normalization is
    applied after convolution. The default value is <code>self.use_batch_norm</code>.</li>
<li><code>use_bias</code>: Optional boolean or iterable of booleans determining whether to
    include bias parameters in the convolutional layers. Default
    is constructed by reversing <code>self.use_bias</code>.</li>
<li><code>batch_norm_config</code>: Optional mapping of additional configuration for the
    <code>snt.BatchNorm</code> modules. Default is <code>self.batch_norm_config</code>.</li>
<li><code>data_format</code>: Optional string, one of "NCHW" or "NHWC". Specifies whether
    the channel dimension of the input and output is the last dimension.
    Default is <code>self._data_format</code>.</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the
    <code>tf.get_variable</code> documentation for information about the
    custom_getter API.</li>
</ul>
<h5 id="returns_615">Returns:</h5>
<p>Matching <code>ConvNet2D</code> module.</p>
<h5 id="raises_544">Raises:</h5>
<ul>
<li><code>ValueError</code>: If output_channels is specified and its length does not match
    the number of layers.</li>
</ul>
<h4 id="netsconvnet2dtransposeuse_batch_norm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=404"><code>nets.ConvNet2DTranspose.use_batch_norm</code></a><a id="nets.ConvNet2DTranspose.use_batch_norm" /></h4>
<h4 id="netsconvnet2dtransposeuse_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/convnet.py?l=400"><code>nets.ConvNet2DTranspose.use_bias</code></a><a id="nets.ConvNet2DTranspose.use_bias" /></h4>
<h4 id="netsconvnet2dtransposevariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>nets.ConvNet2DTranspose.variable_scope</code></a><a id="nets.ConvNet2DTranspose.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_616">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_545">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsconvnet2dtransposevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>nets.ConvNet2DTranspose.variables</code></a><a id="nets.ConvNet2DTranspose.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_617">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_546">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-netsdilation"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/dilation.py?q=class:Dilation"><code>class nets.Dilation</code></a><a id="nets.Dilation" /></h3>
<p>A convolutional module for per-pixel classification.</p>
<p>Consists of 8 convolutional layers, 4 of which are dilated. When applied to
the output of a model like VGG-16 (before fully connected layers), can be used
to make predictions on a per-pixel basis.</p>
<p>Note that the default initializers for the 'basic' model size require that
the number of input channels be equal to the number of output classes, and the
initializers for the 'large' model require it be a multiple.</p>
<p>Based on:
  'Multi-Scale Context Aggregation by Dilated Convolutions'
  Fisher Yu, Vladlen Koltun, ICLR 2016
  https://arxiv.org/abs/1511.07122</p>
<p>Properties:
  conv_modules: list of sonnet modules. The 8 convolution layers used in the
    Dilation module.</p>
<h4 id="netsdilation__init__num_output_classes-initializersnone-regularizersnone-model_sizebasic-namedilation"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/dilation.py?l=227"><code>nets.Dilation.__init__(num_output_classes, initializers=None, regularizers=None, model_size='basic', name='dilation')</code></a><a id="nets.Dilation.__init__" /></h4>
<p>Creates a dilation module.</p>
<h5 id="args_312">Args:</h5>
<ul>
<li><code>num_output_classes</code>: Int. Number of output classes to predict for
    each pixel in an image.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize filters (with key
    'w') or biases (with key 'b'). The default initializer makes this module
    equivalent to the identity.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the weights
    (with key 'w') or biases (with key 'b'). As a default, no regularizers
    are used. A regularizer should be a function that takes a single
    <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output, e.g. the L1
    and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>model_size</code>: string. One of 'basic' or 'large'.</li>
<li><code>name</code>: string. Name of module.</li>
</ul>
<h4 id="netsdilation__call__images"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/dilation.py?l=257"><code>nets.Dilation.__call__(images)</code></a><a id="nets.Dilation.__call__" /></h4>
<p>Build dilation module.</p>
<h5 id="args_313">Args:</h5>
<ul>
<li><code>images</code>: Tensor of shape [batch_size, height, width, depth]
    and dtype float32. Represents a set of images with an arbitrary depth.
    Note that when using the default initializer, depth must equal
    num_output_classes.</li>
</ul>
<h5 id="returns_618">Returns:</h5>
<p>Tensor of shape [batch_size, height, width, num_output_classes] and dtype
    float32. Represents, for each image and pixel, logits for per-class
    predictions.</p>
<h5 id="raises_547">Raises:</h5>
<ul>
<li><code>IncompatibleShapeError</code>: If images is not rank 4.</li>
<li><code>ValueError</code>: If model_size is not one of 'basic' or 'large'.</li>
</ul>
<h4 id="netsdilationconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>nets.Dilation.connected_subgraphs</code></a><a id="nets.Dilation.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="netsdilationconv_modules"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/dilation.py?l=347"><code>nets.Dilation.conv_modules</code></a><a id="nets.Dilation.conv_modules" /></h4>
<h4 id="netsdilationdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>nets.Dilation.defun()</code></a><a id="nets.Dilation.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="netsdilationdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>nets.Dilation.defun_wrapped</code></a><a id="nets.Dilation.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="netsdilationget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>nets.Dilation.get_all_variables(collection='trainable_variables')</code></a><a id="nets.Dilation.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_314">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_619">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_548">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsdilationget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>nets.Dilation.get_possible_initializer_keys(cls)</code></a><a id="nets.Dilation.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_620">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="netsdilationget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>nets.Dilation.get_variables(collection='trainable_variables')</code></a><a id="nets.Dilation.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_315">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_621">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_549">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsdilationgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>nets.Dilation.graph</code></a><a id="nets.Dilation.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="netsdilationis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>nets.Dilation.is_connected</code></a><a id="nets.Dilation.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="netsdilationlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>nets.Dilation.last_connected_subgraph</code></a><a id="nets.Dilation.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_622">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_550">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsdilationmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>nets.Dilation.module_name</code></a><a id="nets.Dilation.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="netsdilationname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>nets.Dilation.name_scopes</code></a><a id="nets.Dilation.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="netsdilationnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>nets.Dilation.non_trainable_variables</code></a><a id="nets.Dilation.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_623">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_551">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsdilationscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>nets.Dilation.scope_name</code></a><a id="nets.Dilation.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="netsdilationtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>nets.Dilation.trainable_variables</code></a><a id="nets.Dilation.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_624">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_552">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsdilationvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>nets.Dilation.variable_scope</code></a><a id="nets.Dilation.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_625">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_553">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsdilationvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>nets.Dilation.variables</code></a><a id="nets.Dilation.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_626">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_554">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-netsmlp"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?q=class:MLP"><code>class nets.MLP</code></a><a id="nets.MLP" /></h3>
<p>A Multi-Layer perceptron module.</p>
<h4 id="netsmlp__init__output_sizes-activationrelu-activate_finalfalse-initializersnone-partitionersnone-regularizersnone-use_biastrue-use_dropoutfalse-custom_getternone-namemlp"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=35"><code>nets.MLP.__init__(output_sizes, activation=relu, activate_final=False, initializers=None, partitioners=None, regularizers=None, use_bias=True, use_dropout=False, custom_getter=None, name='mlp')</code></a><a id="nets.MLP.__init__" /></h4>
<p>Constructs an MLP module.</p>
<h5 id="args_316">Args:</h5>
<ul>
<li><code>output_sizes</code>: An iterable of output dimensionalities as defined in
    <code>basic.Linear</code>. Output size can be defined either as number or via a
    callable. In the latter case, since the function invocation is deferred
    to graph construction time, the user must only ensure that entries can
    be called when build is called. Each entry in the iterable defines
    properties in the corresponding linear layer.</li>
<li><code>activation</code>: An activation op. The activation is applied to intermediate
    layers, and optionally to the output of the final layer.</li>
<li><code>activate_final</code>: Boolean determining if the activation is applied to
    the output of the final layer. Default <code>False</code>.</li>
<li><code>initializers</code>: Optional dict containing ops to initialize the linear
    layers' weights (with key 'w') or biases (with key 'b').</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition the
    linear layers' weights (with key 'w') or biases (with key 'b').</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the linear layers'
    weights (with key 'w') and the biases (with key 'b'). As a default, no
    regularizers are used. A regularizer should be a function that takes
    a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code> output, e.g.
    the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>use_bias</code>: Whether to include bias parameters in the linear layers.
    Default <code>True</code>.</li>
<li><code>use_dropout</code>: Whether to perform dropout on the linear layers.
    Default <code>False</code>.</li>
<li><code>custom_getter</code>: Callable or dictionary of callables to use as
    custom getters inside the module. If a dictionary, the keys
    correspond to regexes to match variable names. See the <code>tf.get_variable</code>
    documentation for information about the custom_getter API.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_555">Raises:</h5>
<ul>
<li><code>KeyError</code>: If initializers contains any keys other than 'w' or 'b'.</li>
<li><code>KeyError</code>: If regularizers contains any keys other than 'w' or 'b'.</li>
<li><code>ValueError</code>: If output_sizes is empty.</li>
<li><code>TypeError</code>: If <code>activation</code> is not callable; or if <code>output_sizes</code> is not
    iterable.</li>
</ul>
<h4 id="netsmlp__call__inputs-is_trainingtrue-dropout_keep_prob05"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=145"><code>nets.MLP.__call__(inputs, is_training=True, dropout_keep_prob=0.5)</code></a><a id="nets.MLP.__call__" /></h4>
<p>Assembles the <code>MLP</code> and connects it to the graph.</p>
<h5 id="args_317">Args:</h5>
<ul>
<li><code>inputs</code>: A 2D Tensor of size <code>[batch_size, input_size]</code>.</li>
<li><code>is_training</code>: A bool or tf.Bool Tensor. Indicates whether we are
    currently training. Defaults to <code>True</code>.</li>
<li><code>dropout_keep_prob</code>: The probability that each element is kept when
    both <code>use_dropout</code> and <code>is_training</code> are True. Defaults to 0.5.</li>
</ul>
<h5 id="returns_627">Returns:</h5>
<p>A 2D Tensor of size <code>[batch_size, output_sizes[-1]]</code>.</p>
<h4 id="netsmlpactivate_final"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=225"><code>nets.MLP.activate_final</code></a><a id="nets.MLP.activate_final" /></h4>
<h4 id="netsmlpactivation"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=221"><code>nets.MLP.activation</code></a><a id="nets.MLP.activation" /></h4>
<h4 id="netsmlpclonenamenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=267"><code>nets.MLP.clone(name=None)</code></a><a id="nets.MLP.clone" /></h4>
<p>Creates a new MLP with the same structure.</p>
<h5 id="args_318">Args:</h5>
<ul>
<li><code>name</code>: Optional string specifying the name of the new module. The default
    name is constructed by appending "_clone" to the original name.</li>
</ul>
<h5 id="returns_628">Returns:</h5>
<p>A cloned <code>MLP</code> module.</p>
<h4 id="netsmlpconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>nets.MLP.connected_subgraphs</code></a><a id="nets.MLP.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="netsmlpdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>nets.MLP.defun()</code></a><a id="nets.MLP.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="netsmlpdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>nets.MLP.defun_wrapped</code></a><a id="nets.MLP.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="netsmlpget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>nets.MLP.get_all_variables(collection='trainable_variables')</code></a><a id="nets.MLP.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_319">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_629">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_556">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsmlpget_possible_initializer_keyscls-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=141"><code>nets.MLP.get_possible_initializer_keys(cls, use_bias=True)</code></a><a id="nets.MLP.get_possible_initializer_keys" /></h4>
<h4 id="netsmlpget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>nets.MLP.get_variables(collection='trainable_variables')</code></a><a id="nets.MLP.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_320">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_630">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_557">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsmlpgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>nets.MLP.graph</code></a><a id="nets.MLP.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="netsmlpinitializers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=206"><code>nets.MLP.initializers</code></a><a id="nets.MLP.initializers" /></h4>
<p>Returns the intializers dictionary.</p>
<h4 id="netsmlpinput_shape"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=230"><code>nets.MLP.input_shape</code></a><a id="nets.MLP.input_shape" /></h4>
<p>Returns shape of input <code>Tensor</code> passed at last call to <code>build</code>.</p>
<h4 id="netsmlpis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>nets.MLP.is_connected</code></a><a id="nets.MLP.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="netsmlplast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>nets.MLP.last_connected_subgraph</code></a><a id="nets.MLP.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_631">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_558">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsmlplayers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=176"><code>nets.MLP.layers</code></a><a id="nets.MLP.layers" /></h4>
<p>Returns a tuple containing the linear layers of the <code>MLP</code>.</p>
<h4 id="netsmlpmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>nets.MLP.module_name</code></a><a id="nets.MLP.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="netsmlpname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>nets.MLP.name_scopes</code></a><a id="nets.MLP.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="netsmlpnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>nets.MLP.non_trainable_variables</code></a><a id="nets.MLP.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_632">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_559">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsmlpoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=186"><code>nets.MLP.output_size</code></a><a id="nets.MLP.output_size" /></h4>
<p>Returns the size of the module output, not including the batch dimension.</p>
<p>This allows the MLP to be used inside a DeepRNN.</p>
<h5 id="returns_633">Returns:</h5>
<p>The scalar size of the module output.</p>
<h4 id="netsmlpoutput_sizes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=181"><code>nets.MLP.output_sizes</code></a><a id="nets.MLP.output_sizes" /></h4>
<p>Returns a tuple of all output sizes of all the layers.</p>
<h4 id="netsmlppartitioners"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=211"><code>nets.MLP.partitioners</code></a><a id="nets.MLP.partitioners" /></h4>
<p>Returns the partitioners dictionary.</p>
<h4 id="netsmlpregularizers"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=216"><code>nets.MLP.regularizers</code></a><a id="nets.MLP.regularizers" /></h4>
<p>Returns the regularizers dictionary.</p>
<h4 id="netsmlpscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>nets.MLP.scope_name</code></a><a id="nets.MLP.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="netsmlptrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>nets.MLP.trainable_variables</code></a><a id="nets.MLP.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_634">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_560">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsmlptransposenamenone-activate_finalnone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=237"><code>nets.MLP.transpose(name=None, activate_final=None)</code></a><a id="nets.MLP.transpose" /></h4>
<p>Returns transposed <code>MLP</code>.</p>
<h5 id="args_321">Args:</h5>
<ul>
<li><code>name</code>: Optional string specifying the name of the transposed module. The
    default name is constructed by appending "_transpose"
    to <code>self.module_name</code>.</li>
<li><code>activate_final</code>: Optional boolean determining if the activation and batch
    normalization, if turned on, are applied to the final layer.</li>
</ul>
<h5 id="returns_635">Returns:</h5>
<p>Matching transposed <code>MLP</code> module.</p>
<h4 id="netsmlpuse_bias"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=198"><code>nets.MLP.use_bias</code></a><a id="nets.MLP.use_bias" /></h4>
<h4 id="netsmlpuse_dropout"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/mlp.py?l=202"><code>nets.MLP.use_dropout</code></a><a id="nets.MLP.use_dropout" /></h4>
<h4 id="netsmlpvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>nets.MLP.variable_scope</code></a><a id="nets.MLP.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_636">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_561">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsmlpvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>nets.MLP.variables</code></a><a id="nets.MLP.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_637">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_562">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-netsvectorquantizer"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py?q=class:VectorQuantizer"><code>class nets.VectorQuantizer</code></a><a id="nets.VectorQuantizer" /></h3>
<p>Sonnet module representing the VQ-VAE layer.</p>
<p>Implements the algorithm presented in
'Neural Discrete Representation Learning' by van den Oord et al.
https://arxiv.org/abs/1711.00937</p>
<p>Input any tensor to be quantized. Last dimension will be used as space in
which to quantize. All other dimensions will be flattened and will be seen
as different examples to quantize.</p>
<p>The output tensor will have the same shape as the input.</p>
<p>For example a tensor with shape [16, 32, 32, 64] will be reshaped into
[16384, 64] and all 16384 vectors (each of 64 dimensions)  will be quantized
independently.</p>
<p>Args:
  embedding_dim: integer representing the dimensionality of the tensors in the
    quantized space. Inputs to the modules must be in this format as well.
  num_embeddings: integer, the number of vectors in the quantized space.
  commitment_cost: scalar which controls the weighting of the loss terms
    (see equation 4 in the paper - this variable is Beta).</p>
<h4 id="netsvectorquantizer__init__embedding_dim-num_embeddings-commitment_cost-namevq_layer"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py?l=54"><code>nets.VectorQuantizer.__init__(embedding_dim, num_embeddings, commitment_cost, name='vq_layer')</code></a><a id="nets.VectorQuantizer.__init__" /></h4>
<h4 id="netsvectorquantizer__call__inputs-is_training"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py?l=66"><code>nets.VectorQuantizer.__call__(inputs, is_training)</code></a><a id="nets.VectorQuantizer.__call__" /></h4>
<p>Connects the module to some inputs.</p>
<h5 id="args_322">Args:</h5>
<ul>
<li><code>inputs</code>: Tensor, final dimension must be equal to embedding_dim. All other
    leading dimensions will be flattened and treated as a large batch.</li>
<li><code>is_training</code>: boolean, whether this connection is to training data.</li>
</ul>
<h5 id="returns_638">Returns:</h5>
<p>dict containing the following keys and values:</p>
<ul>
<li><code>quantize</code>: Tensor containing the quantized version of the input.</li>
<li><code>loss</code>: Tensor containing the loss to optimize.</li>
<li><code>perplexity</code>: Tensor containing the perplexity of the encodings.</li>
<li><code>encodings</code>: Tensor containing the discrete encodings, ie which element
      of the quantized space each input element was mapped to.</li>
<li><code>encoding_indices</code>: Tensor containing the discrete encoding indices, ie
      which element of the quantized space each input element was mapped to.</li>
</ul>
<h4 id="netsvectorquantizerconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>nets.VectorQuantizer.connected_subgraphs</code></a><a id="nets.VectorQuantizer.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="netsvectorquantizerdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>nets.VectorQuantizer.defun()</code></a><a id="nets.VectorQuantizer.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="netsvectorquantizerdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>nets.VectorQuantizer.defun_wrapped</code></a><a id="nets.VectorQuantizer.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="netsvectorquantizerembeddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py?l=114"><code>nets.VectorQuantizer.embeddings</code></a><a id="nets.VectorQuantizer.embeddings" /></h4>
<h4 id="netsvectorquantizerget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>nets.VectorQuantizer.get_all_variables(collection='trainable_variables')</code></a><a id="nets.VectorQuantizer.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_323">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_639">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_563">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsvectorquantizerget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>nets.VectorQuantizer.get_possible_initializer_keys(cls)</code></a><a id="nets.VectorQuantizer.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_640">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="netsvectorquantizerget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>nets.VectorQuantizer.get_variables(collection='trainable_variables')</code></a><a id="nets.VectorQuantizer.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_324">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_641">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_564">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsvectorquantizergraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>nets.VectorQuantizer.graph</code></a><a id="nets.VectorQuantizer.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="netsvectorquantizeris_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>nets.VectorQuantizer.is_connected</code></a><a id="nets.VectorQuantizer.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="netsvectorquantizerlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>nets.VectorQuantizer.last_connected_subgraph</code></a><a id="nets.VectorQuantizer.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_642">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_565">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsvectorquantizermodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>nets.VectorQuantizer.module_name</code></a><a id="nets.VectorQuantizer.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="netsvectorquantizername_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>nets.VectorQuantizer.name_scopes</code></a><a id="nets.VectorQuantizer.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="netsvectorquantizernon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>nets.VectorQuantizer.non_trainable_variables</code></a><a id="nets.VectorQuantizer.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_643">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_566">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsvectorquantizerquantizeencoding_indices"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py?l=118"><code>nets.VectorQuantizer.quantize(encoding_indices)</code></a><a id="nets.VectorQuantizer.quantize" /></h4>
<h4 id="netsvectorquantizerscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>nets.VectorQuantizer.scope_name</code></a><a id="nets.VectorQuantizer.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="netsvectorquantizertrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>nets.VectorQuantizer.trainable_variables</code></a><a id="nets.VectorQuantizer.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_644">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_567">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsvectorquantizervariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>nets.VectorQuantizer.variable_scope</code></a><a id="nets.VectorQuantizer.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_645">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_568">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsvectorquantizervariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>nets.VectorQuantizer.variables</code></a><a id="nets.VectorQuantizer.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_646">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_569">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="class-netsvectorquantizerema"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py?q=class:VectorQuantizerEMA"><code>class nets.VectorQuantizerEMA</code></a><a id="nets.VectorQuantizerEMA" /></h3>
<p>Sonnet module representing the VQ-VAE layer.</p>
<p>Implements a slightly modified version of the algorithm presented in
'Neural Discrete Representation Learning' by van den Oord et al.
https://arxiv.org/abs/1711.00937</p>
<p>The difference between VectorQuantizerEMA and VectorQuantizer is that
this module uses exponential moving averages to update the embedding vectors
instead of an auxiliary loss. This has the advantage that the embedding
updates are independent of the choice of optimizer (SGD, RMSProp, Adam, K-Fac,
...) used for the encoder, decoder and other parts of the architecture. For
most experiments the EMA version trains faster than the non-EMA version.</p>
<p>Input any tensor to be quantized. Last dimension will be used as space in
which to quantize. All other dimensions will be flattened and will be seen
as different examples to quantize.</p>
<p>The output tensor will have the same shape as the input.</p>
<p>For example a tensor with shape [16, 32, 32, 64] will be reshaped into
[16384, 64] and all 16384 vectors (each of 64 dimensions)  will be quantized
independently.</p>
<p>Args:
  embedding_dim: integer representing the dimensionality of the tensors in the
    quantized space. Inputs to the modules must be in this format as well.
  num_embeddings: integer, the number of vectors in the quantized space.
  commitment_cost: scalar which controls the weighting of the loss terms (see
    equation 4 in the paper).
  decay: float, decay for the moving averages.
  epsilon: small float constant to avoid numerical instability.</p>
<h4 id="netsvectorquantizerema__init__embedding_dim-num_embeddings-commitment_cost-decay-epsilon1e-05-namevectorquantizerema"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py?l=158"><code>nets.VectorQuantizerEMA.__init__(embedding_dim, num_embeddings, commitment_cost, decay, epsilon=1e-05, name='VectorQuantizerEMA')</code></a><a id="nets.VectorQuantizerEMA.__init__" /></h4>
<h4 id="netsvectorquantizerema__call__inputs-is_training"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py?l=181"><code>nets.VectorQuantizerEMA.__call__(inputs, is_training)</code></a><a id="nets.VectorQuantizerEMA.__call__" /></h4>
<p>Connects the module to some inputs.</p>
<h5 id="args_325">Args:</h5>
<ul>
<li><code>inputs</code>: Tensor, final dimension must be equal to embedding_dim. All other
    leading dimensions will be flattened and treated as a large batch.</li>
<li><code>is_training</code>: boolean, whether this connection is to training data. When
    this is set to False, the internal moving average statistics will not be
    updated.</li>
</ul>
<h5 id="returns_647">Returns:</h5>
<p>dict containing the following keys and values:</p>
<ul>
<li><code>quantize</code>: Tensor containing the quantized version of the input.</li>
<li><code>loss</code>: Tensor containing the loss to optimize.</li>
<li><code>perplexity</code>: Tensor containing the perplexity of the encodings.</li>
<li><code>encodings</code>: Tensor containing the discrete encodings, ie which element
      of the quantized space each input element was mapped to.</li>
<li><code>encoding_indices</code>: Tensor containing the discrete encoding indices, ie
      which element of the quantized space each input element was mapped to.</li>
</ul>
<h4 id="netsvectorquantizeremaconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>nets.VectorQuantizerEMA.connected_subgraphs</code></a><a id="nets.VectorQuantizerEMA.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="netsvectorquantizeremadefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>nets.VectorQuantizerEMA.defun()</code></a><a id="nets.VectorQuantizerEMA.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="netsvectorquantizeremadefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>nets.VectorQuantizerEMA.defun_wrapped</code></a><a id="nets.VectorQuantizerEMA.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="netsvectorquantizeremaembeddings"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py?l=254"><code>nets.VectorQuantizerEMA.embeddings</code></a><a id="nets.VectorQuantizerEMA.embeddings" /></h4>
<h4 id="netsvectorquantizeremaget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>nets.VectorQuantizerEMA.get_all_variables(collection='trainable_variables')</code></a><a id="nets.VectorQuantizerEMA.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_326">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_648">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_570">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsvectorquantizeremaget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>nets.VectorQuantizerEMA.get_possible_initializer_keys(cls)</code></a><a id="nets.VectorQuantizerEMA.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_649">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="netsvectorquantizeremaget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>nets.VectorQuantizerEMA.get_variables(collection='trainable_variables')</code></a><a id="nets.VectorQuantizerEMA.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_327">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_650">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_571">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsvectorquantizeremagraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>nets.VectorQuantizerEMA.graph</code></a><a id="nets.VectorQuantizerEMA.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="netsvectorquantizeremais_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>nets.VectorQuantizerEMA.is_connected</code></a><a id="nets.VectorQuantizerEMA.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="netsvectorquantizeremalast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>nets.VectorQuantizerEMA.last_connected_subgraph</code></a><a id="nets.VectorQuantizerEMA.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_651">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_572">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsvectorquantizeremamodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>nets.VectorQuantizerEMA.module_name</code></a><a id="nets.VectorQuantizerEMA.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="netsvectorquantizeremaname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>nets.VectorQuantizerEMA.name_scopes</code></a><a id="nets.VectorQuantizerEMA.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="netsvectorquantizeremanon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>nets.VectorQuantizerEMA.non_trainable_variables</code></a><a id="nets.VectorQuantizerEMA.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_652">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_573">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsvectorquantizeremaquantizeencoding_indices"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py?l=258"><code>nets.VectorQuantizerEMA.quantize(encoding_indices)</code></a><a id="nets.VectorQuantizerEMA.quantize" /></h4>
<h4 id="netsvectorquantizeremascope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>nets.VectorQuantizerEMA.scope_name</code></a><a id="nets.VectorQuantizerEMA.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="netsvectorquantizerematrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>nets.VectorQuantizerEMA.trainable_variables</code></a><a id="nets.VectorQuantizerEMA.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_653">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_574">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsvectorquantizeremavariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>nets.VectorQuantizerEMA.variable_scope</code></a><a id="nets.VectorQuantizerEMA.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_654">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_575">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="netsvectorquantizeremavariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>nets.VectorQuantizerEMA.variables</code></a><a id="nets.VectorQuantizerEMA.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_655">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_576">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h3 id="netsidentity_kernel_initializershape-dtypetffloat32-partition_infonone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/dilation.py?l=74"><code>nets.identity_kernel_initializer(shape, dtype=tf.float32, partition_info=None)</code></a><a id="nets.identity_kernel_initializer" /></h3>
<p>An initializer for constructing identity convolution kernels.</p>
<p>Constructs a convolution kernel such that applying it is the same as an
identity operation on the input. Formally, the kernel has entry [i, j, in,
out] = 1 if in equals out and i and j are the middle of the kernel and 0
otherwise.</p>
<h5 id="args_328">Args:</h5>
<ul>
<li><code>shape</code>: List of integers. Represents shape of result.</li>
<li><code>dtype</code>: data type for values in result.</li>
<li><code>partition_info</code>: Partition information for initializer functions. Ignored.</li>
</ul>
<h5 id="returns_656">Returns:</h5>
<p>Tensor of desired shape and dtype such that applying it as a convolution
    kernel results in the identity operation.</p>
<h5 id="raises_577">Raises:</h5>
<ul>
<li><code>ValueError</code>: If shape does not define a valid kernel.
              If filter width and height differ.
              If filter width and height are not odd numbers.
              If number of input and output channels differ.</li>
</ul>
<h3 id="netsnoisy_identity_kernel_initializerbase_num_channels-stddev1e-08"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/dilation.py?l=121"><code>nets.noisy_identity_kernel_initializer(base_num_channels, stddev=1e-08)</code></a><a id="nets.noisy_identity_kernel_initializer" /></h3>
<p>Build an initializer for constructing near-identity convolution kernels.</p>
<p>Construct a convolution kernel where in_channels and out_channels are
multiples of base_num_channels, but need not be equal. This initializer is
essentially the same as identity_kernel_initializer, except that magnitude
is "spread out" across multiple copies of the input.</p>
<h5 id="args_329">Args:</h5>
<ul>
<li><code>base_num_channels</code>: int. Number that divides both in_channels and
    out_channels.</li>
<li><code>stddev</code>: float. Standard deviation of truncated normal noise added to
    off-entries to break ties.</li>
</ul>
<h5 id="returns_657">Returns:</h5>
<p>Initializer function for building a noisy identity kernel.</p>
<h3 id="class-protosmodule_pb2nesteddata"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/protos/module_pb2.py?q=class:NestedData"><code>class protos.module_pb2.NestedData</code></a><a id="protos.module_pb2.NestedData" /></h3>
<h3 id="class-protosmodule_pb2sonnetmodule"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/protos/module_pb2.py?q=class:SonnetModule"><code>class protos.module_pb2.SonnetModule</code></a><a id="protos.module_pb2.SonnetModule" /></h3>
<h3 id="protosmodule_pb2_bx"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/protos/module_pb2.py?l=6"><code>protos.module_pb2._b(x)</code></a><a id="protos.module_pb2._b" /></h3>
<h3 id="class-pythonmodulesattentionattentionoutput"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/attention.py?q=class:AttentionOutput"><code>class python.modules.attention.AttentionOutput</code></a><a id="python.modules.attention.AttentionOutput" /></h3>
<p>AttentionOutput(read, weights, weight_logits)</p>
<h4 id="pythonmodulesattentionattentionoutputread"><code>python.modules.attention.AttentionOutput.read</code><a id="python.modules.attention.AttentionOutput.read" /></h4>
<p>Alias for field number 0</p>
<h4 id="pythonmodulesattentionattentionoutputweight_logits"><code>python.modules.attention.AttentionOutput.weight_logits</code><a id="python.modules.attention.AttentionOutput.weight_logits" /></h4>
<p>Alias for field number 2</p>
<h4 id="pythonmodulesattentionattentionoutputweights"><code>python.modules.attention.AttentionOutput.weights</code><a id="python.modules.attention.AttentionOutput.weights" /></h4>
<p>Alias for field number 1</p>
<h3 id="pythonmodulesbaseget_connection_stack"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=71"><code>python.modules.base.get_connection_stack()</code></a><a id="python.modules.base.get_connection_stack" /></h3>
<h3 id="pythonmodulesbaseget_module_stack"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=70"><code>python.modules.base.get_module_stack()</code></a><a id="python.modules.base.get_module_stack" /></h3>
<h3 id="class-pythonmodulesbase_infoconnectedsubgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base_info.py?q=class:ConnectedSubGraph"><code>class python.modules.base_info.ConnectedSubGraph</code></a><a id="python.modules.base_info.ConnectedSubGraph" /></h3>
<p>ConnectedSubGraph(module, name_scope, inputs, outputs)</p>
<h4 id="pythonmodulesbase_infoconnectedsubgraphinputs"><code>python.modules.base_info.ConnectedSubGraph.inputs</code><a id="python.modules.base_info.ConnectedSubGraph.inputs" /></h4>
<p>Alias for field number 2</p>
<h4 id="pythonmodulesbase_infoconnectedsubgraphmodule"><code>python.modules.base_info.ConnectedSubGraph.module</code><a id="python.modules.base_info.ConnectedSubGraph.module" /></h4>
<p>Alias for field number 0</p>
<h4 id="pythonmodulesbase_infoconnectedsubgraphname_scope"><code>python.modules.base_info.ConnectedSubGraph.name_scope</code><a id="python.modules.base_info.ConnectedSubGraph.name_scope" /></h4>
<p>Alias for field number 1</p>
<h4 id="pythonmodulesbase_infoconnectedsubgraphoutputs"><code>python.modules.base_info.ConnectedSubGraph.outputs</code><a id="python.modules.base_info.ConnectedSubGraph.outputs" /></h4>
<p>Alias for field number 3</p>
<h3 id="class-pythonmodulesbase_infomoduleinfo"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base_info.py?q=class:ModuleInfo"><code>class python.modules.base_info.ModuleInfo</code></a><a id="python.modules.base_info.ModuleInfo" /></h3>
<p>ModuleInfo(module_name, scope_name, class_name, connected_subgraphs)</p>
<h4 id="pythonmodulesbase_infomoduleinfoclass_name"><code>python.modules.base_info.ModuleInfo.class_name</code><a id="python.modules.base_info.ModuleInfo.class_name" /></h4>
<p>Alias for field number 2</p>
<h4 id="pythonmodulesbase_infomoduleinfoconnected_subgraphs"><code>python.modules.base_info.ModuleInfo.connected_subgraphs</code><a id="python.modules.base_info.ModuleInfo.connected_subgraphs" /></h4>
<p>Alias for field number 3</p>
<h4 id="pythonmodulesbase_infomoduleinfomodule_name"><code>python.modules.base_info.ModuleInfo.module_name</code><a id="python.modules.base_info.ModuleInfo.module_name" /></h4>
<p>Alias for field number 0</p>
<h4 id="pythonmodulesbase_infomoduleinfoscope_name"><code>python.modules.base_info.ModuleInfo.scope_name</code><a id="python.modules.base_info.ModuleInfo.scope_name" /></h4>
<p>Alias for field number 1</p>
<h3 id="pythonmodulesbasiccalculate_bias_shapeinput_shape-bias_dims"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=464"><code>python.modules.basic.calculate_bias_shape(input_shape, bias_dims)</code></a><a id="python.modules.basic.calculate_bias_shape" /></h3>
<p>Calculate <code>bias_shape</code> based on the <code>input_shape</code> and <code>bias_dims</code>.</p>
<h5 id="args_330">Args:</h5>
<ul>
<li><code>input_shape</code>: Shape of the input being passed into the module. The leading
      dimension is the minibatch size.</li>
<li><code>bias_dims</code>: The dimensions that bias should be applied over. The remaining
      dimensions will get broadcasted over.</li>
</ul>
<h5 id="returns_658">Returns:</h5>
<ul>
<li><code>bias_shape</code>: Tuple corresponding to the shape of bias Variable to create.</li>
</ul>
<h5 id="raises_578">Raises:</h5>
<ul>
<li><code>ValueError</code>: If the user attempts to add bias over the minibatch dimension,
      e.g. <code>bias_dims=[0]</code>.</li>
</ul>
<h3 id="pythonmodulesbasiccreate_bias_initializerunused_bias_shape-dtypetffloat32"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=129"><code>python.modules.basic.create_bias_initializer(unused_bias_shape, dtype=tf.float32)</code></a><a id="python.modules.basic.create_bias_initializer" /></h3>
<p>Returns a default initializer for the biases of a linear/AddBias module.</p>
<h3 id="pythonmodulesbasiccreate_linear_initializerinput_size-dtypetffloat32"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=123"><code>python.modules.basic.create_linear_initializer(input_size, dtype=tf.float32)</code></a><a id="python.modules.basic.create_linear_initializer" /></h3>
<p>Returns a default initializer for weights of a linear module.</p>
<h3 id="pythonmodulesbatch_normcreate_beta_initializer"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=35"><code>python.modules.batch_norm.create_beta_initializer()</code></a><a id="python.modules.batch_norm.create_beta_initializer" /></h3>
<p>Returns a default initializer for the <code>beta</code> in batch norm.</p>
<h3 id="pythonmodulesbatch_normcreate_gamma_initializer"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=40"><code>python.modules.batch_norm.create_gamma_initializer()</code></a><a id="python.modules.batch_norm.create_gamma_initializer" /></h3>
<p>Returns a default initializer for the <code>gamma</code> in batch norm.</p>
<h3 id="pythonmodulesbatch_normcreate_mean_initializer"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=45"><code>python.modules.batch_norm.create_mean_initializer()</code></a><a id="python.modules.batch_norm.create_mean_initializer" /></h3>
<p>Returns a default initializer for the <code>moving_mean</code> in batch norm.</p>
<h3 id="pythonmodulesbatch_normcreate_variance_initializer"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm.py?l=50"><code>python.modules.batch_norm.create_variance_initializer()</code></a><a id="python.modules.batch_norm.create_variance_initializer" /></h3>
<p>Returns a default initializer for the <code>moving_variance</code> in batch norm.</p>
<h3 id="pythonmodulesbatch_norm_v2create_beta_initializer"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=48"><code>python.modules.batch_norm_v2.create_beta_initializer()</code></a><a id="python.modules.batch_norm_v2.create_beta_initializer" /></h3>
<p>Returns a default initializer for the <code>beta</code> in batch norm.</p>
<h3 id="pythonmodulesbatch_norm_v2create_gamma_initializer"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=53"><code>python.modules.batch_norm_v2.create_gamma_initializer()</code></a><a id="python.modules.batch_norm_v2.create_gamma_initializer" /></h3>
<p>Returns a default initializer for the <code>gamma</code> in batch norm.</p>
<h3 id="pythonmodulesbatch_norm_v2create_mean_initializer"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=58"><code>python.modules.batch_norm_v2.create_mean_initializer()</code></a><a id="python.modules.batch_norm_v2.create_mean_initializer" /></h3>
<p>Returns a default initializer for the <code>moving_mean</code> in batch norm.</p>
<h3 id="pythonmodulesbatch_norm_v2create_variance_initializer"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/batch_norm_v2.py?l=63"><code>python.modules.batch_norm_v2.create_variance_initializer()</code></a><a id="python.modules.batch_norm_v2.create_variance_initializer" /></h3>
<p>Returns a default initializer for the <code>moving_variance</code> in batch norm.</p>
<h3 id="pythonmodulesconvcreate_bias_initializerunused_bias_shape-dtypetffloat32"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=302"><code>python.modules.conv.create_bias_initializer(unused_bias_shape, dtype=tf.float32)</code></a><a id="python.modules.conv.create_bias_initializer" /></h3>
<p>Returns a default initializer for the biases of a convolutional module.</p>
<h3 id="pythonmodulesconvcreate_weight_initializerfan_in_shape-dtypetffloat32"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/conv.py?l=296"><code>python.modules.conv.create_weight_initializer(fan_in_shape, dtype=tf.float32)</code></a><a id="python.modules.conv.create_weight_initializer" /></h3>
<p>Returns a default initializer for the weights of a convolutional module.</p>
<h3 id="class-pythonmodulesgated_rnnconvlstm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?q=class:ConvLSTM"><code>class python.modules.gated_rnn.ConvLSTM</code></a><a id="python.modules.gated_rnn.ConvLSTM" /></h3>
<p>Convolutional LSTM.</p>
<h4 id="pythonmodulesgated_rnnconvlstm__init__conv_ndims-input_shape-output_channels-kernel_shape-stride1-rate1-paddingsame-use_biastrue-legacy_bias_behaviourtrue-forget_bias10-initializersnone-partitionersnone-regularizersnone-use_layer_normfalse-custom_getternone-nameconv_lstm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1225"><code>python.modules.gated_rnn.ConvLSTM.__init__(conv_ndims, input_shape, output_channels, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, legacy_bias_behaviour=True, forget_bias=1.0, initializers=None, partitioners=None, regularizers=None, use_layer_norm=False, custom_getter=None, name='conv_lstm')</code></a><a id="python.modules.gated_rnn.ConvLSTM.__init__" /></h4>
<p>Construct ConvLSTM.</p>
<h5 id="args_331">Args:</h5>
<ul>
<li><code>conv_ndims</code>: Convolution dimensionality (1, 2 or 3).</li>
<li><code>input_shape</code>: Shape of the input as an iterable, excluding the batch size.</li>
<li><code>output_channels</code>: Number of output channels of the conv LSTM.</li>
<li><code>kernel_shape</code>: Sequence of kernel sizes (of size conv_ndims), or integer
      that is used to define kernel size in all dimensions.</li>
<li><code>stride</code>: Sequence of kernel strides (of size conv_ndims), or integer that
      is used to define stride in all dimensions.</li>
<li><code>rate</code>: Sequence of dilation rates (of size conv_ndims), or integer that is
      used to define dilation rate in all dimensions. 1 corresponds to a
      standard convolution, while rate &gt; 1 corresponds to a dilated
      convolution. Cannot be &gt; 1 if any of stride is also &gt; 1.</li>
<li><code>padding</code>: Padding algorithm, either <code>snt.SAME</code> or <code>snt.VALID</code>.</li>
<li><code>use_bias</code>: Use bias in convolutions.</li>
<li><code>legacy_bias_behaviour</code>: If True, bias is applied to both input and hidden
    convolutions, creating a redundant bias variable. If False, bias is only
    applied to input convolution, removing the redundancy.</li>
<li><code>forget_bias</code>: Forget bias.</li>
<li><code>initializers</code>: Dict containing ops to initialize the convolutional weights.</li>
<li><code>partitioners</code>: Optional dict containing partitioners to partition
    the convolutional weights and biases. As a default, no partitioners are
    used.</li>
<li><code>regularizers</code>: Optional dict containing regularizers for the convolutional
    weights and biases. As a default, no regularizers are used.</li>
<li><code>use_layer_norm</code>: Boolean that indicates whether to apply layer
    normalization. This is applied across the entire layer, normalizing
    over all non-batch dimensions.</li>
<li><code>custom_getter</code>: Callable that takes as a first argument the true getter,
    and allows overwriting the internal get_variable method. See the
    <code>tf.get_variable</code> documentation for more details.</li>
<li><code>name</code>: Name of the module.</li>
</ul>
<h5 id="raises_579">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>skip_connection</code> is <code>True</code> and stride is different from 1
    or if <code>input_shape</code> is incompatible with <code>conv_ndims</code>.</li>
</ul>
<h4 id="pythonmodulesgated_rnnconvlstm__call__inputs-state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1364"><code>python.modules.gated_rnn.ConvLSTM.__call__(inputs, state)</code></a><a id="python.modules.gated_rnn.ConvLSTM.__call__" /></h4>
<h4 id="pythonmodulesgated_rnnconvlstmconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>python.modules.gated_rnn.ConvLSTM.connected_subgraphs</code></a><a id="python.modules.gated_rnn.ConvLSTM.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="pythonmodulesgated_rnnconvlstmconvolutions"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1347"><code>python.modules.gated_rnn.ConvLSTM.convolutions</code></a><a id="python.modules.gated_rnn.ConvLSTM.convolutions" /></h4>
<h4 id="pythonmodulesgated_rnnconvlstmdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>python.modules.gated_rnn.ConvLSTM.defun()</code></a><a id="python.modules.gated_rnn.ConvLSTM.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="pythonmodulesgated_rnnconvlstmdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>python.modules.gated_rnn.ConvLSTM.defun_wrapped</code></a><a id="python.modules.gated_rnn.ConvLSTM.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="pythonmodulesgated_rnnconvlstmget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>python.modules.gated_rnn.ConvLSTM.get_all_variables(collection='trainable_variables')</code></a><a id="python.modules.gated_rnn.ConvLSTM.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_332">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_659">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_580">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnconvlstmget_possible_initializer_keyscls-conv_ndims-use_biastrue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1209"><code>python.modules.gated_rnn.ConvLSTM.get_possible_initializer_keys(cls, conv_ndims, use_bias=True)</code></a><a id="python.modules.gated_rnn.ConvLSTM.get_possible_initializer_keys" /></h4>
<h4 id="pythonmodulesgated_rnnconvlstmget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>python.modules.gated_rnn.ConvLSTM.get_variables(collection='trainable_variables')</code></a><a id="python.modules.gated_rnn.ConvLSTM.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_333">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_660">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_581">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnconvlstmgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>python.modules.gated_rnn.ConvLSTM.graph</code></a><a id="python.modules.gated_rnn.ConvLSTM.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="pythonmodulesgated_rnnconvlstminitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=203"><code>python.modules.gated_rnn.ConvLSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)</code></a><a id="python.modules.gated_rnn.ConvLSTM.initial_state" /></h4>
<p>Builds the default start state for an RNNCore.</p>
<h5 id="args_334">Args:</h5>
<ul>
<li><code>batch_size</code>: An int, or scalar int32 Tensor representing the batch size.</li>
<li><code>dtype</code>: The data type to use for the state.</li>
<li><code>trainable</code>: Boolean that indicates whether to learn the initial state.
    Note that intializers and regularizers will be ignored if
    <code>trainable=False</code>.</li>
<li><code>trainable_initializers</code>: An initializer function or nested structure of
      functions with same structure as the <code>state_size</code> property of the
      core, to be used as initializers of the initial state variable.</li>
<li><code>trainable_regularizers</code>: Optional regularizer function or nested structure
    of functions with the same structure as the <code>state_size</code> property of the
    core, to be used as regularizers of the initial state variable. As a
    default, no regularizers are used. A regularizer should be a function
    that takes a single <code>Tensor</code> as an input and returns a scalar <code>Tensor</code>
    output, e.g. the L1 and L2 regularizers in <code>tf.contrib.layers</code>.</li>
<li><code>name</code>: Optional string used to prefix the initial state variable names, in
      the case of a trainable initial state. If not provided, defaults to
      the name of the module.</li>
</ul>
<h5 id="returns_661">Returns:</h5>
<p>A tensor or nested tuple of tensors with same structure and shape as the
  <code>state_size</code> property of the core.</p>
<h5 id="raises_582">Raises:</h5>
<ul>
<li><code>ValueError</code>: if the user passes initializers that are not functions.</li>
<li><code>ValueError</code>: if the user passes regularizers that are not functions.</li>
</ul>
<h4 id="pythonmodulesgated_rnnconvlstmis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>python.modules.gated_rnn.ConvLSTM.is_connected</code></a><a id="python.modules.gated_rnn.ConvLSTM.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="pythonmodulesgated_rnnconvlstmlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>python.modules.gated_rnn.ConvLSTM.last_connected_subgraph</code></a><a id="python.modules.gated_rnn.ConvLSTM.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_662">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_583">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnconvlstmmodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>python.modules.gated_rnn.ConvLSTM.module_name</code></a><a id="python.modules.gated_rnn.ConvLSTM.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="pythonmodulesgated_rnnconvlstmname_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>python.modules.gated_rnn.ConvLSTM.name_scopes</code></a><a id="python.modules.gated_rnn.ConvLSTM.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="pythonmodulesgated_rnnconvlstmnon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>python.modules.gated_rnn.ConvLSTM.non_trainable_variables</code></a><a id="python.modules.gated_rnn.ConvLSTM.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_663">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_584">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnconvlstmoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1358"><code>python.modules.gated_rnn.ConvLSTM.output_size</code></a><a id="python.modules.gated_rnn.ConvLSTM.output_size" /></h4>
<p><code>tf.TensorShape</code> indicating the size of the core output.</p>
<h4 id="pythonmodulesgated_rnnconvlstmscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>python.modules.gated_rnn.ConvLSTM.scope_name</code></a><a id="python.modules.gated_rnn.ConvLSTM.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="pythonmodulesgated_rnnconvlstmstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1351"><code>python.modules.gated_rnn.ConvLSTM.state_size</code></a><a id="python.modules.gated_rnn.ConvLSTM.state_size" /></h4>
<p>Tuple of <code>tf.TensorShape</code>s indicating the size of state tensors.</p>
<h4 id="pythonmodulesgated_rnnconvlstmtrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>python.modules.gated_rnn.ConvLSTM.trainable_variables</code></a><a id="python.modules.gated_rnn.ConvLSTM.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_664">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_585">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnconvlstmuse_layer_norm"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=1396"><code>python.modules.gated_rnn.ConvLSTM.use_layer_norm</code></a><a id="python.modules.gated_rnn.ConvLSTM.use_layer_norm" /></h4>
<p>Boolean indicating whether layer norm is enabled.</p>
<h4 id="pythonmodulesgated_rnnconvlstmvariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>python.modules.gated_rnn.ConvLSTM.variable_scope</code></a><a id="python.modules.gated_rnn.ConvLSTM.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_665">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_586">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnconvlstmvariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>python.modules.gated_rnn.ConvLSTM.variables</code></a><a id="python.modules.gated_rnn.ConvLSTM.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_666">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_587">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnconvlstmzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>python.modules.gated_rnn.ConvLSTM.zero_state(batch_size, dtype)</code></a><a id="python.modules.gated_rnn.ConvLSTM.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_335">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_667">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-pythonmodulesgated_rnnrecurrentdropoutwrapper"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?q=class:RecurrentDropoutWrapper"><code>class python.modules.gated_rnn.RecurrentDropoutWrapper</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper" /></h3>
<p>Wraps an RNNCore so that recurrent dropout can be applied.</p>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapper__init__core-keep_probs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=361"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.__init__(core, keep_probs)</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.__init__" /></h4>
<p>Builds a new wrapper around a given core.</p>
<h5 id="args_336">Args:</h5>
<ul>
<li><code>core</code>: the RNN core to be wrapped.</li>
<li><code>keep_probs</code>: the recurrent dropout keep probabilities to apply.
    This should have the same structure has core.init_state. No dropout is
    applied for leafs set to None.</li>
</ul>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapper__call__inputs-prev_state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=392"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.__call__(inputs, prev_state)</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.__call__" /></h4>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.connected_subgraphs</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.defun()</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.defun_wrapped</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.get_all_variables(collection='trainable_variables')</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_337">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_668">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_588">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.get_possible_initializer_keys(cls)</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_669">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.get_variables(collection='trainable_variables')</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_338">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_670">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_589">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrappergraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.graph</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=404"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None)</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.initial_state" /></h4>
<p>Builds the default start state tensor of zeros.</p>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.is_connected</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.last_connected_subgraph</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_671">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_590">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrappermodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.module_name</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrappername_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.name_scopes</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrappernon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.non_trainable_variables</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_672">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_591">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=428"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.output_size</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.output_size" /></h4>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.scope_name</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=424"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.state_size</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.state_size" /></h4>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrappertrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.trainable_variables</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_673">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_592">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrappervariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.variable_scope</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_674">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_593">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrappervariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.variables</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_675">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_594">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnrecurrentdropoutwrapperzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>python.modules.gated_rnn.RecurrentDropoutWrapper.zero_state(batch_size, dtype)</code></a><a id="python.modules.gated_rnn.RecurrentDropoutWrapper.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_339">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_676">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="class-pythonmodulesgated_rnnzoneoutwrapper"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?q=class:ZoneoutWrapper"><code>class python.modules.gated_rnn.ZoneoutWrapper</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper" /></h3>
<p>Wraps an RNNCore so that zoneout can be applied.</p>
<p>Zoneout was introduced in https://arxiv.org/abs/1606.01305
It consists of randomly freezing some RNN state in the same way recurrent
dropout would replace this state with zero.</p>
<h4 id="pythonmodulesgated_rnnzoneoutwrapper__init__core-keep_probs-is_training"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=459"><code>python.modules.gated_rnn.ZoneoutWrapper.__init__(core, keep_probs, is_training)</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.__init__" /></h4>
<p>Builds a new wrapper around a given core.</p>
<h5 id="args_340">Args:</h5>
<ul>
<li><code>core</code>: the RNN core to be wrapped.</li>
<li><code>keep_probs</code>: the probabilities to use the updated states rather than
    keeping the old state values. This is one minus the probability
    that zoneout gets applied.
    This should have the same structure has core.init_state. No zoneout is
    applied for leafs set to None.</li>
<li><code>is_training</code>: when set, apply some stochastic zoneout. Otherwise perform
    a linear combination of the previous state and the current state based
    on the zoneout probability.</li>
</ul>
<h4 id="pythonmodulesgated_rnnzoneoutwrapper__call__inputs-prev_state"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=480"><code>python.modules.gated_rnn.ZoneoutWrapper.__call__(inputs, prev_state)</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.__call__" /></h4>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperconnected_subgraphs"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=479"><code>python.modules.gated_rnn.ZoneoutWrapper.connected_subgraphs</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.connected_subgraphs" /></h4>
<p>Returns the subgraphs created by this module so far.</p>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperdefun"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=391"><code>python.modules.gated_rnn.ZoneoutWrapper.defun()</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.defun" /></h4>
<p>Wraps this modules call method in a callable graph function.</p>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperdefun_wrapped"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=386"><code>python.modules.gated_rnn.ZoneoutWrapper.defun_wrapped</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.defun_wrapped" /></h4>
<p>Returns boolean indicating whether this module is defun wrapped.</p>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperget_all_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=677"><code>python.modules.gated_rnn.ZoneoutWrapper.get_all_variables(collection='trainable_variables')</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.get_all_variables" /></h4>
<p>Returns all <code>tf.Variable</code>s used when the module is connected.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information.</p>
<h5 id="args_341">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_677">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_595">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperget_possible_initializer_keyscls"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=503"><code>python.modules.gated_rnn.ZoneoutWrapper.get_possible_initializer_keys(cls)</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.get_possible_initializer_keys" /></h4>
<p>Returns the keys the dictionary of variable initializers may contain.</p>
<p>This provides the user with a way of knowing the initializer keys that are
available without having to instantiate a sonnet module. Subclasses may
override this class method if they need additional arguments to determine
what initializer keys may be provided.</p>
<h5 id="returns_678">Returns:</h5>
<p>Set with strings corresponding to the strings that may be passed to the
      constructor.</p>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperget_variablescollectiontrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=640"><code>python.modules.gated_rnn.ZoneoutWrapper.get_variables(collection='trainable_variables')</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.get_variables" /></h4>
<p>Returns tuple of <code>tf.Variable</code>s declared inside this module.</p>
<p>Note that this operates by searching this module's variable scope,
and so does not know about any modules that were constructed elsewhere but
used inside this module.</p>
<p>This method explicitly re-enters the Graph which this module has been
connected to.</p>
<h5 id="args_342">Args:</h5>
<ul>
<li><code>collection</code>: Collection to restrict query to. By default this is
    <code>tf.Graphkeys.TRAINABLE_VARIABLES</code>, which doesn't include non-trainable
    variables such as moving averages.</li>
</ul>
<h5 id="returns_679">Returns:</h5>
<p>A tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_596">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnzoneoutwrappergraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=474"><code>python.modules.gated_rnn.ZoneoutWrapper.graph</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.graph" /></h4>
<p>Returns the Graph instance which the module is connected to, or None.</p>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=501"><code>python.modules.gated_rnn.ZoneoutWrapper.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None)</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.initial_state" /></h4>
<p>Builds the default start state tensor of zeros.</p>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperis_connected"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=469"><code>python.modules.gated_rnn.ZoneoutWrapper.is_connected</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.is_connected" /></h4>
<p>Returns true iff the Module been connected to the Graph at least once.</p>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperlast_connected_subgraph"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=487"><code>python.modules.gated_rnn.ZoneoutWrapper.last_connected_subgraph</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.last_connected_subgraph" /></h4>
<p>Returns the last subgraph created by this module.</p>
<h5 id="returns_680">Returns:</h5>
<p>The last connected subgraph.</p>
<h5 id="raises_597">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnzoneoutwrappermodule_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=464"><code>python.modules.gated_rnn.ZoneoutWrapper.module_name</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.module_name" /></h4>
<p>Returns the name of the Module.</p>
<h4 id="pythonmodulesgated_rnnzoneoutwrappername_scopes"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=430"><code>python.modules.gated_rnn.ZoneoutWrapper.name_scopes</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.name_scopes" /></h4>
<p>Returns a tuple of all name_scopes generated by this module.</p>
<h4 id="pythonmodulesgated_rnnzoneoutwrappernon_trainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=622"><code>python.modules.gated_rnn.ZoneoutWrapper.non_trainable_variables</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.non_trainable_variables" /></h4>
<p>All <strong>non-trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_681">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_598">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperoutput_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=514"><code>python.modules.gated_rnn.ZoneoutWrapper.output_size</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.output_size" /></h4>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperscope_name"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=459"><code>python.modules.gated_rnn.ZoneoutWrapper.scope_name</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.scope_name" /></h4>
<p>Returns the full name of the Module's variable scope.</p>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperstate_size"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/gated_rnn.py?l=510"><code>python.modules.gated_rnn.ZoneoutWrapper.state_size</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.state_size" /></h4>
<h4 id="pythonmodulesgated_rnnzoneoutwrappertrainable_variables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=604"><code>python.modules.gated_rnn.ZoneoutWrapper.trainable_variables</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.trainable_variables" /></h4>
<p>All <strong>trainable</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_682">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_599">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnzoneoutwrappervariable_scope"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=438"><code>python.modules.gated_rnn.ZoneoutWrapper.variable_scope</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.variable_scope" /></h4>
<p>Returns the variable_scope declared by the module.</p>
<p>It is valid for library users to access the internal templated
variable_scope, but only makes sense to do so after connection. Therefore we
raise an error here if the variable_scope is requested before connection.</p>
<p>The only case where it does make sense to access the variable_scope before
connection is to get the post-uniquification name, which we support using
the separate .scope_name property.</p>
<h5 id="returns_683">Returns:</h5>
<ul>
<li><code>variable_scope</code>: <code>tf.VariableScope</code> instance of the internal <code>tf.Template</code>.</li>
</ul>
<h5 id="raises_600">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnzoneoutwrappervariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/base.py?l=585"><code>python.modules.gated_rnn.ZoneoutWrapper.variables</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.variables" /></h4>
<p><strong>All</strong> <code>tf.Variable</code>s used when the module is connected.</p>
<p>This property does not rely on global collections and should generally be
preferred vs. <code>get_variables</code> and <code>get_all_variables</code>.</p>
<p>See the documentation for <code>AbstractModule._capture_variables()</code> for more
information about what variables are captured.</p>
<h5 id="returns_684">Returns:</h5>
<p>A sorted (by variable name) tuple of <code>tf.Variable</code> objects.</p>
<h5 id="raises_601">Raises:</h5>
<ul>
<li><code>NotConnectedError</code>: If the module is not connected to the Graph.</li>
</ul>
<h4 id="pythonmodulesgated_rnnzoneoutwrapperzero_statebatch_size-dtype"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=259"><code>python.modules.gated_rnn.ZoneoutWrapper.zero_state(batch_size, dtype)</code></a><a id="python.modules.gated_rnn.ZoneoutWrapper.zero_state" /></h4>
<p>Return zero-filled state tensor(s).</p>
<h5 id="args_343">Args:</h5>
<ul>
<li><code>batch_size</code>: int, float, or unit Tensor representing the batch size.</li>
<li><code>dtype</code>: the data type to use for the state.</li>
</ul>
<h5 id="returns_685">Returns:</h5>
<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<p>If <code>state_size</code> is a nested list or tuple, then the return value is
  a nested list or tuple (of the same structure) of <code>2-D</code> tensors with
  the shapes <code>[batch_size x s]</code> for each s in <code>state_size</code>.</p>
<h3 id="pythonmoduleslayer_normcreate_beta_initializer"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/layer_norm.py?l=204"><code>python.modules.layer_norm.create_beta_initializer()</code></a><a id="python.modules.layer_norm.create_beta_initializer" /></h3>
<p>Returns a default initializer for the <code>beta</code> in layer norm.</p>
<h3 id="pythonmoduleslayer_normcreate_gamma_initializer"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/layer_norm.py?l=209"><code>python.modules.layer_norm.create_gamma_initializer()</code></a><a id="python.modules.layer_norm.create_gamma_initializer" /></h3>
<p>Returns a default initializer for the <code>gamma</code> in layer norm.</p>
<h3 id="pythonmodulesrnn_corewith_docfn_with_doc_to_copy"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py?l=385"><code>python.modules.rnn_core.with_doc(fn_with_doc_to_copy)</code></a><a id="python.modules.rnn_core.with_doc" /></h3>
<p>Returns a decorator to copy documentation from the given function.</p>
<p>Docstring is copied, including <em>args and </em>*kwargs documentation.</p>
<h5 id="args_344">Args:</h5>
<ul>
<li><code>fn_with_doc_to_copy</code>: Function whose docstring, including <em>args and
    </em>*kwargs documentation, is to be copied.</li>
</ul>
<h5 id="returns_686">Returns:</h5>
<p>Decorated version of <code>wrapper_init</code> with documentation copied from
  <code>fn_with_doc_to_copy</code>.</p>
<h3 id="pythonmodulesutilget_variable_scope_namevalue"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=40"><code>python.modules.util.get_variable_scope_name(value)</code></a><a id="python.modules.util.get_variable_scope_name" /></h3>
<p>Returns the name of the variable scope indicated by the given value.</p>
<h5 id="args_345">Args:</h5>
<ul>
<li><code>value</code>: String, variable scope, or object with <code>variable_scope</code> attribute
  (e.g., Sonnet module).</li>
</ul>
<h5 id="returns_687">Returns:</h5>
<p>The name (a string) of the corresponding variable scope.</p>
<h5 id="raises_602">Raises:</h5>
<ul>
<li><code>ValueError</code>: If <code>value</code> does not identify a variable scope.</li>
</ul>
<h3 id="pythonmodulesutilname_for_callablefunc"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=883"><code>python.modules.util.name_for_callable(func)</code></a><a id="python.modules.util.name_for_callable" /></h3>
<p>Returns a module name for a callable or <code>None</code> if no name can be found.</p>
<h3 id="pythonmodulesutilnotify_about_new_variablescallback"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=82"><code>python.modules.util.notify_about_new_variables(callback)</code></a><a id="python.modules.util.notify_about_new_variables" /></h3>
<p>Calls <code>callback(var)</code> for all newly created variables.</p>
<p>Callback should not modify the variable passed in. Use cases that require
variables to be modified should use <code>variable_creator_scope</code> directly and sit
within the variable creator stack.</p>
<pre><code>&gt;&gt;&gt; variables = []
&gt;&gt;&gt; with notify_about_variables(variables.append):
...   v = tf.Variable(1.0, name='v')
...   w = tf.get_variable('w', [])
&gt;&gt;&gt; assert variables == [v, w]
</code></pre>
<h5 id="args_346">Args:</h5>
<ul>
<li><code>callback</code>: a callable taking a single argument which is a tf.Variable.</li>
</ul>
<h5 id="yields_2">Yields:</h5>
<p><code>None</code> - used for contextmanager API.</p>
<h3 id="pythonmodulesutilsort_by_namevariables"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=937"><code>python.modules.util.sort_by_name(variables)</code></a><a id="python.modules.util.sort_by_name" /></h3>
<p>Returns a tuple of <code>variables</code> sorted ascending by name.</p>
<h3 id="pythonmodulesutilto_snake_casecamel_case"><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/util.py?l=899"><code>python.modules.util.to_snake_case(camel_case)</code></a><a id="python.modules.util.to_snake_case" /></h3>
<p>Returns a CamelCase string as a snake_case string.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../contents/" class="btn btn-neutral" title="Contents"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/deepmind/sonnet/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../contents/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
