<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="None">
  
  <link rel="shortcut icon" href="img/favicon.ico">
  <title>Sonnet - Sonnet Docs</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="css/theme.css" type="text/css" />
  <link rel="stylesheet" href="css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Sonnet";
    var mkdocs_page_input_path = "index.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="js/jquery-2.1.1.min.js" defer></script>
  <script src="js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="." class="icon icon-home"> Sonnet Docs</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1 current">
		
    <a class="current" href=".">Sonnet</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#usage-example">Usage Example</a></li>
    

    <li class="toctree-l2"><a href="#general-principles">General Principles</a></li>
    

    <li class="toctree-l2"><a href="#importing-sonnet">Importing Sonnet</a></li>
    

    <li class="toctree-l2"><a href="#defining-your-own-modules">Defining your own modules</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#inherit-from-sntabstractmodule">Inherit from snt.AbstractModule</a></li>
        
            <li><a class="toctree-l3" href="#call-superclass-constructor">Call superclass constructor</a></li>
        
            <li><a class="toctree-l3" href="#implement-_build-method">Implement _build() method</a></li>
        
            <li><a class="toctree-l3" href="#where-should-the-submodules-be-declared">Where should the submodules be declared?</a></li>
        
            <li><a class="toctree-l3" href="#recurrent-modules">Recurrent Modules</a></li>
        
            <li><a class="toctree-l3" href="#the-transposable-interface">The Transposable interface</a></li>
        
        </ul>
    

    <li class="toctree-l2"><a href="#variable-reuse-with-sntreuse_variables">Variable reuse with @snt.reuse_variables</a></li>
    

    <li class="toctree-l2"><a href="#wrapping-functions-into-sonnet-modules-using-sntmodule">Wrapping functions into Sonnet modules using snt.Module</a></li>
    

    <li class="toctree-l2"><a href="#faq">FAQ</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#q-why-another-tf-library">Q: Why another TF library?</a></li>
        
            <li><a class="toctree-l3" href="#q-can-i-access-different-variables-on-subsequent-calls-to-the-same-build">Q: Can I access different variables on subsequent calls to the same build()?</a></li>
        
            <li><a class="toctree-l3" href="#q-what-if-i-mistakenly-give-two-modules-the-same-name">Q: What if I mistakenly give two modules the same name?</a></li>
        
            <li><a class="toctree-l3" href="#q-do-i-have-to-name-my-modules">Q: Do I have to name my modules?</a></li>
        
            <li><a class="toctree-l3" href="#q-how-do-i-find-out-what-variables-are-declared-by-a-module">Q: How do I find out what variables are declared by a module?</a></li>
        
            <li><a class="toctree-l3" href="#q-should-i-be-putting-calls-to-variable_scope-in-my-code">Q: Should I be putting calls to variable_scope in my code?</a></li>
        
            <li><a class="toctree-l3" href="#q-can-i-mix-this-with-raw-tf-ops">Q: Can I mix this with raw TF ops?</a></li>
        
            <li><a class="toctree-l3" href="#q-should-everything-in-sonnet-be-implemented-as-a-module">Q: Should everything in Sonnet be implemented as a Module?</a></li>
        
            <li><a class="toctree-l3" href="#q-can-i-mix-this-with-other-high-level-tf-apis-eg-tf-slim">Q: Can I mix this with other high level TF APIs, eg TF Slim?</a></li>
        
            <li><a class="toctree-l3" href="#q-shouldnt-i-be-overriding-9595call9595-in-modules">Q: Shouldn't I be overriding __call__ in modules?</a></li>
        
            <li><a class="toctree-l3" href="#q-what-is-the-overhead-of-using-sonnet-vs-other-libraries-vs-raw-tf">Q: What is the overhead of using Sonnet vs other libraries vs raw TF?</a></li>
        
            <li><a class="toctree-l3" href="#q-how-do-i-list-all-the-variables-which-are-used-in-any-way-in-a-module">Q: How do I list all the variables which are used in any way in a Module?</a></li>
        
            <li><a class="toctree-l3" href="#q-how-do-i-serialize-sonnet-module-instances">Q: How do I serialize Sonnet module instances?</a></li>
        
            <li><a class="toctree-l3" href="#q-how-can-i-cite-sonnet">Q: How can I cite Sonnet?</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="INSTALL/">Installing from source</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="contents/">Contents</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="sonnet/">Module Reference</a>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href=".">Sonnet Docs</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".">Docs</a> &raquo;</li>
    
      
    
    <li>Sonnet</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/deepmind/sonnet/edit/master/docs/index.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="_1"><img alt="Sonnet" src="images/sonnet_logo.png" /></h1>
<p>Sonnet is a library built on top of TensorFlow for building complex neural
networks.</p>
<h2 id="usage-example">Usage Example</h2>
<p>The following code constructs a Linear module and connects it to multiple
inputs. The variables (i.e., the weights and biases of the linear
transformation) are automatically shared.</p>
<pre><code class="python">import sonnet as snt

# Provide your own functions to generate data Tensors.
train_data = get_training_data()
test_data = get_test_data()

# Construct the module, providing any configuration necessary.
linear_regression_module = snt.Linear(output_size=FLAGS.output_size)

# Connect the module to some inputs, any number of times.
train_predictions = linear_regression_module(train_data)
test_predictions = linear_regression_module(test_data)
</code></pre>

<p>More usage examples:</p>
<ul>
<li>Generative models</li>
<li><a href="https://github.com/deepmind/sonnet/blob/master/sonnet/examples/">RNNs</a></li>
</ul>
<h2 id="general-principles">General Principles</h2>
<p>The main principle of Sonnet is to first <em>construct</em> Python objects which
represent some part of a neural network, and then separately <em>connect</em> these
objects into the TensorFlow computation graph. The objects are subclasses of
<code>sonnet.AbstractModule</code> and as such are referred to as <code>Modules</code>.</p>
<p>Modules may be connected into the graph multiple times, and any variables
declared in that module will be automatically shared on subsequent connection
calls. Low level aspects of TensorFlow which control variable sharing, including
specifying variable scope names, and using the <code>reuse=</code> flag, are abstracted
away from the user.</p>
<p>Separating configuration and connection allows easy construction of higher-order
Modules, i.e., modules that wrap other modules. For instance,
the <code>BatchApply</code> module merges a number of leading dimensions of a tensor into
a single dimension, connects a provided module, and then splits the leading
dimension of the result to match the input.
At construction time, the inner module is passed in as an argument to the
<code>BatchApply</code> constructor. At run time, the module first performs a reshape
operation on inputs, then applies the module passed into the constructor, and
then inverts the reshape operation.</p>
<p>An additional advantage of representing Modules by Python objects is that it
allows additional methods to be defined where necessary. An example of this is
a module which, after construction, may be connected in a variety of ways while
maintaining weight sharing. For instance, in the case of a generative model, we
may want to sample from the model, or calculate the log probability of a given
observation. Having both connections simultaneously requires weight sharing, and
so these methods depend on the same variables. The variables are conceptually
owned by the object, and are used by different methods of the module.</p>
<h2 id="importing-sonnet">Importing Sonnet</h2>
<p>The recommended way to import Sonnet is to alias it to a variable named <code>snt</code>:</p>
<pre><code class="python">import sonnet as snt
</code></pre>

<p>Every module is then accessible under the namespace <code>snt</code>, and the rest of this
document will use <code>snt</code> for brevity.</p>
<p>The following code constructs a module that is composed of other modules:</p>
<pre><code class="python">import sonnet as snt

# Our data is coming in via multiple inputs, so to apply the same model to each
# we will need to use variable sharing.
train_data = get_training_data()
test_data = get_test_data()

# Make two linear modules, to form a Multi Layer Perceptron. Override the
# default names (which would end up being 'linear', 'linear_1') to provide
# interpretable variable names in TensorBoard / other tools.
lin_to_hidden = snt.Linear(output_size=FLAGS.hidden_size, name='inp_to_hidden')
hidden_to_out = snt.Linear(output_size=FLAGS.output_size, name='hidden_to_out')

# Sequential is a module which applies a number of inner modules or ops in
# sequence to the provided data. Note that raw TF ops such as tanh can be
# used interchangeably with constructed modules, as they contain no variables.
mlp = snt.Sequential([lin_to_hidden, tf.sigmoid, hidden_to_out])

# Connect the sequential into the graph, any number of times.
train_predictions = mlp(train_data)
test_predictions = mlp(test_data)
</code></pre>

<p>The following code adds initializers and regularizers to a Linear module:</p>
<pre><code class="python">import sonnet as snt

train_data = get_training_data()
test_data = get_test_data()

# Initializers and regularizers for the weights and the biasses.
initializers={&quot;w&quot;: tf.truncated_normal_initializer(stddev=1.0),
              &quot;b&quot;: tf.truncated_normal_initializer(stddev=1.0)}
regularizers = {&quot;w&quot;: tf.contrib.layers.l1_regularizer(scale=0.1),
                &quot;b&quot;: tf.contrib.layers.l2_regularizer(scale=0.1)}

linear_regression_module = snt.Linear(output_size=FLAGS.output_size,
                                      initializers=initializers,
                                      regularizers=regularizers)

# Connect the module to some inputs, any number of times.
train_predictions = linear_regression_module(train_data)
test_predictions = linear_regression_module(test_data)

# ...

# Get the regularization losses and add them together.
graph_regularizers = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
total_regularization_loss = tf.reduce_sum(graph_regularizers)

# ...

# When minimizing the loss, minimize also the regularization loss.
train_op = optimizer.minimize(loss + total_regularizer_loss)
</code></pre>

<h2 id="defining-your-own-modules">Defining your own modules</h2>
<h3 id="inherit-from-sntabstractmodule">Inherit from <code>snt.AbstractModule</code></h3>
<p>To define a module, create a new class which inherits from <code>snt.AbstractModule</code>.
The constructor of your class should accept any configuration which defines the
operation of that module, and store it in a member variable prefixed with an
underscore, to indicate that it is private.</p>
<h3 id="call-superclass-constructor">Call superclass constructor</h3>
<p>The first thing the constructor does
should be to call the superclass constructor, passing in the name for the module
- if you forget to do this, the variable sharing will break. A <code>name</code> kwarg
should always be provided as the final one of the list, with the default value
being a <code>snake_case</code> version of the class name.</p>
<pre><code class="python">class MyMLP(snt.AbstractModule):
  &quot;&quot;&quot;Docstring for MyMLP.&quot;&quot;&quot;
  def __init__(self, hidden_size, output_size,
               nonlinearity=tf.tanh, name=&quot;my_mlp&quot;):
    &quot;&quot;&quot;Docstring explaining __init__ args, including types and defaults.&quot;&quot;&quot;
    super(MyMLP, self).__init__(name=name)
    self._hidden_size = hidden_size
    self._output_size = output_size
    self._nonlinearity = nonlinearity
</code></pre>

<h3 id="implement-_build-method">Implement <code>_build()</code> method</h3>
<p>The only other method implementation which must be provided is <code>_build()</code>. This
will be called whenever the module is connected into the <code>tf.Graph</code>. It receives
some input, which may be empty, a single Tensor, or some arbitrary structure
containing multiple Tensors. Multiple Tensors can be provided with either a
tuple or namedtuple, the elements of which in turn can be Tensors or further
tuples / namedtuples. Most input Tensors require a batch dimension, and if a
Tensor has a color channel then it <em>must</em> be the last dimension. While in many
cases the library will not explicitly prevent you, the use of lists and dicts is
not supported, as the mutability of these structures can lead to subtle bugs.</p>
<pre><code class="python">  # Following on from code snippet above..
  def _build(self, inputs):
    &quot;&quot;&quot;Compute output Tensor from input Tensor.&quot;&quot;&quot;
    lin_x_to_h = snt.Linear(output_size=self._hidden_size, name=&quot;x_to_h&quot;)
    lin_h_to_o = snt.Linear(output_size=self._output_size, name=&quot;h_to_o&quot;)
    return lin_h_to_o(self._nonlinearity(lin_x_to_h(inputs)))
</code></pre>

<p>The <code>_build</code> method may include any or all of the following processes:</p>
<ul>
<li>Construct and use internal modules</li>
<li>Use modules which already exist, and were passed into the constructor</li>
<li>Create variables directly.</li>
</ul>
<p>If you create variables yourself, it is <em>crucial</em>
to create them with <code>tf.get_variable</code>. Calling the <code>tf.Variable</code> constructor
directly will only work the first time the module is connected, but on the
second call you will receive an error message "Trainable variable created when
calling a template after the first time".</p>
<p>The modules in the above example are created separately, passing in various
configurations, and then the final line connects them all into the graph. The
return line should be read from right to left - the inputs Tensor is passed into
the first Linear, <code>lin_x_to_h</code>, the output of which is passed into whatever
nonlinearity was stored in the constructor, the output of which goes through
another Linear to produce the result. Note that we give short meaningful names
to the internal Linear instances.</p>
<p>Note that the nonlinearity above can be either a raw TF op, eg <code>tf.tanh</code> or
<code>tf.sigmoid</code>, or an instance of a Sonnet module. In keeping with Python
standards, we may choose to not check this explicitly, and so we may receive
an error when <code>_build</code> is called. It is also acceptable to add constraints and
sanity checking inside <code>__init__</code>.</p>
<p>Note that in the above code, new instances of <code>snt.Linear</code> are
generated each time <code>_build()</code> is called, and you may think this will create
different, unshared variables. This is not the case - only 4 variables (2 for
each <code>Linear</code>) will be created, no matter how many times the MLP instance is
connected into the graph. How this is works is a low level TF detail, and
subject to change - see
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py">tf.variable_op_scope</a>
for details.</p>
<h3 id="where-should-the-submodules-be-declared">Where should the submodules be declared?</h3>
<p>Note that modules may use other modules which they receive already externally
constructed - eg Sequential etc. The submodules we discuss in this section are
any Modules which are <em>constructed</em> inside the code of another Module, which we
will refer to as the Parent Module. An example is an LSTM, where most
implementations will internally construct one or more Linear modules to contain
the weights.</p>
<p>It's recommended that submodules are created in <code>_build()</code>. Doing it this way
means you get the correct nesting of variable scopes, eg:</p>
<pre><code class="python">class ParentModule(snt.AbstractModule):
  def __init__(self, hidden_size, name=&quot;parent_module&quot;):
    super(ParentModule, self).__init__(name=name)
    self._hidden_size = hidden_size

  def _build(self, inputs):
    lin_mod = snt.Linear(self._hidden_size)  # Construct submodule...
    return tf.relu(lin_mod(inputs))          # then connect it.
</code></pre>

<p>The variables created by the Linear will have a name something like
<code>parent_module/linear/w</code>, which is what you probably want in this kind of
situation.</p>
<p>Some users prefer for practical or stylistic reasons to construct everything in
the constructor, before anything is used. This is fine, but for proper variable
nesting  <em>any submodules must be constructed inside a
<code>self._enter_variable_scope</code> call</em>.</p>
<pre><code class="python">class OtherParentModule(snt.AbstractModule):
  def __init__(self, hidden_size, name=&quot;other_parent_module&quot;):
    super(OtherParentModule, self).__init__(name=name)
    self._hidden_size = hidden_size
    with self._enter_variable_scope():  # This line is crucial!
      self._lin_mod = snt.Linear(self._hidden_size)  # Construct submodule here.

  def _build(self, inputs):
    return tf.relu(self._lin_mod(inputs))  # Connect previously constructed mod.
</code></pre>

<p>The above example is fine, and will have the same variable names etc. Different
people prefer different styles and both of the above are considered correct.</p>
<p>The pitfall here is forgetting to call <code>self._enter_variable_scope()</code>. Things
will still "work" but the scopes will not be nested as you might expected:</p>
<pre><code class="python">class WrongModule(snt.AbstractModule):
  def __init__(self, hidden_size, name=&quot;wrong_module&quot;):
    super(WrongModule, self).__init__(name=name)
    self._hidden_size = hidden_size
    self._lin_mod = snt.Linear(self._hidden_size)  # Construct submodule here.

  def _build(self, inputs):
    return tf.relu(self._lin_mod(inputs))  # Connect previously constructed mod.
</code></pre>

<p>The above example works okay in terms of the resulting network's calculations,
but is considered a bug due to the resulting flat (instead of hierarchical)
variable namespace. The variables in the linear will be called <code>"linear/w"</code>
which is completely disjoint from the <code>"wrong_module"</code> namespace.</p>
<h3 id="recurrent-modules">Recurrent Modules</h3>
<h4 id="usage">Usage</h4>
<p>Sonnet includes recurrent core modules (also called <a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell">"cells"</a>
in TensorFlow terminology), which perform one time step of computation. These
are ready to be unrolled in time using TensorFlow's <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn">unrolling operations</a>.</p>
<p>One example of an LSTM that is unrolled in time is the following:</p>
<pre><code class="python">hidden_size = 5
batch_size = 20
# input_sequence should be a tensor of size
# [time_steps, batch_size, input_features]
input_sequence = ...
lstm = snt.LSTM(hidden_size)
initial_state = lstm.initial_state(batch_size)
output_sequence, final_state = tf.nn.dynamic_rnn(
    lstm, input_sequence, initial_state=initial_state, time_major=True)
</code></pre>

<p>The <code>batch_size</code> parameter passed to the <code>initial_state()</code> method can also be an
<code>int32</code> Tensor.</p>
<p>For a more comprehensive demonstration on the usage of recurrent modules, a
fully-documented <a href="https://github.com/deepmind/sonnet/blob/master/sonnet/examples/rnn_shakespeare.py">example of a deep LSTM with skip connections trained on the
Shakespeare dataset</a>
is available.</p>
<h4 id="defining-your-own-recurrent-modules">Defining your own recurrent modules</h4>
<p>A recurrent module is any subclass of
<a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/rnn_core.py"><code>snt.RNNCore</code></a>,
which inherits from <code>snt.AbstractModule</code> and has an interface compatible with
<code>tf.nn.rnn_cell.RNNCell</code>. This allows us to use the variable sharing model from
Sonnet whilst also using the cores inside TensorFlow's RNN Containers.</p>
<pre><code class="python">class Add1RNN(snt.RNNCore):
  &quot;&quot;&quot;Simple core that adds 1 to its state and produces zero outputs.

  This core computes the following:

  (`input`, (`state1`, `state2`)) -&gt; (`output`, (`next_state1`, `next_state2`))

  where all the elements are tensors, next_statei` = `statei` + 1, and
  `output` = 0. All the outputs (`state` and `output`) are of size
  (`batch_size`, `hidden_size`), where `hidden_size` is a size that is
  specified in the constructor.
  &quot;&quot;&quot;

  def __init__(self, hidden_size, name=&quot;add1_rnn&quot;):
    &quot;&quot;&quot;Constructor of the module.

    Args:
      hidden_size: an int, size of the outputs of the module (without batch
          size).
      name: the name of the module.
    &quot;&quot;&quot;
    super(Add1RNN, self).__init__(name=name)
    self._hidden_size = hidden_size

  def _build(self, inputs, state):
    &quot;&quot;&quot;Builds a TF subgraph that performs one timestep of computation.&quot;&quot;&quot;
    batch_size = tf.TensorShape([inputs.get_shape()[0]])
    outputs = tf.zeros(shape=batch_size.concatenate(self.output_size))
    state1, state2 = state
    next_state = (state1 + 1, state2 + 1)
    return outputs, next_state

  @property
  def state_size(self):
    &quot;&quot;&quot;Returns a description of the state size, without batch dimension.&quot;&quot;&quot;
    return (tf.TensorShape([self._hidden_size]),
            tf.TensorShape([self._hidden_size]))

  @property
  def output_size(self):
    &quot;&quot;&quot;Returns a description of the output size, without batch dimension.&quot;&quot;&quot;
    return tf.TensorShape([self._hidden_size])

  def initial_state(self, batch_size, dtype):
    &quot;&quot;&quot;Returns an initial state with zeros, for a batch size and data type.

    NOTE: This method is here only for illustrative purposes, the corresponding
    method in its superclass should be already doing this.
    &quot;&quot;&quot;
    sz1, sz2 = self.state_size
    # Prepend batch size to the state shape, and create zeros.
    return (tf.zeros([batch_size] + sz1.as_list(), dtype=dtype),
            tf.zeros([batch_size] + sz2.as_list(), dtype=dtype))

</code></pre>

<p>Apart from the <code>_build</code> method from <code>snt.AbstractModule</code>, a recurrent module
must also implement the <code>state_size</code> and <code>output_size</code> properties, which provide
the expected size of the recurrent state, and an example of it, respectively.
<code>snt.RNNCore</code> defines a <code>initial_state</code> method that can be used to generate a
zero initial state or a trainable initial state (based on the aforementioned
properties). Optionally, any recurrent module can define its own <code>initial_state</code>
method. Note that a <code>zero_state</code> method is also available (as in
<code>tf.nn.rnn_cell.RNNCell</code>) to produce a correctly sized state value filled with
zeros. In some situations (LSTM, etc) it may be acceptable to begin with a state
containing all zeros, but in other situations this is too limiting, and we may
want to (eg) fill some part of the state with random noise.</p>
<p>A common option is to make the initial state of an RNN trainable, meaning the
state is produced from some <code>tf.Variable</code>s which are trained via
backpropagation. If a core supports this, it should provide kwargs <code>trainable=</code>
and <code>name=</code> for <code>initial_state()</code>. The <code>name=</code> kwarg can be used to provide a
prefix for the (potentially multiple) variable name(s) which will be created.</p>
<h3 id="the-transposable-interface">The Transposable interface</h3>
<p>Sonnet defines an interface for modules supporting <em>transposition</em>, called
<a href="https://github.com/deepmind/sonnet/blob/master/sonnet/docs/sonnet.md?#Transposable"><code>snt.Transposable</code></a>.
Transposition is a flexible concept (e.g. not necessarily
related to matrix transposition as defined in algebra), and in this context
it entails the definition of a new module with attributes which are somehow
related to the original module, <em>without</em> strictly implying any form of variable
sharing. For example, given a <code>snt.Linear</code> which maps from input size <em>A</em> to
output size <em>B</em>, via transposition we will return
another <code>snt.Linear</code> module whose weight matrix shape is the transpose of the
original one, thus mapping from input size <em>B</em> to output size <em>A</em>; given a
<code>snt.Conv2D</code> module we will return a matching <code>snt.Conv2DTranspose</code> module.</p>
<p>The <code>snt.Transposable</code> interface requires that transposable modules implement a
method called <code>transpose</code>, returning a module which is the transposed version of
the one the method is called on. Whilst <em>not</em> enforced by Sonnet, chaining the
method twice should be expected to return a module with the same specifications
as the original module.</p>
<p>When implementing a transposable module, special care is required to ensure that
parameters needed to instantiate the module are provided as functions whose
evaluation is <em>deferred</em> to graph construction time. This mechanism allows for
transposed modules to be instantiated <em>before</em> the original module is connected
to the graph. An example of this behavior can be found in <a href="https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/basic.py?l=272"><code>snt.Linear</code></a>,
where the <code>output_size</code> argument of the transposed module is defined as a
<code>lambda</code> returning the <code>input_shape</code> property of the original module;
upon evaluation <code>input_shape</code> will raise an error unless the module has not been
connected to the graph, but this is not an issue since the <code>lambda</code> is not
called until the transposed module is connected to the graph.</p>
<h2 id="variable-reuse-with-sntreuse_variables">Variable reuse with <code>@snt.reuse_variables</code></h2>
<p>Some use cases require a <code>tf.VariableScope</code> to be shared across multiple
methods, which isn't possible with <code>snt.AbstractModule</code>.
For example, a generative model may define a <code>sample()</code> and <code>log_pdf()</code> method
that share parts of the same <code>tf.Graph</code>.</p>
<p>Adding the <code>@snt.reuse_variables</code> decorator to a method will
enable variable reuse in much the same manner as <code>_build()</code>. The most notable
difference is that a single <code>tf.VariableScope</code> will be used across different
decorated methods and each decorated method has its own <code>reuse</code> flag that is
used to enter the variable scope.</p>
<pre><code class="python">class Reusable(object):

  def __init__(self, name):
    with tf.variable_scope(None, default_name=name) as vs:
      self.variable_scope = vs

  @snt.reuse_variables
  def reusable_var(self):
    return tf.get_variable(&quot;a&quot;, shape=[1])

obj = Reusable(&quot;reusable&quot;)
a1 = obj.reusable_var()
a2 = obj.reusable_var()
# a1 == a2


class NaiveAutoEncoder(snt.AbstractModule):
  def __init__(self, n_latent, n_out, name=&quot;naive_auto_encoder&quot;):
    super(NaiveAutoEncoder, self).__init__(name=name)
    self._n_latent = n_latent
    self._n_out = n_out

  @snt.reuse_variables
  def encode(self, input):
    &quot;&quot;&quot;Builds the front half of AutoEncoder, inputs -&gt; latents.&quot;&quot;&quot;
    w_enc = tf.get_variable(&quot;w_enc&quot;, shape=[self._n_out, self._n_latent])
    b_enc = tf.get_variable(&quot;b_enc&quot;, shape=[self._n_latent])
    return tf.sigmoid(tf.matmul(input, w_enc) + b_enc)

  @snt.reuse_variables
  def decode(self, latents):
    &quot;&quot;&quot;Builds the back half of AutoEncoder, latents -&gt; reconstruction.&quot;&quot;&quot;
    w_rec = tf.get_variable(&quot;w_dec&quot;, shape=[self._n_latent, self._n_out])
    b_rec = tf.get_variable(&quot;b_dec&quot;, shape=[self._n_out])
    return tf.sigmoid(tf.matmul(latents, w_rec) + b_rec)

  def _build(self, input):
    &quot;&quot;&quot;Builds the 'full' AutoEncoder, ie input -&gt; latents -&gt; reconstruction.&quot;&quot;&quot;
    latents = self.encode(input)
    return self.decode(latents)


batch_size = 5
n_in = 10
n_out = n_in
n_latent = 2
nae = NaiveAutoEncoder(n_latent=n_latent, n_out=n_out)
inputs = tf.placeholder(tf.float32, shape=[batch_size, n_in])
latents = tf.placeholder(tf.float32, shape=[batch_size, n_latent])

# Connecting the default way calls build(), producing 'full' AutoEncoder.
reconstructed_from_input = nae(inputs)

# Connecting with one of the other methods might only require some subset of the
# variables, but sharing will still work.
reconstructed_from_latent = nae.decode(latents)
</code></pre>

<p>In the above example, any variables created by <code>nae.encode()</code>, <code>nae.decode()</code>
or <code>nae()</code> exist in the same <code>tf.VariableScope</code>. In addition, since each
decorated method has its own <code>reuse</code> flag we don't need to worry about having to
create all variables on the first method call, or about the calling order at
all. We can even nest (decorated) methods within other (decorated) methods - the
<code>reuse</code> flag is always set correctly since the variable scope is re-entered for
every method call.</p>
<p>However every decorated method must be the <em>sole</em> owner of its variables. For
example, if we use <code>tf.get_variable("w_dec", ...)</code> inside
<code>NaiveAutoEncoder.encode</code>, this will <em>not</em> do variable sharing. Instead,
TensorFlow will treat <code>tf.get_variable("w_dec", ...)</code> in
<code>NaiveAutoEncoder.enodec</code> and <code>NaiveAutoEncoder.decode()</code> as separate variables
and an error will occur in either<code>obj.a()</code> or <code>obj.build()</code> (whichever is
called second).</p>
<p>See below for an example of bad variable reuse:</p>
<pre><code class="python">class BadReusable(object):

  def __init__(self, name):
    with tf.variable_scope(None, default_name=name) as vs:
      self.variable_scope = vs

  @snt.reuse_variables
  def reusable_var(self):
    return tf.get_variable(&quot;a&quot;, shape=[1])

  @snt.reuse_variables
  def another_reusable_var(self):
    return tf.get_variable(&quot;a&quot;, shape=[1])

obj = BadReusable(&quot;bad_reusable&quot;)
obj.reusable_var()
obj.another_reusable_var()  # Raises a ValueError because `reuse=False`
</code></pre>

<h2 id="wrapping-functions-into-sonnet-modules-using-sntmodule">Wrapping functions into Sonnet modules using <code>snt.Module</code></h2>
<p>Whilst the recommended way of defining new Sonnet modules is to inherit from
<code>snt.AbstractModule</code>, the library also offers an alternative route to succinctly
instantiate modules wrapping user-provided functions.</p>
<p>The <code>snt.Module</code> class constructor takes a callable and returns a Sonnet module.
The provided function is invoked when the module is called, thus specifying how
new nodes are added to the computational graph and how to compute output Tensors
from input Tensors. Please refer to the module <a href="https://github.com/deepmind/sonnet/blob/master/sonnet/docs/sonnet.md?#Module">documentation</a>
for more details and examples.</p>
<h2 id="faq">FAQ</h2>
<h3 id="q-why-another-tf-library">Q: Why <em>another</em> TF library?</h3>
<p>A: The existing libraries were judged insufficiently flexible for the DeepMind
use case where extensive use is made of weight sharing. Making
everything use <code>tf.make_template</code>, and therefore support weight sharing from the
start, seemed to have sufficient benefits to outweight the development cost. The
paradigm of separating configuration from connection also allows easy
composability of modules.</p>
<h3 id="q-can-i-access-different-variables-on-subsequent-calls-to-the-same-build">Q: Can I access different variables on subsequent calls to the same build()?</h3>
<p>A: No. This is enforced by <code>tf.make_template</code>, which considers it an error to
access different / extra variables on subsequent calls.</p>
<h3 id="q-what-if-i-mistakenly-give-two-modules-the-same-name">Q: What if I mistakenly give two modules the same name?</h3>
<p>A: Modules which appear to be constructed with the same name will have distinct
names, and variable scopes. Under the hood, Sonnet uses <code>tf.make_template</code> which
essentially wraps a python function together with some <code>tf.VariableScope</code>,
ensuring that every call to the function happens in the same scope, and that all
calls after the first are set to reuse variables. One feature of the templating
is that it will <code>uniquify</code> any provided names, if they have already been entered
in the same scope. For example:</p>
<pre><code class="python">lin_1 = snt.Linear(output_size=42, name=&quot;linear&quot;)
lin_2 = snt.Linear(output_size=84, name=&quot;linear&quot;)  # this name is already taken.

print(lin_1.name)  # prints &quot;linear&quot;
print(lin_2.name)  # prints &quot;linear_1&quot; - automatically uniquified.
</code></pre>

<p>Note that the .name property is available to see the "post-uniquification" name.</p>
<h3 id="q-do-i-have-to-name-my-modules">Q: Do I <em>have</em> to name my modules?</h3>
<p>A: No. Modules have a default name, which should be the class name in
<code>snake_case</code>, and that will be used as the name with uniquification (see above)
if necessary. However, we recommend providing a name for modules which contain
variables, as the name provided becomes the name of the internal scope, and thus
defines the variable names. Most modules are written to declare internal weights
with names like <code>"w"</code> and <code>"b"</code> for weights and
bias - it's vastly preferable to do have a list of weights like:</p>
<pre><code>sdae/encoder_linear/w
sdae/encoder_linear/b
sdae/decoder_linear/w
sdae/decoder_linear/b
</code></pre>

<p>rather than:</p>
<pre><code>sdae/linear/w
sdae/linear/b
sdae/linear_1/w
sdae/linear_1/b
</code></pre>

<p>The names you choose will appear in TensorBoard.</p>
<h3 id="q-how-do-i-find-out-what-variables-are-declared-by-a-module">Q: How do I find out what variables are declared by a module?</h3>
<p>A: You can query a module to find out all the variables in its scope using
the <code>get_variables()</code> method. Note that this will throw an error if the
module is not connected into the graph, as the variables do not exist at that
point so the relevant scope will be empty.</p>
<h3 id="q-should-i-be-putting-calls-to-variable_scope-in-my-code">Q: Should I be putting calls to <code>variable_scope</code> in my code?</h3>
<p>A: Every module implicitly creates an internal variable_scope, which it
re-enters each time it connects to the graph. Assuming all the variables in your
model are inside Sonnet modules, it is not necessary to use scopes yourself.</p>
<h3 id="q-can-i-mix-this-with-raw-tf-ops">Q: Can I mix this with raw TF ops?</h3>
<p>A: Yes. An op which doesn't declare variables internally, and so is effectively
a pure function, can be used inside module <code>_build</code> implementations, and also
to plumb together values between modules.</p>
<h3 id="q-should-everything-in-sonnet-be-implemented-as-a-module">Q: Should everything in Sonnet be implemented as a Module?</h3>
<p>No, computations which do not create <code>tf.Variable</code>s and do not store internal
configurations <em>can</em> be implemented in the regular TF Op style, ie a python
function that receives input tensors, keyword arguments, and returns tensor
outputs.</p>
<p>If an op is going to create variables (ie call <code>tf.get_variable</code> anywhere
internally, including indirectly) it must be implemented as a subclass of
<code>snt.AbstractModule</code> so that variable sharing is correctly handled.</p>
<p>Note that if a computation doesn't create any Variables, it <em>may</em> still be
desirable to implement it with a Module instead of an Op.</p>
<p>Aside from variable sharing, it may be convenient to use Sonnet Modules in cases
where we wish to attach configuration parameters to an op. An example of this is
the <a href="https://github.com/deepmind/dnc/blob/d3d94b3b1f1efc282481910054f82047caf37f65/dnc/addressing.py#L58">content addressing</a>
modules in the Differentiable Neural Computer.
These modules receive a number of configuration parameters
(size of each word in memory, number of read heads) and some function of these
inputs defines what the valid input size is. We use a <code>snt.Linear</code> of the
correct output size before this module, in order to provide the correct
dimensionality. As a module this is easy - provide the configuration at
construction time, then a method <code>.param_size()</code> which gives the required
input dimensionality. We can then make the correct size of input tensor
and perform the connection.</p>
<pre><code class="python">class CosineWeights(snt.AbstractModule):
  def __init__(self, word_size, num_heads, name=&quot;cosine_weights&quot;):
    super(CosineWeights, self).__init__(name=name)
    self._word_size = word_size
    self._num_heads = num_heads
  def param_size(self):
    &quot;&quot;&quot;Returns the size the 2nd dimension of `cos_params` is required to be.&quot;&quot;&quot;
    return self._num_heads * (1 + self._word_size)
  def _build(self, memory, cos_params):
    &quot;&quot;&quot;cos_params must be `[batch_size, self.param_size()]` shape&quot;&quot;&quot;
    # ...

# Construct the module, then work out the right input size
cos_weights_mod = CosineWeights(word_size=32, num_heads=3)
cosine_params_mod = snt.Linear(output_size=cos_weights_mod.param_size())

cos_params = cosine_params_mod(inputs)  # We know this is now the right shape.
weights = cos_weights_mode(memory, cos_params)
</code></pre>

<p>If the above was implemented as an op <code>cosine_weights(memory, cos_params,
word_size, num_heads)</code> then the logic to indicate the desired size of
<code>cos_params</code> would have to be stored in a separate function. Encapsulating the
related functions into one module results in cleaner code.</p>
<p>Another example of where this flexibility is useful is when an Op has a large
number of arguments which are conceptually configuration, along with some which
are conceptually inputs. We often want to use the same configuration in multiple
places, for different inputs, and so writing a Module which can be constructed
with the configuration and then passed around may be useful.</p>
<pre><code class="python">import functools
# 1. Define our computation as some op
def useful_op(input_a, input_b,
              use_clipping=True, remove_nans=False, solve_agi='maybe'):
  # ...

# 2a). Set the configuration parameters with functools, then pass around.
useful_op_configured = functools.partial(
    useful_op,
    use_clipping=False,
    remove_nans=True,
    solve_agi='definitely')
do_something_a(... , inner_op=useful_op_configured)
do_something_else_a(..., inner_op=useful_op_configured)

# 2b). OR, set the configuration by creating kwargs and pass around both.
op_kwargs = {
    'use_clipping': False,
    'remove_nans': True,
    'solve_agi': 'definitely',
}
do_something_b(..., inner_op=useful_op, inner_op_kwargs=op_kwargs)
do_something_else_b(..., inner_op=useful_op, inner_op_kwargs=op_kwargs)
</code></pre>

<p>Either of the above approaches is valid, but many users dislike the style
of using functools or needing to pass both the Op and a dictionary around.
In which case, rewriting the Op as a Module can be a nice solution - in
particular, the difference between configuration parameters and inputs from
the Graph are now made explicit:</p>
<pre><code class="python">class UsefulModule(snt.AbstractModule):
  def __init__(self, use_clipping=True, remove_nans=False,
               solve_agi='maybe', name='useful_module')
    super(UsefulModule, self).__init__(name=name)
    self._use_clipping = use_clipping
    self._remove_nans = remove_nans
    self._solve_agi = solve_agi
  def _build(self, input_a, input_b):
    #...
</code></pre>

<h3 id="q-can-i-mix-this-with-other-high-level-tf-apis-eg-tf-slim">Q: Can I mix this with other high level TF APIs, eg TF Slim?</h3>
<p>A: Sonnet modules, once constructed, follow the Tensor-In-Tensor-Out principle,
so can be mixed with functions from TF-Slim, etc. Note that this may lead to
unexpected behaviour - TF-Slim controls sharing by passing explicit <code>scope=</code>
and <code>reuse=</code> kwargs into the layer functions - if you use a TF-Slim layer inside
the <code>_build()</code> method of a Sonnet module, then calling it multiple times is not
likely to work correctly.</p>
<h3 id="q-shouldnt-i-be-overriding-9595call9595-in-modules">Q: Shouldn't I be overriding __call__ in modules?</h3>
<p>A: No. <code>AbstractModule.__init__</code> provides an implementation of <code>__call__</code>,
which calls an internal function wrapped in a Template, which in turn wraps the
<code>_build</code> method. Overriding <code>__call__</code> yourself will likely break variable
sharing.</p>
<h3 id="q-what-is-the-overhead-of-using-sonnet-vs-other-libraries-vs-raw-tf">Q: What is the overhead of using Sonnet vs other libraries vs raw TF?</h3>
<p>A: None. Sonnet is only involved when constructing the computation graph.
Once you are at the stage of using <code>Session.run()</code> you are simply executing Ops,
without regard for what library was used to put that graph together.</p>
<h3 id="q-how-do-i-list-all-the-variables-which-are-used-in-any-way-in-a-module">Q: How do I list all the variables which are used <em>in any way</em> in a Module?</h3>
<p>A: You can use <code>get_all_variables()</code> to find all the variables that a module or
any of its submodules have created with <code>tf.get_variable()</code>.</p>
<p>Like <code>get_variables()</code> this returns all variables that are inside of the module's
(variable) scope. However, <code>get_all_variables()</code> also returns all of the
variables from any submodules with disjoint (variable) scopes. These submodules
have either been passed into the module's constructor, or have been constructed
by the module but outside of <code>_build()</code> or <code>_enter_variable_scope()</code>.</p>
<p>Note that by definition this will not return variables that have not been
created by <code>tf.get_variable()</code>. This is relevant for modules that use
<code>@snt.reuse_variables</code>. If a method decorated with <code>@snt.reuse_variable</code> is
not called then <code>get_all_variables()</code> will not return any variables used inside
of it.</p>
<p>Note that by definition this returns <em>all</em> of a module's variables. This means
that a module will return <em>all</em> its submodule's variables, even if it only uses
a subset of the submodule's variables (ie. it does not call a method decorated
by <code>@snt.reuse_variables</code> on the submodule).</p>
<h3 id="q-how-do-i-serialize-sonnet-module-instances">Q: How do I serialize Sonnet module instances?</h3>
<p>A: We do not support serializing module instances, via pickle or any other
method. Modules contain inherently non-serializable artifacts, such as
references to <code>Variable</code>s and <code>Graph</code>s. To save a model, you should instead
serialize the configuration information which will allow you to regenerate the
Graph, such as the name and constructor parameters of some top level module.
This avoids any complications with adding new members to modules, and then
having to deal with old serialized instances where those members don't exist.</p>
<h3 id="q-how-can-i-cite-sonnet">Q: How can I cite Sonnet?</h3>
<p>A: Please use the following BibTeX:</p>
<pre><code>@misc{sonnetblog,
  title=&quot;{Open sourcing Sonnet - a new library for constructing neural networks}&quot;,
  author={Reynolds, Malcolm and Barth-Maron, Gabriel and Besse, Frederic and
      de Las Casas, Diego and Fidjeland, Andreas and Green, Tim and
      Puigdom{\`e}nech, Adri{\`a} and Racani{\`e}re, S{\'e}bastien and Rae, Jack
      and Viola, Fabio},
  howpublished={\url{https://deepmind.com/blog/open-sourcing-sonnet/}},
  year={2017}
}
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="INSTALL/" class="btn btn-neutral float-right" title="Installing from source">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/deepmind/sonnet/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
      
        <span style="margin-left: 15px"><a href="INSTALL/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '.';</script>
    <script src="js/theme.js" defer></script>
      <script src="search/main.js" defer></script>

</body>
</html>

<!--
MkDocs version : 1.0.4
Build Date UTC : 2019-07-05 15:27:23
-->
