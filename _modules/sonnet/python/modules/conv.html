
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sonnet.python.modules.conv &#8212; sonnet git documentation</title>
    <link rel="stylesheet" href="../../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../',
        VERSION:     'git',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sonnet.python.modules.conv</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2017 The Sonnet Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Implementation of convolutional Sonnet modules.</span>

<span class="sd">Classes defining convolutional operations, inheriting from `snt.Module`, with</span>
<span class="sd">easy weight sharing.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numbers</span>

<span class="c1"># Dependency imports</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">base</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">util</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>


<span class="c1"># Strings for TensorFlow convolution padding modes. See the following</span>
<span class="c1"># documentation for an explanation of VALID versus SAME:</span>
<span class="c1"># https://www.tensorflow.org/api_guides/python/nn#Convolution</span>
<span class="n">SAME</span> <span class="o">=</span> <span class="s2">&quot;SAME&quot;</span>
<span class="n">VALID</span> <span class="o">=</span> <span class="s2">&quot;VALID&quot;</span>
<span class="n">ALLOWED_PADDINGS</span> <span class="o">=</span> <span class="p">{</span><span class="n">SAME</span><span class="p">,</span> <span class="n">VALID</span><span class="p">}</span>

<span class="n">DATA_FORMAT_NCHW</span> <span class="o">=</span> <span class="s2">&quot;NCHW&quot;</span>
<span class="n">DATA_FORMAT_NHWC</span> <span class="o">=</span> <span class="s2">&quot;NHWC&quot;</span>
<span class="n">SUPPORTED_DATA_FORMATS</span> <span class="o">=</span> <span class="p">{</span><span class="n">DATA_FORMAT_NCHW</span><span class="p">,</span> <span class="n">DATA_FORMAT_NHWC</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">_default_transpose_size</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">padding</span><span class="o">=</span><span class="n">SAME</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns default (maximal) output shape for a transpose convolution.</span>

<span class="sd">  In general, there are multiple possible output shapes that a transpose</span>
<span class="sd">  convolution with a given `input_shape` can map to. This function returns the</span>
<span class="sd">  output shape which evenly divides the stride to produce the input shape in</span>
<span class="sd">  a forward convolution, i.e. the maximal valid output shape with the given</span>
<span class="sd">  configuration:</span>

<span class="sd">  if the padding type is SAME then:  output_shape = input_shape * stride</span>
<span class="sd">  if the padding type is VALID then: output_shape = input_shape * stride +</span>
<span class="sd">                                                    kernel_shape - 1</span>

<span class="sd">  See the following documentation for an explanation of VALID versus SAME</span>
<span class="sd">  padding modes:</span>
<span class="sd">  https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#convolution</span>

<span class="sd">  Args:</span>
<span class="sd">    input_shape: Sequence of sizes of each dimension of the input, excluding</span>
<span class="sd">      batch and channel dimensions.</span>
<span class="sd">    stride: Sequence or integer of kernel strides, excluding batch and channel</span>
<span class="sd">      dimension strides.</span>
<span class="sd">    kernel_shape: Sequence or integer of kernel sizes.</span>
<span class="sd">    padding: Padding algorithm, either `snt.SAME` or `snt.VALID`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    output_shape: A tuple of sizes for a transposed convolution that divide</span>
<span class="sd">      evenly with the given strides, kernel shapes, and padding algorithm.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: if `input_shape` is not a Sequence;</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">input_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;input_shape is None; if using Sonnet, are you sure you &quot;</span>
                      <span class="s2">&quot;have connected the module to inputs?&quot;</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;input_shape is of type </span><span class="si">{}</span><span class="s2">, must be a sequence.&quot;</span>
                    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)))</span>

  <span class="n">input_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
  <span class="n">stride</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">input_length</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">)</span>
  <span class="n">padding</span> <span class="o">=</span> <span class="n">_verify_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>

  <span class="n">output_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">stride</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">padding</span> <span class="o">==</span> <span class="n">VALID</span><span class="p">:</span>
    <span class="n">kernel_shape</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">,</span> <span class="n">input_length</span><span class="p">,</span>
                                                    <span class="s2">&quot;kernel&quot;</span><span class="p">)</span>
    <span class="n">output_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">output_shape</span>


<span class="k">def</span> <span class="nf">_fill_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Idempotentally converts an integer to a tuple of integers of a given size.</span>

<span class="sd">  This is used to allow shorthand notation for various configuration parameters.</span>
<span class="sd">  A user can provide either, for example, `2` or `[2, 2]` as a kernel shape, and</span>
<span class="sd">  this function returns `(2, 2)` in both cases. Passing `[1, 2]` will return</span>
<span class="sd">  `(1, 2)`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x: An integer or an iterable of integers</span>
<span class="sd">    n: An integer, the size of the desired output list</span>

<span class="sd">  Returns:</span>
<span class="sd">    If `x` is an integer, a tuple of size `n` containing `n` copies of `x`.</span>
<span class="sd">    If `x` is an iterable of integers of size `n`, it returns `tuple(x)`.</span>

<span class="sd">  Raises:</span>
<span class="sd">    TypeError: If n is not a positive integer;</span>
<span class="sd">      or if x is neither integer nor an iterable of size n.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span> <span class="ow">or</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;n must be a positive integer&quot;</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span> <span class="ow">and</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">,)</span> <span class="o">*</span> <span class="n">n</span>
  <span class="k">elif</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="n">n</span> <span class="ow">and</span>
        <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)</span> <span class="ow">and</span>
        <span class="nb">all</span><span class="p">(</span><span class="n">v</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;x is </span><span class="si">{}</span><span class="s2">, must be either a positive integer &quot;</span>
                    <span class="s2">&quot;or an iterable of positive integers of size </span><span class="si">{}</span><span class="s2">&quot;</span>
                    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">parameter_label</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Expands x if necessary into a `n`-D kernel shape and reports errors.&quot;&quot;&quot;</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">_fill_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span><span class="s2">&quot;Invalid &quot;</span> <span class="o">+</span> <span class="n">parameter_label</span> <span class="o">+</span> <span class="s2">&quot; shape: &quot;</span>
                                      <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_verify_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Verifies that the provided padding is supported. Returns padding.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">padding</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ALLOWED_PADDINGS</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;Padding must be member of &#39;</span><span class="si">{}</span><span class="s2">&#39;, not </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">ALLOWED_PADDINGS</span><span class="p">,</span> <span class="n">padding</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">padding</span>


<span class="k">def</span> <span class="nf">_fill_and_one_pad_stride</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Expands the provided stride to size n and pads it with 1s.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
      <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">n</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">_fill_shape</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
  <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">==</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">stride</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
        <span class="s2">&quot;stride is </span><span class="si">{}</span><span class="s2"> (</span><span class="si">{}</span><span class="s2">), must be either a positive integer or an iterable of&quot;</span>
        <span class="s2">&quot; positive integers of size </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span> <span class="n">n</span><span class="p">))</span>


<div class="viewcode-block" id="create_weight_initializer"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.create_weight_initializer">[docs]</a><span class="k">def</span> <span class="nf">create_weight_initializer</span><span class="p">(</span><span class="n">fan_in_shape</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a default initializer for the weights of a convolutional module.&quot;&quot;&quot;</span>
  <span class="n">stddev</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">fan_in_shape</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">stddev</span><span class="p">)</span></div>


<div class="viewcode-block" id="create_bias_initializer"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.create_bias_initializer">[docs]</a><span class="k">def</span> <span class="nf">create_bias_initializer</span><span class="p">(</span><span class="n">unused_bias_shape</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a default initializer for the biases of a convolutional module.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">()</span></div>


<div class="viewcode-block" id="Conv2D"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv2D">[docs]</a><span class="k">class</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">Transposable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Spatial convolution and dilated convolution module, including bias.</span>

<span class="sd">  This acts as a light wrapper around the TensorFlow ops `tf.nn.convolution`</span>
<span class="sd">  abstracting away variable creation and sharing.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">padding</span><span class="o">=</span><span class="n">SAME</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">data_format</span><span class="o">=</span><span class="n">DATA_FORMAT_NHWC</span><span class="p">,</span> <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_2d&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a Conv2D module.</span>

<span class="sd">    See the following documentation for an explanation of VALID versus SAME</span>
<span class="sd">    padding modes:</span>
<span class="sd">    https://www.tensorflow.org/api_guides/python/nn#Convolution</span>

<span class="sd">    Args:</span>
<span class="sd">      output_channels: Number of output channels. `output_channels` can be</span>
<span class="sd">          either a number or a callable. In the latter case, since the function</span>
<span class="sd">          invocation is deferred to graph construction time, the user must only</span>
<span class="sd">          ensure that output_channels can be called, returning an integer,</span>
<span class="sd">          when `build` is called.</span>
<span class="sd">      kernel_shape: Sequence of kernel sizes (of size 2), or integer that is</span>
<span class="sd">          used to define kernel size in all dimensions.</span>
<span class="sd">      stride: Sequence of kernel strides (of size 2), or integer that is used to</span>
<span class="sd">          define stride in all dimensions.</span>
<span class="sd">      rate: Sequence of dilation rates (of size 2), or integer that is used to</span>
<span class="sd">          define dilation rate in all dimensions. 1 corresponds to standard 2D</span>
<span class="sd">          convolution, `rate &gt; 1` corresponds to dilated convolution. Cannot be</span>
<span class="sd">          &gt; 1 if any of `stride` is also &gt; 1.</span>
<span class="sd">      padding: Padding algorithm, either `snt.SAME` or `snt.VALID`.</span>
<span class="sd">      use_bias: Whether to include bias parameters. Default `True`.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">          key &#39;w&#39;) or biases (with key &#39;b&#39;). The default initializer for the</span>
<span class="sd">          weights is a truncated normal initializer, which is commonly used</span>
<span class="sd">          when the inputs are zero centered (see</span>
<span class="sd">          https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for</span>
<span class="sd">          the bias is a zero initializer.</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">          weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">          partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      mask: Optional 2D or 4D array, tuple or numpy array containing values to</span>
<span class="sd">          multiply the weights by component-wise.</span>
<span class="sd">      data_format: A string. Specifies whether the channel dimension</span>
<span class="sd">          of the input and output is the last dimension (default, NHWC), or the</span>
<span class="sd">          second dimension (&quot;NCHW&quot;).</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.IncompatibleShapeError: If the given kernel shape is not an integer;</span>
<span class="sd">          or if the given kernel shape is not a sequence of two integers.</span>
<span class="sd">      base.IncompatibleShapeError: If the given stride is not an integer; or if</span>
<span class="sd">          the given stride is not a sequence of two integers.</span>
<span class="sd">      base.IncompatibleShapeError: If the given rate is not an integer; or if</span>
<span class="sd">          the given rate is not a sequence of two integers.</span>
<span class="sd">      base.IncompatibleShapeError: If a mask is given and its rank is neither 2</span>
<span class="sd">          nor 4.</span>
<span class="sd">      base.NotSupportedError: If rate in any dimension and the stride in any</span>
<span class="sd">          dimension are simultaneously &gt; 1.</span>
<span class="sd">      ValueError: If the given padding is not `snt.VALID` or `snt.SAME`.</span>
<span class="sd">      ValueError: If the given data_format is not a supported format (see</span>
<span class="sd">        SUPPORTED_DATA_FORMATS).</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contain any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">      TypeError: If mask is given and is not an array, tuple or a numpy array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
                                                          <span class="s2">&quot;kernel&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">data_format</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">SUPPORTED_DATA_FORMATS</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid data_format </span><span class="si">{:s}</span><span class="s2">. Allowed formats &quot;</span>
                       <span class="s2">&quot;</span><span class="si">{:s}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="n">SUPPORTED_DATA_FORMATS</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span> <span class="o">=</span> <span class="n">data_format</span>
    <span class="c1"># The following is for backwards-compatibility from when we used to accept</span>
    <span class="c1"># 4-strides of the form [1, m, n, 1].</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">stride</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">rate</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;rate&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span><span class="p">):</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">NotSupportedError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot have stride &gt; 1 with rate &gt; 1&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span> <span class="o">=</span> <span class="n">_verify_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid type for mask: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">mask</span><span class="p">)))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asanyarray</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
      <span class="n">mask_rank</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">ndim</span>
      <span class="k">if</span> <span class="n">mask_rank</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">mask_rank</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid mask rank: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mask_rank</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_mask</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="Conv2D.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv2D.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_bias</span> <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">}</span></div>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the Conv2D module into the graph, with input Tensor `inputs`.</span>

<span class="sd">    If this is not the first time the module has been connected to the graph,</span>
<span class="sd">    the input Tensor provided here must have the same final 3 dimensions, in</span>
<span class="sd">    order for the existing variables to be the correct size for the</span>
<span class="sd">    multiplication. The batch size may differ for each connection.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A 4D Tensor of shape [batch_size, input_height, input_width,</span>
<span class="sd">          input_channels] or [batch_size, input_channels, input_height,</span>
<span class="sd">          input_width](NCHW).</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 4D Tensor of shape [batch_size, output_height, output_width,</span>
<span class="sd">          output_channels] or [batch_size, out_channels, out_height, out_width].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If connecting the module into the graph any time after the</span>
<span class="sd">          first time and the inferred size of the input does not match previous</span>
<span class="sd">          invocations.</span>
<span class="sd">      base.IncompatibleShapeError: If the input tensor has the wrong number</span>
<span class="sd">          of dimensions.</span>
<span class="sd">      base.IncompatibleShapeError: If a mask is present and its shape is</span>
<span class="sd">          incompatible with the shape of the weights.</span>
<span class="sd">      base.UnderspecifiedError: If the input tensor has an unknown</span>
<span class="sd">          `input_channels`.</span>
<span class="sd">      TypeError: If input Tensor dtype is not `tf.float32`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Handle input whose shape is unknown during graph creation.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Input Tensor must have shape (batch_size, input_height, input_&quot;</span>
          <span class="s2">&quot;width, input_channels) or (batch_size, input_channels, input_height,&quot;</span>
          <span class="s2">&quot; input_width) but was </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span> <span class="o">==</span> <span class="n">DATA_FORMAT_NCHW</span><span class="p">:</span>
      <span class="n">input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">input_channels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">UnderspecifiedError</span><span class="p">(</span>
          <span class="s2">&quot;Number of input channels must be known at module build time&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span> <span class="o">=</span> <span class="n">input_channels</span>

    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;Input must have dtype tf.float32, but dtype was </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
              <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">)</span>

    <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,)</span>

    <span class="k">if</span> <span class="s2">&quot;w&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_weight_initializer</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>

    <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_bias_initializer</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span>
                              <span class="n">shape</span><span class="o">=</span><span class="n">weight_shape</span><span class="p">,</span>
                              <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span>
                              <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                              <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">mask_rank</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mask</span><span class="o">.</span><span class="n">ndim</span>
      <span class="n">mask_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mask</span><span class="o">.</span><span class="n">shape</span>
      <span class="k">if</span> <span class="n">mask_rank</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">mask_shape</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">:</span>
          <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
              <span class="s2">&quot;Invalid mask shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mask_shape</span><span class="p">))</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
      <span class="k">elif</span> <span class="n">mask_rank</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">mask_shape</span> <span class="o">!=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">):</span>
          <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
              <span class="s2">&quot;Invalid mask shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mask_shape</span><span class="p">))</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mask</span>
      <span class="n">w</span> <span class="o">*=</span> <span class="n">mask</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span>
                                <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">,</span> <span class="n">dilation_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rate</span><span class="p">,</span>
                                <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span>
                                <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                                <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of output channels.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">kernel_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the kernel shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">stride</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the stride.&quot;&quot;&quot;</span>
    <span class="c1"># Backwards compatibility with old stride format.</span>

    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">rate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the dilation rate.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the padding algorithm.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">w</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the weight matrix.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">b</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Variable object containing the bias, from the most recent __call__.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.NotConnectedError: If the module has not been connected to the graph</span>
<span class="sd">          yet, meaning the variables do not exist.</span>
<span class="sd">      AttributeError: If the module does not use bias.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
          <span class="s2">&quot;No bias Variable in Conv2D Module when `use_bias=False`.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if bias Variable is present in the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initializers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioners</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the partitioners dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the regularizers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">mask</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the mask.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mask</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">data_format</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the data format.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span>

<div class="viewcode-block" id="Conv2D.clone"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv2D.clone">[docs]</a>  <span class="k">def</span> <span class="nf">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a cloned `Conv2D` module.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Optional string assigning name of cloned module. The default name</span>
<span class="sd">        is constructed by appending &quot;_clone&quot; to `self.module_name`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      `Conv2D` module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;_clone&quot;</span>

    <span class="k">return</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,</span>
                  <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">,</span>
                  <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                  <span class="n">rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rate</span><span class="p">,</span>
                  <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                  <span class="n">use_bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">,</span>
                  <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">initializers</span><span class="p">,</span>
                  <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">partitioners</span><span class="p">,</span>
                  <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regularizers</span><span class="p">,</span>
                  <span class="n">mask</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">,</span>
                  <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">,</span>
                  <span class="n">custom_getter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

  <span class="c1"># Implements Transposable interface.</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the input shape.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="c1"># Implements Transposable interface.</span>
<div class="viewcode-block" id="Conv2D.transpose"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv2D.transpose">[docs]</a>  <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns matching `Conv2DTranspose` module.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Optional string assigning name of transpose module. The default name</span>
<span class="sd">        is constructed by appending &quot;_transpose&quot; to `self.name`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      `Conv2DTranspose` module.</span>

<span class="sd">    Raises:</span>
<span class="sd">     base.NotSupportedError: If `rate` in any dimension &gt; 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span><span class="p">):</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">NotSupportedError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot transpose a dilated convolution module.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;_transpose&quot;</span>

    <span class="k">def</span> <span class="nf">output_shape</span><span class="p">():</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span> <span class="o">!=</span> <span class="n">DATA_FORMAT_NCHW</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span><span class="p">,</span>
                           <span class="n">output_shape</span><span class="o">=</span><span class="n">output_shape</span><span class="p">,</span>
                           <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">,</span>
                           <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                           <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                           <span class="n">use_bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">,</span>
                           <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">initializers</span><span class="p">,</span>
                           <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">partitioners</span><span class="p">,</span>
                           <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regularizers</span><span class="p">,</span>
                           <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span><span class="p">,</span>
                           <span class="n">custom_getter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">,</span>
                           <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Conv2DTranspose"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv2DTranspose">[docs]</a><span class="k">class</span> <span class="nc">Conv2DTranspose</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">Transposable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Spatial transposed / reverse / up 2D convolution module, including bias.</span>

<span class="sd">  This acts as a light wrapper around the TensorFlow op `tf.nn.conv2d_transpose`</span>
<span class="sd">  abstracting away variable creation and sharing.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">SAME</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">data_format</span><span class="o">=</span><span class="n">DATA_FORMAT_NHWC</span><span class="p">,</span> <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_2d_transpose&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a `Conv2DTranspose module`.</span>

<span class="sd">    See the following documentation for an explanation of VALID versus SAME</span>
<span class="sd">    padding modes:</span>
<span class="sd">    https://www.tensorflow.org/api_guides/python/nn#Convolution</span>

<span class="sd">    Args:</span>
<span class="sd">      output_channels: Number of output channels.</span>
<span class="sd">          Can be either a number or a callable. In the latter case, since the</span>
<span class="sd">          function invocation is deferred to graph construction time, the user</span>
<span class="sd">          must only ensure `output_channels` can be called, returning an</span>
<span class="sd">          integer, when build is called.</span>
<span class="sd">      output_shape: Output shape of transpose convolution.</span>
<span class="sd">          Can be either an iterable of integers or a callable. In the latter</span>
<span class="sd">          case, since the function invocation is deferred to graph construction</span>
<span class="sd">          time, the user must only ensure that `output_shape` can be called,</span>
<span class="sd">          returning an iterable of format `(out_height, out_width)` when `build`</span>
<span class="sd">          is called. Note that `output_shape` defines the size of output signal</span>
<span class="sd">          domain, as opposed to the shape of the output `Tensor`. If a None</span>
<span class="sd">          value is given, a default shape is automatically calculated (see</span>
<span class="sd">          docstring of _default_transpose_size function for more details).</span>
<span class="sd">      kernel_shape: Sequence of kernel sizes (of size 2), or integer that is</span>
<span class="sd">          used to define kernel size in all dimensions.</span>
<span class="sd">      stride: Sequence of kernel strides (of size 2), or integer that is used to</span>
<span class="sd">          define stride in all dimensions.</span>
<span class="sd">      padding: Padding algorithm, either `snt.SAME` or `snt.VALID`.</span>
<span class="sd">      use_bias: Whether to include bias parameters. Default `True`.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">          key &#39;w&#39;) or biases (with key &#39;b&#39;).</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">          weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">          partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      data_format: A string. Specifies whether the channel dimension</span>
<span class="sd">          of the input and output is the last dimension (default, NHWC), or the</span>
<span class="sd">          second dimension (&quot;NCHW&quot;).</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.IncompatibleShapeError: If the given kernel shape is neither an</span>
<span class="sd">          integer nor a sequence of two integers.</span>
<span class="sd">      base.IncompatibleShapeError: If the given stride is neither an integer nor</span>
<span class="sd">          a sequence of two or four integers.</span>
<span class="sd">      ValueError: If the given padding is not `snt.VALID` or `snt.SAME`.</span>
<span class="sd">      ValueError: If the given data_format is not a supported format (see</span>
<span class="sd">        SUPPORTED_DATA_FORMATS).</span>
<span class="sd">      ValueError: If the given kernel_shape is `None`.</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contain any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Conv2DTranspose</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
                                          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>

    <span class="k">if</span> <span class="n">output_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_use_default_output_shape</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_use_default_output_shape</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">output_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="n">output_shape</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
                                                              <span class="s2">&quot;output_shape&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">data_format</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">SUPPORTED_DATA_FORMATS</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid data_format </span><span class="si">{:s}</span><span class="s2">. Allowed formats &quot;</span>
                       <span class="s2">&quot;</span><span class="si">{:s}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="n">SUPPORTED_DATA_FORMATS</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span> <span class="o">=</span> <span class="n">data_format</span>

    <span class="k">if</span> <span class="n">kernel_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`kernel_shape` cannot be None.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
                                                          <span class="s2">&quot;kernel&quot;</span><span class="p">)</span>
    <span class="c1"># We want to support passing native strides akin to [1, m, n, 1].</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid stride: First and last element must be 1.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="n">_fill_and_one_pad_stride</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span> <span class="o">=</span> <span class="n">_verify_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>

<div class="viewcode-block" id="Conv2DTranspose.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv2DTranspose.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_bias</span> <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">}</span></div>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the Conv2DTranspose module into the graph.</span>

<span class="sd">    If this is not the first time the module has been connected to the graph,</span>
<span class="sd">    the input Tensor provided here must have the same final 3 dimensions, in</span>
<span class="sd">    order for the existing variables to be the correct size for the</span>
<span class="sd">    multiplication. The batch size may differ for each connection.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A 4D Tensor of shape [batch_size, input_height, input_width,</span>
<span class="sd">          input_channels].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 4D Tensor of shape [batch_size, output_height, output_width,</span>
<span class="sd">          output_channels].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If connecting the module into the graph any time after the</span>
<span class="sd">          first time and the inferred size of the input does not match previous</span>
<span class="sd">          invocations.</span>
<span class="sd">      base.IncompatibleShapeError: If the input tensor has the wrong number of</span>
<span class="sd">          dimensions; or if the input tensor has an unknown `input_channels`; or</span>
<span class="sd">          or if `output_shape` is an iterable and is not in the format</span>
<span class="sd">          `(out_height, out_width)`.</span>
<span class="sd">      TypeError: If input Tensor dtype is not `tf.float32`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Handle input whose shape is unknown during graph creation.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Input Tensor must have shape (batch_size, input_height, &quot;</span>
          <span class="s2">&quot;input_width, input_channels)&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span> <span class="o">==</span> <span class="n">DATA_FORMAT_NCHW</span><span class="p">:</span>
      <span class="n">input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">input_channels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Number of input channels must be known at module build time&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input must have dtype tf.float32, but dtype was &quot;</span> <span class="o">+</span>
                      <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_default_output_shape</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="p">(</span>
          <span class="k">lambda</span><span class="p">:</span> <span class="n">_default_transpose_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># pylint: disable=g-long-lambda</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                                          <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">,</span>
                                          <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span><span class="s2">&quot;Output shape must be specified as &quot;</span>
                                        <span class="s2">&quot;(output_height, output_width)&quot;</span><span class="p">)</span>

    <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">)</span>

    <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,)</span>

    <span class="k">if</span> <span class="s2">&quot;w&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="n">fan_in_shape</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">weight_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_weight_initializer</span><span class="p">(</span><span class="n">fan_in_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_bias_initializer</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span>
                              <span class="n">shape</span><span class="o">=</span><span class="n">weight_shape</span><span class="p">,</span>
                              <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span>
                              <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                              <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="c1"># Use tensorflow shape op to manipulate inputs shape, so that unknown batch</span>
    <span class="c1"># size - which can happen when using input placeholders - is handled</span>
    <span class="c1"># correcly.</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">)</span>
    <span class="n">out_channels</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span> <span class="o">==</span> <span class="n">DATA_FORMAT_NCHW</span><span class="p">:</span>
      <span class="n">out_shape_tuple</span> <span class="o">=</span> <span class="n">out_channels</span> <span class="o">+</span> <span class="n">out_shape</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">out_shape_tuple</span> <span class="o">=</span> <span class="n">out_shape</span> <span class="o">+</span> <span class="n">out_channels</span>

    <span class="n">conv_output_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">out_shape_tuple</span><span class="p">)</span>
    <span class="n">output_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">conv_output_shape</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d_transpose</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">,</span>
                                     <span class="n">output_shape</span><span class="p">,</span>
                                     <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span>
                                     <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">,</span>
                                     <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span>
                                <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                                <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span><span class="p">)</span>

    <span class="c1"># Recover output tensor shape value and pass it to set_shape in order to</span>
    <span class="c1"># enable shape inference.</span>
    <span class="n">batch_size_value</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span> <span class="o">==</span> <span class="n">DATA_FORMAT_NCHW</span><span class="p">:</span>
      <span class="n">output_shape_value</span> <span class="o">=</span> <span class="p">((</span><span class="n">batch_size_value</span><span class="p">,)</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,)</span> <span class="o">+</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">output_shape_value</span> <span class="o">=</span> <span class="p">((</span><span class="n">batch_size_value</span><span class="p">,)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">+</span>
                            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,))</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">output_shape_value</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of output channels.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">kernel_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the kernel shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">stride</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the stride.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the output shape.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span><span class="p">())</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the padding algorithm.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">w</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the weight matrix.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">b</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Variable object containing the bias, from the most recent __call__.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.NotConnectedError: If the module has not been connected to the graph</span>
<span class="sd">          yet, meaning the variables do not exist.</span>
<span class="sd">      AttributeError: If the module does not use bias.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
          <span class="s2">&quot;No bias Variable in Conv2DTranspose Module when `use_bias=False`.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if bias Variable is present in the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initializers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioners</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the partitioners dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the regularizers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span>

  <span class="c1"># Implements Transposable interface.</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the input shape.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="c1"># Implements Transposable interface.</span>
<div class="viewcode-block" id="Conv2DTranspose.transpose"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv2DTranspose.transpose">[docs]</a>  <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns matching `Conv2D` module.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Optional string assigning name of transpose module. The default name</span>
<span class="sd">          is constructed by appending &quot;_transpose&quot; to `self.name`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      `Conv2D` module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;_transpose&quot;</span>
    <span class="k">return</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                  <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">,</span>
                  <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                  <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                  <span class="n">use_bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">,</span>
                  <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">initializers</span><span class="p">,</span>
                  <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">partitioners</span><span class="p">,</span>
                  <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regularizers</span><span class="p">,</span>
                  <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span><span class="p">,</span>
                  <span class="n">custom_getter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div></div>



<div class="viewcode-block" id="Conv1D"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv1D">[docs]</a><span class="k">class</span> <span class="nc">Conv1D</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">Transposable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;1D convolution module, including optional bias.</span>

<span class="sd">  This acts as a light wrapper around the TensorFlow op `tf.nn.convolution`,</span>
<span class="sd">  abstracting away variable creation and sharing.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">padding</span><span class="o">=</span><span class="n">SAME</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_1d&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a Conv1D module.</span>

<span class="sd">    See the following documentation for an explanation of VALID versus SAME</span>
<span class="sd">    padding modes:</span>
<span class="sd">    https://www.tensorflow.org/api_guides/python/nn#Convolution</span>

<span class="sd">    Args:</span>
<span class="sd">      output_channels: Number of output channels. `output_channels` can be</span>
<span class="sd">          either a number or a callable. In the latter case, since the function</span>
<span class="sd">          invocation is deferred to graph construction time, the user must only</span>
<span class="sd">          ensure that output_channels can be called, returning an integer,</span>
<span class="sd">          when `build` is called.</span>
<span class="sd">      kernel_shape: Sequence of kernel sizes (of size 1), or integer that is</span>
<span class="sd">          used to define kernel size in all dimensions.</span>
<span class="sd">      stride: Sequence of kernel strides (of size 1), or integer that is used to</span>
<span class="sd">          define stride in all dimensions.</span>
<span class="sd">      rate: Sequence of dilation rates (of size 1), or integer that is used to</span>
<span class="sd">          define dilation rate in all dimensions. 1 corresponds to standard 2D</span>
<span class="sd">          convolution, `rate &gt; 1` corresponds to dilated convolution. Cannot be</span>
<span class="sd">          &gt; 1 if any of `stride` is also &gt; 1.</span>
<span class="sd">      padding: Padding algorithm, either `snt.SAME` or `snt.VALID`.</span>
<span class="sd">      use_bias: Whether to include bias parameters. Default `True`.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">          key &#39;w&#39;) or biases (with key &#39;b&#39;). The default initializer for the</span>
<span class="sd">          weights is a truncated normal initializer, which is commonly used</span>
<span class="sd">          when the inputs are zero centered (see</span>
<span class="sd">          https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for</span>
<span class="sd">          the bias is a zero initializer.</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">          weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">          partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.IncompatibleShapeError: If the given kernel shape is not an integer;</span>
<span class="sd">          or if the given kernel shape is not a sequence of two integers.</span>
<span class="sd">      base.IncompatibleShapeError: If the given stride is not an integer; or if</span>
<span class="sd">          the given stride is not a sequence of two or four integers.</span>
<span class="sd">      base.IncompatibleShapeError: If the given rate is not an integer; or if</span>
<span class="sd">          the given rate is not a sequence of two integers.</span>
<span class="sd">      base.NotSupportedError: If rate in any dimension and the stride in any</span>
<span class="sd">          dimension are simultaneously &gt; 1.</span>
<span class="sd">      ValueError: If the given padding is not `snt.VALID` or `snt.SAME`.</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contain any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Conv1D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                                                          <span class="s2">&quot;kernel&quot;</span><span class="p">)</span>
    <span class="c1"># The following is for backwards-compatibility from when we used to accept</span>
    <span class="c1"># 3-strides of the form [1, m, 1].</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">stride</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">rate</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;rate&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span><span class="p">):</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">NotSupportedError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot have stride &gt; 1 with rate &gt; 1&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span> <span class="o">=</span> <span class="n">_verify_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>

<div class="viewcode-block" id="Conv1D.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv1D.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_bias</span> <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">}</span></div>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the Conv1D module into the graph, with input Tensor `inputs`.</span>

<span class="sd">    If this is not the first time the module has been connected to the graph,</span>
<span class="sd">    the input Tensor provided here must have the same final 2 dimensions, in</span>
<span class="sd">    order for the existing variables to be the correct size for the</span>
<span class="sd">    multiplication. The batch size may differ for each connection.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A 3D Tensor of shape [batch_size, input_length, input_channels].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 3D Tensor of shape [batch_size, output_length, output_channels].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If connecting the module into the graph any time after the</span>
<span class="sd">          first time and the inferred size of the input does not match previous</span>
<span class="sd">          invocations.</span>
<span class="sd">      base.IncompatibleShapeError: If the input tensor has the wrong number</span>
<span class="sd">          of dimensions.</span>
<span class="sd">      base.IncompatibleShapeError: If a mask is present and its shape is</span>
<span class="sd">          incompatible with the shape of the weights.</span>
<span class="sd">      base.UnderspecifiedError: If the input tensor has an unknown</span>
<span class="sd">          `input_channels`.</span>
<span class="sd">      TypeError: If input Tensor dtype is not `tf.float32`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Handle input whose shape is unknown during graph creation.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Input Tensor must have shape (batch_size, input_length, input_&quot;</span>
          <span class="s2">&quot;channels)&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">UnderspecifiedError</span><span class="p">(</span>
          <span class="s2">&quot;Number of input channels must be known at module build time&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;Input must have dtype tf.float32, but dtype was </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
              <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">input_channels</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">)</span>

    <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,)</span>

    <span class="k">if</span> <span class="s2">&quot;w&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_weight_initializer</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>

    <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_bias_initializer</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span>
                              <span class="n">shape</span><span class="o">=</span><span class="n">weight_shape</span><span class="p">,</span>
                              <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span>
                              <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                              <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span>
                                <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">,</span> <span class="n">dilation_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rate</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span>
                                <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                                <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of output channels.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the input shape.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">kernel_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the kernel shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">stride</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the stride.&quot;&quot;&quot;</span>
    <span class="c1"># Backwards compatibility with old stride format.</span>

    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">rate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the dilation rate.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the padding algorithm.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">w</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the weight matrix.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">b</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the bias.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if bias Variable is present in the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initializers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioners</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the partitioners dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the regularizers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span>

  <span class="c1"># Implement Transposable interface</span>
<div class="viewcode-block" id="Conv1D.transpose"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv1D.transpose">[docs]</a>  <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns matching `Conv1DTranspose` module.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Optional string assigning name of transpose module. The default name</span>
<span class="sd">          is constructed by appending &quot;_transpose&quot; to `self.name`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      `Conv1DTranspose` module.</span>

<span class="sd">    Raises:</span>
<span class="sd">     base.NotSupportedError: If `rate` in any dimension &gt; 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span><span class="p">):</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">NotSupportedError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot transpose a dilated convolution module.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;_transpose&quot;</span>
    <span class="k">return</span> <span class="n">Conv1DTranspose</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                           <span class="n">output_shape</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                           <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">,</span>
                           <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                           <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                           <span class="n">use_bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">,</span>
                           <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">initializers</span><span class="p">,</span>
                           <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">partitioners</span><span class="p">,</span>
                           <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regularizers</span><span class="p">,</span>
                           <span class="n">custom_getter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">,</span>
                           <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Conv1DTranspose"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv1DTranspose">[docs]</a><span class="k">class</span> <span class="nc">Conv1DTranspose</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">Transposable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;1D transposed / reverse / up 1D convolution module, including bias.</span>

<span class="sd">  This performs a 1D transpose convolution by lightly wrapping the TensorFlow op</span>
<span class="sd">  `tf.nn.conv2d_transpose`, setting the size of the height dimension of the</span>
<span class="sd">  image to 1.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">SAME</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_1d_transpose&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a Conv1DTranspose module.</span>

<span class="sd">    See the following documentation for an explanation of VALID versus SAME</span>
<span class="sd">    padding modes:</span>
<span class="sd">    https://www.tensorflow.org/api_guides/python/nn#Convolution</span>

<span class="sd">    Args:</span>
<span class="sd">      output_channels: Number of output channels. Can be either a number or a</span>
<span class="sd">          callable. In the latter case, since the function invocation is</span>
<span class="sd">          deferred to graph construction time, the user must only ensure</span>
<span class="sd">          `output_channels` can be called, returning an integer, when build is</span>
<span class="sd">          called.</span>
<span class="sd">      output_shape: Output shape of transpose convolution. Can be either a</span>
<span class="sd">          number or a callable. In the latter case, since the function</span>
<span class="sd">          invocation is deferred to graph construction time, the user must only</span>
<span class="sd">          ensure that `output_shape` can be called, returning an iterable of</span>
<span class="sd">          format `(out_length)` when build is called. If a None</span>
<span class="sd">          value is given, a default shape is automatically calculated (see</span>
<span class="sd">          docstring of _default_transpose_size function for more details).</span>
<span class="sd">      kernel_shape: Sequence of kernel sizes (of size 1), or integer that is</span>
<span class="sd">          used to define kernel size in all dimensions.</span>
<span class="sd">      stride: Sequence of kernel strides (of size 1), or integer that is used to</span>
<span class="sd">          define stride in all dimensions.</span>
<span class="sd">      padding: Padding algorithm, either `snt.SAME` or `snt.VALID`.</span>
<span class="sd">      use_bias: Whether to include bias parameters. Default `True`.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">          key &#39;w&#39;) or biases (with key &#39;b&#39;).</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">          weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">          partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.IncompatibleShapeError: If the given kernel shape is not an integer;</span>
<span class="sd">          or if the given kernel shape is not a sequence of two integers.</span>
<span class="sd">      base.IncompatibleShapeError: If the given stride is not an integer; or if</span>
<span class="sd">          the given stride is not a sequence of two or four integers.</span>
<span class="sd">      ValueError: If the given padding is not `snt.VALID` or `snt.SAME`.</span>
<span class="sd">      ValueError: If the given kernel_shape is `None`.</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contain any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Conv1DTranspose</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
                                          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>

    <span class="k">if</span> <span class="n">output_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_use_default_output_shape</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_use_default_output_shape</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">output_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="n">output_shape</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_shape</span><span class="p">,)</span>
      <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">kernel_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`kernel_shape` cannot be None.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                                                          <span class="s2">&quot;kernel&quot;</span><span class="p">)</span>
    <span class="c1"># We want to support passing &#39;native&#39; strides akin to [1, m, 1].</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid stride: First and last element must be 1.&quot;</span><span class="p">)</span>
      <span class="c1"># Need to make a 4D stride in order to use tf.nn.conv2d_transpose.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">stride</span><span class="p">,)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Need to make a 4D stride in order to use tf.nn.conv2d_transpose.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">_fill_and_one_pad_stride</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span> <span class="o">=</span> <span class="n">_verify_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>

<div class="viewcode-block" id="Conv1DTranspose.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv1DTranspose.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_bias</span> <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">}</span></div>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the Conv1DTranspose module into the graph.</span>

<span class="sd">    If this is not the first time the module has been connected to the graph,</span>
<span class="sd">    the input Tensor provided here must have the same final 2 dimensions, in</span>
<span class="sd">    order for the existing variables to be the correct size for the</span>
<span class="sd">    multiplication. The batch size may differ for each connection.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A 3D Tensor of shape `[batch_size, input_length, input_channels]`.</span>
<span class="sd">    Returns:</span>
<span class="sd">      A 3D Tensor of shape `[batch_size, output_length, output_channels]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If connecting the module into the graph any time after the</span>
<span class="sd">        first time and the inferred size of the input does not match previous</span>
<span class="sd">        invocations.</span>
<span class="sd">      base.IncompatibleShapeError: If the input tensor has the wrong number</span>
<span class="sd">          of dimensions.</span>
<span class="sd">      base.IncompatibleShapeError: If the input tensor has an unknown</span>
<span class="sd">          `input_channels`.</span>
<span class="sd">      base.IncompatibleShapeError: If `output_shape` is not an integer or</span>
<span class="sd">          iterable of length 1.</span>
<span class="sd">      TypeError: If input Tensor dtype is not tf.float32.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Handle input whose shape is unknown during graph creation.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Input Tensor must have shape (batch_size, input_length, &quot;</span>
          <span class="s2">&quot;input_channels)&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">UnderspecifiedError</span><span class="p">(</span>
          <span class="s2">&quot;Number of input channels must be known at module build time&quot;</span><span class="p">)</span>
    <span class="n">input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_default_output_shape</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="p">(</span>
          <span class="k">lambda</span><span class="p">:</span> <span class="n">_default_transpose_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># pylint: disable=g-long-lambda</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                                          <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">,</span>
                                          <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Output shape must be specified as (output_length)&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input must have dtype tf.float32, but dtype was </span><span class="si">{}</span><span class="s2">&quot;</span>
                      <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,</span>
        <span class="n">input_channels</span><span class="p">)</span>

    <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,)</span>

    <span class="k">if</span> <span class="s2">&quot;w&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="n">fan_in_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">weight_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_weight_initializer</span><span class="p">(</span><span class="n">fan_in_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_bias_initializer</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span>
                              <span class="n">shape</span><span class="o">=</span><span class="n">weight_shape</span><span class="p">,</span>
                              <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span>
                              <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                              <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">out_channels</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,)</span>
    <span class="n">out_shape_tuple</span> <span class="o">=</span> <span class="n">out_shape</span> <span class="o">+</span> <span class="n">out_channels</span>
    <span class="n">conv_output_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">out_shape_tuple</span><span class="p">)</span>
    <span class="n">tf_out_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">conv_output_shape</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Add an extra dimension to the input - a height of 1.</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d_transpose</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">,</span>
                                     <span class="n">tf_out_shape</span><span class="p">,</span>
                                     <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span>
                                     <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span>
                                <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                                <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span><span class="p">)</span>

    <span class="c1"># Remove the superfluous height dimension to return a 3D tensor.</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Set the tensor sizes in order for shape inference.</span>
    <span class="n">batch_size_value</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">output_shape_value</span> <span class="o">=</span> <span class="p">((</span><span class="n">batch_size_value</span><span class="p">,)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">+</span>
                          <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,))</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">output_shape_value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of output channels.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">kernel_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the kernel shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">stride</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the stride.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the output shape.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the input shape.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the padding algorithm.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">w</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the weight matrix.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">b</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Variable object containing the bias, from the most recent __call__.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.NotConnectedError: If the module has not been connected to the graph</span>
<span class="sd">          yet, meaning the variables do not exist.</span>
<span class="sd">      AttributeError: If the module does not use bias.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
          <span class="s2">&quot;No bias Variable in Conv1DTranspose Module when `use_bias=False`.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if bias Variable is present in the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initializers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioners</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the partitioners dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the regularizers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span>

  <span class="c1"># Implement Transposable interface.</span>
<div class="viewcode-block" id="Conv1DTranspose.transpose"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv1DTranspose.transpose">[docs]</a>  <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns matching `Conv1D` module.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Optional string assigning name of transpose module. The default name</span>
<span class="sd">        is constructed by appending &quot;_transpose&quot; to `self.name`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      `Conv1D` module.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;_transpose&quot;</span>
    <span class="k">return</span> <span class="n">Conv1D</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                  <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">,</span>
                  <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">[</span><span class="mi">2</span><span class="p">],),</span>
                  <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                  <span class="n">use_bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">,</span>
                  <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">initializers</span><span class="p">,</span>
                  <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">partitioners</span><span class="p">,</span>
                  <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regularizers</span><span class="p">,</span>
                  <span class="n">custom_getter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="CausalConv1D"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.CausalConv1D">[docs]</a><span class="k">class</span> <span class="nc">CausalConv1D</span><span class="p">(</span><span class="n">Conv1D</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;1D convolution module, including optional bias.</span>

<span class="sd">  This acts as a light wrapper around Conv1D ensuring that the outputs at index</span>
<span class="sd">  `i` only depend on indices smaller than `i` (also known as a causal</span>
<span class="sd">  convolution). For further details on the theoretical background, refer to:</span>

<span class="sd">  https://arxiv.org/abs/1610.10099</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">output_channels</span><span class="p">,</span>
               <span class="n">kernel_shape</span><span class="p">,</span>
               <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;causal_conv_1d&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a CausalConv1D module.</span>

<span class="sd">    Args:</span>
<span class="sd">      output_channels: Number of output channels. `output_channels` can be</span>
<span class="sd">          either a number or a callable. In the latter case, since the function</span>
<span class="sd">          invocation is deferred to graph construction time, the user must only</span>
<span class="sd">          ensure that output_channels can be called, returning an integer,</span>
<span class="sd">          when `build` is called.</span>
<span class="sd">      kernel_shape: Sequence of kernel sizes (of size 1), or integer that is</span>
<span class="sd">          used to define kernel size in all dimensions.</span>
<span class="sd">      stride: Sequence of kernel strides (of size 1), or integer that is used to</span>
<span class="sd">          define stride in all dimensions.</span>
<span class="sd">      rate: Sequence of dilation rates (of size 1), or integer that is used to</span>
<span class="sd">          define dilation rate in all dimensions. 1 corresponds to standard 2D</span>
<span class="sd">          convolution, `rate &gt; 1` corresponds to dilated convolution. Cannot be</span>
<span class="sd">          &gt; 1 if any of `stride` is also &gt; 1.</span>
<span class="sd">      use_bias: Whether to include bias parameters. Default `True`.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">          key &#39;w&#39;) or biases (with key &#39;b&#39;). The default initializer for the</span>
<span class="sd">          weights is a truncated normal initializer, which is commonly used</span>
<span class="sd">          when the inputs are zero centered (see</span>
<span class="sd">          https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for</span>
<span class="sd">          the bias is a zero initializer.</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">          weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">          partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.IncompatibleShapeError: If the given kernel shape is not an integer;</span>
<span class="sd">          or if the given kernel shape is not a sequence of two integers.</span>
<span class="sd">      base.IncompatibleShapeError: If the given stride is not an integer; or if</span>
<span class="sd">          the given stride is not a sequence of two or four integers.</span>
<span class="sd">      base.IncompatibleShapeError: If the given rate is not an integer; or if</span>
<span class="sd">          the given rate is not a sequence of two integers.</span>
<span class="sd">      base.NotSupportedError: If rate in any dimension and the stride in any</span>
<span class="sd">          dimension are simultaneously &gt; 1.</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contain any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CausalConv1D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">output_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span>
        <span class="n">kernel_shape</span><span class="o">=</span><span class="n">kernel_shape</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">rate</span><span class="o">=</span><span class="n">rate</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">VALID</span><span class="p">,</span>  <span class="c1"># Can&#39;t be configured by the user.</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
        <span class="n">initializers</span><span class="o">=</span><span class="n">initializers</span><span class="p">,</span>
        <span class="n">partitioners</span><span class="o">=</span><span class="n">partitioners</span><span class="p">,</span>
        <span class="n">regularizers</span><span class="o">=</span><span class="n">regularizers</span><span class="p">,</span>
        <span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the CausalConv1D module into the graph, with `inputs` as input.</span>

<span class="sd">    If this is not the first time the module has been connected to the graph,</span>
<span class="sd">    the input Tensor provided here must have the same final 2 dimensions, in</span>
<span class="sd">    order for the existing variables to be the correct size for the</span>
<span class="sd">    multiplication. The batch size may differ for each connection.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A 3D Tensor of shape [batch_size, input_length, input_channels].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 3D Tensor of shape [batch_size, output_length, output_channels].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If connecting the module into the graph any time after the</span>
<span class="sd">          first time and the inferred size of the input does not match previous</span>
<span class="sd">          invocations.</span>
<span class="sd">      base.IncompatibleShapeError: If the input tensor has the wrong number</span>
<span class="sd">          of dimensions.</span>
<span class="sd">      base.IncompatibleShapeError: If a mask is present and its shape is</span>
<span class="sd">          incompatible with the shape of the weights.</span>
<span class="sd">      base.UnderspecifiedError: If the input tensor has an unknown</span>
<span class="sd">          `input_channels`.</span>
<span class="sd">      TypeError: If input Tensor dtype is not `tf.float32`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Handle input whose shape is unknown during graph creation.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Input Tensor must have shape (batch_size, input_length, input_&quot;</span>
          <span class="s2">&quot;channels)&quot;</span><span class="p">)</span>


    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">UnderspecifiedError</span><span class="p">(</span>
          <span class="s2">&quot;Number of input channels must be known at module build time&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input must have dtype tf.float32, but dtype was </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span>
                      <span class="nb">format</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">)</span>

    <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,)</span>

    <span class="k">if</span> <span class="s2">&quot;w&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_weight_initializer</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>

    <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_bias_initializer</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
        <span class="s2">&quot;w&quot;</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">weight_shape</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span>
        <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="n">pad_amount</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">padded_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">paddings</span><span class="o">=</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">pad_amount</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span>
        <span class="n">padded_inputs</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">VALID</span><span class="p">,</span>
        <span class="n">dilation_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rate</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
          <span class="s2">&quot;b&quot;</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span></div>


<div class="viewcode-block" id="InPlaneConv2D"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.InPlaneConv2D">[docs]</a><span class="k">class</span> <span class="nc">InPlaneConv2D</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Applies an in-plane convolution to each channel with tied filter weights.</span>

<span class="sd">  This acts as a light wrapper around the TensorFlow op</span>
<span class="sd">  `tf.nn.depthwise_conv2d`; it differs from the DepthWiseConv2D module in that</span>
<span class="sd">  it has tied weights (i.e. the same filter) for all the in-out channel pairs.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">SAME</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;in_plane_conv2d&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an InPlaneConv2D module.</span>

<span class="sd">    See the following documentation for an explanation of VALID versus SAME</span>
<span class="sd">    padding modes:</span>
<span class="sd">    https://www.tensorflow.org/api_guides/python/nn#Convolution</span>

<span class="sd">    Args:</span>
<span class="sd">      kernel_shape: Iterable with 2 elements in the layout [filter_height,</span>
<span class="sd">          filter_width]; or integer that is used to define the list in all</span>
<span class="sd">          dimensions.</span>
<span class="sd">      stride: Iterable with 2 or 4 elements of kernel strides, or integer that</span>
<span class="sd">          is used to define stride in all dimensions.</span>
<span class="sd">      padding: Padding algorithm, either `snt.SAME` or `snt.VALID`.</span>
<span class="sd">      use_bias: Whether to include bias parameters. Default `True`.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">          key &#39;w&#39;) or biases (with key &#39;b&#39;).</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition the</span>
<span class="sd">          filters (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">          partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `kernel_shape` is not an integer or a sequence of 2</span>
<span class="sd">          integers.</span>
<span class="sd">      ValueError: If `stride` is neither an integer nor a sequence of 2 or</span>
<span class="sd">          4 integers.</span>
<span class="sd">      ValueError: If stride is a sequence of 4 integers, the first and last</span>
<span class="sd">          dimensions are not equal to 1.</span>
<span class="sd">      ValueError: If `padding` is not `snt.VALID` or `snt.SAME`.</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contain any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">InPlaneConv2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
                                                          <span class="s2">&quot;kernel&quot;</span><span class="p">)</span>
    <span class="c1"># We want to support passing native strides akin to [1, m, n, 1].</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid stride: First and last element must be 1.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="n">_fill_and_one_pad_stride</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span> <span class="o">=</span> <span class="n">_verify_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Determined in build() from the input.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Determined in build() from the input.</span>

<div class="viewcode-block" id="InPlaneConv2D.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.InPlaneConv2D.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_bias</span> <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">}</span></div>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the module into the graph, with input Tensor `inputs`.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A 4D Tensor of shape:</span>
<span class="sd">        [batch_size, input_height, input_width, input_channels].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 4D Tensor of shape:</span>
<span class="sd">        [batch_size, output_height, output_width, input_channels].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If connecting the module into the graph any time after the</span>
<span class="sd">          first time and the inferred input size does not match previous</span>
<span class="sd">          invocations.</span>
<span class="sd">      base.IncompatibleShapeError: If the input tensor has the wrong number</span>
<span class="sd">          of dimensions; or if the input tensor has an unknown `input_channels`.</span>
<span class="sd">      TypeError: If input Tensor dtype is not tf.float32.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Handle input whose shape is unknown during graph creation.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Input Tensor must have shape (batch_size, input_height, &quot;</span>
          <span class="s2">&quot;input_width, input_channels)&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Number of input channels must be known at module build time&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input must have dtype tf.float32, but dtype was &quot;</span> <span class="o">+</span>
                      <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="mi">1</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">)</span>
    <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span><span class="p">,)</span>

    <span class="k">if</span> <span class="s2">&quot;w&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_weight_initializer</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>

    <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_bias_initializer</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span>
                              <span class="n">shape</span><span class="o">=</span><span class="n">weight_shape</span><span class="p">,</span>
                              <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span>
                              <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                              <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="n">tiled_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">depthwise_conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                                     <span class="n">tiled_weights</span><span class="p">,</span>
                                     <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span>
                                     <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span>
                                <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                                <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of input channels.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of output channels i.e. number of input channels.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the input shape.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">kernel_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the kernel shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">stride</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the stride.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the padding algorithm.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">w</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the weight matrix.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">b</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Variable object containing the bias, from the most recent __call__.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.NotConnectedError: If the module has not been connected to the graph</span>
<span class="sd">          yet, meaning the variables do not exist.</span>
<span class="sd">      AttributeError: If the module does not use bias.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
          <span class="s2">&quot;No bias Variable in InPlaneConv2D Module when `use_bias=False`.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if bias Variable is present in the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initializers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioners</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the partitioners dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the regularizers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span></div>


<div class="viewcode-block" id="DepthwiseConv2D"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.DepthwiseConv2D">[docs]</a><span class="k">class</span> <span class="nc">DepthwiseConv2D</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Spatial depthwise 2D convolution module, including bias.</span>

<span class="sd">  This acts as a light wrapper around the TensorFlow ops</span>
<span class="sd">  `tf.nn.depthwise_conv2d`, abstracting away variable creation and sharing.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">channel_multiplier</span><span class="p">,</span>
               <span class="n">kernel_shape</span><span class="p">,</span>
               <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">padding</span><span class="o">=</span><span class="n">SAME</span><span class="p">,</span>
               <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_2d_depthwise&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a DepthwiseConv2D module.</span>

<span class="sd">    See the following documentation for an explanation of VALID versus SAME</span>
<span class="sd">    padding modes:</span>
<span class="sd">    https://www.tensorflow.org/api_guides/python/nn#Convolution</span>

<span class="sd">    Args:</span>
<span class="sd">      channel_multiplier: Number of channels to expand convolution to. Must be</span>
<span class="sd">          an integer. Must be &gt; 0. When `channel_multiplier` is set to 1, apply</span>
<span class="sd">          a different filter to each input channel producing one output channel</span>
<span class="sd">          per input channel. Numbers larger than 1 cause multiple different</span>
<span class="sd">          filters to be applied to each input channel, with their outputs being</span>
<span class="sd">          concatenated together, producing `channel_multiplier` *</span>
<span class="sd">          `input_channels` output channels.</span>
<span class="sd">      kernel_shape: Iterable with 2 elements in the following layout:</span>
<span class="sd">          [filter_height, filter_width] or integer that is</span>
<span class="sd">          used to define the list in all dimensions.</span>
<span class="sd">      stride: Iterable with 2 or 4 elements of kernel strides, or integer that</span>
<span class="sd">          is used to define stride in all dimensions. Layout of list:</span>
<span class="sd">          In case of 4 elements: `[1, stride_height, stride_widith, 1]`</span>
<span class="sd">          In case of 2 elements: `[stride_height, stride_width]`.</span>
<span class="sd">      padding: Padding algorithm, either `snt.SAME` or `snt.VALID`.</span>
<span class="sd">      use_bias: Whether to include bias parameters. Default `True`.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">          key &#39;w&#39;) or biases (with key &#39;b&#39;).</span>
<span class="sd">      partitioners: Optional dict containing partitioners for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.IncompatibleShapeError: If `kernel_shape` is not an integer or a</span>
<span class="sd">          sequence of 3 integers.</span>
<span class="sd">      base.IncompatibleShapeError: If `stride` is neither an integer nor a</span>
<span class="sd">          sequence of 2 or 4 integers.</span>
<span class="sd">      base.IncompatibleShapeError: If `stride` is a sequence of 4 integers and</span>
<span class="sd">          `stride[0] != stride[3]`.</span>
<span class="sd">      ValueError: if `channel_multiplier` is not an integer &gt;= 1.</span>
<span class="sd">      ValueError: If `padding` is not `snt.VALID` or `snt.SAME`.</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contain any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">DepthwiseConv2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
                                          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">channel_multiplier</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span> <span class="ow">or</span>
        <span class="n">channel_multiplier</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;channel_multiplier (=</span><span class="si">%d</span><span class="s2">), must be integer &gt;= 1&quot;</span> <span class="o">%</span>
                       <span class="n">channel_multiplier</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_channel_multiplier</span> <span class="o">=</span> <span class="n">channel_multiplier</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
                                                          <span class="s2">&quot;kernel&quot;</span><span class="p">)</span>
    <span class="c1"># We want to support passing native strides akin to [1, m, n, 1]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid stride: First and last element must be 1.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="n">_fill_and_one_pad_stride</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span> <span class="o">=</span> <span class="n">_verify_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Determined in build() from the input.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Determined in build() from the input.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Ditto, determined from the input and kernel.</span>

<div class="viewcode-block" id="DepthwiseConv2D.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.DepthwiseConv2D.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_bias</span> <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">}</span></div>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the module into the graph, with input Tensor `inputs`.</span>

<span class="sd">    If this is not the first time the module has been connected to the graph,</span>
<span class="sd">    the input Tensor provided here must have the same final 3 dimensions, in</span>
<span class="sd">    order for the existing variables to be the correct size for the</span>
<span class="sd">    multiplication. The batch size may differ for each connection.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A 4D Tensor of shape:</span>
<span class="sd">        `[batch_size, input_height, input_width, input_channels]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 4D Tensor of shape:</span>
<span class="sd">        `[batch_size, output_height, output_width, output_channels]`, where</span>
<span class="sd">        `output_channels = input_channels * channel_multiplier`;</span>
<span class="sd">        see `kernel_shape`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If connecting the module into the graph any time after the</span>
<span class="sd">          first time and the inferred size of the input does not match previous</span>
<span class="sd">          invocations.</span>
<span class="sd">      base.IncompatibleShapeError: If the input tensor has the wrong number</span>
<span class="sd">          of dimensions; or if the input tensor has an unknown `input_channels`.</span>
<span class="sd">      TypeError: If input Tensor dtype is not `tf.float32`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Handle input whose shape is unknown during graph creation.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Input Tensor must have shape (batch_size, input_height, &quot;</span>
          <span class="s2">&quot;input_width, input_channels)&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Number of input channels must be known at module build time&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input must have dtype tf.float32, but dtype was &quot;</span> <span class="o">+</span>
                      <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="c1"># For depthwise conv, output_channels = in_channels * channel_multiplier.</span>
    <span class="c1"># By default, depthwise conv applies a different filter to every input</span>
    <span class="c1"># channel. If channel_multiplier &gt; 1, one input channel is used to produce</span>
    <span class="c1"># `channel_multiplier` outputs, which are then concatenated together.</span>
    <span class="c1"># This results in:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_channel_multiplier</span>

    <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_channel_multiplier</span><span class="p">)</span>

    <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">,)</span>

    <span class="k">if</span> <span class="s2">&quot;w&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_weight_initializer</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">[:</span><span class="mi">3</span><span class="p">])</span>

    <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_bias_initializer</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span>
                              <span class="n">shape</span><span class="o">=</span><span class="n">weight_shape</span><span class="p">,</span>
                              <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span>
                              <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                              <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">depthwise_conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">,</span>
                                     <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span>
                                     <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span>
                                <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                                <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of input channels.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of output channels.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the input shape.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">kernel_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the kernel shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">channel_multiplier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the channel multiplier.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_channel_multiplier</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">stride</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the stride.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the padding algorithm.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">w</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the weight matrix.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">b</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Variable object containing the bias, from the most recent __call__.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.NotConnectedError: If the module has not been connected to the graph</span>
<span class="sd">          yet, meaning the variables do not exist.</span>
<span class="sd">      AttributeError: If the module does not use bias.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
          <span class="s2">&quot;No bias Variable in DepthwiseConv2D Module when `use_bias=False`.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if bias Variable is present in the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initializers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioners</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the partitioners dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the regularizers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span></div>


<div class="viewcode-block" id="SeparableConv2D"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.SeparableConv2D">[docs]</a><span class="k">class</span> <span class="nc">SeparableConv2D</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Performs an in-plane convolution to each channel independently.</span>

<span class="sd">  This acts as a light wrapper around the TensorFlow op</span>
<span class="sd">  `tf.nn.separable_conv2d`, abstracting away variable creation and sharing.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">output_channels</span><span class="p">,</span>
               <span class="n">channel_multiplier</span><span class="p">,</span>
               <span class="n">kernel_shape</span><span class="p">,</span>
               <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">padding</span><span class="o">=</span><span class="n">SAME</span><span class="p">,</span>
               <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Separable_conv2d&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a SeparableConv2D module.</span>

<span class="sd">    See the following documentation for an explanation of VALID versus SAME</span>
<span class="sd">    padding modes:</span>
<span class="sd">    https://www.tensorflow.org/api_guides/python/nn#Convolution</span>

<span class="sd">    Args:</span>
<span class="sd">      output_channels: Number of output channels. Must be an integer.</span>
<span class="sd">      channel_multiplier: Number of channels to expand pointwise (depthwise)</span>
<span class="sd">          convolution to. Must be an integer. Must be &gt; 0.</span>
<span class="sd">          When `channel_multiplier` is set to 1, applies a different filter to</span>
<span class="sd">          each input channel. Numbers larger than 1 cause the filter to be</span>
<span class="sd">          applied to `channel_multiplier` input channels. Outputs are</span>
<span class="sd">          concatenated together.</span>
<span class="sd">      kernel_shape: List with 2 elements in the following layout:</span>
<span class="sd">          [filter_height, filter_width] or integer that is</span>
<span class="sd">          used to define the list in all dimensions.</span>
<span class="sd">      stride: List with 4 elements of kernel strides, or integer that is used to</span>
<span class="sd">          define stride in all dimensions. Layout of list:</span>
<span class="sd">          [1, stride_y, stride_x, 1].</span>
<span class="sd">      padding: Padding algorithm, either `snt.SAME` or `snt.VALID`.</span>
<span class="sd">      use_bias: Whether to include bias parameters. Default `True`.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">          keys &#39;w_dw&#39; for depthwise and &#39;w_pw&#39; for pointwise) or biases</span>
<span class="sd">          (with key &#39;b&#39;).</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition the</span>
<span class="sd">          filters (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">          partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with keys &#39;w_dw&#39; for depthwise and &#39;w_pw&#39; for pointwise) and the</span>
<span class="sd">        biases (with key &#39;b&#39;). As a default, no regularizers are used.</span>
<span class="sd">        A regularizer should be a function that takes a single `Tensor` as an</span>
<span class="sd">        input and returns a scalar `Tensor` output, e.g. the L1 and L2</span>
<span class="sd">        regularizers in `tf.contrib.layers`.</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If either `output_channels` or `channel_multiplier` is not an</span>
<span class="sd">          integer or less than 1.</span>
<span class="sd">      base.IncompatibleShapeError: If `kernel_shape` is not an integer or a</span>
<span class="sd">          list of 3 integers.</span>
<span class="sd">      base.IncompatibleShapeError: If `stride` is neither an integer nor a</span>
<span class="sd">          list of 2 or 4 integers.</span>
<span class="sd">      ValueError: If `padding` is not `snt.VALID` or `snt.SAME`;</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contain any</span>
<span class="sd">        keys other than &#39;w_dw&#39;, &#39;w_pw&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SeparableConv2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
                                          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span> <span class="ow">or</span> <span class="n">output_channels</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;output_channels (=</span><span class="si">{}</span><span class="s2">), must be integer &gt;= 1&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
          <span class="n">output_channels</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>

    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">channel_multiplier</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span> <span class="ow">or</span>
        <span class="n">channel_multiplier</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;channel_multiplier (</span><span class="si">{}</span><span class="s2">), must be integer &gt;= 1&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
          <span class="n">channel_multiplier</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_channel_multiplier</span> <span class="o">=</span> <span class="n">channel_multiplier</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
                                                          <span class="s2">&quot;kernel&quot;</span><span class="p">)</span>
    <span class="c1"># We want to support passing native strides akin to [1, m, n, 1].</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid stride: First and last element must be 1.&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span> <span class="ow">and</span>
              <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid stride: stride[1] and [2] must be integer.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="n">_fill_and_one_pad_stride</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span> <span class="o">=</span> <span class="n">_verify_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Determined in build() from the input.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Determined in build() from the input.</span>

<div class="viewcode-block" id="SeparableConv2D.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.SeparableConv2D.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;w_dw&quot;</span><span class="p">,</span> <span class="s2">&quot;w_pw&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_bias</span> <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;w_dw&quot;</span><span class="p">,</span> <span class="s2">&quot;w_pw&quot;</span><span class="p">}</span></div>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the module into the graph, with input Tensor `inputs`.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A 4D Tensor of shape:</span>
<span class="sd">          [batch_size, input_height, input_width, input_channels].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 4D Tensor of shape:</span>
<span class="sd">          [batch_size, output_height, output_width, output_channels].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If connecting the module into the graph any time after the</span>
<span class="sd">          first time and the inferred input size does not match previous</span>
<span class="sd">          invocations.</span>
<span class="sd">      ValueError: If `channel_multiplier` * `input_channels` &gt;</span>
<span class="sd">          `output_channels`, which means that the separable convolution is</span>
<span class="sd">          overparameterized.</span>
<span class="sd">      base.IncompatibleShapeError: If the input tensor has the wrong number</span>
<span class="sd">          of dimensions; or if the input tensor has an unknown `input_channels`.</span>
<span class="sd">      TypeError: If input Tensor dtype is not tf.float32.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Handle input whose shape is unknown during graph creation.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Input Tensor must have shape (batch_size, input_height, &quot;</span>
          <span class="s2">&quot;input_width, input_channels)&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Number of input channels must be known at module build time&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input must have dtype tf.float32, but dtype was &quot;</span> <span class="o">+</span>
                      <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="n">depthwise_weight_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                              <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_channel_multiplier</span><span class="p">)</span>
    <span class="n">pointwise_input_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_channel_multiplier</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span>
    <span class="n">pointwise_weight_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pointwise_input_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">)</span>
    <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">,)</span>

    <span class="k">if</span> <span class="s2">&quot;w_dw&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="n">fan_in_shape</span> <span class="o">=</span> <span class="n">depthwise_weight_shape</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w_dw&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_weight_initializer</span><span class="p">(</span><span class="n">fan_in_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;w_pw&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="n">fan_in_shape</span> <span class="o">=</span> <span class="n">pointwise_weight_shape</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w_pw&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_weight_initializer</span><span class="p">(</span><span class="n">fan_in_shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_bias_initializer</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_w_dw</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
        <span class="s2">&quot;w_dw&quot;</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">depthwise_weight_shape</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w_dw&quot;</span><span class="p">],</span>
        <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w_dw&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w_dw&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_w_pw</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
        <span class="s2">&quot;w_pw&quot;</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">pointwise_weight_shape</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w_pw&quot;</span><span class="p">],</span>
        <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w_pw&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w_pw&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">separable_conv2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">_w_dw</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">_w_pw</span><span class="p">,</span>
                                     <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span>
                                     <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span>
                                <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                                <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of input channels.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_channels</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of output channels.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">channel_multiplier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the channel multiplier.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_channel_multiplier</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the input shape.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">kernel_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the kernel shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">stride</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the stride.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the padding algorithm.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">w_dw</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the depthwise weight matrix.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w_dw</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">w_pw</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the pointwise weight matrix.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w_pw</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">b</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Variable object containing the bias, from the most recent __call__.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.NotConnectedError: If the module has not been connected to the graph</span>
<span class="sd">          yet, meaning the variables do not exist.</span>
<span class="sd">      AttributeError: If the module does not use bias.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
          <span class="s2">&quot;No bias Variable in SeparableConv2D Module when `use_bias=False`.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if bias Variable is present in the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initializers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioners</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the partitioners dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the regularizers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span></div>



<div class="viewcode-block" id="Conv3D"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv3D">[docs]</a><span class="k">class</span> <span class="nc">Conv3D</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Volumetric convolution module, including optional bias.</span>

<span class="sd">  This acts as a light wrapper around the TensorFlow op `tf.nn.conv3d`,</span>
<span class="sd">  abstracting away variable creation and sharing.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">padding</span><span class="o">=</span><span class="n">SAME</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_3d&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a Conv3D module.</span>

<span class="sd">    See the following documentation for an explanation of VALID versus SAME</span>
<span class="sd">    padding modes:</span>
<span class="sd">    https://www.tensorflow.org/api_guides/python/nn#Convolution</span>

<span class="sd">    Args:</span>
<span class="sd">      output_channels: Number of output channels. `output_channels` can be</span>
<span class="sd">          either a number or a callable. In the latter case, since the function</span>
<span class="sd">          invocation is deferred to graph construction time, the user must only</span>
<span class="sd">          ensure that output_channels can be called, returning an integer,</span>
<span class="sd">          when `build` is called.</span>
<span class="sd">      kernel_shape: Sequence of kernel sizes (of size 3), or integer that is</span>
<span class="sd">          used to define kernel size in all dimensions.</span>
<span class="sd">      stride: Sequence of kernel strides (of size 3), or integer that is used to</span>
<span class="sd">          define stride in all dimensions.</span>
<span class="sd">      rate: Sequence of dilation rates (of size 3), or integer that is used to</span>
<span class="sd">          define dilation rate in all dimensions. 1 corresponds to standard 2D</span>
<span class="sd">          convolution, `rate &gt; 1` corresponds to dilated convolution. Cannot be</span>
<span class="sd">          &gt; 1 if any of `stride` is also &gt; 1.</span>
<span class="sd">      padding: Padding algorithm, either `snt.SAME` or `snt.VALID`.</span>
<span class="sd">      use_bias: Whether to include bias parameters. Default `True`.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">          key &#39;w&#39;) or biases (with key &#39;b&#39;). The default initializer for the</span>
<span class="sd">          weights is a truncated normal initializer, which is commonly used</span>
<span class="sd">          when the inputs are zero centered (see</span>
<span class="sd">          https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for</span>
<span class="sd">          the bias is a zero initializer.</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">          weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">          partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.IncompatibleShapeError: If the given kernel shape is not an integer;</span>
<span class="sd">          or if the given kernel shape is not a sequence of two integers.</span>
<span class="sd">      base.IncompatibleShapeError: If the given stride is not an integer; or if</span>
<span class="sd">          the given stride is not a sequence of two or four integers.</span>
<span class="sd">      base.IncompatibleShapeError: If the given rate is not an integer; or if</span>
<span class="sd">          the given rate is not a sequence of two integers.</span>
<span class="sd">      base.NotSupportedError: If rate in any dimension and the stride in any</span>
<span class="sd">          dimension are simultaneously &gt; 1.</span>
<span class="sd">      ValueError: If the given padding is not `snt.VALID` or `snt.SAME`.</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contain any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Conv3D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span>
                                                          <span class="s2">&quot;kernel&quot;</span><span class="p">)</span>
    <span class="c1"># The following is for backwards-compatibility from when we used to accept</span>
    <span class="c1"># 3-strides of the form [1, m, n, o, 1].</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">stride</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">rate</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;rate&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span><span class="p">):</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">NotSupportedError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot have stride &gt; 1 with rate &gt; 1&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span> <span class="o">=</span> <span class="n">_verify_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>

<div class="viewcode-block" id="Conv3D.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv3D.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_bias</span> <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">}</span></div>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the Conv3D module into the graph, with input Tensor `inputs`.</span>

<span class="sd">    If this is not the first time the module has been connected to the graph,</span>
<span class="sd">    the input Tensor provided here must have the same final dimension</span>
<span class="sd">    (i.e. `input_channels`), in order for the existing variables to be the</span>
<span class="sd">    correct size for the multiplication. The batch size may differ for each</span>
<span class="sd">    connection.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A 5D Tensor of shape `[batch_size, input_depth, input_height,</span>
<span class="sd">        input_width, input_channels]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 5D Tensor of shape `[batch_size, output_depth, output_height,</span>
<span class="sd">        output_width, output_channels]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If connecting the module into the graph any time after the</span>
<span class="sd">          first time and the inferred size of the input does not match previous</span>
<span class="sd">          invocations.</span>
<span class="sd">      base.IncompatibleShapeError: If the input tensor has the wrong number</span>
<span class="sd">          of dimensions.</span>
<span class="sd">      base.UnderspecifiedError: If the input tensor has an unknown</span>
<span class="sd">          `input_channels`.</span>
<span class="sd">      TypeError: If input Tensor dtype is not `tf.float32`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Handle input whose shape is unknown during graph creation.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Input Tensor must have shape (batch_size, input_depth, &quot;</span>
          <span class="s2">&quot;input_height, input_width, input_channels)&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">UnderspecifiedError</span><span class="p">(</span>
          <span class="s2">&quot;Number of input channels must be known at module build time&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
          <span class="s2">&quot;Input must have dtype tf.float32, but dtype was </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
              <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
        <span class="n">input_channels</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">)</span>

    <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,)</span>

    <span class="k">if</span> <span class="s2">&quot;w&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_weight_initializer</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">[:</span><span class="mi">4</span><span class="p">])</span>

    <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_bias_initializer</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span>
                              <span class="n">shape</span><span class="o">=</span><span class="n">weight_shape</span><span class="p">,</span>
                              <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span>
                              <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                              <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">convolution</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span>
                                <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">,</span> <span class="n">dilation_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rate</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span>
                                <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                                <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of output channels.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the input shape.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">kernel_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the kernel shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">stride</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the stride.&quot;&quot;&quot;</span>
    <span class="c1"># Backwards compatibility with old stride format.</span>

    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the padding algorithm.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">w</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the weight matrix.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">b</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the bias.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
          <span class="s2">&quot;No bias Variable in Conv2D Module when `use_bias=False`.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if bias Variable is present in the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initializers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioners</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the partitioners dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the regularizers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span>

  <span class="c1"># Implements Transposable interface.</span>
<div class="viewcode-block" id="Conv3D.transpose"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv3D.transpose">[docs]</a>  <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns matching `Conv3DTranspose` module.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Optional string assigning name of transpose module. The default name</span>
<span class="sd">        is constructed by appending &quot;_transpose&quot; to `self.name`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      `Conv3DTranspose` module.</span>

<span class="sd">    Raises:</span>
<span class="sd">     base.NotSupportedError: If `rate` in any dimension &gt; 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rate</span><span class="p">):</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">NotSupportedError</span><span class="p">(</span>
          <span class="s2">&quot;Cannot transpose a dilated convolution module.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;_transpose&quot;</span>
    <span class="k">return</span> <span class="n">Conv3DTranspose</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                           <span class="n">output_shape</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                           <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">,</span>
                           <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                           <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                           <span class="n">use_bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">,</span>
                           <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">initializers</span><span class="p">,</span>
                           <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">partitioners</span><span class="p">,</span>
                           <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regularizers</span><span class="p">,</span>
                           <span class="n">custom_getter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">,</span>
                           <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Conv3DTranspose"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv3DTranspose">[docs]</a><span class="k">class</span> <span class="nc">Conv3DTranspose</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">Transposable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Volumetric transposed / reverse / up 3D convolution module, including bias.</span>

<span class="sd">  This acts as a light wrapper around the TensorFlow op `tf.nn.conv3d_transpose`</span>
<span class="sd">  abstracting away variable creation and sharing.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">SAME</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_3d_transpose&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a `Conv3DTranspose` module.</span>

<span class="sd">    See the following documentation for an explanation of VALID versus SAME</span>
<span class="sd">    padding modes:</span>
<span class="sd">    https://www.tensorflow.org/api_guides/python/nn#Convolution</span>

<span class="sd">    Args:</span>
<span class="sd">      output_channels: Number of output channels. `output_channels` can be</span>
<span class="sd">        either a number or a callable. In the latter case, since the function</span>
<span class="sd">        invocation is deferred to graph construction time, the user must only</span>
<span class="sd">        ensure `output_channels` can be called, returning an integer, when</span>
<span class="sd">        `build` is called.</span>
<span class="sd">      output_shape: Output shape of transpose convolution.</span>
<span class="sd">          Can be either an iterable of integers or a callable. In the latter</span>
<span class="sd">          case, since the function invocation is deferred to graph construction</span>
<span class="sd">          time, the user must only ensure that `output_shape` can be called,</span>
<span class="sd">          returning an iterable of format `(out_depth, out_height, out_width)`</span>
<span class="sd">          when `build` is called. Note that `output_shape` defines the size of</span>
<span class="sd">          output signal domain, as opposed to the shape of the output `Tensor`.</span>
<span class="sd">          If a None value is given, a default shape is automatically calculated</span>
<span class="sd">          (see docstring of _default_transpose_size function for more details).</span>
<span class="sd">      kernel_shape: Sequence of kernel sizes (of size 3), or integer that is</span>
<span class="sd">          used to define kernel size in all dimensions.</span>
<span class="sd">      stride: Sequence of kernel strides (of size 3), or integer that is used to</span>
<span class="sd">          define stride in all dimensions.</span>
<span class="sd">      padding: Padding algorithm, either `snt.SAME` or `snt.VALID`.</span>
<span class="sd">      use_bias: Whether to include bias parameters. Default `True`.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">        key &#39;w&#39;) or biases (with key &#39;b&#39;).</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">          weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">          partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      module.IncompatibleShapeError: If the given kernel shape is neither an</span>
<span class="sd">          integer nor a sequence of three integers.</span>
<span class="sd">      module.IncompatibleShapeError: If the given stride is neither an integer</span>
<span class="sd">          nor a sequence of three or five integers.</span>
<span class="sd">      ValueError: If the given padding is not `snt.VALID` or `snt.SAME`.</span>
<span class="sd">      ValueError: If the given kernel_shape is `None`.</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contain any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Conv3DTranspose</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
                                          <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>

    <span class="k">if</span> <span class="n">output_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_use_default_output_shape</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_use_default_output_shape</span> <span class="o">=</span> <span class="kc">False</span>
      <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">output_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="n">output_shape</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">output_shape</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span>
                                                              <span class="s2">&quot;output_shape&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">kernel_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`kernel_shape` cannot be None.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span> <span class="o">=</span> <span class="n">_fill_and_verify_parameter_shape</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span>
                                                          <span class="s2">&quot;kernel&quot;</span><span class="p">)</span>
    <span class="c1"># We want to support passing native strides akin to [1, m, n, o, 1].</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
            <span class="s2">&quot;Invalid stride: First and last element must be 1.&quot;</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="n">_fill_and_one_pad_stride</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span> <span class="o">=</span> <span class="n">_verify_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>

<div class="viewcode-block" id="Conv3DTranspose.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv3DTranspose.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_bias</span> <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">}</span></div>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the Conv3DTranspose module into the graph.</span>

<span class="sd">    If this is not the first time the module has been connected to the graph,</span>
<span class="sd">    the input Tensor provided here must have the same final dimension</span>
<span class="sd">    (i.e. `input_channels`), in order for the existing variables to be the</span>
<span class="sd">    correct size for the multiplication. The batch size may differ for each</span>
<span class="sd">    connection.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A 5D Tensor of shape [batch_size, input_depth, input_height,</span>
<span class="sd">        input_width, input_channels].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 5D Tensor of shape [batch_size, output_depth, output_height,</span>
<span class="sd">        output_width, output_channels].</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If connecting the module into the graph any time after the</span>
<span class="sd">          first time and the inferred size of the input does not match previous</span>
<span class="sd">          invocations.</span>
<span class="sd">      module.IncompatibleShapeError: If the input tensor has the wrong number of</span>
<span class="sd">          dimensions; or if the input tensor has an unknown `input_channels`; or</span>
<span class="sd">          or if `output_shape` is an iterable and is not in the format</span>
<span class="sd">          `(out_height, out_width)`.</span>
<span class="sd">      TypeError: If input Tensor dtype is not `tf.float32`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Handle input whose shape is unknown during graph creation.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Input Tensor must have shape (batch_size, input_depth, &quot;</span>
          <span class="s2">&quot;input_height, input_width, input_channels)&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Number of input channels must be known at module build time&quot;</span><span class="p">)</span>
    <span class="n">input_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input must have dtype tf.float32, but dtype was &quot;</span> <span class="o">+</span>
                      <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_default_output_shape</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="p">(</span>
          <span class="k">lambda</span><span class="p">:</span> <span class="n">_default_transpose_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># pylint: disable=g-long-lambda</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                                          <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">,</span>
                                          <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span><span class="s2">&quot;Output shape must be specified as &quot;</span>
                                        <span class="s2">&quot;(output_depth, output_height, &quot;</span>
                                        <span class="s2">&quot;output_width)&quot;</span><span class="p">)</span>

    <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,</span>
                    <span class="n">input_channels</span><span class="p">)</span>

    <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,)</span>

    <span class="k">if</span> <span class="s2">&quot;w&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="n">fan_in</span> <span class="o">=</span> <span class="n">weight_shape</span><span class="p">[:</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">weight_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">],)</span>
      <span class="n">stddev</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">fan_in</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">stddev</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="n">stddev</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">stddev</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span>
                              <span class="n">shape</span><span class="o">=</span><span class="n">weight_shape</span><span class="p">,</span>
                              <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span>
                              <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                              <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="c1"># Use tensorflow shape op to manipulate inputs shape, so that unknown batch</span>
    <span class="c1"># size - which can happen when using input placeholders - is handled</span>
    <span class="c1"># correcly.</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">conv_output_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,))</span>
    <span class="n">output_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">conv_output_shape</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv3d_transpose</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">,</span>
                                     <span class="n">output_shape</span><span class="p">,</span>
                                     <span class="n">strides</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span>
                                     <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span>
                                <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                                <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span><span class="p">)</span>

    <span class="c1"># Recover output tensor shape value and pass it to set_shape in order to</span>
    <span class="c1"># enable shape inference.</span>
    <span class="n">batch_size_value</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">output_shape_value</span> <span class="o">=</span> <span class="p">((</span><span class="n">batch_size_value</span><span class="p">,)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">+</span>
                          <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_channels</span><span class="p">,))</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">output_shape_value</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the number of output channels.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">kernel_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the kernel shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">stride</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the stride.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the output shape.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span><span class="p">())</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the padding algorithm.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">w</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the weight matrix.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">b</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Variable object containing the bias, from the most recent __call__.</span>

<span class="sd">    Raises:</span>
<span class="sd">      module.NotConnectedError: If the module has not been connected to the</span>
<span class="sd">          graph yet, meaning the variables do not exist.</span>
<span class="sd">      AttributeError: If the module does not use bias.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
          <span class="s2">&quot;No bias Variable in Conv3DTranspose Module when `use_bias=False`.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if bias Variable is present in the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the intializers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioners</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the partitioners dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the regularizers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the input shape.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="c1"># Implement Transposable interface</span>
<div class="viewcode-block" id="Conv3DTranspose.transpose"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.conv.Conv3DTranspose.transpose">[docs]</a>  <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns transposed Conv3DTranspose module, i.e. a Conv3D module.&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;_transpose&quot;</span>
    <span class="k">return</span> <span class="n">Conv3D</span><span class="p">(</span><span class="n">output_channels</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                  <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shape</span><span class="p">,</span>
                  <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                  <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                  <span class="n">use_bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">,</span>
                  <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">initializers</span><span class="p">,</span>
                  <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">partitioners</span><span class="p">,</span>
                  <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">regularizers</span><span class="p">,</span>
                  <span class="n">custom_getter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_custom_getter</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  <li><a href="../../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Sonnet Authors.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    

    
  </body>
</html>