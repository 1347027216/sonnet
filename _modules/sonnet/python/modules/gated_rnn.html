
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sonnet.python.modules.gated_rnn &#8212; sonnet git documentation</title>
    <link rel="stylesheet" href="../../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../',
        VERSION:     'git',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sonnet.python.modules.gated_rnn</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2017 The Sonnet Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;LSTM based modules for TensorFlow snt.</span>

<span class="sd">This python module contains LSTM-like cores that fall under the broader group</span>
<span class="sd">of RNN cores.  In general, initializers for the gate weights and other</span>
<span class="sd">model parameters may be passed to the constructor.</span>

<span class="sd">Typical usage example of the standard LSTM without peephole connections:</span>

<span class="sd">  ```</span>
<span class="sd">  import sonnet as snt</span>

<span class="sd">  hidden_size = 10</span>
<span class="sd">  batch_size = 2</span>

<span class="sd">  # Simple LSTM op on some input</span>
<span class="sd">  rnn = snt.LSTM(hidden_size)</span>
<span class="sd">  input = tf.placeholder(tf.float32, shape=[batch_size, hidden_size])</span>
<span class="sd">  out, next_state = rnn(input, rnn.initial_state())</span>
<span class="sd">  ```</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span>

<span class="c1"># Dependency imports</span>

<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="n">xrange</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">base</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">basic</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">batch_norm</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">conv</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">layer_norm</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">rnn_core</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">util</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="k">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">deprecation</span>


<span class="k">class</span> <span class="nc">_BaseLSTM</span><span class="p">(</span><span class="n">rnn_core</span><span class="o">.</span><span class="n">RNNCore</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Base implementation underlying BatchNormLSTM and LSTM.</span>

<span class="sd">  This exists as a temporary measure during the deprecation process of batch</span>
<span class="sd">  norm options to LSTM. Once deprecation is complete, it will be removed and</span>
<span class="sd">  both BatchNormLSTM and LSTM replaced with simpler direct implementations.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Keys that may be provided for parameter initializers.</span>
  <span class="n">W_GATES</span> <span class="o">=</span> <span class="s2">&quot;w_gates&quot;</span>  <span class="c1"># weight for gates</span>
  <span class="n">B_GATES</span> <span class="o">=</span> <span class="s2">&quot;b_gates&quot;</span>  <span class="c1"># bias of gates</span>
  <span class="n">W_F_DIAG</span> <span class="o">=</span> <span class="s2">&quot;w_f_diag&quot;</span>  <span class="c1"># weight for prev_cell -&gt; forget gate peephole</span>
  <span class="n">W_I_DIAG</span> <span class="o">=</span> <span class="s2">&quot;w_i_diag&quot;</span>  <span class="c1"># weight for prev_cell -&gt; input gate peephole</span>
  <span class="n">W_O_DIAG</span> <span class="o">=</span> <span class="s2">&quot;w_o_diag&quot;</span>  <span class="c1"># weight for prev_cell -&gt; output gate peephole</span>
  <span class="n">GAMMA_H</span> <span class="o">=</span> <span class="s2">&quot;gamma_h&quot;</span>  <span class="c1"># batch norm scaling for previous_hidden -&gt; gates</span>
  <span class="n">GAMMA_X</span> <span class="o">=</span> <span class="s2">&quot;gamma_x&quot;</span>  <span class="c1"># batch norm scaling for input -&gt; gates</span>
  <span class="n">GAMMA_C</span> <span class="o">=</span> <span class="s2">&quot;gamma_c&quot;</span>  <span class="c1"># batch norm scaling for cell -&gt; output</span>
  <span class="n">BETA_C</span> <span class="o">=</span> <span class="s2">&quot;beta_c&quot;</span>  <span class="c1"># (batch norm) bias for cell -&gt; output</span>
  <span class="n">POSSIBLE_INITIALIZER_KEYS</span> <span class="o">=</span> <span class="p">{</span><span class="n">W_GATES</span><span class="p">,</span> <span class="n">B_GATES</span><span class="p">,</span> <span class="n">W_F_DIAG</span><span class="p">,</span> <span class="n">W_I_DIAG</span><span class="p">,</span> <span class="n">W_O_DIAG</span><span class="p">,</span>
                               <span class="n">GAMMA_H</span><span class="p">,</span> <span class="n">GAMMA_X</span><span class="p">,</span> <span class="n">GAMMA_C</span><span class="p">,</span> <span class="n">BETA_C</span><span class="p">}</span>
  <span class="c1"># Keep old name for backwards compatibility</span>

  <span class="n">POSSIBLE_KEYS</span> <span class="o">=</span> <span class="n">POSSIBLE_INITIALIZER_KEYS</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">hidden_size</span><span class="p">,</span>
               <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">use_peepholes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">use_batch_norm_h</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">use_batch_norm_x</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">use_batch_norm_c</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">use_layer_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">max_unique_stats</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">hidden_clip_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">cell_clip_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lstm&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;See `__init__` of `LSTM` and `BatchNormLSTM` for docs.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">_BaseLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span> <span class="o">=</span> <span class="n">use_peepholes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_max_unique_stats</span> <span class="o">=</span> <span class="n">max_unique_stats</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_h</span> <span class="o">=</span> <span class="n">use_batch_norm_h</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_x</span> <span class="o">=</span> <span class="n">use_batch_norm_x</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_c</span> <span class="o">=</span> <span class="n">use_batch_norm_c</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_layer_norm</span> <span class="o">=</span> <span class="n">use_layer_norm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_clip_value</span> <span class="o">=</span> <span class="n">hidden_clip_value</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip_value</span> <span class="o">=</span> <span class="n">cell_clip_value</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span>
        <span class="n">use_peepholes</span><span class="o">=</span><span class="n">use_peepholes</span><span class="p">,</span> <span class="n">use_batch_norm_h</span><span class="o">=</span><span class="n">use_batch_norm_h</span><span class="p">,</span>
        <span class="n">use_batch_norm_x</span><span class="o">=</span><span class="n">use_batch_norm_x</span><span class="p">,</span> <span class="n">use_batch_norm_c</span><span class="o">=</span><span class="n">use_batch_norm_c</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span><span class="n">initializers</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span><span class="n">partitioners</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span><span class="n">regularizers</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">max_unique_stats</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max_unique_stats must be &gt;= 1&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">max_unique_stats</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span>
        <span class="n">use_batch_norm_h</span> <span class="ow">or</span> <span class="n">use_batch_norm_x</span> <span class="ow">or</span> <span class="n">use_batch_norm_c</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max_unique_stats specified but batch norm disabled&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_batch_norm_h</span> <span class="ow">and</span> <span class="n">use_layer_norm</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Only one of use_batch_norm_h and layer_norm is allowed.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_batch_norm_x</span> <span class="ow">and</span> <span class="n">use_layer_norm</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Only one of use_batch_norm_x and layer_norm is allowed.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_batch_norm_c</span> <span class="ow">and</span> <span class="n">use_layer_norm</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Only one of use_batch_norm_c and layer_norm is allowed.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">hidden_clip_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">hidden_clip_value</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The value of hidden_clip_value should be nonnegative.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cell_clip_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">cell_clip_value</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The value of cell_clip_value should be nonnegative.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">use_batch_norm_h</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batch_norm_h</span> <span class="o">=</span> <span class="n">_BaseLSTM</span><span class="o">.</span><span class="n">IndexedStatsBatchNorm</span><span class="p">(</span><span class="n">max_unique_stats</span><span class="p">,</span>
                                                           <span class="s2">&quot;batch_norm_h&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_batch_norm_x</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batch_norm_x</span> <span class="o">=</span> <span class="n">_BaseLSTM</span><span class="o">.</span><span class="n">IndexedStatsBatchNorm</span><span class="p">(</span><span class="n">max_unique_stats</span><span class="p">,</span>
                                                           <span class="s2">&quot;batch_norm_x&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">use_batch_norm_c</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_batch_norm_c</span> <span class="o">=</span> <span class="n">_BaseLSTM</span><span class="o">.</span><span class="n">IndexedStatsBatchNorm</span><span class="p">(</span><span class="n">max_unique_stats</span><span class="p">,</span>
                                                           <span class="s2">&quot;batch_norm_c&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">with_batch_norm_control</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="n">test_local_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wraps this RNNCore with the additional control input to the `BatchNorm`s.</span>

<span class="sd">    Example usage:</span>

<span class="sd">      lstm = snt.BatchNormLSTM(4)</span>
<span class="sd">      is_training = tf.placeholder(tf.bool)</span>
<span class="sd">      rnn_input = ...</span>
<span class="sd">      my_rnn = rnn.rnn(lstm.with_batch_norm_control(is_training), rnn_input)</span>

<span class="sd">    Args:</span>
<span class="sd">      is_training: Boolean that indicates whether we are in</span>
<span class="sd">        training mode or testing mode. When in training mode, the batch norm</span>
<span class="sd">        statistics are taken from the given batch, and moving statistics are</span>
<span class="sd">        updated. When in testing mode, the moving statistics are not updated,</span>
<span class="sd">        and in addition if `test_local_stats` is False then the moving</span>
<span class="sd">        statistics are used for the batch statistics. See the `BatchNorm` module</span>
<span class="sd">        for more details.</span>
<span class="sd">      test_local_stats: Boolean scalar indicated whether to use local</span>
<span class="sd">        batch statistics in test mode.</span>

<span class="sd">    Returns:</span>
<span class="sd">      RNNCell wrapping this class with the extra input(s) added.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_BaseLSTM</span><span class="o">.</span><span class="n">CellWithExtraInput</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                                        <span class="n">is_training</span><span class="o">=</span><span class="n">is_training</span><span class="p">,</span>
                                        <span class="n">test_local_stats</span><span class="o">=</span><span class="n">test_local_stats</span><span class="p">)</span>

  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span>
      <span class="bp">cls</span><span class="p">,</span> <span class="n">use_peepholes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_batch_norm_h</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_batch_norm_x</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="n">use_batch_norm_c</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the keys the dictionary of variable initializers may contain.</span>

<span class="sd">    The set of all possible initializer keys are:</span>
<span class="sd">      w_gates:  weight for gates</span>
<span class="sd">      b_gates:  bias of gates</span>
<span class="sd">      w_f_diag: weight for prev_cell -&gt; forget gate peephole</span>
<span class="sd">      w_i_diag: weight for prev_cell -&gt; input gate peephole</span>
<span class="sd">      w_o_diag: weight for prev_cell -&gt; output gate peephole</span>
<span class="sd">      gamma_h:  batch norm scaling for previous_hidden -&gt; gates</span>
<span class="sd">      gamma_x:  batch norm scaling for input -&gt; gates</span>
<span class="sd">      gamma_c:  batch norm scaling for cell -&gt; output</span>
<span class="sd">      beta_c:   batch norm bias for cell -&gt; output</span>

<span class="sd">    Args:</span>
<span class="sd">      cls:The class.</span>
<span class="sd">      use_peepholes: Boolean that indicates whether peephole connections are</span>
<span class="sd">        used.</span>
<span class="sd">      use_batch_norm_h: Boolean that indicates whether to apply batch</span>
<span class="sd">        normalization at the previous_hidden -&gt; gates contribution. If you are</span>
<span class="sd">        experimenting with batch norm then this may be the most effective to</span>
<span class="sd">        turn on.</span>
<span class="sd">      use_batch_norm_x: Boolean that indicates whether to apply batch</span>
<span class="sd">        normalization at the input -&gt; gates contribution.</span>
<span class="sd">      use_batch_norm_c: Boolean that indicates whether to apply batch</span>
<span class="sd">        normalization at the cell -&gt; output contribution.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Set with strings corresponding to the strings that may be passed to the</span>
<span class="sd">        constructor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">use_peepholes</span><span class="p">:</span>
      <span class="n">possible_keys</span><span class="o">.</span><span class="n">difference_update</span><span class="p">(</span>
          <span class="p">{</span><span class="bp">cls</span><span class="o">.</span><span class="n">W_F_DIAG</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">W_I_DIAG</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">W_O_DIAG</span><span class="p">})</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">use_batch_norm_h</span><span class="p">:</span>
      <span class="n">possible_keys</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">GAMMA_H</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">use_batch_norm_x</span><span class="p">:</span>
      <span class="n">possible_keys</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">GAMMA_X</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">use_batch_norm_c</span><span class="p">:</span>
      <span class="n">possible_keys</span><span class="o">.</span><span class="n">difference_update</span><span class="p">({</span><span class="bp">cls</span><span class="o">.</span><span class="n">GAMMA_C</span><span class="p">,</span> <span class="bp">cls</span><span class="o">.</span><span class="n">BETA_C</span><span class="p">})</span>
    <span class="k">return</span> <span class="n">possible_keys</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">prev_state</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">test_local_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the LSTM module into the graph.</span>

<span class="sd">    If this is not the first time the module has been connected to the graph,</span>
<span class="sd">    the Tensors provided as inputs and state must have the same final</span>
<span class="sd">    dimension, in order for the existing variables to be the correct size for</span>
<span class="sd">    their corresponding multiplications. The batch size may differ for each</span>
<span class="sd">    connection.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: Tensor of size `[batch_size, input_size]`.</span>
<span class="sd">      prev_state: Tuple (prev_hidden, prev_cell), or if batch norm is enabled</span>
<span class="sd">        and `max_unique_stats &gt; 1`, then (prev_hidden, prev_cell, time_step).</span>
<span class="sd">        Here, prev_hidden and prev_cell are tensors of size</span>
<span class="sd">        `[batch_size, hidden_size]`, and time_step is used to indicate the</span>
<span class="sd">        current RNN step.</span>
<span class="sd">      is_training: Boolean indicating whether we are in training mode (as</span>
<span class="sd">        opposed to testing mode), passed to the batch norm</span>
<span class="sd">        modules. Note to use this you must wrap the cell via the</span>
<span class="sd">        `with_batch_norm_control` function.</span>
<span class="sd">      test_local_stats: Boolean indicating whether to use local batch statistics</span>
<span class="sd">        in test mode. See the `BatchNorm` documentation for more on this.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple (output, next_state) where &#39;output&#39; is a Tensor of size</span>
<span class="sd">      `[batch_size, hidden_size]` and &#39;next_state&#39; is a tuple</span>
<span class="sd">      (next_hidden, next_cell) or (next_hidden, next_cell, time_step + 1),</span>
<span class="sd">      where next_hidden and next_cell have size `[batch_size, hidden_size]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If connecting the module into the graph any time after the</span>
<span class="sd">        first time, and the inferred size of the inputs does not match previous</span>
<span class="sd">        invocations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">use_batch_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_c</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_h</span>
    <span class="n">use_batch_norm</span> <span class="o">=</span> <span class="n">use_batch_norm</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_x</span>
    <span class="k">if</span> <span class="n">use_batch_norm</span> <span class="ow">and</span> <span class="n">is_training</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Boolean is_training flag must be explicitly specified &quot;</span>
                       <span class="s2">&quot;when using batch normalization.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_unique_stats</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">prev_hidden</span><span class="p">,</span> <span class="n">prev_cell</span> <span class="o">=</span> <span class="n">prev_state</span>
      <span class="n">time_step</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">prev_hidden</span><span class="p">,</span> <span class="n">prev_cell</span><span class="p">,</span> <span class="n">time_step</span> <span class="o">=</span> <span class="n">prev_state</span>

    <span class="c1"># pylint: disable=invalid-unary-operand-type</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_clip_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">prev_hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span>
          <span class="n">prev_hidden</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_clip_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_clip_value</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">prev_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span>
          <span class="n">prev_cell</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip_value</span><span class="p">)</span>
    <span class="c1"># pylint: enable=invalid-unary-operand-type</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_create_gate_variables</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">(),</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_create_batch_norm_variables</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># pylint false positive: calling module of same file;</span>
    <span class="c1"># pylint: disable=not-callable</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_h</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_x</span><span class="p">:</span>
      <span class="n">gates_h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">prev_hidden</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w_h</span><span class="p">)</span>
      <span class="n">gates_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w_x</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_h</span><span class="p">:</span>
        <span class="n">gates_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma_h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_norm_h</span><span class="p">(</span><span class="n">gates_h</span><span class="p">,</span>
                                                     <span class="n">time_step</span><span class="p">,</span>
                                                     <span class="n">is_training</span><span class="p">,</span>
                                                     <span class="n">test_local_stats</span><span class="p">)</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_x</span><span class="p">:</span>
        <span class="n">gates_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma_x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_norm_x</span><span class="p">(</span><span class="n">gates_x</span><span class="p">,</span>
                                                     <span class="n">time_step</span><span class="p">,</span>
                                                     <span class="n">is_training</span><span class="p">,</span>
                                                     <span class="n">test_local_stats</span><span class="p">)</span>
      <span class="n">gates</span> <span class="o">=</span> <span class="n">gates_h</span> <span class="o">+</span> <span class="n">gates_x</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Parameters of gates are concatenated into one multiply for efficiency.</span>
      <span class="n">inputs_and_hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">prev_hidden</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">gates</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs_and_hidden</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w_xh</span><span class="p">)</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_layer_norm</span><span class="p">:</span>
        <span class="n">gates</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">()(</span><span class="n">gates</span><span class="p">)</span>

    <span class="n">gates</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

    <span class="c1"># i = input_gate, j = next_input, f = forget_gate, o = output_gate</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">gates</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>  <span class="c1"># diagonal connections</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_create_peephole_variables</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
      <span class="n">f</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w_f_diag</span> <span class="o">*</span> <span class="n">prev_cell</span>
      <span class="n">i</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w_i_diag</span> <span class="o">*</span> <span class="n">prev_cell</span>

    <span class="n">forget_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span>
    <span class="n">next_cell</span> <span class="o">=</span> <span class="n">forget_mask</span> <span class="o">*</span> <span class="n">prev_cell</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
    <span class="n">cell_output</span> <span class="o">=</span> <span class="n">next_cell</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_c</span><span class="p">:</span>
      <span class="n">cell_output</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta_c</span>
                     <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma_c</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_norm_c</span><span class="p">(</span><span class="n">cell_output</span><span class="p">,</span>
                                                          <span class="n">time_step</span><span class="p">,</span>
                                                          <span class="n">is_training</span><span class="p">,</span>
                                                          <span class="n">test_local_stats</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
      <span class="n">cell_output</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w_o_diag</span> <span class="o">*</span> <span class="n">cell_output</span>
    <span class="n">next_hidden</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">cell_output</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_unique_stats</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">next_hidden</span><span class="p">,</span> <span class="p">(</span><span class="n">next_hidden</span><span class="p">,</span> <span class="n">next_cell</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">next_hidden</span><span class="p">,</span> <span class="p">(</span><span class="n">next_hidden</span><span class="p">,</span> <span class="n">next_cell</span><span class="p">,</span> <span class="n">time_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_create_batch_norm_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize the variables used for the `BatchNorm`s (if any).&quot;&quot;&quot;</span>
    <span class="c1"># The paper recommends a value of 0.1 for good gradient flow through the</span>
    <span class="c1"># tanh nonlinearity (although doesn&#39;t say whether this is for all gammas,</span>
    <span class="c1"># or just some).</span>
    <span class="n">gamma_initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_h</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_gamma_h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">GAMMA_H</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">],</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">GAMMA_H</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="p">),</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">GAMMA_H</span><span class="p">),</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">GAMMA_H</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_x</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_gamma_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">GAMMA_X</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">],</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">GAMMA_X</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="p">),</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">GAMMA_X</span><span class="p">),</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">GAMMA_X</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_c</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_gamma_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">GAMMA_C</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">],</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">GAMMA_C</span><span class="p">,</span> <span class="n">gamma_initializer</span><span class="p">),</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">GAMMA_C</span><span class="p">),</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">GAMMA_C</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_beta_c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">BETA_C</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">],</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">BETA_C</span><span class="p">),</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">BETA_C</span><span class="p">),</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">BETA_C</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_create_gate_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize the variables used for the gates.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Rank of shape must be </span><span class="si">{}</span><span class="s2"> not: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)))</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="n">input_shape</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>

    <span class="n">b_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">]</span>

    <span class="n">equiv_input_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span> <span class="o">+</span> <span class="n">input_size</span>
    <span class="n">initializer</span> <span class="o">=</span> <span class="n">basic</span><span class="o">.</span><span class="n">create_linear_initializer</span><span class="p">(</span><span class="n">equiv_input_size</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_h</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_x</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_w_h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">W_GATES</span> <span class="o">+</span> <span class="s2">&quot;_H&quot;</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">],</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_GATES</span><span class="p">,</span> <span class="n">initializer</span><span class="p">),</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_GATES</span><span class="p">),</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_GATES</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_w_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">W_GATES</span> <span class="o">+</span> <span class="s2">&quot;_X&quot;</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">],</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_GATES</span><span class="p">,</span> <span class="n">initializer</span><span class="p">),</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_GATES</span><span class="p">),</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_GATES</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_w_xh</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">W_GATES</span><span class="p">,</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span> <span class="o">+</span> <span class="n">input_size</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">],</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
          <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_GATES</span><span class="p">,</span> <span class="n">initializer</span><span class="p">),</span>
          <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_GATES</span><span class="p">),</span>
          <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_GATES</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">B_GATES</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">b_shape</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">B_GATES</span><span class="p">,</span> <span class="n">initializer</span><span class="p">),</span>
        <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">B_GATES</span><span class="p">),</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">B_GATES</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_create_peephole_variables</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize the variables used for the peephole connections.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_w_f_diag</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_F_DIAG</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">],</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_F_DIAG</span><span class="p">),</span>
        <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_F_DIAG</span><span class="p">),</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_F_DIAG</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_w_i_diag</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_I_DIAG</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">],</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_I_DIAG</span><span class="p">),</span>
        <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_I_DIAG</span><span class="p">),</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_I_DIAG</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_w_o_diag</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_O_DIAG</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">],</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_O_DIAG</span><span class="p">),</span>
        <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_O_DIAG</span><span class="p">),</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_O_DIAG</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">initial_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">trainable_initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">trainable_regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Builds the default start state tensor of zeros.</span>

<span class="sd">    Args:</span>
<span class="sd">      batch_size: An int, float or scalar Tensor representing the batch size.</span>
<span class="sd">      dtype: The data type to use for the state.</span>
<span class="sd">      trainable: Boolean that indicates whether to learn the initial state.</span>
<span class="sd">      trainable_initializers: An optional pair of initializers for the</span>
<span class="sd">          initial hidden state and cell state.</span>
<span class="sd">      trainable_regularizers: Optional regularizer function or nested structure</span>
<span class="sd">        of functions with the same structure as the `state_size` property of the</span>
<span class="sd">        core, to be used as regularizers of the initial state variable. A</span>
<span class="sd">        regularizer should be a function that takes a single `Tensor` as an</span>
<span class="sd">        input and returns a scalar `Tensor` output, e.g. the L1 and L2</span>
<span class="sd">        regularizers in `tf.contrib.layers`.</span>
<span class="sd">      name: Optional string used to prefix the initial state variable names, in</span>
<span class="sd">          the case of a trainable initial state. If not provided, defaults to</span>
<span class="sd">          the name of the module.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tensor tuple `([batch_size, state_size], [batch_size, state_size], ?)`</span>
<span class="sd">      filled with zeros, with the third entry present when batch norm is enabled</span>
<span class="sd">      with `max_unique_stats &gt; 1&#39;, with value `0` (representing the time step).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_unique_stats</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">_BaseLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">initial_state</span><span class="p">(</span>
          <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span>
          <span class="n">trainable_initializers</span><span class="o">=</span><span class="n">trainable_initializers</span><span class="p">,</span>
          <span class="n">trainable_regularizers</span><span class="o">=</span><span class="n">trainable_regularizers</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_initial_state_scope</span><span class="p">(</span><span class="n">name</span><span class="p">)):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">trainable</span><span class="p">:</span>
          <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># We have to manually create the state ourselves so we don&#39;t create a</span>
          <span class="c1"># variable that never gets used for the third entry.</span>
          <span class="n">state</span> <span class="o">=</span> <span class="n">rnn_core</span><span class="o">.</span><span class="n">trainable_initial_state</span><span class="p">(</span>
              <span class="n">batch_size</span><span class="p">,</span>
              <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">]),</span>
               <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">])),</span>
              <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
              <span class="n">initializers</span><span class="o">=</span><span class="n">trainable_initializers</span><span class="p">,</span>
              <span class="n">regularizers</span><span class="o">=</span><span class="n">trainable_regularizers</span><span class="p">,</span>
              <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initial_state_scope</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Tuple of `tf.TensorShape`s indicating the size of state tensors.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_unique_stats</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">]),</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">]))</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">]),</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">]),</span>
              <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;`tf.TensorShape` indicating the size of the core output.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">])</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">use_peepholes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Boolean indicating whether peephole connections are used.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">use_batch_norm_h</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Boolean indicating whether batch norm for hidden -&gt; gates is enabled.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_h</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">use_batch_norm_x</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Boolean indicating whether batch norm for input -&gt; gates is enabled.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_x</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">use_batch_norm_c</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Boolean indicating whether batch norm for cell -&gt; output is enabled.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm_c</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">use_layer_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Boolean indicating whether layer norm is enabled.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_layer_norm</span>

  <span class="k">class</span> <span class="nc">IndexedStatsBatchNorm</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;BatchNorm module where batch statistics are selected by an input index.</span>

<span class="sd">    This is used by LSTM+batchnorm, where we have distinct batch norm statistics</span>
<span class="sd">    for the first `max_unique_stats` time steps, and then use the final set of</span>
<span class="sd">    statistics for subsequent time steps.</span>

<span class="sd">    The module has as input (x, index, is_training, test_local_stats). During</span>
<span class="sd">    training or when test_local_stats=True, the output is simply batchnorm(x)</span>
<span class="sd">    (where mean(x) and stddev(x) are used), and during training the</span>
<span class="sd">    `BatchNorm` module accumulates statistics in mean_i, etc, where</span>
<span class="sd">    i = min(index, max_unique_stats - 1).</span>

<span class="sd">    During testing with test_local_stats=False, the output is batchnorm(x),</span>
<span class="sd">    where mean_i and stddev_i are used instead of mean(x) and stddev(x).</span>

<span class="sd">    See the `BatchNorm` module for more on is_training and test_local_stats.</span>

<span class="sd">    No offset `beta` or scaling `gamma` are learnt.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_unique_stats</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Create an IndexedStatsBatchNorm.</span>

<span class="sd">      Args:</span>
<span class="sd">        max_unique_stats: number of different indices to have statistics for;</span>
<span class="sd">          indices beyond this will use the final statistics.</span>
<span class="sd">        name: Name of the module.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">_BaseLSTM</span><span class="o">.</span><span class="n">IndexedStatsBatchNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_max_unique_stats</span> <span class="o">=</span> <span class="n">max_unique_stats</span>

    <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="n">test_local_stats</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Add the IndexedStatsBatchNorm module to the graph.</span>

<span class="sd">      Args:</span>
<span class="sd">        inputs: Tensor to apply batch norm to.</span>
<span class="sd">        index: Scalar TensorFlow int32 value to select the batch norm index.</span>
<span class="sd">        is_training: Boolean to indicate to `snt.BatchNorm` if we are</span>
<span class="sd">          currently training.</span>
<span class="sd">        test_local_stats: Boolean to indicate to `snt.BatchNorm` if batch</span>
<span class="sd">          normalization should  use local batch statistics at test time.</span>

<span class="sd">      Returns:</span>
<span class="sd">        Output of batch norm operation.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">def</span> <span class="nf">create_batch_norm</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">batch_norm</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="n">offset</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="n">test_local_stats</span><span class="p">)</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_unique_stats</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">pred_fn_pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">index</span><span class="p">),</span> <span class="n">create_batch_norm</span><span class="p">)</span>
                         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_unique_stats</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">case</span><span class="p">(</span><span class="n">pred_fn_pairs</span><span class="p">,</span> <span class="n">create_batch_norm</span><span class="p">)</span>
        <span class="n">out</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>  <span class="c1"># needed for tf.case shape inference</span>
        <span class="k">return</span> <span class="n">out</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">create_batch_norm</span><span class="p">()</span>

  <span class="k">class</span> <span class="nc">CellWithExtraInput</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wraps an RNNCell to create a new RNNCell with extra input appended.</span>

<span class="sd">    This will pass the additional input `args` and `kwargs` to the __call__</span>
<span class="sd">    function of the RNNCell after the input and prev_state inputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Construct the CellWithExtraInput.</span>

<span class="sd">      Args:</span>
<span class="sd">        cell: The RNNCell to wrap (typically a snt.RNNCore).</span>
<span class="sd">        *args: Extra arguments to pass to __call__.</span>
<span class="sd">        **kwargs: Extra keyword arguments to pass to __call__.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span> <span class="o">=</span> <span class="n">cell</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_args</span> <span class="o">=</span> <span class="n">args</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_args</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_kwargs</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Tuple indicating the size of nested state tensors.&quot;&quot;&quot;</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">state_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;`tf.TensorShape` indicating the size of the core output.&quot;&quot;&quot;</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">output_size</span>


<div class="viewcode-block" id="LSTM"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.gated_rnn.LSTM">[docs]</a><span class="k">class</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">_BaseLSTM</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;LSTM recurrent network cell with optional peepholes &amp; layer normalization.</span>

<span class="sd">  The implementation is based on: http://arxiv.org/abs/1409.2329. We add</span>
<span class="sd">  forget_bias (default: 1) to the biases of the forget gate in order to</span>
<span class="sd">  reduce the scale of forgetting in the beginning of the training.</span>

<span class="sd">  #### Layer normalization</span>

<span class="sd">  This is described in https://arxiv.org/pdf/1607.06450.pdf</span>

<span class="sd">  #### Peep-hole connections</span>

<span class="sd">  Peep-hole connections may optionally be used by specifying a flag in the</span>
<span class="sd">  constructor. These connections can aid increasing the precision of output</span>
<span class="sd">  timing, for more details see:</span>

<span class="sd">    https://research.google.com/pubs/archive/43905.pdf</span>

<span class="sd">  Attributes:</span>
<span class="sd">    state_size: Tuple of `tf.TensorShape`s indicating the size of state tensors.</span>
<span class="sd">    output_size: `tf.TensorShape` indicating the size of the core output.</span>
<span class="sd">    use_peepholes: Boolean indicating whether peephole connections are used.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="nd">@deprecation</span><span class="o">.</span><span class="n">deprecated_args</span><span class="p">(</span>
      <span class="s2">&quot;2017-09-18&quot;</span><span class="p">,</span>
      <span class="s2">&quot;Please switch from LSTM to BatchNormLSTM if you need batch norm &quot;</span>
      <span class="s2">&quot;functionality.&quot;</span><span class="p">,</span>
      <span class="s2">&quot;use_batch_norm_h&quot;</span><span class="p">,</span> <span class="s2">&quot;use_batch_norm_x&quot;</span><span class="p">,</span> <span class="s2">&quot;use_batch_norm_c&quot;</span><span class="p">,</span>
      <span class="s2">&quot;max_unique_stats&quot;</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">hidden_size</span><span class="p">,</span>
               <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">use_peepholes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">use_batch_norm_h</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">use_batch_norm_x</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">use_batch_norm_c</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">use_layer_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">max_unique_stats</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">hidden_clip_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">cell_clip_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lstm&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct LSTM.</span>

<span class="sd">    Args:</span>
<span class="sd">      hidden_size: (int) Hidden size dimensionality.</span>
<span class="sd">      forget_bias: (float) Bias for the forget activation.</span>
<span class="sd">      initializers: Dict containing ops to initialize the weights.</span>
<span class="sd">        This dictionary may contain any of the keys returned by</span>
<span class="sd">        `LSTM.get_possible_initializer_keys`.</span>
<span class="sd">        The gamma and beta variables control batch normalization values for</span>
<span class="sd">        different batch norm transformations inside the cell; see the paper for</span>
<span class="sd">        details.</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">        the weights and biases. As a default, no partitioners are used. This</span>
<span class="sd">        dict may contain any of the keys returned by</span>
<span class="sd">        `LSTM.get_possible_initializer_keys`.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the weights and</span>
<span class="sd">        biases. As a default, no regularizers are used. This dict may contain</span>
<span class="sd">        any of the keys returned by</span>
<span class="sd">        `LSTM.get_possible_initializer_keys`.</span>
<span class="sd">      use_peepholes: Boolean that indicates whether peephole connections are</span>
<span class="sd">        used.</span>
<span class="sd">      use_batch_norm_h: Boolean that indicates whether to apply batch</span>
<span class="sd">        normalization at the previous_hidden -&gt; gates contribution.</span>
<span class="sd">        This is deprecated and will be removed in a future sonnet version.</span>
<span class="sd">        Please switch to `BatchNormLSTM` if you require it.</span>
<span class="sd">      use_batch_norm_x: Boolean that indicates whether to apply batch</span>
<span class="sd">        normalization at the input -&gt; gates contribution.</span>
<span class="sd">        This is deprecated and will be removed in a future sonnet version.</span>
<span class="sd">        Please switch to `BatchNormLSTM` if you require it.</span>
<span class="sd">      use_batch_norm_c: Boolean that indicates whether to apply batch</span>
<span class="sd">        normalization at the cell -&gt; output contribution.</span>
<span class="sd">        This is deprecated and will be removed in a future sonnet version.</span>
<span class="sd">        Please switch to `BatchNormLSTM` if you require it.</span>
<span class="sd">      use_layer_norm: Boolean that indicates whether to apply layer</span>
<span class="sd">        normalization.</span>
<span class="sd">      max_unique_stats: The maximum number of steps to use unique batch norm</span>
<span class="sd">        statistics for. (See module description above for more details.)</span>
<span class="sd">        This is deprecated and will be removed in a future sonnet version.</span>
<span class="sd">        Please switch to `BatchNormLSTM` if you require it.</span>
<span class="sd">      hidden_clip_value: Optional number; if set, then the LSTM hidden state</span>
<span class="sd">        vector is clipped by this value.</span>
<span class="sd">      cell_clip_value: Optional number; if set, then the LSTM cell vector is</span>
<span class="sd">        clipped by this value.</span>
<span class="sd">      name: name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      KeyError: if `initializers` contains any keys not returned by</span>
<span class="sd">        `LSTM.get_possible_initializer_keys`.</span>
<span class="sd">      KeyError: if `partitioners` contains any keys not returned by</span>
<span class="sd">        `LSTM.get_possible_initializer_keys`.</span>
<span class="sd">      KeyError: if `regularizers` contains any keys not returned by</span>
<span class="sd">        `LSTM.get_possible_initializer_keys`.</span>
<span class="sd">      ValueError: if a peephole initializer is passed in the initializer list,</span>
<span class="sd">        but `use_peepholes` is False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">LSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">forget_bias</span><span class="o">=</span><span class="n">forget_bias</span><span class="p">,</span>
        <span class="n">initializers</span><span class="o">=</span><span class="n">initializers</span><span class="p">,</span>
        <span class="n">partitioners</span><span class="o">=</span><span class="n">partitioners</span><span class="p">,</span>
        <span class="n">regularizers</span><span class="o">=</span><span class="n">regularizers</span><span class="p">,</span>
        <span class="n">use_peepholes</span><span class="o">=</span><span class="n">use_peepholes</span><span class="p">,</span>
        <span class="n">use_batch_norm_h</span><span class="o">=</span><span class="n">use_batch_norm_h</span><span class="p">,</span>
        <span class="n">use_batch_norm_x</span><span class="o">=</span><span class="n">use_batch_norm_x</span><span class="p">,</span>
        <span class="n">use_batch_norm_c</span><span class="o">=</span><span class="n">use_batch_norm_c</span><span class="p">,</span>
        <span class="n">use_layer_norm</span><span class="o">=</span><span class="n">use_layer_norm</span><span class="p">,</span>
        <span class="n">max_unique_stats</span><span class="o">=</span><span class="n">max_unique_stats</span><span class="p">,</span>
        <span class="n">hidden_clip_value</span><span class="o">=</span><span class="n">hidden_clip_value</span><span class="p">,</span>
        <span class="n">cell_clip_value</span><span class="o">=</span><span class="n">cell_clip_value</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="BatchNormLSTM"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.gated_rnn.BatchNormLSTM">[docs]</a><span class="k">class</span> <span class="nc">BatchNormLSTM</span><span class="p">(</span><span class="n">_BaseLSTM</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;LSTM recurrent network cell with optional peepholes, batch normalization.</span>

<span class="sd">  The base implementation is based on: http://arxiv.org/abs/1409.2329. We add</span>
<span class="sd">  forget_bias (default: 1) to the biases of the forget gate in order to</span>
<span class="sd">  reduce the scale of forgetting in the beginning of the training.</span>

<span class="sd">  #### Peep-hole connections</span>

<span class="sd">  Peep-hole connections may optionally be used by specifying a flag in the</span>
<span class="sd">  constructor. These connections can aid increasing the precision of output</span>
<span class="sd">  timing, for more details see:</span>

<span class="sd">    https://research.google.com/pubs/archive/43905.pdf</span>

<span class="sd">  #### Batch normalization</span>

<span class="sd">  The batch norm transformation (in training mode) is</span>
<span class="sd">    batchnorm(x) = gamma * (x - mean(x)) / stddev(x) + beta,</span>
<span class="sd">  where gamma is a learnt scaling factor and beta is a learnt offset.</span>

<span class="sd">  Batch normalization may optionally be used at different places in the LSTM by</span>
<span class="sd">  specifying flag(s) in the constructor. These are applied when calculating</span>
<span class="sd">  the gate activations and cell-to-hidden transformation. The set-up is based on</span>

<span class="sd">    https://arxiv.org/pdf/1603.09025.pdf</span>

<span class="sd">  ##### Batch normalization: where to apply?</span>

<span class="sd">  Batch norm can be applied in three different places in the LSTM:</span>

<span class="sd">    (h) To the W_h h_{t-1} contribution to the gates from the previous hiddens.</span>
<span class="sd">    (x) To the W_x x_t contribution to the gates from the current input.</span>
<span class="sd">    (c) To the cell value c_t when calculating the output h_t from the cell.</span>

<span class="sd">  (The notation here is consistent with the Recurrent Batch Normalization</span>
<span class="sd">  paper). Each of these can be controlled individually, because batch norm is</span>
<span class="sd">  expensive, and not all are necessary. The paper doesn&#39;t mention the relative</span>
<span class="sd">  effects of these different batch norms; however, experimentation with a</span>
<span class="sd">  shallow LSTM for the `permuted_mnist` sequence task suggests that (h) is the</span>
<span class="sd">  most important and the other two can be left off. For other tasks or deeper</span>
<span class="sd">  (stacked) LSTMs, other batch norm combinations may be more effective.</span>

<span class="sd">  ##### Batch normalization: collecting stats (training vs test)</span>

<span class="sd">  When switching to testing (see `LSTM.with_batch_norm_control`), we can use a</span>
<span class="sd">  mean and stddev learnt from the training data instead of using the statistics</span>
<span class="sd">  from the test data. (This both increases test accuracy because the statistics</span>
<span class="sd">  have less variance, and if the test data does not have the same distribution</span>
<span class="sd">  as the training data then we must use the training statistics to ensure the</span>
<span class="sd">  effective network does not change when switching to testing anyhow.)</span>

<span class="sd">  This does however introduces a slight subtlety. The first few time steps of</span>
<span class="sd">  the RNN tend to have varying statistics (mean and variance) before settling</span>
<span class="sd">  down to a steady value. Therefore in general, better performance is obtained</span>
<span class="sd">  by using separate statistics for the first few time steps, and then using the</span>
<span class="sd">  final set of statistics for all subsequent time steps. This is controlled by</span>
<span class="sd">  the parameter `max_unique_stats`. (We can&#39;t have an unbounded number of</span>
<span class="sd">  distinct statistics for both technical reasons and also for the case where</span>
<span class="sd">  test sequences are longer than anything seen in training.)</span>

<span class="sd">  You may be fine leaving it at its default value of 1. Small values (like 10)</span>
<span class="sd">  may achieve better performance on some tasks when testing with cached</span>
<span class="sd">  statistics.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    state_size: Tuple of `tf.TensorShape`s indicating the size of state tensors.</span>
<span class="sd">    output_size: `tf.TensorShape` indicating the size of the core output.</span>
<span class="sd">    use_peepholes: Boolean indicating whether peephole connections are used.</span>
<span class="sd">    use_batch_norm_h: Boolean indicating whether batch norm (h) is enabled.</span>
<span class="sd">    use_batch_norm_x: Boolean indicating whether batch norm (x) is enabled.</span>
<span class="sd">    use_batch_norm_c: Boolean indicating whether batch norm (c) is enabled.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">hidden_size</span><span class="p">,</span>
               <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">use_peepholes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">use_batch_norm_h</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">use_batch_norm_x</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">use_batch_norm_c</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">max_unique_stats</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">hidden_clip_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">cell_clip_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;batch_norm_lstm&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct `BatchNormLSTM`.</span>

<span class="sd">    Args:</span>
<span class="sd">      hidden_size: (int) Hidden size dimensionality.</span>
<span class="sd">      forget_bias: (float) Bias for the forget activation.</span>
<span class="sd">      initializers: Dict containing ops to initialize the weights.</span>
<span class="sd">        This dictionary may contain any of the keys returned by</span>
<span class="sd">        `BatchNormLSTM.get_possible_initializer_keys`.</span>
<span class="sd">        The gamma and beta variables control batch normalization values for</span>
<span class="sd">        different batch norm transformations inside the cell; see the paper for</span>
<span class="sd">        details.</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">        the weights and biases. As a default, no partitioners are used. This</span>
<span class="sd">        dict may contain any of the keys returned by</span>
<span class="sd">        `BatchNormLSTM.get_possible_initializer_keys`.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the weights and</span>
<span class="sd">        biases. As a default, no regularizers are used. This dict may contain</span>
<span class="sd">        any of the keys returned by</span>
<span class="sd">        `BatchNormLSTM.get_possible_initializer_keys`.</span>
<span class="sd">      use_peepholes: Boolean that indicates whether peephole connections are</span>
<span class="sd">        used.</span>
<span class="sd">      use_batch_norm_h: Boolean that indicates whether to apply batch</span>
<span class="sd">        normalization at the previous_hidden -&gt; gates contribution. If you are</span>
<span class="sd">        experimenting with batch norm then this may be the most effective to</span>
<span class="sd">        use, and is enabled by default.</span>
<span class="sd">      use_batch_norm_x: Boolean that indicates whether to apply batch</span>
<span class="sd">        normalization at the input -&gt; gates contribution.</span>
<span class="sd">      use_batch_norm_c: Boolean that indicates whether to apply batch</span>
<span class="sd">        normalization at the cell -&gt; output contribution.</span>
<span class="sd">      max_unique_stats: The maximum number of steps to use unique batch norm</span>
<span class="sd">        statistics for. (See module description above for more details.)</span>
<span class="sd">      hidden_clip_value: Optional number; if set, then the LSTM hidden state</span>
<span class="sd">        vector is clipped by this value.</span>
<span class="sd">      cell_clip_value: Optional number; if set, then the LSTM cell vector is</span>
<span class="sd">        clipped by this value.</span>
<span class="sd">      name: name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      KeyError: if `initializers` contains any keys not returned by</span>
<span class="sd">        `BatchNormLSTM.get_possible_initializer_keys`.</span>
<span class="sd">      KeyError: if `partitioners` contains any keys not returned by</span>
<span class="sd">        `BatchNormLSTM.get_possible_initializer_keys`.</span>
<span class="sd">      KeyError: if `regularizers` contains any keys not returned by</span>
<span class="sd">        `BatchNormLSTM.get_possible_initializer_keys`.</span>
<span class="sd">      ValueError: if a peephole initializer is passed in the initializer list,</span>
<span class="sd">        but `use_peepholes` is False.</span>
<span class="sd">      ValueError: if a batch norm initializer is passed in the initializer list,</span>
<span class="sd">        but batch norm is disabled.</span>
<span class="sd">      ValueError: if none of the `use_batch_norm_*` options are True.</span>
<span class="sd">      ValueError: if `max_unique_stats` is &lt; 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">([</span><span class="n">use_batch_norm_h</span><span class="p">,</span> <span class="n">use_batch_norm_x</span><span class="p">,</span> <span class="n">use_batch_norm_c</span><span class="p">]):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;At least one use_batch_norm_* option is required for &quot;</span>
                       <span class="s2">&quot;BatchNormLSTM&quot;</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BatchNormLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">forget_bias</span><span class="o">=</span><span class="n">forget_bias</span><span class="p">,</span>
        <span class="n">initializers</span><span class="o">=</span><span class="n">initializers</span><span class="p">,</span>
        <span class="n">partitioners</span><span class="o">=</span><span class="n">partitioners</span><span class="p">,</span>
        <span class="n">regularizers</span><span class="o">=</span><span class="n">regularizers</span><span class="p">,</span>
        <span class="n">use_peepholes</span><span class="o">=</span><span class="n">use_peepholes</span><span class="p">,</span>
        <span class="n">use_batch_norm_h</span><span class="o">=</span><span class="n">use_batch_norm_h</span><span class="p">,</span>
        <span class="n">use_batch_norm_x</span><span class="o">=</span><span class="n">use_batch_norm_x</span><span class="p">,</span>
        <span class="n">use_batch_norm_c</span><span class="o">=</span><span class="n">use_batch_norm_c</span><span class="p">,</span>
        <span class="n">max_unique_stats</span><span class="o">=</span><span class="n">max_unique_stats</span><span class="p">,</span>
        <span class="n">hidden_clip_value</span><span class="o">=</span><span class="n">hidden_clip_value</span><span class="p">,</span>
        <span class="n">cell_clip_value</span><span class="o">=</span><span class="n">cell_clip_value</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="c1"># Overriding because the default for use_batch_norm_h is True here.</span>
<div class="viewcode-block" id="BatchNormLSTM.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.gated_rnn.BatchNormLSTM.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span>
      <span class="bp">cls</span><span class="p">,</span> <span class="n">use_peepholes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_batch_norm_h</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_batch_norm_x</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="n">use_batch_norm_c</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">BatchNormLSTM</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span>
        <span class="n">use_peepholes</span><span class="o">=</span><span class="n">use_peepholes</span><span class="p">,</span> <span class="n">use_batch_norm_h</span><span class="o">=</span><span class="n">use_batch_norm_h</span><span class="p">,</span>
        <span class="n">use_batch_norm_x</span><span class="o">=</span><span class="n">use_batch_norm_x</span><span class="p">,</span> <span class="n">use_batch_norm_c</span><span class="o">=</span><span class="n">use_batch_norm_c</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ConvLSTM"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.gated_rnn.ConvLSTM">[docs]</a><span class="k">class</span> <span class="nc">ConvLSTM</span><span class="p">(</span><span class="n">rnn_core</span><span class="o">.</span><span class="n">RNNCore</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Convolutional LSTM.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="ConvLSTM.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.gated_rnn.ConvLSTM.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">conv_ndims</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">conv_class</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_get_conv_class</span><span class="p">(</span><span class="n">conv_ndims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conv_class</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="n">use_bias</span><span class="p">)</span></div>

  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">_get_conv_class</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">conv_ndims</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">conv_ndims</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">conv</span><span class="o">.</span><span class="n">Conv1D</span>
    <span class="k">elif</span> <span class="n">conv_ndims</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">conv</span><span class="o">.</span><span class="n">Conv2D</span>
    <span class="k">elif</span> <span class="n">conv_ndims</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">conv</span><span class="o">.</span><span class="n">Conv3D</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid convolution dimensionality.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">conv_ndims</span><span class="p">,</span>
               <span class="n">input_shape</span><span class="p">,</span>
               <span class="n">output_channels</span><span class="p">,</span>
               <span class="n">kernel_shape</span><span class="p">,</span>
               <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">padding</span><span class="o">=</span><span class="n">conv</span><span class="o">.</span><span class="n">SAME</span><span class="p">,</span>
               <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">skip_connection</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_lstm&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct ConvLSTM.</span>

<span class="sd">    Args:</span>
<span class="sd">      conv_ndims: Convolution dimensionality (1, 2 or 3).</span>
<span class="sd">      input_shape: Shape of the input as tuple, excluding the batch size.</span>
<span class="sd">      output_channels: Number of output channels of the conv LSTM.</span>
<span class="sd">      kernel_shape: Sequence of kernel sizes (of size 2), or integer that is</span>
<span class="sd">          used to define kernel size in all dimensions.</span>
<span class="sd">      stride: Sequence of kernel strides (of size 2), or integer that is used to</span>
<span class="sd">          define stride in all dimensions.</span>
<span class="sd">      padding: Padding algorithm, either `snt.SAME` or `snt.VALID`.</span>
<span class="sd">      use_bias: Use bias in convolutions.</span>
<span class="sd">      skip_connection: If set to `True`, concatenate the input to the output</span>
<span class="sd">          of the conv LSTM. Default: `False`.</span>
<span class="sd">      forget_bias: Forget bias.</span>
<span class="sd">      initializers: Dict containing ops to initialize the convolutional weights.</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">        the convolutional weights and biases. As a default, no partitioners are</span>
<span class="sd">        used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the convolutional</span>
<span class="sd">        weights and biases. As a default, no regularizers are used.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `skip_connection` is `True` and stride is different from 1</span>
<span class="sd">        or if `input_shape` is incompatible with `conv_ndims`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ConvLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_conv_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_conv_class</span><span class="p">(</span><span class="n">conv_ndims</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">skip_connection</span> <span class="ow">and</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`stride` needs to be 1 when using skip connection&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">conv_ndims</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid input_shape </span><span class="si">{}</span><span class="s2"> for conv_ndims=</span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
          <span class="n">input_shape</span><span class="p">,</span> <span class="n">conv_ndims</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_conv_ndims</span> <span class="o">=</span> <span class="n">conv_ndims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="n">input_shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span> <span class="o">=</span> <span class="n">kernel_shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">=</span> <span class="n">stride</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_padding</span> <span class="o">=</span> <span class="n">padding</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_skip_connection</span> <span class="o">=</span> <span class="n">skip_connection</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">initializers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">partitioners</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">regularizers</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_total_output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_total_output_channels</span> <span class="o">//=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stride</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_skip_connection</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_total_output_channels</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_convolutions</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_new_convolution</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_new_convolution</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv_class</span><span class="p">(</span>
        <span class="n">output_channels</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">,</span>
        <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_padding</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">,</span>
        <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">,</span>
        <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="p">,</span>
        <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv&quot;</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">convolutions</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convolutions</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Tuple of `tf.TensorShape`s indicating the size of state tensors.&quot;&quot;&quot;</span>
    <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span>
                                 <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">,))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;`tf.TensorShape` indicating the size of the core output.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span>
                          <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_output_channels</span><span class="p">,))</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="n">hidden</span><span class="p">,</span> <span class="n">cell</span> <span class="o">=</span> <span class="n">state</span>
    <span class="n">input_conv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convolutions</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span>
    <span class="n">hidden_conv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convolutions</span><span class="p">[</span><span class="s2">&quot;hidden&quot;</span><span class="p">]</span>
    <span class="n">next_hidden</span> <span class="o">=</span> <span class="n">input_conv</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">+</span> <span class="n">hidden_conv</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
    <span class="n">gates</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">next_hidden</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                     <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_conv_ndims</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">input_gate</span><span class="p">,</span> <span class="n">next_input</span><span class="p">,</span> <span class="n">forget_gate</span><span class="p">,</span> <span class="n">output_gate</span> <span class="o">=</span> <span class="n">gates</span>
    <span class="n">next_cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">forget_gate</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span> <span class="o">*</span> <span class="n">cell</span>
    <span class="n">next_cell</span> <span class="o">+=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">input_gate</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">next_input</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">next_cell</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">output_gate</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_skip_connection</span><span class="p">:</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">output</span><span class="p">,</span> <span class="n">inputs</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">next_cell</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv1DLSTM"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.gated_rnn.Conv1DLSTM">[docs]</a><span class="k">class</span> <span class="nc">Conv1DLSTM</span><span class="p">(</span><span class="n">ConvLSTM</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;1D convolutional LSTM.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="Conv1DLSTM.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.gated_rnn.Conv1DLSTM.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Conv1DLSTM</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">use_bias</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_1d_lstm&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct Conv1DLSTM. See `snt.ConvLSTM` for more details.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Conv1DLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">conv_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv2DLSTM"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.gated_rnn.Conv2DLSTM">[docs]</a><span class="k">class</span> <span class="nc">Conv2DLSTM</span><span class="p">(</span><span class="n">ConvLSTM</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;2D convolutional LSTM.&quot;&quot;&quot;</span>


<div class="viewcode-block" id="Conv2DLSTM.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.gated_rnn.Conv2DLSTM.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Conv2DLSTM</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">use_bias</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_2d_lstm&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct Conv2DLSTM. See `snt.ConvLSTM` for more details.&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Conv2DLSTM</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">conv_ndims</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="GRU"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.gated_rnn.GRU">[docs]</a><span class="k">class</span> <span class="nc">GRU</span><span class="p">(</span><span class="n">rnn_core</span><span class="o">.</span><span class="n">RNNCore</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;GRU recurrent network cell.</span>

<span class="sd">  The implementation is based on: https://arxiv.org/pdf/1412.3555v1.pdf.</span>

<span class="sd">  Attributes:</span>
<span class="sd">    state_size: Integer indicating the size of state tensor.</span>
<span class="sd">    output_size: Integer indicating the size of the core output.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># Keys that may be provided for parameter initializers.</span>
  <span class="n">WZ</span> <span class="o">=</span> <span class="s2">&quot;wz&quot;</span>  <span class="c1"># weight for input -&gt; update cell</span>
  <span class="n">UZ</span> <span class="o">=</span> <span class="s2">&quot;uz&quot;</span>  <span class="c1"># weight for prev_state -&gt; update cell</span>
  <span class="n">BZ</span> <span class="o">=</span> <span class="s2">&quot;bz&quot;</span>  <span class="c1"># bias for update_cell</span>
  <span class="n">WR</span> <span class="o">=</span> <span class="s2">&quot;wr&quot;</span>  <span class="c1"># weight for input -&gt; reset cell</span>
  <span class="n">UR</span> <span class="o">=</span> <span class="s2">&quot;ur&quot;</span>  <span class="c1"># weight for prev_state -&gt; reset cell</span>
  <span class="n">BR</span> <span class="o">=</span> <span class="s2">&quot;br&quot;</span>  <span class="c1"># bias for reset cell</span>
  <span class="n">WH</span> <span class="o">=</span> <span class="s2">&quot;wh&quot;</span>  <span class="c1"># weight for input -&gt; candidate activation</span>
  <span class="n">UH</span> <span class="o">=</span> <span class="s2">&quot;uh&quot;</span>  <span class="c1"># weight for prev_state -&gt; candidate activation</span>
  <span class="n">BH</span> <span class="o">=</span> <span class="s2">&quot;bh&quot;</span>  <span class="c1"># bias for candidate activation</span>
  <span class="n">POSSIBLE_INITIALIZER_KEYS</span> <span class="o">=</span> <span class="p">{</span><span class="n">WZ</span><span class="p">,</span> <span class="n">UZ</span><span class="p">,</span> <span class="n">BZ</span><span class="p">,</span> <span class="n">WR</span><span class="p">,</span> <span class="n">UR</span><span class="p">,</span> <span class="n">BR</span><span class="p">,</span> <span class="n">WH</span><span class="p">,</span> <span class="n">UH</span><span class="p">,</span> <span class="n">BH</span><span class="p">}</span>
  <span class="c1"># Keep old name for backwards compatibility</span>

  <span class="n">POSSIBLE_KEYS</span> <span class="o">=</span> <span class="n">POSSIBLE_INITIALIZER_KEYS</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gru&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct GRU.</span>

<span class="sd">    Args:</span>
<span class="sd">      hidden_size: (int) Hidden size dimensionality.</span>
<span class="sd">      initializers: Dict containing ops to initialize the weights. This</span>
<span class="sd">        dict may contain any of the keys returned by</span>
<span class="sd">        `GRU.get_possible_initializer_keys`.</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">        the weights and biases. As a default, no partitioners are used. This</span>
<span class="sd">        dict may contain any of the keys returned by</span>
<span class="sd">        `GRU.get_possible_initializer_keys`</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the weights and</span>
<span class="sd">        biases. As a default, no regularizers are used. This</span>
<span class="sd">        dict may contain any of the keys returned by</span>
<span class="sd">        `GRU.get_possible_initializer_keys`</span>
<span class="sd">      name: name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      KeyError: if `initializers` contains any keys not returned by</span>
<span class="sd">        `GRU.get_possible_initializer_keys`.</span>
<span class="sd">      KeyError: if `partitioners` contains any keys not returned by</span>
<span class="sd">        `GRU.get_possible_initializer_keys`.</span>
<span class="sd">      KeyError: if `regularizers` contains any keys not returned by</span>
<span class="sd">        `GRU.get_possible_initializer_keys`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">GRU</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>

<div class="viewcode-block" id="GRU.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.gated_rnn.GRU.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the keys the dictionary of variable initializers may contain.</span>

<span class="sd">    The set of all possible initializer keys are:</span>
<span class="sd">      wz: weight for input -&gt; update cell</span>
<span class="sd">      uz: weight for prev_state -&gt; update cell</span>
<span class="sd">      bz: bias for update_cell</span>
<span class="sd">      wr: weight for input -&gt; reset cell</span>
<span class="sd">      ur: weight for prev_state -&gt; reset cell</span>
<span class="sd">      br: bias for reset cell</span>
<span class="sd">      wh: weight for input -&gt; candidate activation</span>
<span class="sd">      uh: weight for prev_state -&gt; candidate activation</span>
<span class="sd">      bh: bias for candidate activation</span>

<span class="sd">    Returns:</span>
<span class="sd">      Set with strings corresponding to the strings that may be passed to the</span>
<span class="sd">        constructor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">GRU</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span></div>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">prev_state</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the GRU module into the graph.</span>

<span class="sd">    If this is not the first time the module has been connected to the graph,</span>
<span class="sd">    the Tensors provided as inputs and state must have the same final</span>
<span class="sd">    dimension, in order for the existing variables to be the correct size for</span>
<span class="sd">    their corresponding multiplications. The batch size may differ for each</span>
<span class="sd">    connection.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: Tensor of size `[batch_size, input_size]`.</span>
<span class="sd">      prev_state: Tensor of size `[batch_size, hidden_size]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A tuple (output, next_state) where `output` is a Tensor of size</span>
<span class="sd">      `[batch_size, hidden_size]` and `next_state` is a Tensor of size</span>
<span class="sd">      `[batch_size, hidden_size]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If connecting the module into the graph any time after the</span>
<span class="sd">        first time, and the inferred size of the inputs does not match previous</span>
<span class="sd">        invocations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">)</span>
    <span class="n">u_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">)</span>
    <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">,)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_wz</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">WZ</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">WZ</span><span class="p">),</span>
                               <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">WZ</span><span class="p">),</span>
                               <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">WZ</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_uz</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">UZ</span><span class="p">,</span> <span class="n">u_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">UZ</span><span class="p">),</span>
                               <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">UZ</span><span class="p">),</span>
                               <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">UZ</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_bz</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">BZ</span><span class="p">,</span> <span class="n">bias_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">BZ</span><span class="p">),</span>
                               <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">BZ</span><span class="p">),</span>
                               <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">BZ</span><span class="p">))</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wz</span><span class="p">)</span> <span class="o">+</span>
                   <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">prev_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uz</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bz</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_wr</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">WR</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">WR</span><span class="p">),</span>
                               <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">WR</span><span class="p">),</span>
                               <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">WR</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ur</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">UR</span><span class="p">,</span> <span class="n">u_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">UR</span><span class="p">),</span>
                               <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">UR</span><span class="p">),</span>
                               <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">UR</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_br</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">BR</span><span class="p">,</span> <span class="n">bias_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">BR</span><span class="p">),</span>
                               <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">BR</span><span class="p">),</span>
                               <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">BR</span><span class="p">))</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wr</span><span class="p">)</span> <span class="o">+</span>
                   <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">prev_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ur</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_br</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_wh</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">WH</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">WH</span><span class="p">),</span>
                               <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">WH</span><span class="p">),</span>
                               <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">WH</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_uh</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">UH</span><span class="p">,</span> <span class="n">u_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">UH</span><span class="p">),</span>
                               <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">UH</span><span class="p">),</span>
                               <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">UH</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_bh</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">BH</span><span class="p">,</span> <span class="n">bias_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                               <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">BH</span><span class="p">),</span>
                               <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">BH</span><span class="p">),</span>
                               <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">GRU</span><span class="o">.</span><span class="n">BH</span><span class="p">))</span>
    <span class="n">h_twiddle</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wh</span><span class="p">)</span> <span class="o">+</span>
                        <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">r</span> <span class="o">*</span> <span class="n">prev_state</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uh</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bh</span><span class="p">)</span>

    <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span> <span class="o">*</span> <span class="n">prev_state</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">h_twiddle</span>
    <span class="k">return</span> <span class="n">state</span><span class="p">,</span> <span class="n">state</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">])</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_hidden_size</span><span class="p">])</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  <li><a href="../../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Sonnet Authors.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    

    
  </body>
</html>