
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sonnet.python.modules.basic &#8212; sonnet git documentation</title>
    <link rel="stylesheet" href="../../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../',
        VERSION:     'git',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sonnet.python.modules.basic</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2017 The Sonnet Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Basic Modules for TensorFlow snt.</span>

<span class="sd">Modules defining the simplest building blocks for Neural Networks.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numbers</span>

<span class="c1"># Dependency imports</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="n">xrange</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">base</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">util</span>
<span class="kn">from</span> <span class="nn">sonnet.python.ops</span> <span class="k">import</span> <span class="n">nest</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>


<div class="viewcode-block" id="merge_leading_dims"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.merge_leading_dims">[docs]</a><span class="k">def</span> <span class="nf">merge_leading_dims</span><span class="p">(</span><span class="n">array_or_tensor</span><span class="p">,</span> <span class="n">n_dims</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Merge the first dimensions of a tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    array_or_tensor: Tensor to have its first dimensions merged. Can also</span>
<span class="sd">        be an array or numerical value, which will be converted to a tensor</span>
<span class="sd">        for batch application, if needed.</span>
<span class="sd">    n_dims: Number of dimensions to merge.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Either the input value converted to a Tensor, with the requested dimensions</span>
<span class="sd">    merged, or the unmodified input value if the input has less than `n_dims`</span>
<span class="sd">    dimensions.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If the rank of `array_or_tensor` is not well-defined.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">array_or_tensor</span><span class="p">)</span>
  <span class="n">tensor_shape_static</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>

  <span class="c1"># Check if the rank of the input tensor is well-defined.</span>
  <span class="k">if</span> <span class="n">tensor_shape_static</span><span class="o">.</span><span class="n">dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Can&#39;t merge leading dimensions of tensor of unknown &quot;</span>
                     <span class="s2">&quot;rank.&quot;</span><span class="p">)</span>
  <span class="n">tensor_shape_list</span> <span class="o">=</span> <span class="n">tensor_shape_static</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>

  <span class="c1"># We can only merge the n_dims leading dimensions if the rank of the given</span>
  <span class="c1"># tensor is sufficiently large.</span>
  <span class="k">if</span> <span class="n">n_dims</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor_shape_list</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">array_or_tensor</span>

  <span class="k">if</span> <span class="n">tensor_shape_static</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">():</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">tensor_shape_list</span><span class="p">[:</span><span class="n">n_dims</span><span class="p">])]</span> <span class="o">+</span> <span class="n">tensor_shape_list</span><span class="p">[</span><span class="n">n_dims</span><span class="p">:])</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>

  <span class="c1"># Shape can&#39;t be inferred statically.</span>
  <span class="n">tensor_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
  <span class="n">new_first_dim</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_prod</span><span class="p">(</span><span class="n">tensor_shape</span><span class="p">[:</span><span class="n">n_dims</span><span class="p">],</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">other_dims</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="p">[</span><span class="n">n_dims</span><span class="p">:]</span>
  <span class="n">new_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">new_first_dim</span><span class="p">,</span> <span class="n">other_dims</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">new_size</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">tensor_shape_list</span><span class="p">[:</span><span class="n">n_dims</span><span class="p">]):</span>
    <span class="n">merged_leading_size</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">tensor_shape_list</span><span class="p">[:</span><span class="n">n_dims</span><span class="p">])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">merged_leading_size</span> <span class="o">=</span> <span class="kc">None</span>
  <span class="c1"># We need to set the result size of this, as otherwise we won&#39;t be able to</span>
  <span class="c1"># pass to e.g. a Linear. Here we need to know at least the rank of the tensor.</span>
  <span class="n">result</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="n">merged_leading_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">tensor_shape_list</span><span class="p">[</span><span class="n">n_dims</span><span class="p">:])</span>
  <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="split_leading_dim"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.split_leading_dim">[docs]</a><span class="k">def</span> <span class="nf">split_leading_dim</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">n_dims</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Split the first dimension of a tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    tensor: Tensor to have its first dimension split.</span>
<span class="sd">    inputs: Original reference input to look the dimensions of.</span>
<span class="sd">    n_dims: Number of dimensions to split.</span>

<span class="sd">  Returns:</span>
<span class="sd">    The input tensor, with its first dimension split.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">input_shape_static</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
  <span class="n">input_shape_list</span> <span class="o">=</span> <span class="n">input_shape_static</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
  <span class="n">tensor_shape_static</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span>
  <span class="n">tensor_shape_list</span> <span class="o">=</span> <span class="n">tensor_shape_static</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">input_shape_static</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">()</span>
      <span class="ow">and</span> <span class="n">tensor_shape_static</span><span class="o">.</span><span class="n">is_fully_defined</span><span class="p">()):</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="n">input_shape_list</span><span class="p">[:</span><span class="n">n_dims</span><span class="p">]</span> <span class="o">+</span> <span class="n">tensor_shape_list</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>

  <span class="c1"># Shape can&#39;t be inferred statically.</span>
  <span class="n">dims_after_first</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
  <span class="n">split_sizes</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[:</span><span class="n">n_dims</span><span class="p">]</span>
  <span class="n">known_split_sizes</span> <span class="o">=</span> <span class="n">input_shape_list</span><span class="p">[:</span><span class="n">n_dims</span><span class="p">]</span>
  <span class="n">known_dims_after_first</span> <span class="o">=</span> <span class="n">tensor_shape_list</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
  <span class="n">output_size</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">split_sizes</span><span class="p">,</span> <span class="n">dims_after_first</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
  <span class="n">result</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">known_split_sizes</span> <span class="o">+</span> <span class="n">known_dims_after_first</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">result</span></div>


<div class="viewcode-block" id="create_linear_initializer"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.create_linear_initializer">[docs]</a><span class="k">def</span> <span class="nf">create_linear_initializer</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a default initializer for weights of a linear module.&quot;&quot;&quot;</span>
  <span class="n">stddev</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">stddev</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="create_bias_initializer"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.create_bias_initializer">[docs]</a><span class="k">def</span> <span class="nf">create_bias_initializer</span><span class="p">(</span><span class="n">unused_bias_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a default initializer for the biases of a linear/AddBias module.&quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="Linear"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.Linear">[docs]</a><span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">Transposable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Linear module, optionally including bias.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">output_size</span><span class="p">,</span>
               <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a Linear module.</span>

<span class="sd">    Args:</span>
<span class="sd">      output_size: Output dimensionality. `output_size` can be either an integer</span>
<span class="sd">          or a callable. In the latter case, since the function invocation is</span>
<span class="sd">          deferred to graph construction time, the user must only ensure that</span>
<span class="sd">          output_size can be called, returning an integer, when build is called.</span>
<span class="sd">      use_bias: Whether to include bias parameters. Default `True`.</span>
<span class="sd">      initializers: Optional dict containing initializers to initialize the</span>
<span class="sd">          weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). The default</span>
<span class="sd">          initializer for the weights is a truncated normal initializer, which</span>
<span class="sd">          is commonly used when the inputs are zero centered (see</span>
<span class="sd">          https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for</span>
<span class="sd">          the bias is a zero initializer.</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">          weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">          partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the weights</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contains any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Linear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">output_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_possible_initializer_keys</span><span class="p">(</span><span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span><span class="p">)</span>

<div class="viewcode-block" id="Linear.get_possible_initializer_keys"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.Linear.get_possible_initializer_keys">[docs]</a>  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">get_possible_initializer_keys</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_bias</span> <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">}</span></div>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the Linear module into the graph, with input Tensor `inputs`.</span>

<span class="sd">    If this is not the first time the module has been connected to the graph,</span>
<span class="sd">    the Tensor provided here must have the same final dimension, in order for</span>
<span class="sd">    the existing variables to be the correct size for the multiplication. The</span>
<span class="sd">    batch size may differ for each connection.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A 2D Tensor of size [batch_size, input_size].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 2D Tensor of size [batch_size, output_size].</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.IncompatibleShapeError: If the input is not a 2-D `Tensor` with</span>
<span class="sd">          the size of the second dimension specified.</span>
<span class="sd">      base.IncompatibleShapeError: If reconnecting an already connected module</span>
<span class="sd">          into the graph, and the shape of the input is not compatible with</span>
<span class="sd">          previous inputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">: rank of shape must be 2 not: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">scope_name</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)))</span>

    <span class="k">if</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">: Input size must be specified at module build time&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">scope_name</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">: Input shape must be [batch_size, </span><span class="si">{}</span><span class="s2">] not: [batch_size, </span><span class="si">{}</span><span class="s2">]&quot;</span>
          <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scope_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="n">input_shape</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>

    <span class="k">if</span> <span class="s2">&quot;w&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_linear_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                                          <span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_bias_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                                        <span class="n">dtype</span><span class="p">)</span>

    <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span>
                              <span class="n">shape</span><span class="o">=</span><span class="n">weight_shape</span><span class="p">,</span>
                              <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                              <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span>
                              <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                              <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span>
                                <span class="n">shape</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span>
                                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                                <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span>
                                <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                                <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
      <span class="n">outputs</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">w</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the weight matrix.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Variable object containing the weights, from the most recent __call__.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.NotConnectedError: If the module has not been connected to the</span>
<span class="sd">          graph yet, meaning the variables do not exist.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">b</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Variable object containing the bias, from the most recent __call__.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.NotConnectedError: If the module has not been connected to the</span>
<span class="sd">          graph yet, meaning the variables do not exist.</span>
<span class="sd">      AttributeError: If the module does not use bias.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
          <span class="s2">&quot;No bias Variable in Linear Module when `use_bias=False`.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the module output size.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">has_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns `True` if bias Variable is present in the module.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the initializers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioners</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the partitioners dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the regularizers dictionary.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span>

<div class="viewcode-block" id="Linear.clone"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.Linear.clone">[docs]</a>  <span class="k">def</span> <span class="nf">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a cloned `Linear` module.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Optional string assigning name of cloned module. The default name</span>
<span class="sd">          is constructed by appending &quot;_clone&quot; to `self.module_name`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Cloned `Linear` module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;_clone&quot;</span>
    <span class="k">return</span> <span class="n">Linear</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span>
                  <span class="n">use_bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">,</span>
                  <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">,</span>
                  <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="p">,</span>
                  <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>

  <span class="c1"># Implements Transposable interface.</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns shape of input `Tensor` passed at last call to `build`.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="c1"># Implements Transposable interface</span>
<div class="viewcode-block" id="Linear.transpose"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.Linear.transpose">[docs]</a>  <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns transposed `Linear` module.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Optional string assigning name of transpose module. The default name</span>
<span class="sd">          is constructed by appending &quot;_transpose&quot; to `self.module_name`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Transposed `Linear` module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;_transpose&quot;</span>
    <span class="k">return</span> <span class="n">Linear</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                  <span class="n">use_bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">,</span>
                  <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">,</span>
                  <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="p">,</span>
                  <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="p">,</span>
                  <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="calculate_bias_shape"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.calculate_bias_shape">[docs]</a><span class="k">def</span> <span class="nf">calculate_bias_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">bias_dims</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Calculate `bias_shape` based on the `input_shape` and `bias_dims`.</span>

<span class="sd">  Args:</span>
<span class="sd">    input_shape: Shape of the input being passed into the module. The leading</span>
<span class="sd">        dimension is the minibatch size.</span>
<span class="sd">    bias_dims: The dimensions that bias should be applied over. The remaining</span>
<span class="sd">        dimensions will get broadcasted over.</span>

<span class="sd">  Returns:</span>
<span class="sd">    bias_shape: Tuple corresponding to the shape of bias Variable to create.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If the user attempts to add bias over the minibatch dimension,</span>
<span class="sd">        e.g. `bias_dims=[0]`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">input_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
  <span class="c1"># If None, default is to use all dimensions.</span>
  <span class="k">if</span> <span class="n">bias_dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
  <span class="c1"># If empty list, use a scalar bias.</span>
  <span class="k">elif</span> <span class="ow">not</span> <span class="n">bias_dims</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">()</span>
  <span class="c1"># Otherwise, calculate bias_shape from bias_dims.</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">bias_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_rank</span>
    <span class="c1"># Populate bias dimensions.</span>
    <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">bias_dims</span><span class="p">:</span>
      <span class="n">dim</span> <span class="o">%=</span> <span class="n">input_rank</span>
      <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot apply bias across the minibatch dimension.&quot;</span><span class="p">)</span>
      <span class="n">bias_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span>
    <span class="c1"># Strip leading unit dimensions.</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">input_rank</span>
    <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_rank</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">bias_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="k">break</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">[</span><span class="n">start</span><span class="p">:])</span>  <span class="c1"># Do not apply across minibatch dimension.</span></div>


<div class="viewcode-block" id="AddBias"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.AddBias">[docs]</a><span class="k">class</span> <span class="nc">AddBias</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">Transposable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;AddBias module.&quot;&quot;&quot;</span>

  <span class="n">POSSIBLE_INITIALIZER_KEYS</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;b&quot;</span><span class="p">}</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">output_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">bias_dims</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;add&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs an AddBias module that supports broadcasting.</span>

<span class="sd">    Args:</span>
<span class="sd">      output_shape: Output dimensionality. `output_shape` can be either `None`,</span>
<span class="sd">          a `tuple`, or a `callable`. In the latter case, since the function</span>
<span class="sd">          invocation is deferred to graph construction time, the user must only</span>
<span class="sd">          ensure that `output_shape` can be called, returning a tuple, when</span>
<span class="sd">          build is called. If `output_shape` is left as `None`, the size will be</span>
<span class="sd">          directly inferred by the input.</span>
<span class="sd">      bias_dims: List of which dimensions to retain from the input shape when</span>
<span class="sd">          constructing the bias. The remaining dimensions will get broadcasted</span>
<span class="sd">          over (given size of 1), and leading dimensions will be removed</span>
<span class="sd">          completely. For example, for an input of [batch_size, dim1_size,</span>
<span class="sd">          dim2_size, dim3_size] and `bias_dims=[1, 3]`, the resulting</span>
<span class="sd">          bias will have shape [dim1_size, 1, dim2_size]. The default is to</span>
<span class="sd">          retain all dimensions apart from the minibatch dimension. Trying to</span>
<span class="sd">          retain the bias shape over the minibatch dimension, e.g.</span>
<span class="sd">          `bias_dims=[0]`, will result in an error at build time. See the</span>
<span class="sd">          &#39;Example Usage&#39; section below for more information.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the biases</span>
<span class="sd">          (with key &#39;b&#39;). The default initializer for the bias is a zero</span>
<span class="sd">          initializer.</span>
<span class="sd">      partitioners: Optional dict containing a partitioner to partition</span>
<span class="sd">          the bias (with key &#39;b&#39;). As a default, no partitioner is used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers of the biases</span>
<span class="sd">        (with key &#39;b&#39;). As a default, no regularizers are used. A regularizer</span>
<span class="sd">        should be a function that takes a single `Tensor` as an input and</span>
<span class="sd">        returns a scalar `Tensor` output, e.g. the L1 and L2 regularizers in</span>
<span class="sd">        `tf.contrib.layers`.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Example Usage:</span>

<span class="sd">    ```python</span>
<span class="sd">    # Create a 4D input Tensor.</span>
<span class="sd">    input = tf.random_normal(</span>
<span class="sd">        shape=(batch_size, dim1_size, dim2_size, dim3_size)))</span>

<span class="sd">    # Create a scalar bias:</span>
<span class="sd">    scalar_bias = snt.AddBias(bias_dims=[])</span>
<span class="sd">    scalar_bias_output = scalar_bias(input)</span>
<span class="sd">    scalar_bias.b.get_shape()  # ()</span>

<span class="sd">    # Create a bias over all non-minibatch dimensions:</span>
<span class="sd">    all_bias = snt.AddBias()  # or snt.AddBias(bias_dims=None)</span>
<span class="sd">    all_bias_output = all_bias(input)</span>
<span class="sd">    all_bias.b.get_shape()  # (dim1_size, dim2_size, dim3_size)</span>

<span class="sd">    # Create a bias over the last non-minibatch dimension:</span>
<span class="sd">    last_bias = snt.AddBias(bias_dims=[-1])</span>
<span class="sd">    last_bias_output = last_bias(input)</span>
<span class="sd">    last_bias.b.get_shape()  # (dim3_size)</span>

<span class="sd">    # Create a bias over the first non-minibatch dimension:</span>
<span class="sd">    first_bias = snt.AddBias(bias_dims=[1])</span>
<span class="sd">    first_bias_output = first_bias(input)</span>
<span class="sd">    first_bias.b.get_shape()  # (dim1_size, 1, 1)</span>

<span class="sd">    # Subtract and later add the same learned bias:</span>
<span class="sd">    bias = snt.AddBias()</span>
<span class="sd">    hidden1 = bias(input, multiplier=-1)</span>
<span class="sd">    # ...</span>
<span class="sd">    reconstructed_input = bias(hidden4)</span>

<span class="sd">    ```</span>

<span class="sd">    Raises:</span>
<span class="sd">      KeyError: If `initializers` contains any keys other than &#39;b&#39;.</span>
<span class="sd">      KeyError: If `partitioners` contains any keys other than &#39;b&#39;.</span>
<span class="sd">      KeyError: If `regularizers` contains any keys other than &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers are not callable.</span>
<span class="sd">      TypeError: If any of the given partitioners are not callable.</span>
<span class="sd">      TypeError: If any of the given regularizers are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AddBias</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="n">output_shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_bias_dims</span> <span class="o">=</span> <span class="n">bias_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the Add module into the graph, with input Tensor `inputs`.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A Tensor of size `[batch_size, input_size1, ...]`.</span>
<span class="sd">      multiplier: A scalar or Tensor which the bias term is multiplied by</span>
<span class="sd">        before adding it to `inputs`. Anything which works in the expression</span>
<span class="sd">        `bias * multiplier` is acceptable here. This may be useful if you want</span>
<span class="sd">        to add a bias in one place and subtract the same bias in another place</span>
<span class="sd">        via `multiplier=-1`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of size `[batch_size, input_size1, ...]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.IncompatibleShapeError: If the input is not a &gt;= 2D `Tensor`.</span>
<span class="sd">      base.IncompatibleShapeError: If connecting the module into the graph</span>
<span class="sd">          any time after the first time, and the inferred size of the input does</span>
<span class="sd">          not match previous invocations.</span>
<span class="sd">      base.IncompatibleShapeError: If the `output_shape` has been specified</span>
<span class="sd">          but it does not match the input_shape`.</span>
<span class="sd">      base.ParentNotBuiltError: If the module is a transposed and the original</span>
<span class="sd">          untransposed module has not been built.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>
    <span class="n">bias_shape</span> <span class="o">=</span> <span class="n">calculate_bias_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias_dims</span><span class="p">)</span>

    <span class="c1"># Check always contains minibatched input.</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Rank of input shape must be &gt;=2 not: </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)))</span>

    <span class="c1"># Check previous input size is same as new input size.</span>
    <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
        <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span><span class="s2">&quot;Input shape has changed.&quot;</span><span class="p">)</span>

    <span class="c1"># If transposed, make sure that the original Module is built.</span>
    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span><span class="p">()</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">ParentNotBuiltError</span><span class="p">(</span>
            <span class="s2">&quot;Build the original untransposed module before building this one.&quot;</span><span class="p">)</span>

    <span class="c1"># If output_shape specified, check that it matches input_shape.</span>
    <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">!=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Input shape must be </span><span class="si">{}</span><span class="s2"> not: </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_shape</span><span class="p">,</span>
                                                   <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="n">input_shape</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>

    <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_bias_initializer</span><span class="p">(</span><span class="n">bias_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
        <span class="s2">&quot;b&quot;</span><span class="p">,</span>
        <span class="n">shape</span><span class="o">=</span><span class="n">bias_shape</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span>
        <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

    <span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>
    <span class="k">if</span> <span class="n">multiplier</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">bias</span> <span class="o">*=</span> <span class="n">multiplier</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">+</span> <span class="n">bias</span>
    <span class="k">return</span> <span class="n">outputs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">b</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the bias.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Variable object containing the bias, from the most recent __call__.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.NotConnectedError: If the module has not been connected to the</span>
<span class="sd">          graph yet, meaning the variables do not exist.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_b</span>

  <span class="c1"># Implements Transposable interface.</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns shape of input `Tensor` passed at last call to `build`.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="c1"># Implements Transposable interface</span>
<div class="viewcode-block" id="AddBias.transpose"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.AddBias.transpose">[docs]</a>  <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns transposed `AddBias` module.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Optional string assigning name of transpose module. The default name</span>
<span class="sd">          is constructed by appending &quot;_transpose&quot; to `self.module_name`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Transposed `AddBias` module.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;_transpose&quot;</span>
    <span class="k">return</span> <span class="n">AddBias</span><span class="p">(</span><span class="n">output_shape</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">,</span>
                   <span class="n">bias_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_bias_dims</span><span class="p">,</span>
                   <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">,</span>
                   <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="p">,</span>
                   <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BatchReshape"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.BatchReshape">[docs]</a><span class="k">class</span> <span class="nc">BatchReshape</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">Transposable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Reshapes input Tensor, preserving the batch dimension.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">preserve_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;batch_reshape&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a BatchReshape module.</span>

<span class="sd">    Args:</span>
<span class="sd">      shape: Shape to reshape the input Tensor to while preserving its</span>
<span class="sd">          first `preserve_dims` dimensions; `shape` can be either a tuple/list,</span>
<span class="sd">          or a callable that returns the actual shape. The callable does not</span>
<span class="sd">          need to be ready to return something meaningful at construction time,</span>
<span class="sd">          but it will be required to be able to do so when the module is</span>
<span class="sd">          connected to the graph. When the special value -1 appears in `shape`</span>
<span class="sd">          the corresponding size is automatically inferred. Note that -1 can</span>
<span class="sd">          only appear once in `shape`. To flatten all non-batch dimensions,</span>
<span class="sd">          the snt.BatchFlatten module can also be used.</span>
<span class="sd">      preserve_dims: Number of leading dimensions that will not be reshaped.</span>
<span class="sd">          For example, given an input Tensor with shape `[B, H, W, C, D]`,</span>
<span class="sd">          and argument `shape` equal to `(-1, D)`:</span>
<span class="sd">            * `preserve_dims=1` will return a Tensor with shape `[B, H*W*C, D]`.</span>
<span class="sd">            * `preserve_dims=2` will return a Tensor with</span>
<span class="sd">                shape `[B, H, W*C, D]`.</span>
<span class="sd">            * `preserve_dims=3` will return a Tensor with</span>
<span class="sd">                shape `[B, H, W, C, D]`.</span>
<span class="sd">            * `preserve_dims=4` will return a Tensor with</span>
<span class="sd">                shape `[B, H, W, C, 1, D]`.</span>
<span class="sd">            * `preserve_dims&gt;=5` will throw an error on build unless D=1.</span>
<span class="sd">          The preserved dimensions can be unknown at building time.</span>
<span class="sd">      name: Name of the module.</span>
<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `preserve_dims &lt;= 0`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BatchReshape</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="n">shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_preserve_dims</span> <span class="o">=</span> <span class="n">preserve_dims</span>
    <span class="k">if</span> <span class="n">preserve_dims</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Argument preserve_dims should be &gt;= 1.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Replaces the -1 wildcard in the output shape vector.</span>

<span class="sd">    This function infers the correct output shape given the input dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">      dimensions: List of input non-batch dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Tuple of non-batch output dimensions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Size of input</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dimensions</span><span class="p">)</span>
    <span class="c1"># Size of output where defined</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)))</span>
    <span class="c1"># Replace wildcard</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span>
    <span class="n">v</span><span class="p">[</span><span class="n">v</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">m</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the module into the graph, with input Tensor `inputs`.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A Tensor of shape [b_1, b_2, ..., b_preserve_dims,</span>
<span class="sd">                                 b_preserve_dims+1, ...].</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape [b_1, b_2, ..., b_preserve_dims,</span>
<span class="sd">                         b_reshape_1, b_reshape_2, ...],</span>
<span class="sd">        with reshaping defined by the constructor `shape` parameter.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If output shape is incompatible with input shape; or if</span>
<span class="sd">          shape array contains non numeric entries; or if shape array contains</span>
<span class="sd">          more than 1 wildcard -1; or if the input array contains unknown,</span>
<span class="sd">          non-preserved dimensions (except when the unknown dimension is the</span>
<span class="sd">          only non-preserved dimension and doesn&#39;t actually need reshaping).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">full_input_shape</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_input_shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_preserve_dims</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input tensor has </span><span class="si">{}</span><span class="s2"> dimensions, should have at least &quot;</span>
                       <span class="s2">&quot;as many as preserve_dims=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="nb">len</span><span class="p">(</span><span class="n">full_input_shape</span><span class="p">),</span>
                           <span class="bp">self</span><span class="o">.</span><span class="n">_preserve_dims</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="n">full_input_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_preserve_dims</span><span class="p">:]</span>

    <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">())</span>

    <span class="c1"># Special-case of 1 non-preserved dimension, where no reshape is necessary.</span>
    <span class="c1"># This is useful if the non-preserved dimension of `inputs` is unknown</span>
    <span class="c1"># at build time.</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">inputs</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unknown non-preserved dimensions are not allowed &quot;</span>
                           <span class="s2">&quot;in the input to BatchReshape unless it is only one &quot;</span>
                           <span class="s2">&quot;and the desired shape is (-1,).&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Output shape is incompatible with input shape&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">([</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">x</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">]):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Desired shape can only contain positive integral numbers &quot;</span>
          <span class="s2">&quot;and the wildcard -1. Given shape </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">))</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;Wildcard -1 can appear only once in desired output shape. &quot;</span>
          <span class="s2">&quot;Given shape </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">))</span>

    <span class="n">preserved_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">_preserve_dims</span><span class="p">]</span>
    <span class="c1"># Slicing the shape tensor loses information, we keep it in a list.</span>
    <span class="n">preserved_shape_list</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[:</span><span class="bp">self</span><span class="o">.</span><span class="n">_preserve_dims</span><span class="p">]</span>

    <span class="c1"># Except in the case above where no reshape is needed, we do not allow</span>
    <span class="c1"># unknown non-preserved dimensions in the input.</span>
    <span class="k">if</span> <span class="kc">None</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unknown non-preserved dimensions are not allowed in &quot;</span>
                       <span class="s2">&quot;the input to BatchReshape unless it is only one and the&quot;</span>
                       <span class="s2">&quot; desired shape is (-1,). The offending non-preserved &quot;</span>
                       <span class="s2">&quot;input shape is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">trailing_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">trailing_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">trailing_shape</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Output shape is incompatible with input shape&quot;</span><span class="p">)</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">preserved_shape</span><span class="p">,</span> <span class="n">trailing_shape</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>

    <span class="c1"># Include shape information that was lost when we sliced the shape tensor.</span>
    <span class="n">shape_list</span> <span class="o">=</span> <span class="n">preserved_shape_list</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">trailing_shape</span><span class="p">)</span>
    <span class="n">output</span><span class="o">.</span><span class="n">set_shape</span><span class="p">(</span><span class="n">shape_list</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="c1"># Implements Transposable interface.</span>
<div class="viewcode-block" id="BatchReshape.transpose"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.BatchReshape.transpose">[docs]</a>  <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns transpose batch reshape.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;_transpose&quot;</span>
    <span class="k">return</span> <span class="n">BatchReshape</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="p">,</span>
                        <span class="n">preserve_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_preserve_dims</span><span class="p">,</span>
                        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BatchFlatten"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.BatchFlatten">[docs]</a><span class="k">class</span> <span class="nc">BatchFlatten</span><span class="p">(</span><span class="n">BatchReshape</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Flattens the input Tensor, preserving the batch dimension(s).&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preserve_dims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;batch_flatten&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a BatchFlatten module.</span>

<span class="sd">    Args:</span>
<span class="sd">      preserve_dims: Number of leading dimensions that will not be reshaped.</span>
<span class="sd">          For example, given an input Tensor with shape `[B, H, W, C]`:</span>
<span class="sd">            * `preserve_dims=1` will return a Tensor with shape `[B, H*W*C]`.</span>
<span class="sd">            * `preserve_dims=2` will return a Tensor with</span>
<span class="sd">                shape `[B, H, W*C]`.</span>
<span class="sd">            * `preserve_dims=3` will return the input itself,</span>
<span class="sd">                shape `[B, H, W, C]`.</span>
<span class="sd">            * `preserve_dims=4` will  return a Tensor with</span>
<span class="sd">                shape `[B, H, W, C, 1]`.</span>
<span class="sd">            * `preserve_dims&gt;=5` will throw an error on build.</span>
<span class="sd">          The preserved dimensions can be unknown at building time.</span>
<span class="sd">      name: Name of the module.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BatchFlatten</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span> <span class="n">preserve_dims</span><span class="o">=</span><span class="n">preserve_dims</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="FlattenTrailingDimensions"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.FlattenTrailingDimensions">[docs]</a><span class="k">class</span> <span class="nc">FlattenTrailingDimensions</span><span class="p">(</span><span class="n">BatchReshape</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Flattens trailing dimensions of a Tensor.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim_from</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;batch_dim_from&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a FlattenTrailingDimensions module.</span>

<span class="sd">    For example, given an input Tensor with shape `[B, H, W, C]`:</span>

<span class="sd">      * `dim_from=1` will return a Tensor with shape `[B, H*W*C]`.</span>
<span class="sd">      * `dim_from=2` will return a Tensor with shape `[B, H, W*C]`.</span>
<span class="sd">      * `dim_from=3` will return the input itself.</span>
<span class="sd">      * `dim_from=4` will return a Tensor with shape `[B, H, W, C, 1]`.</span>
<span class="sd">      * `dim_from&gt;=5` will generate a ValueError when building the module.</span>
<span class="sd">      The preserved dimensions can be unknown at building time.</span>

<span class="sd">    Equivalent to BatchFlatten(preserve_dims=dim_from, name=name).</span>

<span class="sd">    Args:</span>
<span class="sd">      dim_from: All dimensions after and including `dim_from` will</span>
<span class="sd">          be flattened into a single dimension.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `dim_from &lt;= 0`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim_from</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Argument dim_from should be &gt;= 1.&quot;</span><span class="p">)</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FlattenTrailingDimensions</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,),</span> <span class="n">preserve_dims</span><span class="o">=</span><span class="n">dim_from</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="TrainableVariable"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.TrainableVariable">[docs]</a><span class="k">class</span> <span class="nc">TrainableVariable</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Provides learnable parameter Tensor.&quot;&quot;&quot;</span>

  <span class="n">POSSIBLE_INITIALIZER_KEYS</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">}</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">shape</span><span class="p">,</span>
               <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;trainable_variable&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a TrainableVariable module.</span>

<span class="sd">    Args:</span>
<span class="sd">      shape: Tensor shape.</span>
<span class="sd">      dtype: Tensor data type.</span>
<span class="sd">      initializers: Optional dictionary containing ops to initialize the weight</span>
<span class="sd">          Tensor, with key &#39;w&#39;.</span>
<span class="sd">      partitioners: Optional dict containing a partitioner to partition</span>
<span class="sd">          the weight (with key &#39;w&#39;). As a default, no partitioner is used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the weights</span>
<span class="sd">        (with key &#39;w&#39;). As a default, no regularizers are used. A regularizer</span>
<span class="sd">        should be a function that takes a single `Tensor` as an input and</span>
<span class="sd">        returns a scalar `Tensor` output, e.g. the L1 and L2 regularizers in</span>
<span class="sd">        `tf.contrib.layers`.</span>
<span class="sd">      custom_getter: Optional callable or dictionary of callables to use as</span>
<span class="sd">        custom_getter for the module.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      KeyError: If `initializers` contains any keys other than &#39;w&#39;.</span>
<span class="sd">      KeyError: If `partitioners` contains any keys other than &#39;w&#39;.</span>
<span class="sd">      KeyError: If `regularizers` contains any keys other than &#39;w&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers are not callable.</span>
<span class="sd">      TypeError: If any of the given partitioners are not callable.</span>
<span class="sd">      TypeError: If any of the given regularizers are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TrainableVariable</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
                                            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">dtype</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the TrainableTensor module into the graph.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of shape as determined in the constructor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s2">&quot;w&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">:</span>
      <span class="n">stddev</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">stddev</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span>
                              <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_shape</span><span class="p">,</span>
                              <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
                              <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span>
                              <span class="n">partitioner</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                              <span class="n">regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">w</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the Variable containing the weights Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Variable object containing the weights, from the most recent __call__.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.Error: If the module has not been connected to the graph yet,</span>
<span class="sd">          meaning the variables do not exist.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_w</span></div>


<div class="viewcode-block" id="BatchApply"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.BatchApply">[docs]</a><span class="k">class</span> <span class="nc">BatchApply</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Merges a number of leading dimensions of an input tensor to manipulate it.</span>

<span class="sd">  Merges a number of leading dimensions of a tensor into a single dimension,</span>
<span class="sd">  connects the provided module, then splits the leading dimension of the</span>
<span class="sd">  result to match the input.</span>

<span class="sd">  Input tensors whose rank is smaller than the number of dimensions to collapse</span>
<span class="sd">  (e.g. all scalar values, which are tensors of rank 0), are passed unaltered to</span>
<span class="sd">  the provided module.</span>

<span class="sd">  This is useful for applying some module to each timestep of a Time x Batch x N</span>
<span class="sd">  tensor. If a module is hard coded to only support 2D (Batch x N) then the</span>
<span class="sd">  full 3D Tensor cannot be provided. BatchApply will &#39;merge&#39; the first two</span>
<span class="sd">  dimensions of the sequence tensor by reshaping to a (Time * Batch) x N Tensor,</span>
<span class="sd">  and then the internal module can be applied. The result of that operation is</span>
<span class="sd">  reshaped such that its first dimensions are split to match the leading</span>
<span class="sd">  dimensions of the input.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_or_op</span><span class="p">,</span> <span class="n">n_dims</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">input_example_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;batch_apply&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructor of the module.</span>

<span class="sd">    Args:</span>
<span class="sd">      module_or_op: Module or tensorflow op to apply to an input tensor.</span>
<span class="sd">      n_dims: Number of dimensions to merge before using module on the input</span>
<span class="sd">          of BatchApply.</span>
<span class="sd">      input_example_index: Index of input that has same shape for the first</span>
<span class="sd">          `n_dims` dimensions as `module_or_op` output(s). This is used for</span>
<span class="sd">          unflattening the output(s) if static shape inference is not possible.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If n_dims is not an integer.</span>
<span class="sd">      ValueError: If n_dims is not greater than zero.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BatchApply</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">n_dims</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;n_dims should be an integer, it is a </span><span class="si">%s</span><span class="s2"> instead.&quot;</span> <span class="o">%</span>
                      <span class="nb">type</span><span class="p">(</span><span class="n">n_dims</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">n_dims</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_dims should be greater than zero.&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_module</span> <span class="o">=</span> <span class="n">module_or_op</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_n_dims</span> <span class="o">=</span> <span class="n">n_dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_example_index</span> <span class="o">=</span> <span class="n">input_example_index</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the BatchApply module into the graph.</span>

<span class="sd">    Args:</span>
<span class="sd">      *args: a Tensor or a nested list or dictionary of Tensors. The input</span>
<span class="sd">          tensors will have their first dimensions merged, then an op or a</span>
<span class="sd">          module will be called on the input. The first dimension of the output</span>
<span class="sd">          tensor(s) will be split again based on the leading dimensions of the</span>
<span class="sd">          first input tensor.</span>
<span class="sd">      **kwargs: Dictionary of named arguments; used in the same way as `*args`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor or nested list or dictionary of Tensors as a result of applying</span>
<span class="sd">      the process above. (&quot;None&quot; return values are also supported.)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">flattened</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten_iterable</span><span class="p">([</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">])</span>
    <span class="n">merged_flattened</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">merge_leading_dims</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_dims</span><span class="p">)</span> <span class="k">if</span> <span class="n">inp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">flattened</span><span class="p">]</span>
    <span class="n">merged_args</span><span class="p">,</span> <span class="n">merged_kwargs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_iterable_as</span><span class="p">([</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">],</span>
                                                       <span class="n">merged_flattened</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_module</span><span class="p">(</span><span class="o">*</span><span class="n">merged_args</span><span class="p">,</span> <span class="o">**</span><span class="n">merged_kwargs</span><span class="p">)</span>

    <span class="c1"># Unmerging takes the sizes of the leading dimensions from an input example</span>
    <span class="c1"># with equal shape for the leading `n_dims` dimensions. Typically this is</span>
    <span class="c1"># the first input.</span>
    <span class="n">example_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">flattened</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_example_index</span><span class="p">])</span>
    <span class="k">def</span> <span class="nf">_split_to_original_leading_dims</span><span class="p">(</span><span class="n">result</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">split_leading_dim</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">example_input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_dims</span><span class="p">)</span>

    <span class="n">flat_results</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten_iterable</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
    <span class="n">flat_unmerged_results</span> <span class="o">=</span> <span class="p">[</span><span class="n">_split_to_original_leading_dims</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
                             <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">flat_results</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_iterable_as</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">flat_unmerged_results</span><span class="p">)</span></div>


<div class="viewcode-block" id="SliceByDim"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.SliceByDim">[docs]</a><span class="k">class</span> <span class="nc">SliceByDim</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Slices a tensor along specific dimensions.</span>

<span class="sd">  The user can slice a tensor by specifying only the list of dimensions that</span>
<span class="sd">  they want to slice, together with the lists of integers containing the</span>
<span class="sd">  beginning indices of the slicing, and the size of the slices. Hence, with</span>
<span class="sd">  `SliceByDim` slicing can be performed without knowing in advance the rank of</span>
<span class="sd">  the input tensor.</span>

<span class="sd">  Tensorflow also offers a built-in op performing slicing, `tf.slice`. However,</span>
<span class="sd">  `tf.slice` requires all the slicing dimensions to be specified, using</span>
<span class="sd">  wildcards when no slicing is required. For example, with `tf.slice`, slicing</span>
<span class="sd">  half a 5D tensor along dimension `1` would be:</span>

<span class="sd">  ```python</span>
<span class="sd">  output = tf.slice(inputs,</span>
<span class="sd">                    begin=[0, 0, 0, 0, 0],</span>
<span class="sd">                    size=[-1, inputs.get_shape()[1].value//2, -1, -1, -1])</span>
<span class="sd">  ```</span>

<span class="sd">  The same operation using `SliceByDim` would be:</span>

<span class="sd">  ```python</span>
<span class="sd">  output = SliceByDim(dims=[1], begin=[0], size=[x.get_shape()[1].value//2])(x)</span>
<span class="sd">  ```</span>

<span class="sd">  `SliceByDim` can be used to specify multiple slicing dimensions, for example:</span>

<span class="sd">  ```python</span>
<span class="sd">  output = SliceByDim(dims=[1, 3], begin=[0, 0], size=[12, 24])(x)</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;slice_by_dim&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs the `SliceByDim` module.</span>

<span class="sd">    Args:</span>
<span class="sd">      dims: The dimensions to slice along, as a list of unique integers.</span>
<span class="sd">          Negative integers index from the final dimension backwards, as in</span>
<span class="sd">          python arrays.</span>
<span class="sd">      begin: The beginning indices of the slicing, as a list of integers. Must</span>
<span class="sd">          be the same length as the `dims` list.</span>
<span class="sd">      size: The size of the slices, as a list of integers. Must be the same</span>
<span class="sd">          length as the `dims` list.</span>
<span class="sd">      name: The name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `dims` has non-unique integers, or if the size of `begin`</span>
<span class="sd">          is different from the size of `dims`, or if the size of `size` is</span>
<span class="sd">          different from the size of `dims`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SliceByDim</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dims</span> <span class="o">=</span> <span class="n">dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_begin</span> <span class="o">=</span> <span class="n">begin</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_size</span> <span class="o">=</span> <span class="n">size</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dims must not have any repeated integers.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">begin</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;begin must have the same length as dims: </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)))</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;size must have the same length as dims: </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)))</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the SliceByDim module into the graph.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: `Tensor` to slice. Its rank must be greater than the maximum</span>
<span class="sd">          dimension specified in `dims` (plus one as python is 0 indexed).</span>

<span class="sd">    Returns:</span>
<span class="sd">      The sliced tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `inputs` tensor has insufficient rank.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape_inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_inputs</span><span class="p">)</span>

    <span class="c1"># Checks that the rank of the tensor.</span>
    <span class="n">max_dim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dims</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="n">max_dim</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Rank of inputs must be at least </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_dim</span><span class="p">))</span>

    <span class="c1"># Builds default lists for begin and size to pass to `tf.slice`.</span>
    <span class="n">full_begin</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">rank</span>
    <span class="n">full_size</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">rank</span>

    <span class="c1"># Updates lists with what the user provided.</span>
    <span class="k">for</span> <span class="n">dim</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dims</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_begin</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">):</span>
      <span class="n">full_begin</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">begin</span>
      <span class="n">full_size</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">size</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">begin</span><span class="o">=</span><span class="n">full_begin</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">full_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="TileByDim"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.TileByDim">[docs]</a><span class="k">class</span> <span class="nc">TileByDim</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Tile a tensor along specific dimensions.</span>

<span class="sd">  The user can tile a tensor by specifying only the list of dimensions that</span>
<span class="sd">  they want to tile, together with the lists of integers containing the</span>
<span class="sd">  multiples of the tiling. Hence, with `TileByDim` tiling can be performed</span>
<span class="sd">  without knowing in advance the rank of the input tensor.</span>

<span class="sd">  Tensorflow also offers a built-in op performing tiling, `tf.tile`. However,</span>
<span class="sd">  `tf.tile` requires all the tiling dimensions to be specified, using `1`</span>
<span class="sd">  when no tiling is required. For example, with tf.tiling, tiling a 5D</span>
<span class="sd">  tensor along dimension `1`, by `2` would be:</span>

<span class="sd">  ```python</span>
<span class="sd">  output = tf.tile(inputs, multiples=[1, 2, 1, 1, 1])</span>
<span class="sd">  ```</span>

<span class="sd">  The same operation using `TileByDim` would be:</span>

<span class="sd">  ```python</span>
<span class="sd">  output = TileByDim(dims=[1], multiples=[2])(x)</span>
<span class="sd">  ```</span>

<span class="sd">  `TileByDim` can be used to specify multiple tiling dimensions, for example:</span>

<span class="sd">  ```python</span>
<span class="sd">  output = TileByDim(dims=[1, 3], multiples=[2, 4])(x)</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">multiples</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;tile_by_dim&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs the `TileByDim` module.</span>

<span class="sd">    Args:</span>
<span class="sd">      dims: The dimensions to tile along, as a list of unique integers.</span>
<span class="sd">      multiples: The multiple of the tiling, as a list of integers. Must</span>
<span class="sd">          be the same length as the `dims` list.</span>
<span class="sd">      name: The name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `dims` has non-unique integers, or if the size of</span>
<span class="sd">          `multiples` is different from the size of `dims`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TileByDim</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dims</span> <span class="o">=</span> <span class="n">dims</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_multiples</span> <span class="o">=</span> <span class="n">multiples</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span><span class="o">.</span><span class="n">size</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dims must not have any repeated integers.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">multiples</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;multiples must have the same length as dims: </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)))</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the `TileByDim` module into the graph.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: `Tensor` to tile.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The tiled tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape_inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_inputs</span><span class="p">)</span>

    <span class="c1"># Builds default lists for multiples to pass to `tf.tile`.</span>
    <span class="n">full_multiples</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">rank</span>

    <span class="c1"># Updates lists with what the user provided.</span>
    <span class="k">for</span> <span class="n">dim</span><span class="p">,</span> <span class="n">multiple</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_dims</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multiples</span><span class="p">):</span>
      <span class="n">full_multiples</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">multiple</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">multiples</span><span class="o">=</span><span class="n">full_multiples</span><span class="p">)</span></div>


<div class="viewcode-block" id="MergeDims"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.MergeDims">[docs]</a><span class="k">class</span> <span class="nc">MergeDims</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Merges a tensor or nested list of tensors along a range of dimensions.</span>

<span class="sd">  Tensors are reshaped by specifying the range of dimensions to merge.</span>
<span class="sd">  Hence, the reshape can be performed without knowing in advance the rank of</span>
<span class="sd">  the input tensor.</span>

<span class="sd">  For example, merging dimensions 1, 2 and 3 together can be performed by</span>
<span class="sd">  calling:</span>

<span class="sd">  ```python</span>
<span class="sd">  output = MergeDims(start=1, size=3)(x)</span>
<span class="sd">  ```</span>

<span class="sd">  A nested list of tensors can be merged:</span>

<span class="sd">  ```python</span>
<span class="sd">  x = [tf.random_uniform(shape=[5, 5]), [tf.random_uniform(shape=[3, 3, 3])]]</span>
<span class="sd">  output = MergeDims(start=0, size=2)(x)</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;merge_dims&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs the MergeDims module.</span>

<span class="sd">    Args:</span>
<span class="sd">      start: Start of the range of dimensions to merge.</span>
<span class="sd">      size: Size the range of dimensions to merge.</span>
<span class="sd">      name: The name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `size` is not strictly greater than 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MergeDims</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_start</span> <span class="o">=</span> <span class="n">start</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_size</span> <span class="o">=</span> <span class="n">size</span>

    <span class="c1"># Checks for non consecutive integers.</span>
    <span class="k">if</span> <span class="n">size</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`size` should be strictly greater than 1.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_merge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
    <span class="n">output_shape</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Rank of inputs must be at least </span><span class="si">{}</span><span class="s2">.&quot;</span>
                       <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">))</span>

    <span class="c1"># Update the shape of the merged dimensions.</span>
    <span class="n">output_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_start</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">_start</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">output_shape</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the MergeDims module into the graph.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: Tensor or a nested list of Tensors to merge. Its rank must be</span>
<span class="sd">          greater than or equal to `start` + `size`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The merged Tensor or a nested list of merged Tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If any of the `inputs` tensors has insufficient rank.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">nest</span><span class="o">.</span><span class="n">is_sequence</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
      <span class="n">merged_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_merge</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)]</span>
      <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">merged_tensors</span><span class="p">)</span>

    <span class="c1"># inputs is a single tf.Tensor</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span></div>


<div class="viewcode-block" id="SelectInput"><a class="viewcode-back" href="../../../../api/sonnet.python.modules.html#sonnet.python.modules.basic.SelectInput">[docs]</a><span class="k">class</span> <span class="nc">SelectInput</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Returns a subset of its inputs in an arbitrarily nested configuration.</span>

<span class="sd">  This module can be used for multiple purposes.</span>

<span class="sd">  The basic usage is to select a tensor or a subset of tensors:</span>

<span class="sd">  ```</span>
<span class="sd">  output = snt.SelectInput(idx=0, name=&#39;select&#39;)(input0, input1)</span>
<span class="sd">  ==&gt; input0</span>

<span class="sd">  output = snt.SelectInput(idx=[0, 2], name=&#39;select&#39;)(input0, input1, input2)</span>
<span class="sd">  ==&gt; (input0, input2)</span>
<span class="sd">  ```</span>

<span class="sd">  Another usage is to change the orders of the input tensors:</span>

<span class="sd">  ```</span>
<span class="sd">  output = snt.SelectInput(idx=[1, 0], name=&#39;select&#39;)(input0, input1)</span>
<span class="sd">  ==&gt; (input1, input0)</span>
<span class="sd">  ```</span>

<span class="sd">  Another usage is to duplicate an input:</span>

<span class="sd">  ```</span>
<span class="sd">  output = snt.SelectInput(idx=[0, 0], name=&#39;select&#39;)(input0)</span>
<span class="sd">  ==&gt; (input0, input0)</span>
<span class="sd">  ```</span>

<span class="sd">  Another usage is to add arbitrary nesting:</span>

<span class="sd">  ```</span>
<span class="sd">  output = snt.SelectInput(</span>
<span class="sd">      idx=[0, [1, [2]]], name=&#39;select&#39;)(input0, input1, input2)</span>
<span class="sd">  ==&gt; (input0, (input1, (input2,)))</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;select_input&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Module constructor.</span>

<span class="sd">    Args:</span>
<span class="sd">      idx: Indexes of the tensors to select. If `idx` is an integer, then</span>
<span class="sd">          a `Tensor` is returned. If `idx` is a (nested) list/tuple, then a</span>
<span class="sd">          (nested) tuple of `Tensor` is returned.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `idx` is not an list, tuple or integer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SelectInput</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_type</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_idx</span> <span class="o">=</span> <span class="n">idx</span>

  <span class="k">def</span> <span class="nf">_check_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;`idx` should be a (nested) array/tuple, or an integer.&quot;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_select</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
      <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_select</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`idx` contains out of bound entries (they should be &quot;</span>
                         <span class="s2">&quot;in the range [0, </span><span class="si">{}</span><span class="s2">))&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
      <span class="c1"># Identity is called otherwise we might get &#39;placeholder is both fed and</span>
      <span class="c1"># fetched&#39; errors in some cases when using a feed_dict.</span>
      <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the module into the graph.</span>

<span class="sd">    Args:</span>
<span class="sd">      *inputs: `Tensor` variables to select.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Subset of `inputs` in an arbitrarily nested configuration.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If any entry of `idx` is out of bounds with respect to the</span>
<span class="sd">          size of `inputs`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_select</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_idx</span><span class="p">)</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  <li><a href="../../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Sonnet Authors.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    

    
  </body>
</html>