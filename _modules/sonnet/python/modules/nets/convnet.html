
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sonnet.python.modules.nets.convnet &#8212; sonnet git documentation</title>
    <link rel="stylesheet" href="../../../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for sonnet.python.modules.nets.convnet</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2017 The Sonnet Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;A minimal interface convolutional networks module.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">six</span>

<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="n">xrange</span>  <span class="c1"># pylint: disable=redefined-builtin</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">base</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">batch_norm</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">batch_norm_v2</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">conv</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">util</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">DATA_FORMAT_NCHW</span> <span class="o">=</span> <span class="s2">&quot;NCHW&quot;</span>
<span class="n">DATA_FORMAT_NHWC</span> <span class="o">=</span> <span class="s2">&quot;NHWC&quot;</span>
<span class="n">SUPPORTED_2D_DATA_FORMATS</span> <span class="o">=</span> <span class="p">{</span><span class="n">DATA_FORMAT_NCHW</span><span class="p">,</span> <span class="n">DATA_FORMAT_NHWC</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">_replicate_elements</span><span class="p">(</span><span class="n">input_iterable</span><span class="p">,</span> <span class="n">num_times</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Replicates entry in `input_iterable` if `input_iterable` is of length 1.&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_iterable</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">input_iterable</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">*</span> <span class="n">num_times</span>
  <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">input_iterable</span><span class="p">)</span>


<div class="viewcode-block" id="ConvNet2D"><a class="viewcode-back" href="../../../../../api/sonnet.python.modules.nets.html#sonnet.python.modules.nets.convnet.ConvNet2D">[docs]</a><span class="k">class</span> <span class="nc">ConvNet2D</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">,</span> <span class="n">base</span><span class="o">.</span><span class="n">Transposable</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A 2D Convolutional Network module.&quot;&quot;&quot;</span>

  <span class="n">POSSIBLE_INITIALIZER_KEYS</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">}</span>
  <span class="c1"># Keep old name for backwards compatibility</span>

  <span class="n">POSSIBLE_KEYS</span> <span class="o">=</span> <span class="n">POSSIBLE_INITIALIZER_KEYS</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">output_channels</span><span class="p">,</span>
               <span class="n">kernel_shapes</span><span class="p">,</span>
               <span class="n">strides</span><span class="p">,</span>
               <span class="n">paddings</span><span class="p">,</span>
               <span class="n">rates</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span>
               <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
               <span class="n">activate_final</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">normalization_ctor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">normalization_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">normalize_final</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Deprecated.</span>
               <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">batch_norm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Deprecated.</span>
               <span class="n">data_format</span><span class="o">=</span><span class="n">DATA_FORMAT_NHWC</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_net_2d&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a `ConvNet2D` module.</span>

<span class="sd">    By default, neither batch normalization nor activation are applied to the</span>
<span class="sd">    output of the final layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      output_channels: Iterable of output channels, as defined in</span>
<span class="sd">        `conv.Conv2D`. Output channels can be defined either as number or via a</span>
<span class="sd">        callable. In the latter case, since the function invocation is deferred</span>
<span class="sd">        to graph construction time, the user must only ensure that entries can</span>
<span class="sd">        be called when build is called. Each entry in the iterable defines</span>
<span class="sd">        properties in the corresponding convolutional layer.</span>
<span class="sd">      kernel_shapes: Iterable of kernel sizes as defined in `conv.Conv2D`; if</span>
<span class="sd">        the list contains one element only, the same kernel shape is used in</span>
<span class="sd">        each layer of the network.</span>
<span class="sd">      strides: Iterable of kernel strides as defined in `conv.Conv2D`; if the</span>
<span class="sd">        list contains one element only, the same stride is used in each layer of</span>
<span class="sd">        the network.</span>
<span class="sd">      paddings: Iterable of padding options as defined in `conv.Conv2D`. Each</span>
<span class="sd">        can be `snt.SAME`, `snt.VALID`, `snt.FULL`, `snt.CAUSAL`,</span>
<span class="sd">        `snt.REVERSE_CAUSAL` or a pair of these to use for height and width.</span>
<span class="sd">        If the Iterable contains one element only, the same padding is used in</span>
<span class="sd">        each layer of the network.</span>
<span class="sd">      rates: Iterable of dilation rates as defined in `conv.Conv2D`; if the</span>
<span class="sd">        list contains one element only, the same rate is used in each layer of</span>
<span class="sd">        the network.</span>
<span class="sd">      activation: An activation op.</span>
<span class="sd">      activate_final: Boolean determining if the activation and batch</span>
<span class="sd">        normalization, if turned on, are applied to the final layer.</span>
<span class="sd">      normalization_ctor: Constructor to return a callable which will perform</span>
<span class="sd">        normalization at each layer. Defaults to None / no normalization.</span>
<span class="sd">        Examples of what could go here: `snt.BatchNormV2`, `snt.LayerNorm`. If</span>
<span class="sd">        a string is provided, importlib is used to convert the string to a</span>
<span class="sd">        callable, so either `snt.LayerNorm` or `&quot;snt.LayerNorm&quot;` can be</span>
<span class="sd">        provided.</span>
<span class="sd">      normalization_kwargs: kwargs to be provided to `normalization_ctor` when</span>
<span class="sd">        it is called.</span>
<span class="sd">      normalize_final: Whether to apply normalization after the final conv</span>
<span class="sd">        layer. Default is to take the value of activate_final.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters of</span>
<span class="sd">        the whole network (with key &#39;w&#39;) or biases (with key &#39;b&#39;).</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">          weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">          partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters of the</span>
<span class="sd">        whole network (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes a</span>
<span class="sd">        single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      use_batch_norm: Boolean determining if batch normalization is applied</span>
<span class="sd">        after convolution. Deprecated, use `normalization_ctor` instead.</span>
<span class="sd">      use_bias: Boolean or iterable of booleans determining whether to include</span>
<span class="sd">        bias parameters in the convolutional layers. Default `True`.</span>
<span class="sd">      batch_norm_config: Optional mapping of additional configuration for the</span>
<span class="sd">        `snt.BatchNorm` modules. Deprecated, use `normalization_kwargs` instead.</span>
<span class="sd">      data_format: A string, one of &quot;NCHW&quot; or &quot;NHWC&quot;. Specifies whether the</span>
<span class="sd">        channel dimension of the input and output is the last dimension</span>
<span class="sd">        (default, &quot;NHWC&quot;), or the second dimension (&quot;NCHW&quot;).</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">          custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">          correspond to regexes to match variable names. See the</span>
<span class="sd">          `tf.get_variable` documentation for information about the</span>
<span class="sd">          custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `output_channels` is not iterable; or if `kernel_shapes` is</span>
<span class="sd">        not iterable; or `strides` is not iterable; or `paddings` is not</span>
<span class="sd">        iterable; or if `activation` is not callable.</span>
<span class="sd">      ValueError: If `output_channels` is empty; or if `kernel_shapes` has not</span>
<span class="sd">        length 1 or `len(output_channels)`; or if `strides` has not</span>
<span class="sd">        length 1 or `len(output_channels)`; or if `paddings` has not</span>
<span class="sd">        length 1 or `len(output_channels)`; or if `rates` has not</span>
<span class="sd">        length 1 or `len(output_channels)`; or if the given data_format is not a</span>
<span class="sd">        supported format (&quot;NHWC&quot; or &quot;NCHW&quot;); or if `normalization_ctor` is</span>
<span class="sd">        provided but cannot be mapped to a callable.</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contain any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;output_channels must be iterable&quot;</span><span class="p">)</span>
    <span class="n">output_channels</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_channels</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_shapes</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;kernel_shapes must be iterable&quot;</span><span class="p">)</span>
    <span class="n">kernel_shapes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel_shapes</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">strides</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;strides must be iterable&quot;</span><span class="p">)</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">strides</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;paddings must be iterable&quot;</span><span class="p">)</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">rates</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;rates must be iterable&quot;</span><span class="p">)</span>
    <span class="n">rates</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">rates</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">use_batch_norm</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;use_batch_norm must be a boolean. Per-layer use of &quot;</span>
                      <span class="s2">&quot;batch normalization is not supported. Previously, a &quot;</span>
                      <span class="s2">&quot;test erroneously suggested use_batch_norm can be an &quot;</span>
                      <span class="s2">&quot;iterable of booleans.&quot;</span><span class="p">)</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">ConvNet2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">output_channels</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;output_channels must not be empty&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_channels</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">data_format</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">SUPPORTED_2D_DATA_FORMATS</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid data_format </span><span class="si">{}</span><span class="s2">. Allowed formats &quot;</span>
                       <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="n">SUPPORTED_2D_DATA_FORMATS</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span> <span class="o">=</span> <span class="n">data_format</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">activation</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Input &#39;activation&#39; must be callable&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_activate_final</span> <span class="o">=</span> <span class="n">activate_final</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shapes</span> <span class="o">=</span> <span class="n">_replicate_elements</span><span class="p">(</span><span class="n">kernel_shapes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_layers</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shapes</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_layers</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;kernel_shapes must be of length 1 or len(output_channels)&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_strides</span> <span class="o">=</span> <span class="n">_replicate_elements</span><span class="p">(</span><span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_layers</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_strides</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_layers</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sd">&quot;&quot;&quot;strides must be of length 1 or len(output_channels)&quot;&quot;&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_paddings</span> <span class="o">=</span> <span class="n">_replicate_elements</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_layers</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_paddings</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_layers</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sd">&quot;&quot;&quot;paddings must be of length 1 or len(output_channels)&quot;&quot;&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_rates</span> <span class="o">=</span> <span class="n">_replicate_elements</span><span class="p">(</span><span class="n">rates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_layers</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_rates</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_layers</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="sd">&quot;&quot;&quot;rates must be of length 1 or len(output_channels)&quot;&quot;&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_parse_normalization_kwargs</span><span class="p">(</span>
        <span class="n">use_batch_norm</span><span class="p">,</span> <span class="n">batch_norm_config</span><span class="p">,</span>
        <span class="n">normalization_ctor</span><span class="p">,</span> <span class="n">normalization_kwargs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">normalize_final</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">util</span><span class="o">.</span><span class="n">deprecation_warning</span><span class="p">(</span>
          <span class="s2">&quot;normalize_final is not specified, so using the value of &quot;</span>
          <span class="s2">&quot;activate_final = </span><span class="si">{}</span><span class="s2">. Change your code to set this kwarg explicitly. &quot;</span>
          <span class="s2">&quot;In the future, normalize_final will default to True.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
              <span class="n">activate_final</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_final</span> <span class="o">=</span> <span class="n">activate_final</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># User has provided an override, so don&#39;t link to activate_final.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_final</span> <span class="o">=</span> <span class="n">normalize_final</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">use_bias</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
      <span class="n">use_bias</span> <span class="o">=</span> <span class="p">(</span><span class="n">use_bias</span><span class="p">,)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">use_bias</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;use_bias must be either a bool or an iterable&quot;</span><span class="p">)</span>
      <span class="n">use_bias</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">use_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">_replicate_elements</span><span class="p">(</span><span class="n">use_bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_layers</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_instantiate_layers</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_check_and_assign_normalization_members</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">normalization_ctor</span><span class="p">,</span>
                                              <span class="n">normalization_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Checks that the normalization constructor is callable.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">normalization_ctor</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
      <span class="n">normalization_ctor</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">parse_string_to_constructor</span><span class="p">(</span><span class="n">normalization_ctor</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">normalization_ctor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">normalization_ctor</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;normalization_ctor must be a callable or a string that specifies &quot;</span>
          <span class="s2">&quot;a callable, got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">normalization_ctor</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_ctor</span> <span class="o">=</span> <span class="n">normalization_ctor</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_kwargs</span> <span class="o">=</span> <span class="n">normalization_kwargs</span>

  <span class="k">def</span> <span class="nf">_parse_normalization_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_batch_norm</span><span class="p">,</span> <span class="n">batch_norm_config</span><span class="p">,</span>
                                  <span class="n">normalization_ctor</span><span class="p">,</span> <span class="n">normalization_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sets up normalization, checking old and new flags.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">use_batch_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Delete this whole block when deprecation is done.</span>
      <span class="n">util</span><span class="o">.</span><span class="n">deprecation_warning</span><span class="p">(</span>
          <span class="s2">&quot;`use_batch_norm` kwarg is deprecated. Change your code to instead &quot;</span>
          <span class="s2">&quot;specify `normalization_ctor` and `normalization_kwargs`.&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">use_batch_norm</span><span class="p">:</span>
        <span class="c1"># Explicitly set to False - normalization_{ctor,kwargs} has precedence.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_and_assign_normalization_members</span><span class="p">(</span><span class="n">normalization_ctor</span><span class="p">,</span>
                                                     <span class="n">normalization_kwargs</span> <span class="ow">or</span> <span class="p">{})</span>
      <span class="k">else</span><span class="p">:</span>  <span class="c1"># Explicitly set to true - new kwargs must not be used.</span>
        <span class="k">if</span> <span class="n">normalization_ctor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">normalization_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
              <span class="s2">&quot;if use_batch_norm is specified, normalization_ctor and &quot;</span>
              <span class="s2">&quot;normalization_kwargs must not be.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_and_assign_normalization_members</span><span class="p">(</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span>
                                                     <span class="n">batch_norm_config</span> <span class="ow">or</span> <span class="p">{})</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Old kwargs not set, this block will remain after removing old kwarg.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_check_and_assign_normalization_members</span><span class="p">(</span><span class="n">normalization_ctor</span><span class="p">,</span>
                                                   <span class="n">normalization_kwargs</span> <span class="ow">or</span> <span class="p">{})</span>

  <span class="k">def</span> <span class="nf">_instantiate_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Instantiates all the convolutional modules used in the network.&quot;&quot;&quot;</span>

    <span class="c1"># Here we are entering the module&#39;s variable scope to name our submodules</span>
    <span class="c1"># correctly (not to create variables). As such it&#39;s safe to not check</span>
    <span class="c1"># whether we&#39;re in the same graph. This is important if we&#39;re constructing</span>
    <span class="c1"># the module in one graph and connecting it in another (e.g. with `defun`</span>
    <span class="c1"># the module is created in some default graph, and connected to a capturing</span>
    <span class="c1"># graph in order to turn it into a graph function).</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enter_variable_scope</span><span class="p">(</span><span class="n">check_same_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">conv</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_2d_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                                       <span class="n">output_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                       <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shapes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                       <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_strides</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                       <span class="n">rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rates</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                       <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_paddings</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                       <span class="n">use_bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                       <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">,</span>
                                       <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="p">,</span>
                                       <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="p">,</span>
                                       <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span><span class="p">)</span>
                           <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_layers</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">normalization_build_kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assembles the `ConvNet2D` and connects it to the graph.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A 4D Tensor of shape `[batch_size, input_height, input_width,</span>
<span class="sd">        input_channels]`.</span>
<span class="sd">      **normalization_build_kwargs: kwargs passed to the normalization module</span>
<span class="sd">        at _build time.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 4D Tensor of shape `[batch_size, output_height, output_width,</span>
<span class="sd">        output_channels[-1]]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If `is_training` is not explicitly specified when using</span>
<span class="sd">        batch normalization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_normalization_ctor</span> <span class="ow">in</span> <span class="p">{</span><span class="n">batch_norm</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">,</span>
                                     <span class="n">batch_norm_v2</span><span class="o">.</span><span class="n">BatchNormV2</span><span class="p">}</span> <span class="ow">and</span>
        <span class="s2">&quot;is_training&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">normalization_build_kwargs</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Boolean is_training flag must be explicitly specified &quot;</span>
                       <span class="s2">&quot;when using batch normalization.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">())</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="n">final_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">):</span>
      <span class="n">net</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">final_index</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_final</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_ctor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
          <span class="c1"># The name &#39;batch_norm&#39; is used even if something else like</span>
          <span class="c1"># LayerNorm is being used. This is to avoid breaking old checkpoints.</span>
          <span class="n">normalizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_ctor</span><span class="p">(</span>
              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;batch_norm_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
              <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_normalization_kwargs</span><span class="p">)</span>
          <span class="n">net</span> <span class="o">=</span> <span class="n">normalizer</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="o">**</span><span class="n">normalization_build_kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">normalization_build_kwargs</span><span class="p">:</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;No normalization configured, but extra kwargs &quot;</span>
                <span class="s2">&quot;provided: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">normalization_build_kwargs</span><span class="p">))</span>

      <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">final_index</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activate_final</span><span class="p">:</span>
        <span class="n">net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">net</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">layers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a tuple containing the convolutional layers of the network.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioners</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">strides</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_strides</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">paddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_paddings</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">rates</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rates</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">kernel_shapes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shapes</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_channels</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">l</span><span class="p">()</span> <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">else</span> <span class="n">l</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">])</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">use_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">use_batch_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">util</span><span class="o">.</span><span class="n">deprecation_warning</span><span class="p">(</span>
        <span class="s2">&quot;The `.use_batch_norm` property is deprecated. Check &quot;</span>
        <span class="s2">&quot;`.normalization_ctor` instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_ctor</span> <span class="o">==</span> <span class="n">batch_norm</span><span class="o">.</span><span class="n">BatchNorm</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">batch_norm_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">util</span><span class="o">.</span><span class="n">deprecation_warning</span><span class="p">(</span>
        <span class="s2">&quot;The `.batch_norm_config` property is deprecated. Check &quot;</span>
        <span class="s2">&quot;`.normalization_kwargs` instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_kwargs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">normalization_ctor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_ctor</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">normalization_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalization_kwargs</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">normalize_final</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_final</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">activate_final</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activate_final</span>

  <span class="c1"># Implements Transposable interface.</span>
  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns shape of input `Tensor` passed at last call to `build`.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span>

  <span class="k">def</span> <span class="nf">_transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">transpose_constructor</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">output_channels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">kernel_shapes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">strides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">activate_final</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">normalization_ctor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">normalization_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">normalize_final</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="kc">None</span><span class="p">,):</span>
    <span class="sd">&quot;&quot;&quot;Returns transposed version of this network.</span>

<span class="sd">    Args:</span>
<span class="sd">      transpose_constructor: A method that creates an instance of the transposed</span>
<span class="sd">        network type. The method must accept the same kwargs as this methods</span>
<span class="sd">        with the exception of the `transpose_constructor` argument.</span>
<span class="sd">      name: Optional string specifying the name of the transposed module. The</span>
<span class="sd">        default name is constructed by appending &quot;_transpose&quot;</span>
<span class="sd">        to `self.module_name`.</span>
<span class="sd">      output_channels: Optional iterable of numbers of output channels.</span>
<span class="sd">      kernel_shapes: Optional iterable of kernel sizes. The default value is</span>
<span class="sd">        constructed by reversing `self.kernel_shapes`.</span>
<span class="sd">      strides: Optional iterable of kernel strides. The default value is</span>
<span class="sd">        constructed by reversing `self.strides`.</span>
<span class="sd">      paddings: Optional iterable of padding options, either `snt.SAME` or</span>
<span class="sd">        `snt.VALID`; The default value is constructed by reversing</span>
<span class="sd">        `self.paddings`.</span>
<span class="sd">      activation: Optional activation op. Default value is `self.activation`.</span>
<span class="sd">      activate_final: Optional boolean determining if the activation and batch</span>
<span class="sd">        normalization, if turned on, are applied to the final layer.</span>
<span class="sd">      normalization_ctor: Constructor to return a callable which will perform</span>
<span class="sd">        normalization at each layer. Defaults to None / no normalization.</span>
<span class="sd">        Examples of what could go here: `snt.BatchNormV2`, `snt.LayerNorm`. If</span>
<span class="sd">        a string is provided, importlib is used to convert the string to a</span>
<span class="sd">        callable, so either `snt.LayerNorm` or `&quot;snt.LayerNorm&quot;` can be</span>
<span class="sd">        provided.</span>
<span class="sd">      normalization_kwargs: kwargs to be provided to `normalization_ctor` when</span>
<span class="sd">        it is called.</span>
<span class="sd">      normalize_final: Whether to apply normalization after the final conv</span>
<span class="sd">        layer. Default is to take the value of activate_final.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters of</span>
<span class="sd">        the whole network (with key &#39;w&#39;) or biases (with key &#39;b&#39;). The default</span>
<span class="sd">        value is `self.initializers`.</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">        weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). The default value is</span>
<span class="sd">        `self.partitioners`.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters of the</span>
<span class="sd">        whole network (with key &#39;w&#39;) or biases (with key &#39;b&#39;). The default is</span>
<span class="sd">        `self.regularizers`.</span>
<span class="sd">      use_bias: Optional boolean or iterable of booleans determining whether to</span>
<span class="sd">        include bias parameters in the convolutional layers. Default</span>
<span class="sd">        is constructed by reversing `self.use_bias`.</span>
<span class="sd">      data_format: Optional string, one of &quot;NCHW&quot; or &quot;NHWC&quot;. Specifies whether</span>
<span class="sd">        the channel dimension of the input and output is the last dimension.</span>
<span class="sd">        Default is `self._data_format`.</span>
<span class="sd">    Returns:</span>
<span class="sd">      Matching transposed module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If output_channels is specified and its length does not match</span>
<span class="sd">        the number of layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">data_format</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">data_format</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span>

    <span class="k">if</span> <span class="n">output_channels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">output_channels</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">channel_dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="n">DATA_FORMAT_NHWC</span> <span class="k">else</span> <span class="mi">1</span>
      <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">):</span>
        <span class="n">output_channels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="o">=</span><span class="n">layer</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="n">channel_dim</span><span class="p">])</span>

    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_channels</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">):</span>
      <span class="c1"># Note that we only have to do this check for the output channels. Any</span>
      <span class="c1"># other inconsistencies will be picked up by ConvNet2D.__init__.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Iterable output_channels length must match the &quot;</span>
                       <span class="s2">&quot;number of layers (</span><span class="si">{}</span><span class="s2">), but is </span><span class="si">{}</span><span class="s2"> instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_channels</span><span class="p">)))</span>

    <span class="k">if</span> <span class="n">kernel_shapes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">kernel_shapes</span> <span class="o">=</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_shapes</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">strides</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">strides</span> <span class="o">=</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">paddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">paddings</span> <span class="o">=</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">paddings</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">activation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span>

    <span class="k">if</span> <span class="n">activate_final</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">activate_final</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activate_final</span>

    <span class="k">if</span> <span class="n">normalization_ctor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">normalization_ctor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization_ctor</span>

    <span class="k">if</span> <span class="n">normalization_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">normalization_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalization_kwargs</span>

    <span class="k">if</span> <span class="n">normalize_final</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">normalize_final</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_final</span>

    <span class="k">if</span> <span class="n">initializers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">initializers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">initializers</span>

    <span class="k">if</span> <span class="n">partitioners</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">partitioners</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">partitioners</span>

    <span class="k">if</span> <span class="n">regularizers</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">regularizers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regularizers</span>

    <span class="k">if</span> <span class="n">use_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">use_bias</span> <span class="o">=</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_name</span> <span class="o">+</span> <span class="s2">&quot;_transpose&quot;</span>

    <span class="k">return</span> <span class="n">transpose_constructor</span><span class="p">(</span>
        <span class="n">output_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span>
        <span class="n">kernel_shapes</span><span class="o">=</span><span class="n">kernel_shapes</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
        <span class="n">paddings</span><span class="o">=</span><span class="n">paddings</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">activate_final</span><span class="o">=</span><span class="n">activate_final</span><span class="p">,</span>
        <span class="n">normalization_ctor</span><span class="o">=</span><span class="n">normalization_ctor</span><span class="p">,</span>
        <span class="n">normalization_kwargs</span><span class="o">=</span><span class="n">normalization_kwargs</span><span class="p">,</span>
        <span class="n">normalize_final</span><span class="o">=</span><span class="n">normalize_final</span><span class="p">,</span>
        <span class="n">initializers</span><span class="o">=</span><span class="n">initializers</span><span class="p">,</span>
        <span class="n">partitioners</span><span class="o">=</span><span class="n">partitioners</span><span class="p">,</span>
        <span class="n">regularizers</span><span class="o">=</span><span class="n">regularizers</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
        <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="c1"># Implements Transposable interface.</span>
<div class="viewcode-block" id="ConvNet2D.transpose"><a class="viewcode-back" href="../../../../../api/sonnet.python.modules.nets.html#sonnet.python.modules.nets.convnet.ConvNet2D.transpose">[docs]</a>  <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">output_channels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">kernel_shapes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">strides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">activate_final</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">normalization_ctor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">normalization_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">normalize_final</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">use_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">batch_norm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">data_format</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns transposed version of this network.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Optional string specifying the name of the transposed module. The</span>
<span class="sd">        default name is constructed by appending &quot;_transpose&quot;</span>
<span class="sd">        to `self.module_name`.</span>
<span class="sd">      output_channels: Optional iterable of numbers of output channels.</span>
<span class="sd">      kernel_shapes: Optional iterable of kernel sizes. The default value is</span>
<span class="sd">        constructed by reversing `self.kernel_shapes`.</span>
<span class="sd">      strides: Optional iterable of kernel strides. The default value is</span>
<span class="sd">        constructed by reversing `self.strides`.</span>
<span class="sd">      paddings: Optional iterable of padding options, either `snt.SAME` or</span>
<span class="sd">        `snt.VALID`; The default value is constructed by reversing</span>
<span class="sd">        `self.paddings`.</span>
<span class="sd">      activation: Optional activation op. Default value is `self.activation`.</span>
<span class="sd">      activate_final: Optional boolean determining if the activation and batch</span>
<span class="sd">        normalization, if turned on, are applied to the final layer.</span>
<span class="sd">      normalization_ctor: Constructor to return a callable which will perform</span>
<span class="sd">        normalization at each layer. Defaults to None / no normalization.</span>
<span class="sd">        Examples of what could go here: `snt.BatchNormV2`, `snt.LayerNorm`. If</span>
<span class="sd">        a string is provided, importlib is used to convert the string to a</span>
<span class="sd">        callable, so either `snt.LayerNorm` or `&quot;snt.LayerNorm&quot;` can be</span>
<span class="sd">        provided.</span>
<span class="sd">      normalization_kwargs: kwargs to be provided to `normalization_ctor` when</span>
<span class="sd">        it is called.</span>
<span class="sd">      normalize_final: Whether to apply normalization after the final conv</span>
<span class="sd">        layer. Default is to take the value of activate_final.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters of</span>
<span class="sd">        the whole network (with key &#39;w&#39;) or biases (with key &#39;b&#39;). The default</span>
<span class="sd">        value is `self.initializers`.</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">        weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). The default value is</span>
<span class="sd">        `self.partitioners`.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters of the</span>
<span class="sd">        whole network (with key &#39;w&#39;) or biases (with key &#39;b&#39;). The default is</span>
<span class="sd">        `self.regularizers`.</span>
<span class="sd">      use_batch_norm: Optional boolean determining if batch normalization is</span>
<span class="sd">        applied after convolution. The default value is `self.use_batch_norm`.</span>
<span class="sd">      use_bias: Optional boolean or iterable of booleans determining whether to</span>
<span class="sd">        include bias parameters in the convolutional layers. Default</span>
<span class="sd">        is constructed by reversing `self.use_bias`.</span>
<span class="sd">      batch_norm_config: Optional mapping of additional configuration for the</span>
<span class="sd">        `snt.BatchNorm` modules. Default is `self.batch_norm_config`.</span>
<span class="sd">      data_format: Optional string, one of &quot;NCHW&quot; or &quot;NHWC&quot;. Specifies whether</span>
<span class="sd">        the channel dimension of the input and output is the last dimension.</span>
<span class="sd">        Default is `self._data_format`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Matching `ConvNet2DTranspose` module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If output_channels is specified and its length does not match</span>
<span class="sd">        the number of layers.</span>
<span class="sd">      ValueError: If the given data_format is not a supported format (&quot;NHWC&quot; or</span>
<span class="sd">        &quot;NCHW&quot;).</span>
<span class="sd">      NotImplementedError: If the convolutions are dilated.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">rate</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rates</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">rate</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Transpose dilated convolutions &quot;</span>
                                  <span class="s2">&quot;are not supported&quot;</span><span class="p">)</span>
    <span class="n">output_shapes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">data_format</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">data_format</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span>
    <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="n">DATA_FORMAT_NHWC</span><span class="p">:</span>
      <span class="n">start_dim</span><span class="p">,</span> <span class="n">end_dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">elif</span> <span class="n">data_format</span> <span class="o">==</span> <span class="n">DATA_FORMAT_NCHW</span><span class="p">:</span>
      <span class="n">start_dim</span><span class="p">,</span> <span class="n">end_dim</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid data_format </span><span class="si">{:s}</span><span class="s2">. Allowed formats &quot;</span>
                       <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="n">SUPPORTED_2D_DATA_FORMATS</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">):</span>
      <span class="n">output_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="o">=</span><span class="n">layer</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">input_shape</span><span class="p">[</span><span class="n">start_dim</span><span class="p">:</span><span class="n">end_dim</span><span class="p">])</span>
    <span class="n">transpose_constructor</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">ConvNet2DTranspose</span><span class="p">,</span>
                                              <span class="n">output_shapes</span><span class="o">=</span><span class="n">output_shapes</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose</span><span class="p">(</span>
        <span class="n">transpose_constructor</span><span class="o">=</span><span class="n">transpose_constructor</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">output_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span>
        <span class="n">kernel_shapes</span><span class="o">=</span><span class="n">kernel_shapes</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
        <span class="n">paddings</span><span class="o">=</span><span class="n">paddings</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">activate_final</span><span class="o">=</span><span class="n">activate_final</span><span class="p">,</span>
        <span class="n">normalization_ctor</span><span class="o">=</span><span class="n">normalization_ctor</span><span class="p">,</span>
        <span class="n">normalization_kwargs</span><span class="o">=</span><span class="n">normalization_kwargs</span><span class="p">,</span>
        <span class="n">normalize_final</span><span class="o">=</span><span class="n">normalize_final</span><span class="p">,</span>
        <span class="n">initializers</span><span class="o">=</span><span class="n">initializers</span><span class="p">,</span>
        <span class="n">partitioners</span><span class="o">=</span><span class="n">partitioners</span><span class="p">,</span>
        <span class="n">regularizers</span><span class="o">=</span><span class="n">regularizers</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
        <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ConvNet2DTranspose"><a class="viewcode-back" href="../../../../../api/sonnet.python.modules.nets.html#sonnet.python.modules.nets.convnet.ConvNet2DTranspose">[docs]</a><span class="k">class</span> <span class="nc">ConvNet2DTranspose</span><span class="p">(</span><span class="n">ConvNet2D</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A 2D Transpose-Convolutional Network module.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">output_channels</span><span class="p">,</span>
               <span class="n">output_shapes</span><span class="p">,</span>
               <span class="n">kernel_shapes</span><span class="p">,</span>
               <span class="n">strides</span><span class="p">,</span>
               <span class="n">paddings</span><span class="p">,</span>
               <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
               <span class="n">activate_final</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">normalization_ctor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">normalization_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">normalize_final</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">batch_norm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">data_format</span><span class="o">=</span><span class="n">DATA_FORMAT_NHWC</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_net_2d_transpose&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs a `ConvNetTranspose2D` module.</span>

<span class="sd">    `output_{shapes,channels}` can be defined either as iterable of</span>
<span class="sd">    {iterables,integers} or via a callable. In the latter case, since the</span>
<span class="sd">    function invocation is deferred to graph construction time, the user</span>
<span class="sd">    must only ensure that entries can be called returning meaningful values when</span>
<span class="sd">    build is called. Each entry in the iterable defines properties in the</span>
<span class="sd">    corresponding convolutional layer.</span>

<span class="sd">    By default, neither batch normalization nor activation are applied to the</span>
<span class="sd">    output of the final layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      output_channels: Iterable of numbers of output channels.</span>
<span class="sd">      output_shapes: Iterable of output shapes as defined in</span>
<span class="sd">        `conv.conv2DTranpose`; if the iterable contains one element only, the</span>
<span class="sd">        same shape is used in each layer of the network.</span>
<span class="sd">      kernel_shapes: Iterable of kernel sizes as defined in `conv.Conv2D`; if</span>
<span class="sd">        the list contains one element only, the same kernel shape is used in</span>
<span class="sd">        each layer of the network.</span>
<span class="sd">      strides: Iterable of kernel strides as defined in `conv.Conv2D`; if the</span>
<span class="sd">        list contains one element only, the same stride is used in each layer of</span>
<span class="sd">        the network.</span>
<span class="sd">      paddings: Iterable of padding options, either `snt.SAME` or</span>
<span class="sd">        `snt.VALID`; if the Iterable contains one element only, the same padding</span>
<span class="sd">        is used in each layer of the network.</span>
<span class="sd">      activation: An activation op.</span>
<span class="sd">      activate_final: Boolean determining if the activation and batch</span>
<span class="sd">        normalization, if turned on, are applied to the final layer.</span>
<span class="sd">      normalization_ctor: Constructor to return a callable which will perform</span>
<span class="sd">        normalization at each layer. Defaults to None / no normalization.</span>
<span class="sd">        Examples of what could go here: `snt.BatchNormV2`, `snt.LayerNorm`. If</span>
<span class="sd">        a string is provided, importlib is used to convert the string to a</span>
<span class="sd">        callable, so either `snt.LayerNorm` or `&quot;snt.LayerNorm&quot;` can be</span>
<span class="sd">        provided.</span>
<span class="sd">      normalization_kwargs: kwargs to be provided to `normalization_ctor` when</span>
<span class="sd">        it is called.</span>
<span class="sd">      normalize_final: Whether to apply normalization after the final conv</span>
<span class="sd">        layer. Default is to take the value of activate_final.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters of</span>
<span class="sd">        the whole network (with key &#39;w&#39;) or biases (with key &#39;b&#39;).</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">          weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">          partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters of the</span>
<span class="sd">        whole network (with key &#39;w&#39;) or biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes a</span>
<span class="sd">        single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      use_batch_norm: Boolean determining if batch normalization is applied</span>
<span class="sd">        after convolution.</span>
<span class="sd">      use_bias: Boolean or iterable of booleans determining whether to include</span>
<span class="sd">        bias parameters in the convolutional layers. Default `True`.</span>
<span class="sd">      batch_norm_config: Optional mapping of additional configuration for the</span>
<span class="sd">        `snt.BatchNorm` modules.</span>
<span class="sd">      data_format: A string, one of &quot;NCHW&quot; or &quot;NHWC&quot;. Specifies whether the</span>
<span class="sd">        channel dimension of the input and output is the last dimension</span>
<span class="sd">        (default, &quot;NHWC&quot;), or the second dimension (&quot;NCHW&quot;).</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `output_channels` is not iterable; or if `output_shapes`</span>
<span class="sd">        is not iterable; or if `kernel_shapes` is not iterable; or if `strides`</span>
<span class="sd">        is not iterable; or if `paddings` is not iterable; or if `activation` is</span>
<span class="sd">        not callable.</span>
<span class="sd">      ValueError: If `output_channels` is empty; or if `kernel_shapes` has not</span>
<span class="sd">        length 1 or `len(output_channels)`; or if `strides` has not</span>
<span class="sd">        length 1 or `len(output_channels)`; or if `paddings` has not</span>
<span class="sd">        length 1 or `len(output_channels)`.</span>
<span class="sd">      ValueError: If the given data_format is not a supported format (&quot;NHWC&quot; or</span>
<span class="sd">        &quot;NCHW&quot;).</span>
<span class="sd">      ValueError: If `normalization_ctor` is provided but cannot be converted</span>
<span class="sd">        to a callable.</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contain any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">      TypeError: If any of the given initializers, partitioners or regularizers</span>
<span class="sd">        are not callable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;output_channels must be iterable&quot;</span><span class="p">)</span>
    <span class="n">output_channels</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_channels</span><span class="p">)</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_channels</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_shapes</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Iterable</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;output_shapes must be iterable&quot;</span><span class="p">)</span>
    <span class="n">output_shapes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">output_shapes</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_output_shapes</span> <span class="o">=</span> <span class="n">_replicate_elements</span><span class="p">(</span><span class="n">output_shapes</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_shapes</span><span class="p">)</span> <span class="o">!=</span> <span class="n">num_layers</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s2">&quot;output_shapes must be of length 1 or len(output_channels)&quot;</span><span class="p">)</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">ConvNet2DTranspose</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">output_channels</span><span class="p">,</span>
        <span class="n">kernel_shapes</span><span class="p">,</span>
        <span class="n">strides</span><span class="p">,</span>
        <span class="n">paddings</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">activate_final</span><span class="o">=</span><span class="n">activate_final</span><span class="p">,</span>
        <span class="n">normalization_ctor</span><span class="o">=</span><span class="n">normalization_ctor</span><span class="p">,</span>
        <span class="n">normalization_kwargs</span><span class="o">=</span><span class="n">normalization_kwargs</span><span class="p">,</span>
        <span class="n">normalize_final</span><span class="o">=</span><span class="n">normalize_final</span><span class="p">,</span>
        <span class="n">initializers</span><span class="o">=</span><span class="n">initializers</span><span class="p">,</span>
        <span class="n">partitioners</span><span class="o">=</span><span class="n">partitioners</span><span class="p">,</span>
        <span class="n">regularizers</span><span class="o">=</span><span class="n">regularizers</span><span class="p">,</span>
        <span class="n">use_batch_norm</span><span class="o">=</span><span class="n">use_batch_norm</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
        <span class="n">batch_norm_config</span><span class="o">=</span><span class="n">batch_norm_config</span><span class="p">,</span>
        <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_instantiate_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Instantiates all the convolutional modules used in the network.&quot;&quot;&quot;</span>

    <span class="c1"># See `ConvNet2D._instantiate_layers` for more information about why we are</span>
    <span class="c1"># using `check_same_graph=False`.</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enter_variable_scope</span><span class="p">(</span><span class="n">check_same_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
          <span class="n">conv</span><span class="o">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_2d_transpose_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
                               <span class="n">output_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                               <span class="n">output_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_shapes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                               <span class="n">kernel_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shapes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                               <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_strides</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                               <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_paddings</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                               <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">,</span>
                               <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="p">,</span>
                               <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="p">,</span>
                               <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_format</span><span class="p">,</span>
                               <span class="n">use_bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
          <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_layers</span><span class="p">))</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">output_shapes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">l</span><span class="p">()</span> <span class="k">if</span> <span class="n">callable</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">else</span> <span class="n">l</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_shapes</span><span class="p">])</span>

  <span class="c1"># Implements Transposable interface.</span>
<div class="viewcode-block" id="ConvNet2DTranspose.transpose"><a class="viewcode-back" href="../../../../../api/sonnet.python.modules.nets.html#sonnet.python.modules.nets.convnet.ConvNet2DTranspose.transpose">[docs]</a>  <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">output_channels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">kernel_shapes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">strides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">paddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">activate_final</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">normalization_ctor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">normalization_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">normalize_final</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">use_bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">batch_norm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">data_format</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns transposed version of this network.</span>

<span class="sd">    Args:</span>
<span class="sd">      name: Optional string specifying the name of the transposed module. The</span>
<span class="sd">        default name is constructed by appending &quot;_transpose&quot;</span>
<span class="sd">        to `self.module_name`.</span>
<span class="sd">      output_channels: Optional iterable of numbers of output channels.</span>
<span class="sd">      kernel_shapes: Optional iterable of kernel sizes. The default value is</span>
<span class="sd">        constructed by reversing `self.kernel_shapes`.</span>
<span class="sd">      strides: Optional iterable of kernel strides. The default value is</span>
<span class="sd">        constructed by reversing `self.strides`.</span>
<span class="sd">      paddings: Optional iterable of padding options, either `snt.SAME` or</span>
<span class="sd">        `snt.VALID`; The default value is constructed by reversing</span>
<span class="sd">        `self.paddings`.</span>
<span class="sd">      activation: Optional activation op. Default value is `self.activation`.</span>
<span class="sd">      activate_final: Optional boolean determining if the activation and batch</span>
<span class="sd">        normalization, if turned on, are applied to the final layer.</span>
<span class="sd">      normalization_ctor: Constructor to return a callable which will perform</span>
<span class="sd">        normalization at each layer. Defaults to None / no normalization.</span>
<span class="sd">        Examples of what could go here: `snt.BatchNormV2`, `snt.LayerNorm`. If</span>
<span class="sd">        a string is provided, importlib is used to convert the string to a</span>
<span class="sd">        callable, so either `snt.LayerNorm` or `&quot;snt.LayerNorm&quot;` can be</span>
<span class="sd">        provided.</span>
<span class="sd">      normalization_kwargs: kwargs to be provided to `normalization_ctor` when</span>
<span class="sd">        it is called.</span>
<span class="sd">      normalize_final: Whether to apply normalization after the final conv</span>
<span class="sd">        layer. Default is to take the value of activate_final.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters of</span>
<span class="sd">        the whole network (with key &#39;w&#39;) or biases (with key &#39;b&#39;). The default</span>
<span class="sd">        value is `self.initializers`.</span>
<span class="sd">      partitioners: Optional dict containing partitioners to partition</span>
<span class="sd">        weights (with key &#39;w&#39;) or biases (with key &#39;b&#39;). The default value is</span>
<span class="sd">        `self.partitioners`.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters of the</span>
<span class="sd">        whole network (with key &#39;w&#39;) or biases (with key &#39;b&#39;). The default is</span>
<span class="sd">        `self.regularizers`.</span>
<span class="sd">      use_batch_norm: Optional boolean determining if batch normalization is</span>
<span class="sd">        applied after convolution. The default value is `self.use_batch_norm`.</span>
<span class="sd">      use_bias: Optional boolean or iterable of booleans determining whether to</span>
<span class="sd">        include bias parameters in the convolutional layers. Default</span>
<span class="sd">        is constructed by reversing `self.use_bias`.</span>
<span class="sd">      batch_norm_config: Optional mapping of additional configuration for the</span>
<span class="sd">        `snt.BatchNorm` modules. Default is `self.batch_norm_config`.</span>
<span class="sd">      data_format: Optional string, one of &quot;NCHW&quot; or &quot;NHWC&quot;. Specifies whether</span>
<span class="sd">        the channel dimension of the input and output is the last dimension.</span>
<span class="sd">        Default is `self._data_format`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Matching `ConvNet2D` module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If output_channels is specified and its length does not match</span>
<span class="sd">        the number of layers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">use_batch_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">normalization_ctor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">normalization_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;If use_batch_norm is specified, normalization_ctor and &quot;</span>
            <span class="s2">&quot;normalization_kwargs must not be.&quot;</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">use_batch_norm</span><span class="p">:</span>
        <span class="n">normalization_ctor</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="o">.</span><span class="n">BatchNorm</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">normalization_ctor</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="n">normalization_kwargs</span> <span class="o">=</span> <span class="n">batch_norm_config</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose</span><span class="p">(</span>
        <span class="n">transpose_constructor</span><span class="o">=</span><span class="n">ConvNet2D</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">output_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span>
        <span class="n">kernel_shapes</span><span class="o">=</span><span class="n">kernel_shapes</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span>
        <span class="n">paddings</span><span class="o">=</span><span class="n">paddings</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">activate_final</span><span class="o">=</span><span class="n">activate_final</span><span class="p">,</span>
        <span class="n">normalization_ctor</span><span class="o">=</span><span class="n">normalization_ctor</span><span class="p">,</span>
        <span class="n">normalization_kwargs</span><span class="o">=</span><span class="n">normalization_kwargs</span><span class="p">,</span>
        <span class="n">normalize_final</span><span class="o">=</span><span class="n">normalize_final</span><span class="p">,</span>
        <span class="n">initializers</span><span class="o">=</span><span class="n">initializers</span><span class="p">,</span>
        <span class="n">partitioners</span><span class="o">=</span><span class="n">partitioners</span><span class="p">,</span>
        <span class="n">regularizers</span><span class="o">=</span><span class="n">regularizers</span><span class="p">,</span>
        <span class="n">use_bias</span><span class="o">=</span><span class="n">use_bias</span><span class="p">,</span>
        <span class="n">data_format</span><span class="o">=</span><span class="n">data_format</span><span class="p">)</span></div></div>
</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../../index.html">sonnet</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../index.html">Documentation overview</a><ul>
  <li><a href="../../../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Sonnet Authors.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
    </div>

    

    
  </body>
</html>