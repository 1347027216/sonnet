
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>sonnet.python.modules.nets.alexnet &#8212; sonnet git documentation</title>
    <link rel="stylesheet" href="../../../../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../../',
        VERSION:     'git',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sonnet.python.modules.nets.alexnet</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2017 The Sonnet Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#    http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="c1"># pylint: disable=line-too-long</span>
<span class="sd">&quot;&quot;&quot;Implementation of AlexNet as a Sonnet module.</span>

<span class="sd">`AlexNet` is a Sonnet module that implements two variants of</span>
<span class="sd">   &#39;ImageNet Classification with Deep Convolutional Neural Networks&#39;</span>
<span class="sd">    Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, NIPS 2012</span>
<span class="sd">    http://papers.nips.cc/paper/4824-imagenet-classification-w</span>

<span class="sd">The two modes are FULL and MINI, corresponding to the full dual-gpu version and</span>
<span class="sd">a cut-down version that is able to run on Cifar10.</span>

<span class="sd">AlexNet is no longer state of the art and isn&#39;t considered a good starting point</span>
<span class="sd">for a vision network.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1"># pylint: enable=line-too-long</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="c1"># Dependency imports</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">base</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">basic</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">batch_norm</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">conv</span>
<span class="kn">from</span> <span class="nn">sonnet.python.modules</span> <span class="k">import</span> <span class="n">util</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>


<div class="viewcode-block" id="AlexNet"><a class="viewcode-back" href="../../../../../api/sonnet.python.modules.nets.html#sonnet.python.modules.nets.alexnet.AlexNet">[docs]</a><span class="k">class</span> <span class="nc">AlexNet</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">AbstractModule</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Implementation of AlexNet with full and mini versions.</span>

<span class="sd">  Based on:</span>
<span class="sd">    &#39;ImageNet Classification with Deep Convolutional Neural Networks&#39;</span>
<span class="sd">    Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, NIPS 2012</span>
<span class="sd">    http://papers.nips.cc/paper/4824-imagenet-classification-w</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="n">FULL</span> <span class="o">=</span> <span class="s2">&quot;FULL&quot;</span>
  <span class="n">MINI</span> <span class="o">=</span> <span class="s2">&quot;MINI&quot;</span>

  <span class="n">POSSIBLE_INITIALIZER_KEYS</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">}</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">mode</span><span class="p">,</span>
               <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">batch_norm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">bn_on_fc_layers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;alex_net&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs AlexNet.</span>

<span class="sd">    Args:</span>
<span class="sd">      mode: Construction mode of network: `AlexNet.FULL` or `AlexNet.MINI`.</span>
<span class="sd">      use_batch_norm: Whether to use batch normalization between the output of</span>
<span class="sd">          a layer and the activation function.</span>
<span class="sd">      batch_norm_config: Optional mapping of additional configuration for the</span>
<span class="sd">          `snt.BatchNorm` modules.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">          key &#39;w&#39;) or biases (with key &#39;b&#39;). The default initializers are</span>
<span class="sd">          truncated normal initializers, which are commonly used when the inputs</span>
<span class="sd">          are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf).</span>
<span class="sd">      partitioners: Optional dict containing partitioners for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      bn_on_fc_layers: If `use_batch_norm` is True, add batch normalization to</span>
<span class="sd">        the fully-connected layers. This is deprecated.</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.Error: If the given `mode` is not one of `AlexNet.FULL`,</span>
<span class="sd">        or `AlexNet.MINI`.</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contains any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AlexNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_mode</span> <span class="o">=</span> <span class="n">mode</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm</span> <span class="o">=</span> <span class="n">use_batch_norm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_bn_on_fc_layers</span> <span class="o">=</span> <span class="n">bn_on_fc_layers</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bn_on_fc_layers</span><span class="p">:</span>
      <span class="n">tf</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Using BatchNorm on the fully connected layers in &quot;</span>
                      <span class="s2">&quot;AlexNet is not recommended. &#39;bn_on_fc_layers&#39; is a &quot;</span>
                      <span class="s2">&quot;deprecated option and will likely be removed.&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_batch_norm_config</span> <span class="o">=</span> <span class="n">batch_norm_config</span> <span class="ow">or</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mode</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">FULL</span><span class="p">:</span>
      <span class="c1"># The full AlexNet, i.e. originally ran on two GPUs</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_conv_layers</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
          <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
          <span class="p">(</span><span class="mi">384</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="kc">None</span><span class="p">),</span>
          <span class="p">(</span><span class="mi">384</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="kc">None</span><span class="p">),</span>
          <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
      <span class="p">]</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">_fc_layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4096</span><span class="p">,</span> <span class="mi">4096</span><span class="p">]</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mode</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">MINI</span><span class="p">:</span>
      <span class="c1"># A cut down version of the half net for testing with Cifar10</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_conv_layers</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
          <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
          <span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="kc">None</span><span class="p">),</span>
          <span class="p">(</span><span class="mi">192</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="kc">None</span><span class="p">),</span>
          <span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
      <span class="p">]</span>

      <span class="bp">self</span><span class="o">.</span><span class="n">_fc_layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">Error</span><span class="p">(</span><span class="s2">&quot;AlexNet construction mode &#39;</span><span class="si">{}</span><span class="s2">&#39; not recognised, &quot;</span>
                       <span class="s2">&quot;must be one of: &#39;</span><span class="si">{}</span><span class="s2">&#39;, &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                           <span class="n">mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">FULL</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">MINI</span><span class="p">))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_min_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calc_min_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_conv_layers</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_conv_modules</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_linear_modules</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Keep old name for backwards compatibility</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">possible_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_initializers</span><span class="p">(</span>
        <span class="n">initializers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_partitioners</span><span class="p">(</span>
        <span class="n">partitioners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">check_regularizers</span><span class="p">(</span>
        <span class="n">regularizers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">POSSIBLE_INITIALIZER_KEYS</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_calc_min_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conv_layers</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculates the minimum size of the input layer.</span>

<span class="sd">    Given a set of convolutional layers, calculate the minimum value of</span>
<span class="sd">    the `input_height` and `input_width`, i.e. such that the output has</span>
<span class="sd">    size 1x1. Assumes snt.VALID padding.</span>

<span class="sd">    Args:</span>
<span class="sd">      conv_layers: List of tuples `(output_channels, (kernel_size, stride),</span>
<span class="sd">        (pooling_size, pooling_stride))`</span>

<span class="sd">    Returns:</span>
<span class="sd">      Minimum value of input height and width.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">conv_params</span><span class="p">,</span> <span class="n">max_pooling</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">conv_layers</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">max_pooling</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">max_pooling</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="n">stride</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">conv_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">conv_params</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span> <span class="o">*</span> <span class="n">stride</span> <span class="o">+</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="n">stride</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">input_size</span>

  <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">test_local_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Connects the AlexNet module into the graph.</span>

<span class="sd">    The is_training flag only controls the batch norm settings, if `False` it</span>
<span class="sd">    does not force no dropout by overriding any input `keep_prob`. To avoid any</span>
<span class="sd">    confusion this may cause, if `is_training=False` and `keep_prob` would cause</span>
<span class="sd">    dropout to be applied, an error is thrown.</span>

<span class="sd">    Args:</span>
<span class="sd">      inputs: A Tensor of size [batch_size, input_height, input_width,</span>
<span class="sd">        input_channels], representing a batch of input images.</span>
<span class="sd">      keep_prob: A scalar Tensor representing the dropout keep probability.</span>
<span class="sd">        When `is_training=False` this must be None or 1 to give no dropout.</span>
<span class="sd">      is_training: Boolean to indicate if we are currently training. Must be</span>
<span class="sd">          specified if batch normalization or dropout is used.</span>
<span class="sd">      test_local_stats: Boolean to indicate to `snt.BatchNorm` if batch</span>
<span class="sd">        normalization should  use local batch statistics at test time.</span>
<span class="sd">        By default `True`.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A Tensor of size [batch_size, output_size], where `output_size` depends</span>
<span class="sd">      on the mode the network was constructed in.</span>

<span class="sd">    Raises:</span>
<span class="sd">      base.IncompatibleShapeError: If any of the input image dimensions</span>
<span class="sd">        (input_height, input_width) are too small for the given network mode.</span>
<span class="sd">      ValueError: If `keep_prob` is not None or 1 when `is_training=False`.</span>
<span class="sd">      ValueError: If `is_training` is not explicitly specified when using</span>
<span class="sd">        batch normalization.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check input shape</span>
    <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm</span> <span class="ow">or</span> <span class="n">keep_prob</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_training</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Boolean is_training flag must be explicitly specified &quot;</span>
                       <span class="s2">&quot;when using batch normalization or dropout.&quot;</span><span class="p">)</span>

    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_min_size</span> <span class="ow">or</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_min_size</span><span class="p">:</span>
      <span class="k">raise</span> <span class="n">base</span><span class="o">.</span><span class="n">IncompatibleShapeError</span><span class="p">(</span>
          <span class="s2">&quot;Image shape too small: (</span><span class="si">{:d}</span><span class="s2">, </span><span class="si">{:d}</span><span class="s2">) &lt; </span><span class="si">{:d}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
              <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_min_size</span><span class="p">))</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># Check keep prob</span>
    <span class="k">if</span> <span class="n">keep_prob</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">valid_inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">is_training</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">1.</span><span class="p">))</span>
      <span class="n">keep_prob_check</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">assert_equal</span><span class="p">(</span>
          <span class="n">valid_inputs</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span>
          <span class="n">message</span><span class="o">=</span><span class="s2">&quot;Input `keep_prob` must be None or 1 if `is_training=False`.&quot;</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">([</span><span class="n">keep_prob_check</span><span class="p">]):</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_conv_layers</span><span class="p">):</span>
      <span class="n">output_channels</span><span class="p">,</span> <span class="n">conv_params</span><span class="p">,</span> <span class="n">max_pooling</span> <span class="o">=</span> <span class="n">params</span>

      <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">conv_params</span>

      <span class="n">conv_mod</span> <span class="o">=</span> <span class="n">conv</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
          <span class="n">output_channels</span><span class="o">=</span><span class="n">output_channels</span><span class="p">,</span>
          <span class="n">kernel_shape</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
          <span class="n">padding</span><span class="o">=</span><span class="n">conv</span><span class="o">.</span><span class="n">VALID</span><span class="p">,</span>
          <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">,</span>
          <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="p">,</span>
          <span class="n">regularizers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span><span class="p">)</span>

      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_connected</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_conv_modules</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">conv_mod</span><span class="p">)</span>

      <span class="n">net</span> <span class="o">=</span> <span class="n">conv_mod</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm</span><span class="p">:</span>
        <span class="n">bn</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_norm_config</span><span class="p">)</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">bn</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="n">test_local_stats</span><span class="p">)</span>

      <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">max_pooling</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pooling_kernel_size</span><span class="p">,</span> <span class="n">pooling_stride</span> <span class="o">=</span> <span class="n">max_pooling</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span>
            <span class="n">net</span><span class="p">,</span>
            <span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">pooling_kernel_size</span><span class="p">,</span> <span class="n">pooling_kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">pooling_stride</span><span class="p">,</span> <span class="n">pooling_stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">conv</span><span class="o">.</span><span class="n">VALID</span><span class="p">)</span>

    <span class="n">net</span> <span class="o">=</span> <span class="n">basic</span><span class="o">.</span><span class="n">BatchFlatten</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;flatten&quot;</span><span class="p">)(</span><span class="n">net</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">output_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fc_layers</span><span class="p">):</span>
      <span class="n">linear_mod</span> <span class="o">=</span> <span class="n">basic</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
          <span class="n">name</span><span class="o">=</span><span class="s2">&quot;fc_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">),</span>
          <span class="n">output_size</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
          <span class="n">initializers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span><span class="p">,</span>
          <span class="n">partitioners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span><span class="p">)</span>

      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_connected</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_linear_modules</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">linear_mod</span><span class="p">)</span>

      <span class="n">net</span> <span class="o">=</span> <span class="n">linear_mod</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_batch_norm</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bn_on_fc_layers</span><span class="p">:</span>
        <span class="n">bn</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_norm_config</span><span class="p">)</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">bn</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="n">test_local_stats</span><span class="p">)</span>

      <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">keep_prob</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="n">keep_prob</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">net</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">initializers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">partitioners</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partitioners</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">regularizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_regularizers</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">min_input_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns integer specifying the minimum width and height for the input.</span>

<span class="sd">    Note that the input can be non-square, but both the width and height must</span>
<span class="sd">    be &gt;= this number in size.</span>

<span class="sd">    Returns:</span>
<span class="sd">      The minimum size as an integer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_min_size</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">conv_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns list containing convolutional modules of network.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list containing the Conv2D modules.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv_modules</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">linear_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns list containing linear modules of network.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list containing the Linear modules.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_ensure_is_connected</span><span class="p">()</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear_modules</span></div>


<div class="viewcode-block" id="AlexNetFull"><a class="viewcode-back" href="../../../../../api/sonnet.python.modules.nets.html#sonnet.python.modules.nets.alexnet.AlexNetFull">[docs]</a><span class="k">class</span> <span class="nc">AlexNetFull</span><span class="p">(</span><span class="n">AlexNet</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;AlexNet constructed in the &#39;FULL&#39; mode.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">batch_norm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;alex_net_full&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs AlexNet.</span>

<span class="sd">    Args:</span>
<span class="sd">      use_batch_norm: Whether to use batch normalization between the output of</span>
<span class="sd">          a layer and the activation function.</span>
<span class="sd">      batch_norm_config: Optional mapping of additional configuration for the</span>
<span class="sd">          `snt.BatchNorm` modules.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">          key &#39;w&#39;) or biases (with key &#39;b&#39;). The default initializers are</span>
<span class="sd">          truncated normal initializers, which are commonly used when the inputs</span>
<span class="sd">          are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf).</span>
<span class="sd">      partitioners: Optional dict containing partitioners for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contains any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AlexNetFull</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">FULL</span><span class="p">,</span>
        <span class="n">use_batch_norm</span><span class="o">=</span><span class="n">use_batch_norm</span><span class="p">,</span>
        <span class="n">batch_norm_config</span><span class="o">=</span><span class="n">batch_norm_config</span><span class="p">,</span>
        <span class="n">initializers</span><span class="o">=</span><span class="n">initializers</span><span class="p">,</span>
        <span class="n">partitioners</span><span class="o">=</span><span class="n">partitioners</span><span class="p">,</span>
        <span class="n">regularizers</span><span class="o">=</span><span class="n">regularizers</span><span class="p">,</span>
        <span class="n">bn_on_fc_layers</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="AlexNetMini"><a class="viewcode-back" href="../../../../../api/sonnet.python.modules.nets.html#sonnet.python.modules.nets.alexnet.AlexNetMini">[docs]</a><span class="k">class</span> <span class="nc">AlexNetMini</span><span class="p">(</span><span class="n">AlexNet</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;AlexNet constructed in the &#39;MINI&#39; mode.&quot;&quot;&quot;</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">use_batch_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">batch_norm_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">partitioners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">regularizers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">custom_getter</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s2">&quot;alex_net_mini&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Constructs AlexNet.</span>

<span class="sd">    Args:</span>
<span class="sd">      use_batch_norm: Whether to use batch normalization between the output of</span>
<span class="sd">          a layer and the activation function.</span>
<span class="sd">      batch_norm_config: Optional mapping of additional configuration for the</span>
<span class="sd">          `snt.BatchNorm` modules.</span>
<span class="sd">      initializers: Optional dict containing ops to initialize the filters (with</span>
<span class="sd">          key &#39;w&#39;) or biases (with key &#39;b&#39;). The default initializers are</span>
<span class="sd">          truncated normal initializers, which are commonly used when the inputs</span>
<span class="sd">          are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf).</span>
<span class="sd">      partitioners: Optional dict containing partitioners for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        partitioners are used.</span>
<span class="sd">      regularizers: Optional dict containing regularizers for the filters</span>
<span class="sd">        (with key &#39;w&#39;) and the biases (with key &#39;b&#39;). As a default, no</span>
<span class="sd">        regularizers are used. A regularizer should be a function that takes</span>
<span class="sd">        a single `Tensor` as an input and returns a scalar `Tensor` output, e.g.</span>
<span class="sd">        the L1 and L2 regularizers in `tf.contrib.layers`.</span>
<span class="sd">      custom_getter: Callable or dictionary of callables to use as</span>
<span class="sd">        custom getters inside the module. If a dictionary, the keys</span>
<span class="sd">        correspond to regexes to match variable names. See the `tf.get_variable`</span>
<span class="sd">        documentation for information about the custom_getter API.</span>
<span class="sd">      name: Name of the module.</span>

<span class="sd">    Raises:</span>
<span class="sd">      KeyError: If `initializers`, `partitioners` or `regularizers` contains any</span>
<span class="sd">        keys other than &#39;w&#39; or &#39;b&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AlexNetMini</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">MINI</span><span class="p">,</span>
        <span class="n">use_batch_norm</span><span class="o">=</span><span class="n">use_batch_norm</span><span class="p">,</span>
        <span class="n">batch_norm_config</span><span class="o">=</span><span class="n">batch_norm_config</span><span class="p">,</span>
        <span class="n">initializers</span><span class="o">=</span><span class="n">initializers</span><span class="p">,</span>
        <span class="n">partitioners</span><span class="o">=</span><span class="n">partitioners</span><span class="p">,</span>
        <span class="n">regularizers</span><span class="o">=</span><span class="n">regularizers</span><span class="p">,</span>
        <span class="n">bn_on_fc_layers</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">custom_getter</span><span class="o">=</span><span class="n">custom_getter</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../index.html">Documentation overview</a><ul>
  <li><a href="../../../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Sonnet Authors.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
    </div>

    

    
  </body>
</html>