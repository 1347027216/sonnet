{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Sonnet is a library built on top of TensorFlow for building complex neural networks. Usage Example The following code constructs a Linear module and connects it to multiple inputs. The variables (i.e., the weights and biases of the linear transformation) are automatically shared. import sonnet as snt # Provide your own functions to generate data Tensors. train_data = get_training_data() test_data = get_test_data() # Construct the module, providing any configuration necessary. linear_regression_module = snt.Linear(output_size=FLAGS.output_size) # Connect the module to some inputs, any number of times. train_predictions = linear_regression_module(train_data) test_predictions = linear_regression_module(test_data) More usage examples: Generative models RNNs General Principles The main principle of Sonnet is to first construct Python objects which represent some part of a neural network, and then separately connect these objects into the TensorFlow computation graph. The objects are subclasses of sonnet.AbstractModule and as such are referred to as Modules . Modules may be connected into the graph multiple times, and any variables declared in that module will be automatically shared on subsequent connection calls. Low level aspects of TensorFlow which control variable sharing, including specifying variable scope names, and using the reuse= flag, are abstracted away from the user. Separating configuration and connection allows easy construction of higher-order Modules, i.e., modules that wrap other modules. For instance, the BatchApply module merges a number of leading dimensions of a tensor into a single dimension, connects a provided module, and then splits the leading dimension of the result to match the input. At construction time, the inner module is passed in as an argument to the BatchApply constructor. At run time, the module first performs a reshape operation on inputs, then applies the module passed into the constructor, and then inverts the reshape operation. An additional advantage of representing Modules by Python objects is that it allows additional methods to be defined where necessary. An example of this is a module which, after construction, may be connected in a variety of ways while maintaining weight sharing. For instance, in the case of a generative model, we may want to sample from the model, or calculate the log probability of a given observation. Having both connections simultaneously requires weight sharing, and so these methods depend on the same variables. The variables are conceptually owned by the object, and are used by different methods of the module. Importing Sonnet The recommended way to import Sonnet is to alias it to a variable named snt : import sonnet as snt Every module is then accessible under the namespace snt , and the rest of this document will use snt for brevity. The following code constructs a module that is composed of other modules: import sonnet as snt # Our data is coming in via multiple inputs, so to apply the same model to each # we will need to use variable sharing. train_data = get_training_data() test_data = get_test_data() # Make two linear modules, to form a Multi Layer Perceptron. Override the # default names (which would end up being 'linear', 'linear_1') to provide # interpretable variable names in TensorBoard / other tools. lin_to_hidden = snt.Linear(output_size=FLAGS.hidden_size, name='inp_to_hidden') hidden_to_out = snt.Linear(output_size=FLAGS.output_size, name='hidden_to_out') # Sequential is a module which applies a number of inner modules or ops in # sequence to the provided data. Note that raw TF ops such as tanh can be # used interchangeably with constructed modules, as they contain no variables. mlp = snt.Sequential([lin_to_hidden, tf.sigmoid, hidden_to_out]) # Connect the sequential into the graph, any number of times. train_predictions = mlp(train_data) test_predictions = mlp(test_data) The following code adds initializers and regularizers to a Linear module: import sonnet as snt train_data = get_training_data() test_data = get_test_data() # Initializers and regularizers for the weights and the biasses. initializers={ w : tf.truncated_normal_initializer(stddev=1.0), b : tf.truncated_normal_initializer(stddev=1.0)} regularizers = { w : tf.contrib.layers.l1_regularizer(scale=0.1), b : tf.contrib.layers.l2_regularizer(scale=0.1)} linear_regression_module = snt.Linear(output_size=FLAGS.output_size, initializers=initializers, regularizers=regularizers) # Connect the module to some inputs, any number of times. train_predictions = linear_regression_module(train_data) test_predictions = linear_regression_module(test_data) # ... # Get the regularization losses and add them together. graph_regularizers = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) total_regularization_loss = tf.reduce_sum(graph_regularizers) # ... # When minimizing the loss, minimize also the regularization loss. train_op = optimizer.minimize(loss + total_regularizer_loss) Defining your own modules Inherit from snt.AbstractModule To define a module, create a new class which inherits from snt.AbstractModule . The constructor of your class should accept any configuration which defines the operation of that module, and store it in a member variable prefixed with an underscore, to indicate that it is private. Call superclass constructor The first thing the constructor does should be to call the superclass constructor, passing in the name for the module - if you forget to do this, the variable sharing will break. A name kwarg should always be provided as the final one of the list, with the default value being a snake_case version of the class name. class MyMLP(snt.AbstractModule): Docstring for MyMLP. def __init__(self, hidden_size, output_size, nonlinearity=tf.tanh, name= my_mlp ): Docstring explaining __init__ args, including types and defaults. super(MyMLP, self).__init__(name=name) self._hidden_size = hidden_size self._output_size = output_size self._nonlinearity = nonlinearity Implement _build() method The only other method implementation which must be provided is _build() . This will be called whenever the module is connected into the tf.Graph . It receives some input, which may be empty, a single Tensor, or some arbitrary structure containing multiple Tensors. Multiple Tensors can be provided with either a tuple or namedtuple, the elements of which in turn can be Tensors or further tuples / namedtuples. Most input Tensors require a batch dimension, and if a Tensor has a color channel then it must be the last dimension. While in many cases the library will not explicitly prevent you, the use of lists and dicts is not supported, as the mutability of these structures can lead to subtle bugs. # Following on from code snippet above.. def _build(self, inputs): Compute output Tensor from input Tensor. lin_x_to_h = snt.Linear(output_size=self._hidden_size, name= x_to_h ) lin_h_to_o = snt.Linear(output_size=self._output_size, name= h_to_o ) return lin_h_to_o(self._nonlinearity(lin_x_to_h(inputs))) The _build method may include any or all of the following processes: Construct and use internal modules Use modules which already exist, and were passed into the constructor Create variables directly. If you create variables yourself, it is crucial to create them with tf.get_variable . Calling the tf.Variable constructor directly will only work the first time the module is connected, but on the second call you will receive an error message \"Trainable variable created when calling a template after the first time\". The modules in the above example are created separately, passing in various configurations, and then the final line connects them all into the graph. The return line should be read from right to left - the inputs Tensor is passed into the first Linear, lin_x_to_h , the output of which is passed into whatever nonlinearity was stored in the constructor, the output of which goes through another Linear to produce the result. Note that we give short meaningful names to the internal Linear instances. Note that the nonlinearity above can be either a raw TF op, eg tf.tanh or tf.sigmoid , or an instance of a Sonnet module. In keeping with Python standards, we may choose to not check this explicitly, and so we may receive an error when _build is called. It is also acceptable to add constraints and sanity checking inside __init__ . Note that in the above code, new instances of snt.Linear are generated each time _build() is called, and you may think this will create different, unshared variables. This is not the case - only 4 variables (2 for each Linear ) will be created, no matter how many times the MLP instance is connected into the graph. How this is works is a low level TF detail, and subject to change - see tf.variable_op_scope for details. Where should the submodules be declared? Note that modules may use other modules which they receive already externally constructed - eg Sequential etc. The submodules we discuss in this section are any Modules which are constructed inside the code of another Module, which we will refer to as the Parent Module. An example is an LSTM, where most implementations will internally construct one or more Linear modules to contain the weights. It's recommended that submodules are created in _build() . Doing it this way means you get the correct nesting of variable scopes, eg: class ParentModule(snt.AbstractModule): def __init__(self, hidden_size, name= parent_module ): super(ParentModule, self).__init__(name=name) self._hidden_size = hidden_size def _build(self, inputs): lin_mod = snt.Linear(self._hidden_size) # Construct submodule... return tf.relu(lin_mod(inputs)) # then connect it. The variables created by the Linear will have a name something like parent_module/linear/w , which is what you probably want in this kind of situation. Some users prefer for practical or stylistic reasons to construct everything in the constructor, before anything is used. This is fine, but for proper variable nesting any submodules must be constructed inside a self._enter_variable_scope call . class OtherParentModule(snt.AbstractModule): def __init__(self, hidden_size, name= other_parent_module ): super(OtherParentModule, self).__init__(name=name) self._hidden_size = hidden_size with self._enter_variable_scope(): # This line is crucial! self._lin_mod = snt.Linear(self._hidden_size) # Construct submodule here. def _build(self, inputs): return tf.relu(self._lin_mod(inputs)) # Connect previously constructed mod. The above example is fine, and will have the same variable names etc. Different people prefer different styles and both of the above are considered correct. The pitfall here is forgetting to call self._enter_variable_scope() . Things will still \"work\" but the scopes will not be nested as you might expected: class WrongModule(snt.AbstractModule): def __init__(self, hidden_size, name= wrong_module ): super(WrongModule, self).__init__(name=name) self._hidden_size = hidden_size self._lin_mod = snt.Linear(self._hidden_size) # Construct submodule here. def _build(self, inputs): return tf.relu(self._lin_mod(inputs)) # Connect previously constructed mod. The above example works okay in terms of the resulting network's calculations, but is considered a bug due to the resulting flat (instead of hierarchical) variable namespace. The variables in the linear will be called \"linear/w\" which is completely disjoint from the \"wrong_module\" namespace. Recurrent Modules Usage Sonnet includes recurrent core modules (also called \"cells\" in TensorFlow terminology), which perform one time step of computation. These are ready to be unrolled in time using TensorFlow's unrolling operations . One example of an LSTM that is unrolled in time is the following: hidden_size = 5 batch_size = 20 # input_sequence should be a tensor of size # [time_steps, batch_size, input_features] input_sequence = ... lstm = snt.LSTM(hidden_size) initial_state = lstm.initial_state(batch_size) output_sequence, final_state = tf.nn.dynamic_rnn( lstm, input_sequence, initial_state=initial_state, time_major=True) The batch_size parameter passed to the initial_state() method can also be an int32 Tensor. For a more comprehensive demonstration on the usage of recurrent modules, a fully-documented example of a deep LSTM with skip connections trained on the Shakespeare dataset is available. Defining your own recurrent modules A recurrent module is any subclass of snt.RNNCore , which inherits from snt.AbstractModule and has an interface compatible with tf.nn.rnn_cell.RNNCell . This allows us to use the variable sharing model from Sonnet whilst also using the cores inside TensorFlow's RNN Containers. class Add1RNN(snt.RNNCore): Simple core that adds 1 to its state and produces zero outputs. This core computes the following: (`input`, (`state1`, `state2`)) - (`output`, (`next_state1`, `next_state2`)) where all the elements are tensors, next_statei` = `statei` + 1, and `output` = 0. All the outputs (`state` and `output`) are of size (`batch_size`, `hidden_size`), where `hidden_size` is a size that is specified in the constructor. def __init__(self, hidden_size, name= add1_rnn ): Constructor of the module. Args: hidden_size: an int, size of the outputs of the module (without batch size). name: the name of the module. super(Add1RNN, self).__init__(name=name) self._hidden_size = hidden_size def _build(self, inputs, state): Builds a TF subgraph that performs one timestep of computation. batch_size = tf.TensorShape([inputs.get_shape()[0]]) outputs = tf.zeros(shape=batch_size.concatenate(self.output_size)) state1, state2 = state next_state = (state1 + 1, state2 + 1) return outputs, next_state @property def state_size(self): Returns a description of the state size, without batch dimension. return (tf.TensorShape([self._hidden_size]), tf.TensorShape([self._hidden_size])) @property def output_size(self): Returns a description of the output size, without batch dimension. return tf.TensorShape([self._hidden_size]) def initial_state(self, batch_size, dtype): Returns an initial state with zeros, for a batch size and data type. NOTE: This method is here only for illustrative purposes, the corresponding method in its superclass should be already doing this. sz1, sz2 = self.state_size # Prepend batch size to the state shape, and create zeros. return (tf.zeros([batch_size] + sz1.as_list(), dtype=dtype), tf.zeros([batch_size] + sz2.as_list(), dtype=dtype)) Apart from the _build method from snt.AbstractModule , a recurrent module must also implement the state_size and output_size properties, which provide the expected size of the recurrent state, and an example of it, respectively. snt.RNNCore defines a initial_state method that can be used to generate a zero initial state or a trainable initial state (based on the aforementioned properties). Optionally, any recurrent module can define its own initial_state method. Note that a zero_state method is also available (as in tf.nn.rnn_cell.RNNCell ) to produce a correctly sized state value filled with zeros. In some situations (LSTM, etc) it may be acceptable to begin with a state containing all zeros, but in other situations this is too limiting, and we may want to (eg) fill some part of the state with random noise. A common option is to make the initial state of an RNN trainable, meaning the state is produced from some tf.Variable s which are trained via backpropagation. If a core supports this, it should provide kwargs trainable= and name= for initial_state() . The name= kwarg can be used to provide a prefix for the (potentially multiple) variable name(s) which will be created. The Transposable interface Sonnet defines an interface for modules supporting transposition , called snt.Transposable . Transposition is a flexible concept (e.g. not necessarily related to matrix transposition as defined in algebra), and in this context it entails the definition of a new module with attributes which are somehow related to the original module, without strictly implying any form of variable sharing. For example, given a snt.Linear which maps from input size A to output size B , via transposition we will return another snt.Linear module whose weight matrix shape is the transpose of the original one, thus mapping from input size B to output size A ; given a snt.Conv2D module we will return a matching snt.Conv2DTranspose module. The snt.Transposable interface requires that transposable modules implement a method called transpose , returning a module which is the transposed version of the one the method is called on. Whilst not enforced by Sonnet, chaining the method twice should be expected to return a module with the same specifications as the original module. When implementing a transposable module, special care is required to ensure that parameters needed to instantiate the module are provided as functions whose evaluation is deferred to graph construction time. This mechanism allows for transposed modules to be instantiated before the original module is connected to the graph. An example of this behavior can be found in snt.Linear , where the output_size argument of the transposed module is defined as a lambda returning the input_shape property of the original module; upon evaluation input_shape will raise an error unless the module has not been connected to the graph, but this is not an issue since the lambda is not called until the transposed module is connected to the graph. Variable reuse with @snt.reuse_variables Some use cases require a tf.VariableScope to be shared across multiple methods, which isn't possible with snt.AbstractModule . For example, a generative model may define a sample() and log_pdf() method that share parts of the same tf.Graph . Adding the @snt.reuse_variables decorator to a method will enable variable reuse in much the same manner as _build() . The most notable difference is that a single tf.VariableScope will be used across different decorated methods and each decorated method has its own reuse flag that is used to enter the variable scope. class Reusable(object): def __init__(self, name): with tf.variable_scope(None, default_name=name) as vs: self.variable_scope = vs @snt.reuse_variables def reusable_var(self): return tf.get_variable( a , shape=[1]) obj = Reusable( reusable ) a1 = obj.reusable_var() a2 = obj.reusable_var() # a1 == a2 class NaiveAutoEncoder(snt.AbstractModule): def __init__(self, n_latent, n_out, name= naive_auto_encoder ): super(NaiveAutoEncoder, self).__init__(name=name) self._n_latent = n_latent self._n_out = n_out @snt.reuse_variables def encode(self, input): Builds the front half of AutoEncoder, inputs - latents. w_enc = tf.get_variable( w_enc , shape=[self._n_out, self._n_latent]) b_enc = tf.get_variable( b_enc , shape=[self._n_latent]) return tf.sigmoid(tf.matmul(input, w_enc) + b_enc) @snt.reuse_variables def decode(self, latents): Builds the back half of AutoEncoder, latents - reconstruction. w_rec = tf.get_variable( w_dec , shape=[self._n_latent, self._n_out]) b_rec = tf.get_variable( b_dec , shape=[self._n_out]) return tf.sigmoid(tf.matmul(latents, w_rec) + b_rec) def _build(self, input): Builds the 'full' AutoEncoder, ie input - latents - reconstruction. latents = self.encode(input) return self.decode(latents) batch_size = 5 n_in = 10 n_out = n_in n_latent = 2 nae = NaiveAutoEncoder(n_latent=n_latent, n_out=n_out) inputs = tf.placeholder(tf.float32, shape=[batch_size, n_in]) latents = tf.placeholder(tf.float32, shape=[batch_size, n_latent]) # Connecting the default way calls build(), producing 'full' AutoEncoder. reconstructed_from_input = nae(inputs) # Connecting with one of the other methods might only require some subset of the # variables, but sharing will still work. reconstructed_from_latent = nae.decode(latents) In the above example, any variables created by nae.encode() , nae.decode() or nae() exist in the same tf.VariableScope . In addition, since each decorated method has its own reuse flag we don't need to worry about having to create all variables on the first method call, or about the calling order at all. We can even nest (decorated) methods within other (decorated) methods - the reuse flag is always set correctly since the variable scope is re-entered for every method call. However every decorated method must be the sole owner of its variables. For example, if we use tf.get_variable(\"w_dec\", ...) inside NaiveAutoEncoder.encode , this will not do variable sharing. Instead, TensorFlow will treat tf.get_variable(\"w_dec\", ...) in NaiveAutoEncoder.enodec and NaiveAutoEncoder.decode() as separate variables and an error will occur in either obj.a() or obj.build() (whichever is called second). See below for an example of bad variable reuse: class BadReusable(object): def __init__(self, name): with tf.variable_scope(None, default_name=name) as vs: self.variable_scope = vs @snt.reuse_variables def reusable_var(self): return tf.get_variable( a , shape=[1]) @snt.reuse_variables def another_reusable_var(self): return tf.get_variable( a , shape=[1]) obj = BadReusable( bad_reusable ) obj.reusable_var() obj.another_reusable_var() # Raises a ValueError because `reuse=False` Wrapping functions into Sonnet modules using snt.Module Whilst the recommended way of defining new Sonnet modules is to inherit from snt.AbstractModule , the library also offers an alternative route to succinctly instantiate modules wrapping user-provided functions. The snt.Module class constructor takes a callable and returns a Sonnet module. The provided function is invoked when the module is called, thus specifying how new nodes are added to the computational graph and how to compute output Tensors from input Tensors. Please refer to the module documentation for more details and examples. FAQ Q: Why another TF library? A: The existing libraries were judged insufficiently flexible for the DeepMind use case where extensive use is made of weight sharing. Making everything use tf.make_template , and therefore support weight sharing from the start, seemed to have sufficient benefits to outweight the development cost. The paradigm of separating configuration from connection also allows easy composability of modules. Q: Can I access different variables on subsequent calls to the same build()? A: No. This is enforced by tf.make_template , which considers it an error to access different / extra variables on subsequent calls. Q: What if I mistakenly give two modules the same name? A: Modules which appear to be constructed with the same name will have distinct names, and variable scopes. Under the hood, Sonnet uses tf.make_template which essentially wraps a python function together with some tf.VariableScope , ensuring that every call to the function happens in the same scope, and that all calls after the first are set to reuse variables. One feature of the templating is that it will uniquify any provided names, if they have already been entered in the same scope. For example: lin_1 = snt.Linear(output_size=42, name= linear ) lin_2 = snt.Linear(output_size=84, name= linear ) # this name is already taken. print(lin_1.name) # prints linear print(lin_2.name) # prints linear_1 - automatically uniquified. Note that the .name property is available to see the \"post-uniquification\" name. Q: Do I have to name my modules? A: No. Modules have a default name, which should be the class name in snake_case , and that will be used as the name with uniquification (see above) if necessary. However, we recommend providing a name for modules which contain variables, as the name provided becomes the name of the internal scope, and thus defines the variable names. Most modules are written to declare internal weights with names like \"w\" and \"b\" for weights and bias - it's vastly preferable to do have a list of weights like: sdae/encoder_linear/w sdae/encoder_linear/b sdae/decoder_linear/w sdae/decoder_linear/b rather than: sdae/linear/w sdae/linear/b sdae/linear_1/w sdae/linear_1/b The names you choose will appear in TensorBoard. Q: How do I find out what variables are declared by a module? A: You can query a module to find out all the variables in its scope using the get_variables() method. Note that this will throw an error if the module is not connected into the graph, as the variables do not exist at that point so the relevant scope will be empty. Q: Should I be putting calls to variable_scope in my code? A: Every module implicitly creates an internal variable_scope, which it re-enters each time it connects to the graph. Assuming all the variables in your model are inside Sonnet modules, it is not necessary to use scopes yourself. Q: Can I mix this with raw TF ops? A: Yes. An op which doesn't declare variables internally, and so is effectively a pure function, can be used inside module _build implementations, and also to plumb together values between modules. Q: Should everything in Sonnet be implemented as a Module? No, computations which do not create tf.Variable s and do not store internal configurations can be implemented in the regular TF Op style, ie a python function that receives input tensors, keyword arguments, and returns tensor outputs. If an op is going to create variables (ie call tf.get_variable anywhere internally, including indirectly) it must be implemented as a subclass of snt.AbstractModule so that variable sharing is correctly handled. Note that if a computation doesn't create any Variables, it may still be desirable to implement it with a Module instead of an Op. Aside from variable sharing, it may be convenient to use Sonnet Modules in cases where we wish to attach configuration parameters to an op. An example of this is the content addressing modules in the Differentiable Neural Computer. These modules receive a number of configuration parameters (size of each word in memory, number of read heads) and some function of these inputs defines what the valid input size is. We use a snt.Linear of the correct output size before this module, in order to provide the correct dimensionality. As a module this is easy - provide the configuration at construction time, then a method .param_size() which gives the required input dimensionality. We can then make the correct size of input tensor and perform the connection. class CosineWeights(snt.AbstractModule): def __init__(self, word_size, num_heads, name= cosine_weights ): super(CosineWeights, self).__init__(name=name) self._word_size = word_size self._num_heads = num_heads def param_size(self): Returns the size the 2nd dimension of `cos_params` is required to be. return self._num_heads * (1 + self._word_size) def _build(self, memory, cos_params): cos_params must be `[batch_size, self.param_size()]` shape # ... # Construct the module, then work out the right input size cos_weights_mod = CosineWeights(word_size=32, num_heads=3) cosine_params_mod = snt.Linear(output_size=cos_weights_mod.param_size()) cos_params = cosine_params_mod(inputs) # We know this is now the right shape. weights = cos_weights_mode(memory, cos_params) If the above was implemented as an op cosine_weights(memory, cos_params, word_size, num_heads) then the logic to indicate the desired size of cos_params would have to be stored in a separate function. Encapsulating the related functions into one module results in cleaner code. Another example of where this flexibility is useful is when an Op has a large number of arguments which are conceptually configuration, along with some which are conceptually inputs. We often want to use the same configuration in multiple places, for different inputs, and so writing a Module which can be constructed with the configuration and then passed around may be useful. import functools # 1. Define our computation as some op def useful_op(input_a, input_b, use_clipping=True, remove_nans=False, solve_agi='maybe'): # ... # 2a). Set the configuration parameters with functools, then pass around. useful_op_configured = functools.partial( useful_op, use_clipping=False, remove_nans=True, solve_agi='definitely') do_something_a(... , inner_op=useful_op_configured) do_something_else_a(..., inner_op=useful_op_configured) # 2b). OR, set the configuration by creating kwargs and pass around both. op_kwargs = { 'use_clipping': False, 'remove_nans': True, 'solve_agi': 'definitely', } do_something_b(..., inner_op=useful_op, inner_op_kwargs=op_kwargs) do_something_else_b(..., inner_op=useful_op, inner_op_kwargs=op_kwargs) Either of the above approaches is valid, but many users dislike the style of using functools or needing to pass both the Op and a dictionary around. In which case, rewriting the Op as a Module can be a nice solution - in particular, the difference between configuration parameters and inputs from the Graph are now made explicit: class UsefulModule(snt.AbstractModule): def __init__(self, use_clipping=True, remove_nans=False, solve_agi='maybe', name='useful_module') super(UsefulModule, self).__init__(name=name) self._use_clipping = use_clipping self._remove_nans = remove_nans self._solve_agi = solve_agi def _build(self, input_a, input_b): #... Q: Can I mix this with other high level TF APIs, eg TF Slim? A: Sonnet modules, once constructed, follow the Tensor-In-Tensor-Out principle, so can be mixed with functions from TF-Slim, etc. Note that this may lead to unexpected behaviour - TF-Slim controls sharing by passing explicit scope= and reuse= kwargs into the layer functions - if you use a TF-Slim layer inside the _build() method of a Sonnet module, then calling it multiple times is not likely to work correctly. Q: Shouldn't I be overriding __call__ in modules? A: No. AbstractModule.__init__ provides an implementation of __call__ , which calls an internal function wrapped in a Template, which in turn wraps the _build method. Overriding __call__ yourself will likely break variable sharing. Q: What is the overhead of using Sonnet vs other libraries vs raw TF? A: None. Sonnet is only involved when constructing the computation graph. Once you are at the stage of using Session.run() you are simply executing Ops, without regard for what library was used to put that graph together. Q: How do I list all the variables which are used in any way in a Module? A: You can use get_all_variables() to find all the variables that a module or any of its submodules have created with tf.get_variable() . Like get_variables() this returns all variables that are inside of the module's (variable) scope. However, get_all_variables() also returns all of the variables from any submodules with disjoint (variable) scopes. These submodules have either been passed into the module's constructor, or have been constructed by the module but outside of _build() or _enter_variable_scope() . Note that by definition this will not return variables that have not been created by tf.get_variable() . This is relevant for modules that use @snt.reuse_variables . If a method decorated with @snt.reuse_variable is not called then get_all_variables() will not return any variables used inside of it. Note that by definition this returns all of a module's variables. This means that a module will return all its submodule's variables, even if it only uses a subset of the submodule's variables (ie. it does not call a method decorated by @snt.reuse_variables on the submodule). Q: How do I serialize Sonnet module instances? A: We do not support serializing module instances, via pickle or any other method. Modules contain inherently non-serializable artifacts, such as references to Variable s and Graph s. To save a model, you should instead serialize the configuration information which will allow you to regenerate the Graph, such as the name and constructor parameters of some top level module. This avoids any complications with adding new members to modules, and then having to deal with old serialized instances where those members don't exist. Q: How can I cite Sonnet? A: Please use the following BibTeX: @misc{sonnetblog, title= {Open sourcing Sonnet - a new library for constructing neural networks} , author={Reynolds, Malcolm and Barth-Maron, Gabriel and Besse, Frederic and de Las Casas, Diego and Fidjeland, Andreas and Green, Tim and Puigdom{\\`e}nech, Adri{\\`a} and Racani{\\`e}re, S{\\'e}bastien and Rae, Jack and Viola, Fabio}, howpublished={\\url{https://deepmind.com/blog/open-sourcing-sonnet/}}, year={2017} }","title":"Sonnet"},{"location":"#usage-example","text":"The following code constructs a Linear module and connects it to multiple inputs. The variables (i.e., the weights and biases of the linear transformation) are automatically shared. import sonnet as snt # Provide your own functions to generate data Tensors. train_data = get_training_data() test_data = get_test_data() # Construct the module, providing any configuration necessary. linear_regression_module = snt.Linear(output_size=FLAGS.output_size) # Connect the module to some inputs, any number of times. train_predictions = linear_regression_module(train_data) test_predictions = linear_regression_module(test_data) More usage examples: Generative models RNNs","title":"Usage Example"},{"location":"#general-principles","text":"The main principle of Sonnet is to first construct Python objects which represent some part of a neural network, and then separately connect these objects into the TensorFlow computation graph. The objects are subclasses of sonnet.AbstractModule and as such are referred to as Modules . Modules may be connected into the graph multiple times, and any variables declared in that module will be automatically shared on subsequent connection calls. Low level aspects of TensorFlow which control variable sharing, including specifying variable scope names, and using the reuse= flag, are abstracted away from the user. Separating configuration and connection allows easy construction of higher-order Modules, i.e., modules that wrap other modules. For instance, the BatchApply module merges a number of leading dimensions of a tensor into a single dimension, connects a provided module, and then splits the leading dimension of the result to match the input. At construction time, the inner module is passed in as an argument to the BatchApply constructor. At run time, the module first performs a reshape operation on inputs, then applies the module passed into the constructor, and then inverts the reshape operation. An additional advantage of representing Modules by Python objects is that it allows additional methods to be defined where necessary. An example of this is a module which, after construction, may be connected in a variety of ways while maintaining weight sharing. For instance, in the case of a generative model, we may want to sample from the model, or calculate the log probability of a given observation. Having both connections simultaneously requires weight sharing, and so these methods depend on the same variables. The variables are conceptually owned by the object, and are used by different methods of the module.","title":"General Principles"},{"location":"#importing-sonnet","text":"The recommended way to import Sonnet is to alias it to a variable named snt : import sonnet as snt Every module is then accessible under the namespace snt , and the rest of this document will use snt for brevity. The following code constructs a module that is composed of other modules: import sonnet as snt # Our data is coming in via multiple inputs, so to apply the same model to each # we will need to use variable sharing. train_data = get_training_data() test_data = get_test_data() # Make two linear modules, to form a Multi Layer Perceptron. Override the # default names (which would end up being 'linear', 'linear_1') to provide # interpretable variable names in TensorBoard / other tools. lin_to_hidden = snt.Linear(output_size=FLAGS.hidden_size, name='inp_to_hidden') hidden_to_out = snt.Linear(output_size=FLAGS.output_size, name='hidden_to_out') # Sequential is a module which applies a number of inner modules or ops in # sequence to the provided data. Note that raw TF ops such as tanh can be # used interchangeably with constructed modules, as they contain no variables. mlp = snt.Sequential([lin_to_hidden, tf.sigmoid, hidden_to_out]) # Connect the sequential into the graph, any number of times. train_predictions = mlp(train_data) test_predictions = mlp(test_data) The following code adds initializers and regularizers to a Linear module: import sonnet as snt train_data = get_training_data() test_data = get_test_data() # Initializers and regularizers for the weights and the biasses. initializers={ w : tf.truncated_normal_initializer(stddev=1.0), b : tf.truncated_normal_initializer(stddev=1.0)} regularizers = { w : tf.contrib.layers.l1_regularizer(scale=0.1), b : tf.contrib.layers.l2_regularizer(scale=0.1)} linear_regression_module = snt.Linear(output_size=FLAGS.output_size, initializers=initializers, regularizers=regularizers) # Connect the module to some inputs, any number of times. train_predictions = linear_regression_module(train_data) test_predictions = linear_regression_module(test_data) # ... # Get the regularization losses and add them together. graph_regularizers = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) total_regularization_loss = tf.reduce_sum(graph_regularizers) # ... # When minimizing the loss, minimize also the regularization loss. train_op = optimizer.minimize(loss + total_regularizer_loss)","title":"Importing Sonnet"},{"location":"#defining-your-own-modules","text":"","title":"Defining your own modules"},{"location":"#inherit-from-sntabstractmodule","text":"To define a module, create a new class which inherits from snt.AbstractModule . The constructor of your class should accept any configuration which defines the operation of that module, and store it in a member variable prefixed with an underscore, to indicate that it is private.","title":"Inherit from snt.AbstractModule"},{"location":"#call-superclass-constructor","text":"The first thing the constructor does should be to call the superclass constructor, passing in the name for the module - if you forget to do this, the variable sharing will break. A name kwarg should always be provided as the final one of the list, with the default value being a snake_case version of the class name. class MyMLP(snt.AbstractModule): Docstring for MyMLP. def __init__(self, hidden_size, output_size, nonlinearity=tf.tanh, name= my_mlp ): Docstring explaining __init__ args, including types and defaults. super(MyMLP, self).__init__(name=name) self._hidden_size = hidden_size self._output_size = output_size self._nonlinearity = nonlinearity","title":"Call superclass constructor"},{"location":"#implement-_build-method","text":"The only other method implementation which must be provided is _build() . This will be called whenever the module is connected into the tf.Graph . It receives some input, which may be empty, a single Tensor, or some arbitrary structure containing multiple Tensors. Multiple Tensors can be provided with either a tuple or namedtuple, the elements of which in turn can be Tensors or further tuples / namedtuples. Most input Tensors require a batch dimension, and if a Tensor has a color channel then it must be the last dimension. While in many cases the library will not explicitly prevent you, the use of lists and dicts is not supported, as the mutability of these structures can lead to subtle bugs. # Following on from code snippet above.. def _build(self, inputs): Compute output Tensor from input Tensor. lin_x_to_h = snt.Linear(output_size=self._hidden_size, name= x_to_h ) lin_h_to_o = snt.Linear(output_size=self._output_size, name= h_to_o ) return lin_h_to_o(self._nonlinearity(lin_x_to_h(inputs))) The _build method may include any or all of the following processes: Construct and use internal modules Use modules which already exist, and were passed into the constructor Create variables directly. If you create variables yourself, it is crucial to create them with tf.get_variable . Calling the tf.Variable constructor directly will only work the first time the module is connected, but on the second call you will receive an error message \"Trainable variable created when calling a template after the first time\". The modules in the above example are created separately, passing in various configurations, and then the final line connects them all into the graph. The return line should be read from right to left - the inputs Tensor is passed into the first Linear, lin_x_to_h , the output of which is passed into whatever nonlinearity was stored in the constructor, the output of which goes through another Linear to produce the result. Note that we give short meaningful names to the internal Linear instances. Note that the nonlinearity above can be either a raw TF op, eg tf.tanh or tf.sigmoid , or an instance of a Sonnet module. In keeping with Python standards, we may choose to not check this explicitly, and so we may receive an error when _build is called. It is also acceptable to add constraints and sanity checking inside __init__ . Note that in the above code, new instances of snt.Linear are generated each time _build() is called, and you may think this will create different, unshared variables. This is not the case - only 4 variables (2 for each Linear ) will be created, no matter how many times the MLP instance is connected into the graph. How this is works is a low level TF detail, and subject to change - see tf.variable_op_scope for details.","title":"Implement _build() method"},{"location":"#where-should-the-submodules-be-declared","text":"Note that modules may use other modules which they receive already externally constructed - eg Sequential etc. The submodules we discuss in this section are any Modules which are constructed inside the code of another Module, which we will refer to as the Parent Module. An example is an LSTM, where most implementations will internally construct one or more Linear modules to contain the weights. It's recommended that submodules are created in _build() . Doing it this way means you get the correct nesting of variable scopes, eg: class ParentModule(snt.AbstractModule): def __init__(self, hidden_size, name= parent_module ): super(ParentModule, self).__init__(name=name) self._hidden_size = hidden_size def _build(self, inputs): lin_mod = snt.Linear(self._hidden_size) # Construct submodule... return tf.relu(lin_mod(inputs)) # then connect it. The variables created by the Linear will have a name something like parent_module/linear/w , which is what you probably want in this kind of situation. Some users prefer for practical or stylistic reasons to construct everything in the constructor, before anything is used. This is fine, but for proper variable nesting any submodules must be constructed inside a self._enter_variable_scope call . class OtherParentModule(snt.AbstractModule): def __init__(self, hidden_size, name= other_parent_module ): super(OtherParentModule, self).__init__(name=name) self._hidden_size = hidden_size with self._enter_variable_scope(): # This line is crucial! self._lin_mod = snt.Linear(self._hidden_size) # Construct submodule here. def _build(self, inputs): return tf.relu(self._lin_mod(inputs)) # Connect previously constructed mod. The above example is fine, and will have the same variable names etc. Different people prefer different styles and both of the above are considered correct. The pitfall here is forgetting to call self._enter_variable_scope() . Things will still \"work\" but the scopes will not be nested as you might expected: class WrongModule(snt.AbstractModule): def __init__(self, hidden_size, name= wrong_module ): super(WrongModule, self).__init__(name=name) self._hidden_size = hidden_size self._lin_mod = snt.Linear(self._hidden_size) # Construct submodule here. def _build(self, inputs): return tf.relu(self._lin_mod(inputs)) # Connect previously constructed mod. The above example works okay in terms of the resulting network's calculations, but is considered a bug due to the resulting flat (instead of hierarchical) variable namespace. The variables in the linear will be called \"linear/w\" which is completely disjoint from the \"wrong_module\" namespace.","title":"Where should the submodules be declared?"},{"location":"#recurrent-modules","text":"","title":"Recurrent Modules"},{"location":"#usage","text":"Sonnet includes recurrent core modules (also called \"cells\" in TensorFlow terminology), which perform one time step of computation. These are ready to be unrolled in time using TensorFlow's unrolling operations . One example of an LSTM that is unrolled in time is the following: hidden_size = 5 batch_size = 20 # input_sequence should be a tensor of size # [time_steps, batch_size, input_features] input_sequence = ... lstm = snt.LSTM(hidden_size) initial_state = lstm.initial_state(batch_size) output_sequence, final_state = tf.nn.dynamic_rnn( lstm, input_sequence, initial_state=initial_state, time_major=True) The batch_size parameter passed to the initial_state() method can also be an int32 Tensor. For a more comprehensive demonstration on the usage of recurrent modules, a fully-documented example of a deep LSTM with skip connections trained on the Shakespeare dataset is available.","title":"Usage"},{"location":"#defining-your-own-recurrent-modules","text":"A recurrent module is any subclass of snt.RNNCore , which inherits from snt.AbstractModule and has an interface compatible with tf.nn.rnn_cell.RNNCell . This allows us to use the variable sharing model from Sonnet whilst also using the cores inside TensorFlow's RNN Containers. class Add1RNN(snt.RNNCore): Simple core that adds 1 to its state and produces zero outputs. This core computes the following: (`input`, (`state1`, `state2`)) - (`output`, (`next_state1`, `next_state2`)) where all the elements are tensors, next_statei` = `statei` + 1, and `output` = 0. All the outputs (`state` and `output`) are of size (`batch_size`, `hidden_size`), where `hidden_size` is a size that is specified in the constructor. def __init__(self, hidden_size, name= add1_rnn ): Constructor of the module. Args: hidden_size: an int, size of the outputs of the module (without batch size). name: the name of the module. super(Add1RNN, self).__init__(name=name) self._hidden_size = hidden_size def _build(self, inputs, state): Builds a TF subgraph that performs one timestep of computation. batch_size = tf.TensorShape([inputs.get_shape()[0]]) outputs = tf.zeros(shape=batch_size.concatenate(self.output_size)) state1, state2 = state next_state = (state1 + 1, state2 + 1) return outputs, next_state @property def state_size(self): Returns a description of the state size, without batch dimension. return (tf.TensorShape([self._hidden_size]), tf.TensorShape([self._hidden_size])) @property def output_size(self): Returns a description of the output size, without batch dimension. return tf.TensorShape([self._hidden_size]) def initial_state(self, batch_size, dtype): Returns an initial state with zeros, for a batch size and data type. NOTE: This method is here only for illustrative purposes, the corresponding method in its superclass should be already doing this. sz1, sz2 = self.state_size # Prepend batch size to the state shape, and create zeros. return (tf.zeros([batch_size] + sz1.as_list(), dtype=dtype), tf.zeros([batch_size] + sz2.as_list(), dtype=dtype)) Apart from the _build method from snt.AbstractModule , a recurrent module must also implement the state_size and output_size properties, which provide the expected size of the recurrent state, and an example of it, respectively. snt.RNNCore defines a initial_state method that can be used to generate a zero initial state or a trainable initial state (based on the aforementioned properties). Optionally, any recurrent module can define its own initial_state method. Note that a zero_state method is also available (as in tf.nn.rnn_cell.RNNCell ) to produce a correctly sized state value filled with zeros. In some situations (LSTM, etc) it may be acceptable to begin with a state containing all zeros, but in other situations this is too limiting, and we may want to (eg) fill some part of the state with random noise. A common option is to make the initial state of an RNN trainable, meaning the state is produced from some tf.Variable s which are trained via backpropagation. If a core supports this, it should provide kwargs trainable= and name= for initial_state() . The name= kwarg can be used to provide a prefix for the (potentially multiple) variable name(s) which will be created.","title":"Defining your own recurrent modules"},{"location":"#the-transposable-interface","text":"Sonnet defines an interface for modules supporting transposition , called snt.Transposable . Transposition is a flexible concept (e.g. not necessarily related to matrix transposition as defined in algebra), and in this context it entails the definition of a new module with attributes which are somehow related to the original module, without strictly implying any form of variable sharing. For example, given a snt.Linear which maps from input size A to output size B , via transposition we will return another snt.Linear module whose weight matrix shape is the transpose of the original one, thus mapping from input size B to output size A ; given a snt.Conv2D module we will return a matching snt.Conv2DTranspose module. The snt.Transposable interface requires that transposable modules implement a method called transpose , returning a module which is the transposed version of the one the method is called on. Whilst not enforced by Sonnet, chaining the method twice should be expected to return a module with the same specifications as the original module. When implementing a transposable module, special care is required to ensure that parameters needed to instantiate the module are provided as functions whose evaluation is deferred to graph construction time. This mechanism allows for transposed modules to be instantiated before the original module is connected to the graph. An example of this behavior can be found in snt.Linear , where the output_size argument of the transposed module is defined as a lambda returning the input_shape property of the original module; upon evaluation input_shape will raise an error unless the module has not been connected to the graph, but this is not an issue since the lambda is not called until the transposed module is connected to the graph.","title":"The Transposable interface"},{"location":"#variable-reuse-with-sntreuse_variables","text":"Some use cases require a tf.VariableScope to be shared across multiple methods, which isn't possible with snt.AbstractModule . For example, a generative model may define a sample() and log_pdf() method that share parts of the same tf.Graph . Adding the @snt.reuse_variables decorator to a method will enable variable reuse in much the same manner as _build() . The most notable difference is that a single tf.VariableScope will be used across different decorated methods and each decorated method has its own reuse flag that is used to enter the variable scope. class Reusable(object): def __init__(self, name): with tf.variable_scope(None, default_name=name) as vs: self.variable_scope = vs @snt.reuse_variables def reusable_var(self): return tf.get_variable( a , shape=[1]) obj = Reusable( reusable ) a1 = obj.reusable_var() a2 = obj.reusable_var() # a1 == a2 class NaiveAutoEncoder(snt.AbstractModule): def __init__(self, n_latent, n_out, name= naive_auto_encoder ): super(NaiveAutoEncoder, self).__init__(name=name) self._n_latent = n_latent self._n_out = n_out @snt.reuse_variables def encode(self, input): Builds the front half of AutoEncoder, inputs - latents. w_enc = tf.get_variable( w_enc , shape=[self._n_out, self._n_latent]) b_enc = tf.get_variable( b_enc , shape=[self._n_latent]) return tf.sigmoid(tf.matmul(input, w_enc) + b_enc) @snt.reuse_variables def decode(self, latents): Builds the back half of AutoEncoder, latents - reconstruction. w_rec = tf.get_variable( w_dec , shape=[self._n_latent, self._n_out]) b_rec = tf.get_variable( b_dec , shape=[self._n_out]) return tf.sigmoid(tf.matmul(latents, w_rec) + b_rec) def _build(self, input): Builds the 'full' AutoEncoder, ie input - latents - reconstruction. latents = self.encode(input) return self.decode(latents) batch_size = 5 n_in = 10 n_out = n_in n_latent = 2 nae = NaiveAutoEncoder(n_latent=n_latent, n_out=n_out) inputs = tf.placeholder(tf.float32, shape=[batch_size, n_in]) latents = tf.placeholder(tf.float32, shape=[batch_size, n_latent]) # Connecting the default way calls build(), producing 'full' AutoEncoder. reconstructed_from_input = nae(inputs) # Connecting with one of the other methods might only require some subset of the # variables, but sharing will still work. reconstructed_from_latent = nae.decode(latents) In the above example, any variables created by nae.encode() , nae.decode() or nae() exist in the same tf.VariableScope . In addition, since each decorated method has its own reuse flag we don't need to worry about having to create all variables on the first method call, or about the calling order at all. We can even nest (decorated) methods within other (decorated) methods - the reuse flag is always set correctly since the variable scope is re-entered for every method call. However every decorated method must be the sole owner of its variables. For example, if we use tf.get_variable(\"w_dec\", ...) inside NaiveAutoEncoder.encode , this will not do variable sharing. Instead, TensorFlow will treat tf.get_variable(\"w_dec\", ...) in NaiveAutoEncoder.enodec and NaiveAutoEncoder.decode() as separate variables and an error will occur in either obj.a() or obj.build() (whichever is called second). See below for an example of bad variable reuse: class BadReusable(object): def __init__(self, name): with tf.variable_scope(None, default_name=name) as vs: self.variable_scope = vs @snt.reuse_variables def reusable_var(self): return tf.get_variable( a , shape=[1]) @snt.reuse_variables def another_reusable_var(self): return tf.get_variable( a , shape=[1]) obj = BadReusable( bad_reusable ) obj.reusable_var() obj.another_reusable_var() # Raises a ValueError because `reuse=False`","title":"Variable reuse with @snt.reuse_variables"},{"location":"#wrapping-functions-into-sonnet-modules-using-sntmodule","text":"Whilst the recommended way of defining new Sonnet modules is to inherit from snt.AbstractModule , the library also offers an alternative route to succinctly instantiate modules wrapping user-provided functions. The snt.Module class constructor takes a callable and returns a Sonnet module. The provided function is invoked when the module is called, thus specifying how new nodes are added to the computational graph and how to compute output Tensors from input Tensors. Please refer to the module documentation for more details and examples.","title":"Wrapping functions into Sonnet modules using snt.Module"},{"location":"#faq","text":"","title":"FAQ"},{"location":"#q-why-another-tf-library","text":"A: The existing libraries were judged insufficiently flexible for the DeepMind use case where extensive use is made of weight sharing. Making everything use tf.make_template , and therefore support weight sharing from the start, seemed to have sufficient benefits to outweight the development cost. The paradigm of separating configuration from connection also allows easy composability of modules.","title":"Q: Why another TF library?"},{"location":"#q-can-i-access-different-variables-on-subsequent-calls-to-the-same-build","text":"A: No. This is enforced by tf.make_template , which considers it an error to access different / extra variables on subsequent calls.","title":"Q: Can I access different variables on subsequent calls to the same build()?"},{"location":"#q-what-if-i-mistakenly-give-two-modules-the-same-name","text":"A: Modules which appear to be constructed with the same name will have distinct names, and variable scopes. Under the hood, Sonnet uses tf.make_template which essentially wraps a python function together with some tf.VariableScope , ensuring that every call to the function happens in the same scope, and that all calls after the first are set to reuse variables. One feature of the templating is that it will uniquify any provided names, if they have already been entered in the same scope. For example: lin_1 = snt.Linear(output_size=42, name= linear ) lin_2 = snt.Linear(output_size=84, name= linear ) # this name is already taken. print(lin_1.name) # prints linear print(lin_2.name) # prints linear_1 - automatically uniquified. Note that the .name property is available to see the \"post-uniquification\" name.","title":"Q: What if I mistakenly give two modules the same name?"},{"location":"#q-do-i-have-to-name-my-modules","text":"A: No. Modules have a default name, which should be the class name in snake_case , and that will be used as the name with uniquification (see above) if necessary. However, we recommend providing a name for modules which contain variables, as the name provided becomes the name of the internal scope, and thus defines the variable names. Most modules are written to declare internal weights with names like \"w\" and \"b\" for weights and bias - it's vastly preferable to do have a list of weights like: sdae/encoder_linear/w sdae/encoder_linear/b sdae/decoder_linear/w sdae/decoder_linear/b rather than: sdae/linear/w sdae/linear/b sdae/linear_1/w sdae/linear_1/b The names you choose will appear in TensorBoard.","title":"Q: Do I have to name my modules?"},{"location":"#q-how-do-i-find-out-what-variables-are-declared-by-a-module","text":"A: You can query a module to find out all the variables in its scope using the get_variables() method. Note that this will throw an error if the module is not connected into the graph, as the variables do not exist at that point so the relevant scope will be empty.","title":"Q: How do I find out what variables are declared by a module?"},{"location":"#q-should-i-be-putting-calls-to-variable_scope-in-my-code","text":"A: Every module implicitly creates an internal variable_scope, which it re-enters each time it connects to the graph. Assuming all the variables in your model are inside Sonnet modules, it is not necessary to use scopes yourself.","title":"Q: Should I be putting calls to variable_scope in my code?"},{"location":"#q-can-i-mix-this-with-raw-tf-ops","text":"A: Yes. An op which doesn't declare variables internally, and so is effectively a pure function, can be used inside module _build implementations, and also to plumb together values between modules.","title":"Q: Can I mix this with raw TF ops?"},{"location":"#q-should-everything-in-sonnet-be-implemented-as-a-module","text":"No, computations which do not create tf.Variable s and do not store internal configurations can be implemented in the regular TF Op style, ie a python function that receives input tensors, keyword arguments, and returns tensor outputs. If an op is going to create variables (ie call tf.get_variable anywhere internally, including indirectly) it must be implemented as a subclass of snt.AbstractModule so that variable sharing is correctly handled. Note that if a computation doesn't create any Variables, it may still be desirable to implement it with a Module instead of an Op. Aside from variable sharing, it may be convenient to use Sonnet Modules in cases where we wish to attach configuration parameters to an op. An example of this is the content addressing modules in the Differentiable Neural Computer. These modules receive a number of configuration parameters (size of each word in memory, number of read heads) and some function of these inputs defines what the valid input size is. We use a snt.Linear of the correct output size before this module, in order to provide the correct dimensionality. As a module this is easy - provide the configuration at construction time, then a method .param_size() which gives the required input dimensionality. We can then make the correct size of input tensor and perform the connection. class CosineWeights(snt.AbstractModule): def __init__(self, word_size, num_heads, name= cosine_weights ): super(CosineWeights, self).__init__(name=name) self._word_size = word_size self._num_heads = num_heads def param_size(self): Returns the size the 2nd dimension of `cos_params` is required to be. return self._num_heads * (1 + self._word_size) def _build(self, memory, cos_params): cos_params must be `[batch_size, self.param_size()]` shape # ... # Construct the module, then work out the right input size cos_weights_mod = CosineWeights(word_size=32, num_heads=3) cosine_params_mod = snt.Linear(output_size=cos_weights_mod.param_size()) cos_params = cosine_params_mod(inputs) # We know this is now the right shape. weights = cos_weights_mode(memory, cos_params) If the above was implemented as an op cosine_weights(memory, cos_params, word_size, num_heads) then the logic to indicate the desired size of cos_params would have to be stored in a separate function. Encapsulating the related functions into one module results in cleaner code. Another example of where this flexibility is useful is when an Op has a large number of arguments which are conceptually configuration, along with some which are conceptually inputs. We often want to use the same configuration in multiple places, for different inputs, and so writing a Module which can be constructed with the configuration and then passed around may be useful. import functools # 1. Define our computation as some op def useful_op(input_a, input_b, use_clipping=True, remove_nans=False, solve_agi='maybe'): # ... # 2a). Set the configuration parameters with functools, then pass around. useful_op_configured = functools.partial( useful_op, use_clipping=False, remove_nans=True, solve_agi='definitely') do_something_a(... , inner_op=useful_op_configured) do_something_else_a(..., inner_op=useful_op_configured) # 2b). OR, set the configuration by creating kwargs and pass around both. op_kwargs = { 'use_clipping': False, 'remove_nans': True, 'solve_agi': 'definitely', } do_something_b(..., inner_op=useful_op, inner_op_kwargs=op_kwargs) do_something_else_b(..., inner_op=useful_op, inner_op_kwargs=op_kwargs) Either of the above approaches is valid, but many users dislike the style of using functools or needing to pass both the Op and a dictionary around. In which case, rewriting the Op as a Module can be a nice solution - in particular, the difference between configuration parameters and inputs from the Graph are now made explicit: class UsefulModule(snt.AbstractModule): def __init__(self, use_clipping=True, remove_nans=False, solve_agi='maybe', name='useful_module') super(UsefulModule, self).__init__(name=name) self._use_clipping = use_clipping self._remove_nans = remove_nans self._solve_agi = solve_agi def _build(self, input_a, input_b): #...","title":"Q: Should everything in Sonnet be implemented as a Module?"},{"location":"#q-can-i-mix-this-with-other-high-level-tf-apis-eg-tf-slim","text":"A: Sonnet modules, once constructed, follow the Tensor-In-Tensor-Out principle, so can be mixed with functions from TF-Slim, etc. Note that this may lead to unexpected behaviour - TF-Slim controls sharing by passing explicit scope= and reuse= kwargs into the layer functions - if you use a TF-Slim layer inside the _build() method of a Sonnet module, then calling it multiple times is not likely to work correctly.","title":"Q: Can I mix this with other high level TF APIs, eg TF Slim?"},{"location":"#q-shouldnt-i-be-overriding-9595call9595-in-modules","text":"A: No. AbstractModule.__init__ provides an implementation of __call__ , which calls an internal function wrapped in a Template, which in turn wraps the _build method. Overriding __call__ yourself will likely break variable sharing.","title":"Q: Shouldn't I be overriding __call__ in modules?"},{"location":"#q-what-is-the-overhead-of-using-sonnet-vs-other-libraries-vs-raw-tf","text":"A: None. Sonnet is only involved when constructing the computation graph. Once you are at the stage of using Session.run() you are simply executing Ops, without regard for what library was used to put that graph together.","title":"Q: What is the overhead of using Sonnet vs other libraries vs raw TF?"},{"location":"#q-how-do-i-list-all-the-variables-which-are-used-in-any-way-in-a-module","text":"A: You can use get_all_variables() to find all the variables that a module or any of its submodules have created with tf.get_variable() . Like get_variables() this returns all variables that are inside of the module's (variable) scope. However, get_all_variables() also returns all of the variables from any submodules with disjoint (variable) scopes. These submodules have either been passed into the module's constructor, or have been constructed by the module but outside of _build() or _enter_variable_scope() . Note that by definition this will not return variables that have not been created by tf.get_variable() . This is relevant for modules that use @snt.reuse_variables . If a method decorated with @snt.reuse_variable is not called then get_all_variables() will not return any variables used inside of it. Note that by definition this returns all of a module's variables. This means that a module will return all its submodule's variables, even if it only uses a subset of the submodule's variables (ie. it does not call a method decorated by @snt.reuse_variables on the submodule).","title":"Q: How do I list all the variables which are used in any way in a Module?"},{"location":"#q-how-do-i-serialize-sonnet-module-instances","text":"A: We do not support serializing module instances, via pickle or any other method. Modules contain inherently non-serializable artifacts, such as references to Variable s and Graph s. To save a model, you should instead serialize the configuration information which will allow you to regenerate the Graph, such as the name and constructor parameters of some top level module. This avoids any complications with adding new members to modules, and then having to deal with old serialized instances where those members don't exist.","title":"Q: How do I serialize Sonnet module instances?"},{"location":"#q-how-can-i-cite-sonnet","text":"A: Please use the following BibTeX: @misc{sonnetblog, title= {Open sourcing Sonnet - a new library for constructing neural networks} , author={Reynolds, Malcolm and Barth-Maron, Gabriel and Besse, Frederic and de Las Casas, Diego and Fidjeland, Andreas and Green, Tim and Puigdom{\\`e}nech, Adri{\\`a} and Racani{\\`e}re, S{\\'e}bastien and Rae, Jack and Viola, Fabio}, howpublished={\\url{https://deepmind.com/blog/open-sourcing-sonnet/}}, year={2017} }","title":"Q: How can I cite Sonnet?"},{"location":"INSTALL/","text":"Installing from source To install Sonnet from source, you will need to compile the library using bazel. You should have installed TensorFlow by following the TensorFlow installation instructions . Install bazel Ensure you have a recent version of bazel ( = 0.4.5) and JDK ( = 1.8). If not, follow these directions . (virtualenv TensorFlow installation) Activate virtualenv If using virtualenv, activate your virtualenv for the rest of the installation, otherwise skip this step: $ source $VIRTUALENV_PATH/bin/activate # bash, sh, ksh, or zsh $ source $VIRTUALENV_PATH/bin/activate.csh # csh or tcsh Build and run the installer First clone the Sonnet source code: $ git clone https://github.com/deepmind/sonnet Then run the install script to create a wheel file in a temporary directory: $ mkdir /tmp/sonnet $ bazel build :install $ ./bazel-bin/install /tmp/sonnet To build the GPU accelerated version of Sonnet use: $ SONNET_GPU=1 ./bazel-bin/install /tmp/sonnet By default, the wheel file is built using python . You can optionally specify another python binary in the previous command to build the wheel file, such as python3 : $ ./bazel-bin/install /tmp/sonnet python3 pip install the generated wheel file: $ pip install /tmp/sonnet/*.whl If Sonnet was already installed, uninstall prior to calling pip install on the wheel file: $ pip uninstall dm-sonnet # or dm-sonnet-gpu You can verify that Sonnet has been successfully installed by, for example, trying to instantiate and connect a Linear module: $ cd ~/ $ python import tensorflow as tf import sonnet as snt input_ = tf.zeros(3, 5) output = snt.Linear(10)(input_)","title":"Installing from source"},{"location":"INSTALL/#installing-from-source","text":"To install Sonnet from source, you will need to compile the library using bazel. You should have installed TensorFlow by following the TensorFlow installation instructions .","title":"Installing from source"},{"location":"INSTALL/#install-bazel","text":"Ensure you have a recent version of bazel ( = 0.4.5) and JDK ( = 1.8). If not, follow these directions .","title":"Install bazel"},{"location":"INSTALL/#virtualenv-tensorflow-installation-activate-virtualenv","text":"If using virtualenv, activate your virtualenv for the rest of the installation, otherwise skip this step: $ source $VIRTUALENV_PATH/bin/activate # bash, sh, ksh, or zsh $ source $VIRTUALENV_PATH/bin/activate.csh # csh or tcsh","title":"(virtualenv TensorFlow installation) Activate virtualenv"},{"location":"INSTALL/#build-and-run-the-installer","text":"First clone the Sonnet source code: $ git clone https://github.com/deepmind/sonnet Then run the install script to create a wheel file in a temporary directory: $ mkdir /tmp/sonnet $ bazel build :install $ ./bazel-bin/install /tmp/sonnet To build the GPU accelerated version of Sonnet use: $ SONNET_GPU=1 ./bazel-bin/install /tmp/sonnet By default, the wheel file is built using python . You can optionally specify another python binary in the previous command to build the wheel file, such as python3 : $ ./bazel-bin/install /tmp/sonnet python3 pip install the generated wheel file: $ pip install /tmp/sonnet/*.whl If Sonnet was already installed, uninstall prior to calling pip install on the wheel file: $ pip uninstall dm-sonnet # or dm-sonnet-gpu You can verify that Sonnet has been successfully installed by, for example, trying to instantiate and connect a Linear module: $ cd ~/ $ python import tensorflow as tf import sonnet as snt input_ = tf.zeros(3, 5) output = snt.Linear(10)(input_)","title":"Build and run the installer"},{"location":"contents/","text":"sonnet : AbstractModule ACTCore AddBias AffineGridWarper AffineWarpConstraints AttentiveRead BatchApply BatchFlatten BatchNorm BatchNormLSTM BatchNormV2 BatchReshape BidirectionalRNN CausalConv1D check_initializers check_partitioners check_regularizers clip_gradient ConcatLinear Conv1D Conv1DLSTM Conv1DTranspose Conv2D Conv2DLSTM Conv2DTranspose Conv3D Conv3DTranspose count_variables_by_type custom_getter_router custom_getters.bayes_by_backprop._VariableMetadata custom_getters.bayes_by_backprop.adaptive_gaussian_prior_builder custom_getters.bayes_by_backprop.analytic_kl_builder custom_getters.bayes_by_backprop.bayes_by_backprop_getter custom_getters.bayes_by_backprop.diagonal_gaussian_posterior_builder custom_getters.bayes_by_backprop.EstimatorModes custom_getters.bayes_by_backprop.fixed_gaussian_prior_builder custom_getters.bayes_by_backprop.get_total_kl_cost custom_getters.bayes_by_backprop.get_variable_metadata custom_getters.bayes_by_backprop.inverse_softplus custom_getters.bayes_by_backprop.scale_variable_initializer custom_getters.bayes_by_backprop.stochastic_kl_builder custom_getters.Context custom_getters.non_trainable custom_getters.override_args custom_getters.override_default_args custom_getters.restore_initializer custom_getters.stop_gradient DeepRNN deprecation_warning DepthwiseConv2D DifferentGraphError Embed Error FlattenTrailingDimensions format_variable_map format_variables get_lagrange_multiplier get_normalized_variable_map get_saver get_variables_in_module get_variables_in_scope GridWarper GRU has_variable_scope highway_core_with_recurrent_dropout HighwayCore IncompatibleShapeError InPlaneConv2D LayerNorm Linear log_variables LSTM lstm_with_recurrent_dropout lstm_with_zoneout LSTMBlockCell LSTMState merge_leading_dims MergeDims ModelRNN Module ModuleInfoError MovingAverage nest.assert_same_structure nest.assert_shallow_structure nest.flatten nest.flatten_dict_items nest.flatten_iterable nest.flatten_up_to nest.is_iterable nest.is_sequence nest.map nest.map_up_to nest.pack_iterable_as nest.pack_sequence_as nest.with_deprecation_warning nets.AlexNet nets.AlexNetFull nets.AlexNetMini nets.ConvNet2D nets.ConvNet2DTranspose nets.Dilation nets.identity_kernel_initializer nets.MLP nets.noisy_identity_kernel_initializer nets.VectorQuantizer nets.VectorQuantizerEMA NotConnectedError NotInitializedError NotSupportedError observe_connections OptimizationConstraints ParentNotBuiltError parse_string_to_constructor protos.module_pb2._b protos.module_pb2.NestedData protos.module_pb2.SonnetModule python.modules.attention.AttentionOutput python.modules.base.get_connection_stack python.modules.base.get_module_stack python.modules.base_info.ConnectedSubGraph python.modules.base_info.ModuleInfo python.modules.basic.calculate_bias_shape python.modules.basic.create_bias_initializer python.modules.basic.create_linear_initializer python.modules.batch_norm.create_beta_initializer python.modules.batch_norm.create_gamma_initializer python.modules.batch_norm.create_mean_initializer python.modules.batch_norm.create_variance_initializer python.modules.batch_norm_v2.create_beta_initializer python.modules.batch_norm_v2.create_gamma_initializer python.modules.batch_norm_v2.create_mean_initializer python.modules.batch_norm_v2.create_variance_initializer python.modules.conv.create_bias_initializer python.modules.conv.create_weight_initializer python.modules.gated_rnn.ConvLSTM python.modules.gated_rnn.RecurrentDropoutWrapper python.modules.gated_rnn.ZoneoutWrapper python.modules.layer_norm.create_beta_initializer python.modules.layer_norm.create_gamma_initializer python.modules.rnn_core.with_doc python.modules.spectral_normalization.spectral_norm python.modules.spectral_normalization.SpectralNormWrapper python.modules.util.get_variable_scope_name python.modules.util.name_for_callable python.modules.util.notify_about_new_variables python.modules.util.sort_by_name python.modules.util.to_snake_case RelationalMemory remove_unsupported_kwargs Residual ResidualCore reuse_variables RNNCellWrapper RNNCore scale_gradient SelectInput SeparableConv1D SeparableConv2D Sequential SkipConnectionCore SliceByDim split_leading_dim summarize_variables supports_kwargs TileByDim trainable_initial_state TrainableInitialState TrainableVariable Transposable UnderspecifiedError VanillaRNN variable_map_items wrap_with_spectral_norm","title":"Contents"},{"location":"sonnet/","text":"sonnet - module reference This python module contains Neural Network Modules for TensorFlow. Each module is a Python object which conceptually \"owns\" any variables required in that part of the Neural Network. The __call__ function on the object is used to connect that Module into the Graph, and this may be called repeatedly with sharing automatically taking place. Everything public should be imported by this top level __init__.py so that the library can be used as follows: import sonnet as snt linear = snt.Linear(...) Other Functions and Classes class ACTCore Adaptive computation time core. Implementation of the model described in \"Adaptive Computation Time for Recurrent Neural Networks\" paper, https://arxiv.org/abs/1603.08983. The ACTCore incorporates the pondering RNN of ACT, with different computation times for each element in the mini batch. Each pondering step is performed by the core passed to the constructor of ACTCore . The output of the ACTCore is made of (act_out, (iteration, remainder) , where iteration counts the number of pondering step in each batch element; remainder is the remainder as defined in the ACT paper; act_out is the weighted average output of all pondering steps (see ACT paper for more info). ACTCore.__init__(core, output_size, threshold, get_state_for_halting, max_steps=0, name='act_core') Constructor. Args: core : A sonnet.RNNCore object. This should only take a single Tensor in input, and output only a single flat Tensor . output_size : An integer. The size of each output in the sequence. threshold : A float between 0 and 1. Probability to reach for ACT to stop pondering. get_state_for_halting : A callable that can take the core state and return the input to the halting function. max_steps : Integer = 0, that controls the maximum number of ponder steps. If equal to 0, then this disables control. name : A string. The name of this module. Raises: ValueError : if threshold is not between 0 and 1. ValueError : if core has either nested outputs or outputs that are not one dimensional. ACTCore.__call__(x, prev_state) Connects the core to the graph. Args: x : Input Tensor of shape (batch_size, input_size) . prev_state : Previous state. This could be a Tensor , or a tuple of Tensor s. Returns: The tuple (output, state) for this core. Raises: ValueError : if the Tensor x does not have rank 2. ACTCore.batch_size ACTCore.connected_subgraphs Returns the subgraphs created by this module so far. ACTCore.defun() Wraps this modules call method in a callable graph function. ACTCore.defun_wrapped Returns boolean indicating whether this module is defun wrapped. ACTCore.dtype ACTCore.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ACTCore.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. ACTCore.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ACTCore.graph Returns the Graph instance which the module is connected to, or None. ACTCore.initial_state(*args, **kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. ACTCore.is_connected Returns true iff the Module been connected to the Graph at least once. ACTCore.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. ACTCore.module_name Returns the name of the Module. ACTCore.name_scopes Returns a tuple of all name_scopes generated by this module. ACTCore.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ACTCore.output_size Integer or TensorShape: size of outputs produced by this cell. ACTCore.scope_name Returns the full name of the Module's variable scope. ACTCore.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. ACTCore.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ACTCore.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. ACTCore.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ACTCore.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class AbstractModule Superclass for Sonnet Modules. This class defines the functionality that every module should implement, principally the build method which is wrapped using tf.make_template and called from __call__ . Every time the module is called it will be connected into the graph but using the same shared set of variables, thanks to the template. For this to work correctly, the build implementation in the derived class must access all variables using tf.get_variable , not tf.Variable . The same set of variables must be created each time, if this is not the case an Error will be raised. Every subclass must call this class' __init__ at the start of their __init__ , passing the relevant name. If this step is omitted variable sharing will not work. AbstractModule.__init__(_sentinel=None, custom_getter=None, name=None) Performs the initialisation necessary for all AbstractModule instances. Every subclass of AbstractModule must begin their constructor with a call to this constructor, i.e. super(MySubModule, self).__init__(custom_getter=custom_getter, name=name) . If you instantiate sub-modules in init you must create them within the _enter_variable_scope context manager to ensure they are in the module's variable scope. Alternatively, instantiate sub-modules in _build . Args: _sentinel: Variable that only carries a non-None value if __init__ was called without named parameters. If this is the case, a deprecation warning is issued in form of a ValueError . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of this module. Used to construct the Templated build function. If None the module's class name is used (converted to snake case). Raises: TypeError : If name is not a string. TypeError : If a given custom_getter is not callable. ValueError : If __init__ was called without named arguments. AbstractModule.__call__(*args, **kwargs) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). AbstractModule.connected_subgraphs Returns the subgraphs created by this module so far. AbstractModule.defun() Wraps this modules call method in a callable graph function. AbstractModule.defun_wrapped Returns boolean indicating whether this module is defun wrapped. AbstractModule.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AbstractModule.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. AbstractModule.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AbstractModule.graph Returns the Graph instance which the module is connected to, or None. AbstractModule.is_connected Returns true iff the Module been connected to the Graph at least once. AbstractModule.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. AbstractModule.module_name Returns the name of the Module. AbstractModule.name_scopes Returns a tuple of all name_scopes generated by this module. AbstractModule.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AbstractModule.scope_name Returns the full name of the Module's variable scope. AbstractModule.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AbstractModule.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. AbstractModule.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class AddBias AddBias module. AddBias.__init__(output_shape=None, bias_dims=None, initializers=None, partitioners=None, regularizers=None, name='add') Constructs an AddBias module that supports broadcasting. Args: output_shape : Output dimensionality. output_shape can be either None , a tuple , or a callable . In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_shape can be called, returning a tuple, when build is called. If output_shape is left as None , the size will be directly inferred by the input. bias_dims : List of which dimensions to retain from the input shape when constructing the bias. The remaining dimensions will get broadcasted over (given size of 1), and leading dimensions will be removed completely. For example, for an input of [batch_size, dim1_size, dim2_size, dim3_size] and bias_dims=[1, 3] , the resulting bias will have shape [dim1_size, 1, dim3_size]. The default is to retain all dimensions apart from the minibatch dimension. Trying to retain the bias shape over the minibatch dimension, e.g. bias_dims=[0] , will result in an error at build time. See the 'Example Usage' section below for more information. initializers : Optional dict containing ops to initialize the biases (with key 'b'). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing a partitioner to partition the bias (with key 'b'). As a default, no partitioner is used. regularizers : Optional dict containing regularizers of the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Name of the module. Example Usage: # Create a 4D input Tensor. input = tf.random_normal( shape=(batch_size, dim1_size, dim2_size, dim3_size))) # Create a scalar bias: scalar_bias = snt.AddBias(bias_dims=[]) scalar_bias_output = scalar_bias(input) scalar_bias.b.get_shape() # () # Create a bias over all non-minibatch dimensions: all_bias = snt.AddBias() # or snt.AddBias(bias_dims=None) all_bias_output = all_bias(input) all_bias.b.get_shape() # (dim1_size, dim2_size, dim3_size) # Create a bias over the last non-minibatch dimension: last_bias = snt.AddBias(bias_dims=[-1]) last_bias_output = last_bias(input) last_bias.b.get_shape() # (dim3_size) # Create a bias over the first non-minibatch dimension: first_bias = snt.AddBias(bias_dims=[1]) first_bias_output = first_bias(input) first_bias.b.get_shape() # (dim1_size, 1, 1) # Subtract and later add the same learned bias: bias = snt.AddBias() hidden1 = bias(input, multiplier=-1) # ... reconstructed_input = bias(hidden4) Raises: KeyError : If initializers contains any keys other than 'b'. KeyError : If partitioners contains any keys other than 'b'. KeyError : If regularizers contains any keys other than 'b'. TypeError : If any of the given initializers are not callable. TypeError : If any of the given partitioners are not callable. TypeError : If any of the given regularizers are not callable. AddBias.__call__(inputs, multiplier=1) Connects the Add module into the graph, with input Tensor inputs . Args: inputs : A Tensor of size [batch_size, input_size1, ...] . multiplier : A scalar or Tensor which the bias term is multiplied by before adding it to inputs . Anything which works in the expression bias * multiplier is acceptable here. This may be useful if you want to add a bias in one place and subtract the same bias in another place via multiplier=-1 . Returns: A Tensor of size [batch_size, input_size1, ...] . Raises: base.IncompatibleShapeError: If the input is not a = 2D Tensor . base.IncompatibleShapeError: If connecting the module into the graph any time after the first time, and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the output_shape has been specified but it does not match the input_shape`. base.ParentNotBuiltError: If the module is a transposed and the original untransposed module has not been built. AddBias.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AddBias.connected_subgraphs Returns the subgraphs created by this module so far. AddBias.defun() Wraps this modules call method in a callable graph function. AddBias.defun_wrapped Returns boolean indicating whether this module is defun wrapped. AddBias.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AddBias.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. AddBias.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AddBias.graph Returns the Graph instance which the module is connected to, or None. AddBias.input_shape Returns shape of input Tensor passed at last call to build . AddBias.is_connected Returns true iff the Module been connected to the Graph at least once. AddBias.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. AddBias.module_name Returns the name of the Module. AddBias.name_scopes Returns a tuple of all name_scopes generated by this module. AddBias.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AddBias.scope_name Returns the full name of the Module's variable scope. AddBias.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AddBias.transpose(name=None) Returns transposed AddBias module. Args: name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.module_name . Returns: Transposed AddBias module. AddBias.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. AddBias.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class AffineGridWarper Affine Grid Warper class. The affine grid warper generates a reference grid of n-dimensional points and warps it via an affine transormation model determined by an input parameter Tensor. Some of the transformation parameters can be fixed at construction time via an AffineWarpConstraints object. AffineGridWarper.__init__(source_shape, output_shape, constraints=None, name='affine_grid_warper') Constructs an AffineGridWarper. source_shape and output_shape are used to define the size of the source and output signal domains, as opposed to the shape of the respective Tensors. For example, for an image of size width=W and height=H , {source,output}_shape=[H, W] ; for a volume of size width=W , height=H and depth=D , {source,output}_shape=[H, W, D] . Args: source_shape : Iterable of integers determining the size of the source signal domain. output_shape : Iterable of integers determining the size of the destination resampled signal domain. constraints : Either a double list of shape [N, N+1] defining constraints on the entries of a matrix defining an affine transformation in N dimensions, or an AffineWarpConstraints object. If the double list is passed, a numeric value bakes in a constraint on the corresponding entry in the transformation matrix, whereas None implies that the corresponding entry will be specified at run time. name : Name of module. Raises: Error : If constraints fully define the affine transformation; or if input grid shape and contraints have different dimensionality. TypeError : If output_shape and source_shape are not both iterable. AffineGridWarper.__call__(inputs) Assembles the module network and adds it to the graph. The internal computation graph is assembled according to the set of constraints provided at construction time. Args: inputs : Tensor containing a batch of transformation parameters. Returns: A batch of warped grids. Raises: Error : If the input tensor size is not consistent with the constraints passed at construction time. AffineGridWarper.connected_subgraphs Returns the subgraphs created by this module so far. AffineGridWarper.constraints AffineGridWarper.defun() Wraps this modules call method in a callable graph function. AffineGridWarper.defun_wrapped Returns boolean indicating whether this module is defun wrapped. AffineGridWarper.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AffineGridWarper.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. AffineGridWarper.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AffineGridWarper.graph Returns the Graph instance which the module is connected to, or None. AffineGridWarper.inverse(name=None) Returns a sonnet module to compute inverse affine transforms. The function first assembles a network that given the constraints of the current AffineGridWarper and a set of input parameters, retrieves the coefficients of the corresponding inverse affine transform, then feeds its output into a new AffineGridWarper setup to correctly warp the output space into the source space. Args: name : Name of module implementing the inverse grid transformation. Returns: A sonnet module performing the inverse affine transform of a reference grid of points via an AffineGridWarper module. Raises: tf.errors.UnimplementedError: If the function is called on a non 2D instance of AffineGridWarper. AffineGridWarper.is_connected Returns true iff the Module been connected to the Graph at least once. AffineGridWarper.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. AffineGridWarper.module_name Returns the name of the Module. AffineGridWarper.n_coeff Returns number of coefficients of warping function. AffineGridWarper.name_scopes Returns a tuple of all name_scopes generated by this module. AffineGridWarper.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AffineGridWarper.output_shape Returns a tuple containing the shape of the output grid. AffineGridWarper.psi Returns a list of features used to compute the grid warp. AffineGridWarper.scope_name Returns the full name of the Module's variable scope. AffineGridWarper.source_shape Returns a tuple containing the shape of the source signal. AffineGridWarper.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AffineGridWarper.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. AffineGridWarper.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class AffineWarpConstraints Affine warp contraints class. AffineWarpConstraints allow for very succinct definitions of constraints on the values of entries in affine transform matrices. AffineWarpConstraints.__init__(constraints=((None, None, None), (None, None, None))) Creates a constraint definition for an affine transformation. Args: constraints : A doubly-nested iterable of shape [N, N+1] defining constraints on the entries of a matrix that represents an affine transformation in N dimensions. A numeric value bakes in a constraint on the corresponding entry in the transformation matrix, whereas None implies that the corresponding entry will be specified at run time. Raises: TypeError : If constraints is not a nested iterable. ValueError : If the double iterable constraints has inconsistent dimensions. AffineWarpConstraints.combine_with(additional_constraints) Combines two sets of constraints into a coherent single set. AffineWarpConstraints.constraints AffineWarpConstraints.mask AffineWarpConstraints.no_constraints(num_dim=2) Empty set of constraints for a num_dim-ensional affine transform. AffineWarpConstraints.no_shear_2d() AffineWarpConstraints.no_shear_3d() Assigns contraints on shear components of affine transform in 3d. AffineWarpConstraints.num_dim AffineWarpConstraints.num_free_params AffineWarpConstraints.scale_2d(x=None, y=None) Assigns contraints on scaling components of affine transform in 2d. AffineWarpConstraints.scale_3d(x=None, y=None, z=None) Assigns contraints on scaling components of affine transform in 3d. AffineWarpConstraints.shear_2d(x=None, y=None) Assigns contraints on shear components of affine transform in 2d. AffineWarpConstraints.translation_2d(x=None, y=None) Assign contraints on translation components of affine transform in 2d. AffineWarpConstraints.translation_3d(x=None, y=None, z=None) Assign contraints on translation components of affine transform in 3d. class AttentiveRead A module for reading with attention. This module reads a weighted sum of embeddings from memory, where each memory slot's weight is based on the logit returned by an attention embedding module. A mask may be given to ignore some memory slots (e.g. when attending over variable-length sequences). AttentiveRead.__init__(attention_logit_mod, name='attention') Initialize AttentiveRead module. Args: attention_logit_mod : Module that produces logit corresponding to a memory slot's compatibility. Must map a [batch_size * memory_size, memory_word_size + query_word_size]-shaped Tensor to a [batch_size * memory_size, 1] shape Tensor. name : string. Name for module. AttentiveRead.__call__(memory, query, memory_mask=None) Perform a differentiable read. Args: memory : [batch_size, memory_size, memory_word_size]-shaped Tensor of dtype float32. This represents, for each example and memory slot, a single embedding to attend over. query : [batch_size, query_word_size]-shaped Tensor of dtype float32. Represents, for each example, a single embedding representing a query. memory_mask : None or [batch_size, memory_size]-shaped Tensor of dtype bool. An entry of False indicates that a memory slot should not enter the resulting weighted sum. If None, all memory is used. Returns: An AttentionOutput instance containing: read : [batch_size, memory_word_size]-shaped Tensor of dtype float32. This represents, for each example, a weighted sum of the contents of the memory. weights : [batch_size, memory_size]-shaped Tensor of dtype float32. This represents, for each example and memory slot, the attention weights used to compute the read. weight_logits : [batch_size, memory_size]-shaped Tensor of dtype float32. This represents, for each example and memory slot, the logits of the attention weights, that is, weights is calculated by taking the softmax of the weight logits. Raises: UnderspecifiedError : if memory_word_size or query_word_size can not be inferred. IncompatibleShapeError : if memory, query, memory_mask, or output of attention_logit_mod do not match expected shapes. AttentiveRead.connected_subgraphs Returns the subgraphs created by this module so far. AttentiveRead.defun() Wraps this modules call method in a callable graph function. AttentiveRead.defun_wrapped Returns boolean indicating whether this module is defun wrapped. AttentiveRead.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AttentiveRead.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. AttentiveRead.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AttentiveRead.graph Returns the Graph instance which the module is connected to, or None. AttentiveRead.is_connected Returns true iff the Module been connected to the Graph at least once. AttentiveRead.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. AttentiveRead.module_name Returns the name of the Module. AttentiveRead.name_scopes Returns a tuple of all name_scopes generated by this module. AttentiveRead.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AttentiveRead.scope_name Returns the full name of the Module's variable scope. AttentiveRead.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. AttentiveRead.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. AttentiveRead.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class BatchApply Merges a number of leading dimensions of an input tensor to manipulate it. Merges a number of leading dimensions of a tensor into a single dimension, connects the provided module, then splits the leading dimension of the result to match the input. Input tensors whose rank is smaller than the number of dimensions to collapse (e.g. all scalar values, which are tensors of rank 0), are passed unaltered to the provided module. This is useful for applying some module to each timestep of a Time x Batch x N tensor. If a module is hard coded to only support 2D (Batch x N) then the full 3D Tensor cannot be provided. BatchApply will 'merge' the first two dimensions of the sequence tensor by reshaping to a (Time * Batch) x N Tensor, and then the internal module can be applied. The result of that operation is reshaped such that its first dimensions are split to match the leading dimensions of the input. BatchApply.__init__(module_or_op, n_dims=2, input_example_index=0, name='batch_apply') Constructor of the module. Args: module_or_op : Module or tensorflow op to apply to an input tensor. n_dims : Number of dimensions to merge before using module on the input of BatchApply. input_example_index : Index of input that has same shape for the first n_dims dimensions as module_or_op output(s). This is used for unflattening the output(s) if static shape inference is not possible. name : Name of the module. Raises: TypeError : If n_dims is not an integer. ValueError : If n_dims is not greater than zero. BatchApply.__call__(*args, **kwargs) Connects the BatchApply module into the graph. Args: *args : a Tensor or a nested list or dictionary of Tensors. The input tensors will have their first dimensions merged, then an op or a module will be called on the input. The first dimension of the output tensor(s) will be split again based on the leading dimensions of the first input tensor. **kwargs : Dictionary of named arguments; used in the same way as *args . Returns: A Tensor or nested list or dictionary of Tensors as a result of applying the process above. (\"None\" return values are also supported.) BatchApply.connected_subgraphs Returns the subgraphs created by this module so far. BatchApply.defun() Wraps this modules call method in a callable graph function. BatchApply.defun_wrapped Returns boolean indicating whether this module is defun wrapped. BatchApply.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchApply.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. BatchApply.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchApply.graph Returns the Graph instance which the module is connected to, or None. BatchApply.is_connected Returns true iff the Module been connected to the Graph at least once. BatchApply.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. BatchApply.module_name Returns the name of the Module. BatchApply.name_scopes Returns a tuple of all name_scopes generated by this module. BatchApply.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchApply.scope_name Returns the full name of the Module's variable scope. BatchApply.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchApply.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. BatchApply.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class BatchFlatten Flattens the input Tensor, preserving the batch dimension(s). BatchFlatten.__init__(preserve_dims=1, name='batch_flatten') Constructs a BatchFlatten module. Args: preserve_dims : Number of leading dimensions that will not be reshaped. For example, given an input Tensor with shape [B, H, W, C] : * preserve_dims=1 will return a Tensor with shape [B, H*W*C] . * preserve_dims=2 will return a Tensor with shape [B, H, W*C] . * preserve_dims=3 will return the input itself, shape [B, H, W, C] . * preserve_dims=4 will return a Tensor with shape [B, H, W, C, 1] . * preserve_dims =5 will throw an error on build. The preserved dimensions can be unknown at building time. name : Name of the module. BatchFlatten.__call__(inputs) Connects the module into the graph, with input Tensor inputs . Args: inputs : A Tensor of shape [b_1, b_2, ..., b_preserve_dims, b_preserve_dims+1, ...]. Returns: A Tensor of shape [b_1, b_2, ..., b_preserve_dims, b_reshape_1, b_reshape_2, ...], with reshaping defined by the constructor shape parameter. Raises: ValueError : If output shape is incompatible with input shape; or if shape array contains non numeric entries; or if shape array contains more than 1 wildcard -1; or if the input array contains unknown, non-preserved dimensions (except when the unknown dimension is the only non-preserved dimension and doesn't actually need reshaping). BatchFlatten.connected_subgraphs Returns the subgraphs created by this module so far. BatchFlatten.defun() Wraps this modules call method in a callable graph function. BatchFlatten.defun_wrapped Returns boolean indicating whether this module is defun wrapped. BatchFlatten.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchFlatten.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. BatchFlatten.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchFlatten.graph Returns the Graph instance which the module is connected to, or None. BatchFlatten.input_shape Returns shape of input Tensor passed at last call to build . BatchFlatten.is_connected Returns true iff the Module been connected to the Graph at least once. BatchFlatten.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. BatchFlatten.module_name Returns the name of the Module. BatchFlatten.name_scopes Returns a tuple of all name_scopes generated by this module. BatchFlatten.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchFlatten.scope_name Returns the full name of the Module's variable scope. BatchFlatten.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchFlatten.transpose(name=None) Returns transpose batch reshape. BatchFlatten.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. BatchFlatten.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class BatchNorm Batch normalization module, including optional affine transformation. This module maintains exponential moving averages of the mean and variance, which can be optionally used to normalize at test time. At training time, batch statistics (mean, variance) are not shared between separate connections. The moving averages are shared between separate connections. At both training and test time, the optional affine transformation ( * gamma + beta ) is shared between separate connections. This is also the case for distributed replica training, where the batch statistics are not aggregated across replicas, but the moving averages are shared globally. When connecting the module to the graph, is_training=True means that Update ops are created to update the moving averages with the current batch's statistics. Features are normalized using the current batch's statistics . The test_local_stats setting is ignored. The moving averages are not used. whereas is_training=False means that Update ops are not created. Features are normalized using either: The test batch statistics if test_local_stats=True (default). The moving averages if test_local_stats=False . Local batch statistics are used by default at test time, but the moving averages can be used by specifying a flag when connecting. One often wants to use local batch statistics at test time to track the progress while the model is trained as it would ensure that moving average updates do not affect the training curves. Once the training is finished, it's often advantageous to use moving average statistics, since it would make evaluation agnostic to the batch size, and might even lead to small improvements over the local batch statistics. You can either update the moving averages automatically by setting update_ops_collection=None or by running the ops in the given collection, by default tf.GraphKeys.UPDATE_OPS. For example, to run the updates automatically: bn = BatchNorm(update_ops_collection=None) train_net = bn(train_inputs, is_training=True) this does, however, have the effect of blocking the forwards pass of the network until the update ops have been run and may have a small performance penalty. For example, to run the updates manually: bn = BatchNorm() train_net = bn(train_inputs, is_training=True) ... update_ops = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS)) train_op = tf.group(train_op, update_ops) Then, whenever train_op is run so also are the moving average update ops. Some batch normalization caveats: Batch normalization will remove the effect of adding a bias, so e.g. use_bias=False should be used for an immediately preceding snt.Linear module. If your data batches aren't i.i.d. then batch normalization can allow your network to 'cheat' by using the batch statistics to peek at the rest of the batch. This can exhibit itself as a higher test score with test_local_stats=True than test_local_stats=False . BatchNorm.__init__(axis=None, offset=True, scale=False, decay_rate=0.999, eps=0.001, initializers=None, partitioners=None, regularizers=None, update_ops_collection='update_ops', fused=False, name='batch_norm') Constructs a BatchNorm module. By default reduces over all input tensor dimensions apart from the final dimension. This has the effect of treating pixels in 1D/2D/3D images as additional elements of the minibatch. If this is not the desired behaviour, the user can specify the tensor indices to reduce over with axis . Args: axis : Optional iterable of indices of dimensions to reduce over. By default None and all dimensions except the last are reduced over. offset : Optional boolean to specify whether or not to apply a trained component-wise bias after the batch normalization and scaling. scale : Optional boolean to specify whether or not to apply a trained component-wise scale after the batch normalization. decay_rate : Decay rate of the exponential moving averages of the mean and variance. eps : Small number to avoid dividing by zero when diving by the standard deviation. initializers : Optional dict containing ops to initialize the weights of the affine transform ( gamma and beta ). partitioners : Optional dict containing partitioners to partition the weights of the affine transform ( gamma and beta ). regularizers : Optional dict containing regularizers for the weights of the affine transform ('gamma' and 'beta'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . update_ops_collection : Name of TensorFlow variable collection to add the moving average update ops to. If None , we instead add the update ops as control dependencies of the output of the module. This may result in some slowdown, as the feed-forward of the network is now blocked. By default, tf.GraphKeys.UPDATE_OPS . fused : Use nn.fused_batch_norm if True, nn.batch_normalization otherwise. name : Name of the module. Raises: KeyError : If initializers contains any keys other than gamma , beta , moving_mean or moving_variance . KeyError : If partitioners or regularizers contains any keys other than gamma or beta . TypeError : If any of the given initializers, partitioners or regularizers are not callable. BatchNorm.__call__(input_batch, is_training, test_local_stats=True) Connects the BatchNorm module into the graph. Args: input_batch : A Tensor of arbitrary dimension. By default, the final dimension is not reduced over when computing the minibatch statistics. is_training : A boolean to indicate if the module should be connected in training mode, meaning the moving averages are updated. Can be a Tensor. test_local_stats : A boolean to indicate if local batch statistics should be used when is_training=False . If not, moving averages are used. By default True . Can be a Tensor. Returns: A tensor with the same shape as input_batch . Raises: base.IncompatibleShapeError: If axis is not valid for the input shape or has negative entries. base.NotSupportedError: If input_batch has data type of tf.bfloat16 . BatchNorm.beta BatchNorm.connected_subgraphs Returns the subgraphs created by this module so far. BatchNorm.defun() Wraps this modules call method in a callable graph function. BatchNorm.defun_wrapped Returns boolean indicating whether this module is defun wrapped. BatchNorm.gamma BatchNorm.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNorm.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. BatchNorm.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNorm.graph Returns the Graph instance which the module is connected to, or None. BatchNorm.initializers BatchNorm.is_connected Returns true iff the Module been connected to the Graph at least once. BatchNorm.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNorm.module_name Returns the name of the Module. BatchNorm.moving_mean BatchNorm.moving_variance BatchNorm.name_scopes Returns a tuple of all name_scopes generated by this module. BatchNorm.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNorm.partitioners BatchNorm.regularizers BatchNorm.scope_name Returns the full name of the Module's variable scope. BatchNorm.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNorm.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. BatchNorm.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class BatchNormLSTM LSTM recurrent network cell with optional peepholes, batch normalization. The base implementation is based on: http://arxiv.org/abs/1409.2329. We add forget_bias (default: 1) to the biases of the forget gate in order to reduce the scale of forgetting in the beginning of the training. Peep-hole connections Peep-hole connections may optionally be used by specifying a flag in the constructor. These connections can aid increasing the precision of output timing, for more details see: https://research.google.com/pubs/archive/43905.pdf Batch normalization The batch norm transformation (in training mode) is batchnorm(x) = gamma * (x - mean(x)) / stddev(x) + beta, where gamma is a learnt scaling factor and beta is a learnt offset. Batch normalization may optionally be used at different places in the LSTM by specifying flag(s) in the constructor. These are applied when calculating the gate activations and cell-to-hidden transformation. The set-up is based on https://arxiv.org/pdf/1603.09025.pdf Batch normalization: where to apply? Batch norm can be applied in three different places in the LSTM: (h) To the W_h h_{t-1} contribution to the gates from the previous hiddens. (x) To the W_x x_t contribution to the gates from the current input. (c) To the cell value c_t when calculating the output h_t from the cell. (The notation here is consistent with the Recurrent Batch Normalization paper). Each of these can be controlled individually, because batch norm is expensive, and not all are necessary. The paper doesn't mention the relative effects of these different batch norms; however, experimentation with a shallow LSTM for the permuted_mnist sequence task suggests that (h) is the most important and the other two can be left off. For other tasks or deeper (stacked) LSTMs, other batch norm combinations may be more effective. Batch normalization: collecting stats (training vs test) When switching to testing (see LSTM.with_batch_norm_control ), we can use a mean and stddev learnt from the training data instead of using the statistics from the test data. (This both increases test accuracy because the statistics have less variance, and if the test data does not have the same distribution as the training data then we must use the training statistics to ensure the effective network does not change when switching to testing anyhow.) This does however introduces a slight subtlety. The first few time steps of the RNN tend to have varying statistics (mean and variance) before settling down to a steady value. Therefore in general, better performance is obtained by using separate statistics for the first few time steps, and then using the final set of statistics for all subsequent time steps. This is controlled by the parameter max_unique_stats . (We can't have an unbounded number of distinct statistics for both technical reasons and also for the case where test sequences are longer than anything seen in training.) You may be fine leaving it at its default value of 1. Small values (like 10) may achieve better performance on some tasks when testing with cached statistics. Attributes: state_size: Tuple of tf.TensorShape s indicating the size of state tensors. output_size: tf.TensorShape indicating the size of the core output. use_peepholes: Boolean indicating whether peephole connections are used. use_batch_norm_h: Boolean indicating whether batch norm (h) is enabled. use_batch_norm_x: Boolean indicating whether batch norm (x) is enabled. use_batch_norm_c: Boolean indicating whether batch norm (c) is enabled. BatchNormLSTM.__init__(hidden_size, forget_bias=1.0, initializers=None, partitioners=None, regularizers=None, use_peepholes=False, use_batch_norm_h=True, use_batch_norm_x=False, use_batch_norm_c=False, max_unique_stats=1, hidden_clip_value=None, cell_clip_value=None, custom_getter=None, name='batch_norm_lstm') Construct BatchNormLSTM . Args: hidden_size : (int) Hidden size dimensionality. forget_bias : (float) Bias for the forget activation. initializers : Dict containing ops to initialize the weights. This dictionary may contain any of the keys returned by BatchNormLSTM.get_possible_initializer_keys . The gamma and beta variables control batch normalization values for different batch norm transformations inside the cell; see the paper for details. partitioners : Optional dict containing partitioners to partition the weights and biases. As a default, no partitioners are used. This dict may contain any of the keys returned by BatchNormLSTM.get_possible_initializer_keys . regularizers : Optional dict containing regularizers for the weights and biases. As a default, no regularizers are used. This dict may contain any of the keys returned by BatchNormLSTM.get_possible_initializer_keys . use_peepholes : Boolean that indicates whether peephole connections are used. use_batch_norm_h : Boolean that indicates whether to apply batch normalization at the previous_hidden - gates contribution. If you are experimenting with batch norm then this may be the most effective to use, and is enabled by default. use_batch_norm_x : Boolean that indicates whether to apply batch normalization at the input - gates contribution. use_batch_norm_c : Boolean that indicates whether to apply batch normalization at the cell - output contribution. max_unique_stats : The maximum number of steps to use unique batch norm statistics for. (See module description above for more details.) hidden_clip_value : Optional number; if set, then the LSTM hidden state vector is clipped by this value. cell_clip_value : Optional number; if set, then the LSTM cell vector is clipped by this value. custom_getter : Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. See the tf.get_variable documentation for more details. name : Name of the module. Raises: KeyError : if initializers contains any keys not returned by BatchNormLSTM.get_possible_initializer_keys . KeyError : if partitioners contains any keys not returned by BatchNormLSTM.get_possible_initializer_keys . KeyError : if regularizers contains any keys not returned by BatchNormLSTM.get_possible_initializer_keys . ValueError : if a peephole initializer is passed in the initializer list, but use_peepholes is False. ValueError : if a batch norm initializer is passed in the initializer list, but batch norm is disabled. ValueError : if none of the use_batch_norm_* options are True. ValueError : if max_unique_stats is 1. BatchNormLSTM.__call__(inputs, prev_state, is_training=None, test_local_stats=True) Connects the LSTM module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided as inputs and state must have the same final dimension, in order for the existing variables to be the correct size for their corresponding multiplications. The batch size may differ for each connection. Args: inputs : Tensor of size [batch_size, input_size] . prev_state : Tuple (prev_hidden, prev_cell), or if batch norm is enabled and max_unique_stats 1 , then (prev_hidden, prev_cell, time_step). Here, prev_hidden and prev_cell are tensors of size [batch_size, hidden_size] , and time_step is used to indicate the current RNN step. is_training : Boolean indicating whether we are in training mode (as opposed to testing mode), passed to the batch norm modules. Note to use this you must wrap the cell via the with_batch_norm_control function. test_local_stats : Boolean indicating whether to use local batch statistics in test mode. See the BatchNorm documentation for more on this. Returns: A tuple (output, next_state) where 'output' is a Tensor of size [batch_size, hidden_size] and 'next_state' is a tuple (next_hidden, next_cell) or (next_hidden, next_cell, time_step + 1), where next_hidden and next_cell have size [batch_size, hidden_size] . Raises: ValueError : If connecting the module into the graph any time after the first time, and the inferred size of the inputs does not match previous invocations. BatchNormLSTM.connected_subgraphs Returns the subgraphs created by this module so far. BatchNormLSTM.defun() Wraps this modules call method in a callable graph function. BatchNormLSTM.defun_wrapped Returns boolean indicating whether this module is defun wrapped. BatchNormLSTM.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormLSTM.get_possible_initializer_keys(use_peepholes=False, use_batch_norm_h=True, use_batch_norm_x=False, use_batch_norm_c=False) Returns the keys the dictionary of variable initializers may contain. The set of all possible initializer keys are: w_gates : weight for gates b_gates : bias of gates w_f_diag : weight for prev_cell - forget gate peephole w_i_diag : weight for prev_cell - input gate peephole w_o_diag : weight for prev_cell - output gate peephole gamma_h : batch norm scaling for previous_hidden - gates gamma_x : batch norm scaling for input - gates gamma_c : batch norm scaling for cell - output beta_c : batch norm bias for cell - output Args: cls:The class. use_peepholes : Boolean that indicates whether peephole connections are used. use_batch_norm_h : Boolean that indicates whether to apply batch normalization at the previous_hidden - gates contribution. If you are experimenting with batch norm then this may be the most effective to turn on. use_batch_norm_x : Boolean that indicates whether to apply batch normalization at the input - gates contribution. use_batch_norm_c : Boolean that indicates whether to apply batch normalization at the cell - output contribution. Returns: Set with strings corresponding to the strings that may be passed to the constructor. BatchNormLSTM.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormLSTM.graph Returns the Graph instance which the module is connected to, or None. BatchNormLSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None) Builds the default start state tensor of zeros. Args: batch_size : An int, float or scalar Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. trainable_initializers : An optional pair of initializers for the initial hidden state and cell state. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor tuple ([batch_size, state_size], [batch_size, state_size], ?) filled with zeros, with the third entry present when batch norm is enabled with max_unique_stats 1', with value 0` (representing the time step). BatchNormLSTM.is_connected Returns true iff the Module been connected to the Graph at least once. BatchNormLSTM.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormLSTM.module_name Returns the name of the Module. BatchNormLSTM.name_scopes Returns a tuple of all name_scopes generated by this module. BatchNormLSTM.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormLSTM.output_size tf.TensorShape indicating the size of the core output. BatchNormLSTM.scope_name Returns the full name of the Module's variable scope. BatchNormLSTM.state_size Tuple of tf.TensorShape s indicating the size of state tensors. BatchNormLSTM.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormLSTM.use_batch_norm_c Boolean indicating whether batch norm for cell - output is enabled. BatchNormLSTM.use_batch_norm_h Boolean indicating whether batch norm for hidden - gates is enabled. BatchNormLSTM.use_batch_norm_x Boolean indicating whether batch norm for input - gates is enabled. BatchNormLSTM.use_peepholes Boolean indicating whether peephole connections are used. BatchNormLSTM.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormLSTM.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormLSTM.with_batch_norm_control(is_training, test_local_stats=True) Wraps this RNNCore with the additional control input to the BatchNorm s. Example usage: lstm = snt.BatchNormLSTM(4) is_training = tf.placeholder(tf.bool) rnn_input = ... my_rnn = rnn.rnn(lstm.with_batch_norm_control(is_training), rnn_input) Args: is_training : Boolean that indicates whether we are in training mode or testing mode. When in training mode, the batch norm statistics are taken from the given batch, and moving statistics are updated. When in testing mode, the moving statistics are not updated, and in addition if test_local_stats is False then the moving statistics are used for the batch statistics. See the BatchNorm module for more details. test_local_stats : Boolean scalar indicated whether to use local batch statistics in test mode. Returns: snt.RNNCore wrapping this class with the extra input(s) added. BatchNormLSTM.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class BatchNormV2 Batch normalization module, including optional affine transformation. This module maintains exponential moving averages of the mean and variance, which can be optionally used to normalize at test time. At training time, batch statistics (mean, variance) are not shared between separate connections. The moving averages are shared between separate connections. At both training and test time, the optional affine transformation ( * gamma + beta ) is shared between separate connections. This is also the case for distributed replica training, where the batch statistics are not aggregated across replicas, but the moving averages are shared globally. When connecting the module to the graph, is_training=True means that Update ops are created to update the moving averages with the current batch's statistics. Features are normalized using the current batch's statistics . The test_local_stats setting is ignored. The moving averages are not used. whereas is_training=False means that Update ops are not created. Features are normalized using either: The moving averages if test_local_stats=False (default). The test batch statistics if test_local_stats=True . The moving averages are used by default at test time, but local batch statistics can be used by specifying a flag when connecting. One often wants to use local batch statistics at test time to track the progress while the model is trained as it would ensure that moving average updates do not affect the training curves. Once the training is finished, it's often advantageous to use moving average statistics, since it would make evaluation agnostic to the batch size, and might even lead to small improvements over the local batch statistics. The moving averages will be updated automatically by default, but not if update_ops_collection is provided: in that case they will only be updated when the ops in that collection are run. For example, to run the updates automatically: bn = BatchNormV2() train_net = bn(train_inputs, is_training=True) this does, however, have the effect of blocking the forwards pass of the network until the update ops have been run and may have a small performance penalty. For example, to run the updates manually: bn = BatchNormV2(update_ops_collection=tf.GraphKeys.UPDATE_OPS) train_net = bn(train_inputs, is_training=True) ... update_ops = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS)) train_op = tf.group(train_op, update_ops) Then, whenever train_op is run so also are the moving average update ops. Some batch normalization caveats: Batch normalization will remove the effect of adding a bias, so e.g. use_bias=False should be used for an immediately preceding snt.Linear module. If your data batches aren't i.i.d. then batch normalization can allow your network to 'cheat' by using the batch statistics to peek at the rest of the batch. This can exhibit itself as a higher test score with test_local_stats=True than test_local_stats=False . BatchNormV2.__init__(data_format=None, offset=True, scale=False, decay_rate=0.999, eps=0.001, initializers=None, partitioners=None, regularizers=None, update_ops_collection=None, fused=True, name='batch_norm') Constructs a BatchNormV2 module. Reduces over all input tensor dimensions apart from the channel dimension. This has the effect of treating pixels in 1D/2D/3D images as additional elements of the minibatch. Args: data_format : The data format. Can be \"NC\", \"NWC\", \"NCW\", \"NHWC\", \"NCHW\", \"NDHWC\", or \"NCDHW\". If not provided we assume the channel dimension is last. offset : Optional boolean to specify whether or not to apply a trained component-wise bias after the batch normalization and scaling. scale : Optional boolean to specify whether or not to apply a trained component-wise scale after the batch normalization. decay_rate : Decay rate of the exponential moving averages of the mean and variance. eps : Small number to avoid dividing by zero when diving by the standard deviation. initializers : Optional dict containing ops to initialize the weights of the affine transform ( gamma and beta ). partitioners : Optional dict containing partitioners to partition the weights of the affine transform ( gamma and beta ). regularizers : Optional dict containing regularizers for the weights of the affine transform (\"gamma\" and \"beta\"). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . update_ops_collection : Optional name of TensorFlow variable collection to add the moving average update ops to. If not provided, we instead add the update ops as control dependencies of the output of the module. This may result in some slowdown, as the feed-forward of the network is now blocked. fused : Use nn.fused_batch_norm if True, nn.batch_normalization otherwise. name : Name of the module. Raises: KeyError : If initializers contains any keys other than gamma , beta , moving_mean or moving_variance . KeyError : If partitioners or regularizers contains any keys other than gamma or beta . TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If data_format is invalid. BatchNormV2.__call__(input_batch, is_training, test_local_stats=False) Connects the BatchNormV2 module into the graph. Args: input_batch : A Tensor of the same dimension as len(data_format) . is_training : A boolean to indicate if the module should be connected in training mode, meaning the moving averages are updated. Can be a Tensor. test_local_stats : A boolean to indicate if local batch statistics should be used when is_training=False . If not, moving averages are used. By default False . Can be a Tensor. Returns: A tensor with the same shape as input_batch . Raises: base.IncompatibleShapeError: If data_format is not valid for the input shape. base.NotSupportedError: If input_batch has data type of tf.bfloat16 . BatchNormV2.beta BatchNormV2.connected_subgraphs Returns the subgraphs created by this module so far. BatchNormV2.defun() Wraps this modules call method in a callable graph function. BatchNormV2.defun_wrapped Returns boolean indicating whether this module is defun wrapped. BatchNormV2.gamma BatchNormV2.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormV2.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. BatchNormV2.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormV2.graph Returns the Graph instance which the module is connected to, or None. BatchNormV2.initializers BatchNormV2.is_connected Returns true iff the Module been connected to the Graph at least once. BatchNormV2.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormV2.module_name Returns the name of the Module. BatchNormV2.moving_mean BatchNormV2.moving_variance BatchNormV2.name_scopes Returns a tuple of all name_scopes generated by this module. BatchNormV2.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormV2.partitioners BatchNormV2.regularizers BatchNormV2.scope_name Returns the full name of the Module's variable scope. BatchNormV2.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormV2.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. BatchNormV2.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class BatchReshape Reshapes input Tensor, preserving the batch dimension. BatchReshape.__init__(shape, preserve_dims=1, name='batch_reshape') Constructs a BatchReshape module. Args: shape : Shape to reshape the input Tensor to while preserving its first preserve_dims dimensions; shape can be either a tuple/list, or a callable that returns the actual shape. The callable does not need to be ready to return something meaningful at construction time, but it will be required to be able to do so when the module is connected to the graph. When the special value -1 appears in shape the corresponding size is automatically inferred. Note that -1 can only appear once in shape . To flatten all non-batch dimensions, the snt.BatchFlatten module can also be used. preserve_dims : Number of leading dimensions that will not be reshaped. For example, given an input Tensor with shape [B, H, W, C, D] , and argument shape equal to (-1, D) : * preserve_dims=1 will return a Tensor with shape [B, H*W*C, D] . * preserve_dims=2 will return a Tensor with shape [B, H, W*C, D] . * preserve_dims=3 will return a Tensor with shape [B, H, W, C, D] . * preserve_dims=4 will return a Tensor with shape [B, H, W, C, 1, D] . * preserve_dims =5 will throw an error on build unless D=1. The preserved dimensions can be unknown at building time. name : Name of the module. Raises: ValueError : If preserve_dims = 0 . BatchReshape.__call__(inputs) Connects the module into the graph, with input Tensor inputs . Args: inputs : A Tensor of shape [b_1, b_2, ..., b_preserve_dims, b_preserve_dims+1, ...]. Returns: A Tensor of shape [b_1, b_2, ..., b_preserve_dims, b_reshape_1, b_reshape_2, ...], with reshaping defined by the constructor shape parameter. Raises: ValueError : If output shape is incompatible with input shape; or if shape array contains non numeric entries; or if shape array contains more than 1 wildcard -1; or if the input array contains unknown, non-preserved dimensions (except when the unknown dimension is the only non-preserved dimension and doesn't actually need reshaping). BatchReshape.connected_subgraphs Returns the subgraphs created by this module so far. BatchReshape.defun() Wraps this modules call method in a callable graph function. BatchReshape.defun_wrapped Returns boolean indicating whether this module is defun wrapped. BatchReshape.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchReshape.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. BatchReshape.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchReshape.graph Returns the Graph instance which the module is connected to, or None. BatchReshape.input_shape Returns shape of input Tensor passed at last call to build . BatchReshape.is_connected Returns true iff the Module been connected to the Graph at least once. BatchReshape.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. BatchReshape.module_name Returns the name of the Module. BatchReshape.name_scopes Returns a tuple of all name_scopes generated by this module. BatchReshape.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchReshape.scope_name Returns the full name of the Module's variable scope. BatchReshape.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BatchReshape.transpose(name=None) Returns transpose batch reshape. BatchReshape.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. BatchReshape.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class BidirectionalRNN Bidirectional RNNCore that processes the sequence forwards and backwards. Based upon the encoder implementation in: https://arxiv.org/abs/1409.0473 This interface of this module is different than the typical ones found in the RNNCore family. The primary difference is that it is pre-conditioned on the full input sequence in order to produce a full sequence of outputs and states concatenated along the feature dimension among the forward and backward cores. BidirectionalRNN.__init__(forward_core, backward_core, name='bidir_rnn') Construct a Bidirectional RNN core. Args: forward_core : callable RNNCore module that computes forward states. backward_core : callable RNNCore module that computes backward states. name : name of the module. Raises: ValueError : if not all the modules are recurrent. BidirectionalRNN.__call__(input_sequence, state) Connects the BidirectionalRNN module into the graph. Args: input_sequence : tensor (time, batch, [feature_1, ..]). It must be time_major. state : tuple of states for the forward and backward cores. Returns: A dict with forward/backard states and output sequences: \"outputs\":{ \"forward\": ..., \"backward\": ...}, \"state\": { \"forward\": ..., \"backward\": ...} Raises: ValueError : in case time dimension is not statically known. BidirectionalRNN.connected_subgraphs Returns the subgraphs created by this module so far. BidirectionalRNN.defun() Wraps this modules call method in a callable graph function. BidirectionalRNN.defun_wrapped Returns boolean indicating whether this module is defun wrapped. BidirectionalRNN.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BidirectionalRNN.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. BidirectionalRNN.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BidirectionalRNN.graph Returns the Graph instance which the module is connected to, or None. BidirectionalRNN.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None) Builds the default start state for a BidirectionalRNN. The Bidirectional RNN flattens the states of its forward and backward cores and concatentates them. Args: batch_size : An int, float or scalar Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: Tuple of initial states from forward and backward RNNs. BidirectionalRNN.is_connected Returns true iff the Module been connected to the Graph at least once. BidirectionalRNN.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. BidirectionalRNN.module_name Returns the name of the Module. BidirectionalRNN.name_scopes Returns a tuple of all name_scopes generated by this module. BidirectionalRNN.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BidirectionalRNN.output_size Flattened output size of cores. BidirectionalRNN.scope_name Returns the full name of the Module's variable scope. BidirectionalRNN.state_size Flattened state size of cores. BidirectionalRNN.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. BidirectionalRNN.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. BidirectionalRNN.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class CausalConv1D 1D convolution module, including optional bias. This is deprecated, please use the padding=CAUSAL argument to Conv1D. This acts as a light wrapper around _ConvND ensuring that the outputs at index i only depend on indices smaller than i (also known as a causal convolution). For further details on the theoretical background, refer to: https://arxiv.org/abs/1610.10099 CausalConv1D.__init__(output_channels, kernel_shape, stride=1, rate=1, use_bias=True, initializers=None, partitioners=None, regularizers=None, mask=None, padding='CAUSAL', data_format='NWC', custom_getter=None, name='causal_conv_1d') Constructs a CausalConv1D module. This is deprecated, please use the padding=CAUSAL argument to Conv1D. Args: output_channels : Number of output channels. output_channels can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_channels can be called, returning an integer, when build is called. kernel_shape : Sequence of kernel sizes (of size 1), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 1), or integer that is used to define stride in all dimensions. rate : Sequence of dilation rates (of size 1), or integer that is used to define dilation rate in all dimensions. 1 corresponds to standard convolution, rate 1 corresponds to dilated convolution. Cannot be 1 if any of stride is also 1. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializer for the weights is a truncated normal initializer, which is commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . mask : A convertible to a 3D tensor which is multiplied component-wise with the weights (Optional). padding : Padding algorithm. Should be snt.CAUSAL . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NWC), or the second dimension (NCW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. base.IncompatibleShapeError: If the given rate is not an integer; or if the given rate is not a sequence of two integers. base.IncompatibleShapeError: If a mask is a TensorFlow Tensor with a not fully defined shape. base.NotSupportedError: If rate in any dimension and the stride in any dimension are simultaneously 1. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. TypeError : If mask is given and it is not convertible to a Tensor. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_1D_DATA_FORMATS ). CausalConv1D.__call__(inputs) Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection. Args: inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Returns: A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels]. Raises: ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . CausalConv1D.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias. CausalConv1D.clone(name=None) Returns a cloned _ConvND module. Args: name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name . Returns: A copy of the current class. CausalConv1D.connected_subgraphs Returns the subgraphs created by this module so far. CausalConv1D.conv_op_padding Returns the padding algorithm used for the underlying convolution op. CausalConv1D.data_format Returns the data format. CausalConv1D.defun() Wraps this modules call method in a callable graph function. CausalConv1D.defun_wrapped Returns boolean indicating whether this module is defun wrapped. CausalConv1D.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. CausalConv1D.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. CausalConv1D.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. CausalConv1D.graph Returns the Graph instance which the module is connected to, or None. CausalConv1D.has_bias Returns True if bias Variable is present in the module. CausalConv1D.initializers Returns the initializers dictionary. CausalConv1D.input_channels Returns the number of input channels. CausalConv1D.input_shape Returns the input shape. CausalConv1D.is_connected Returns true iff the Module been connected to the Graph at least once. CausalConv1D.kernel_shape Returns the kernel shape. CausalConv1D.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. CausalConv1D.mask Returns the mask. CausalConv1D.module_name Returns the name of the Module. CausalConv1D.name_scopes Returns a tuple of all name_scopes generated by this module. CausalConv1D.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. CausalConv1D.output_channels Returns the number of output channels. CausalConv1D.padding Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension. Returns: The padding algorithm used, if this is the same for all dimensions. Raises: ValueError : If different padding algorithms are used for different dimensions. CausalConv1D.paddings Returns a tuple with the padding algorithm used for each dimension. CausalConv1D.partitioners Returns the partitioners dictionary. CausalConv1D.rate Returns the dilation rate. CausalConv1D.regularizers Returns the regularizers dictionary. CausalConv1D.scope_name Returns the full name of the Module's variable scope. CausalConv1D.stride Returns the stride. CausalConv1D.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. CausalConv1D.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. CausalConv1D.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. CausalConv1D.w Returns the Variable containing the weight matrix. class ConcatLinear Linear transformation of a number of concatenated inputs. This class ensures that at initialisation, the relative importance of all inputs are similar even if they have very different sizes. This assumes that all inputs have roughly the same range of values. For example, the following code also concatenates a list of inputs and applies a linear transform: inp = tf.concat(input_list, axis=-1) return snt.Linear(output_size)(inp) The issue with the above code is that if input_list is made of two Tensors of very different shapes such as [batch_size, 1] and [batch_size, 128] , then almost no signal will be received from the first Tensor. This class works around this problem by using a weight matrix with relatively larger coefficients for the first Tensor than for the second one. ConcatLinear.__init__(output_size, use_bias=True, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='concat_linear') Constructs a ConcatLinear module. Args: output_size : Output dimensionality. output_size can be either an integer or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_size can be called, returning an integer, when build is called. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing initializers to initialize the weights (with key 'w') or biases (with key 'b'). The default initializer for the weights is a truncated normal initializer, which is commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the weights (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. ConcatLinear.__call__(inputs_list) Connects the module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided here must have the same final dimensions as when called the first time, in order for the existing variables to be the correct size for the multiplication. The batch size may differ for each connection. Args: inputs_list : A list of 2D Tensors of rank 2, with leading batch dimension. Returns: A 2D Tensor of size [batch_size, output_size]. ConcatLinear.connected_subgraphs Returns the subgraphs created by this module so far. ConcatLinear.defun() Wraps this modules call method in a callable graph function. ConcatLinear.defun_wrapped Returns boolean indicating whether this module is defun wrapped. ConcatLinear.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ConcatLinear.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. ConcatLinear.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ConcatLinear.graph Returns the Graph instance which the module is connected to, or None. ConcatLinear.is_connected Returns true iff the Module been connected to the Graph at least once. ConcatLinear.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. ConcatLinear.module_name Returns the name of the Module. ConcatLinear.name_scopes Returns a tuple of all name_scopes generated by this module. ConcatLinear.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ConcatLinear.scope_name Returns the full name of the Module's variable scope. ConcatLinear.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ConcatLinear.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. ConcatLinear.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class Conv1D 1D convolution module, including optional bias. This acts as a light wrapper around the class _ConvND . Conv1D.__init__(output_channels, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, mask=None, data_format='NWC', custom_getter=None, name='conv_1d') Constructs a Conv1D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution Args: output_channels : Number of output channels. output_channels can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_channels can be called, returning an integer, when build is called. kernel_shape : Sequence of kernel sizes (of size 1), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 1), or integer that is used to define stride in all dimensions. rate : Sequence of dilation rates (of size 1), or integer that is used to define dilation rate in all dimensions. 1 corresponds to standard convolution, rate 1 corresponds to dilated convolution. Cannot be 1 if any of stride is also 1. padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 1. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializer for the weights is a truncated normal initializer, which is commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . mask : A convertible to a 3D tensor which is multiplied component-wise with the weights (Optional). data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NWC), or the second dimension (NCW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. base.IncompatibleShapeError: If the given rate is not an integer; or if the given rate is not a sequence of two integers. base.IncompatibleShapeError: If a mask is a TensorFlow Tensor with a not fully defined shape. base.NotSupportedError: If rate in any dimension and the stride in any dimension are simultaneously 1. ValueError : If the given padding is not snt.VALID or snt.SAME . KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. TypeError : If mask is given and it is not convertible to a Tensor. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_1D_DATA_FORMATS ). Conv1D.__call__(inputs) Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection. Args: inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Returns: A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels]. Raises: ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Conv1D.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias. Conv1D.clone(name=None) Returns a cloned _ConvND module. Args: name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name . Returns: A copy of the current class. Conv1D.connected_subgraphs Returns the subgraphs created by this module so far. Conv1D.conv_op_padding Returns the padding algorithm used for the underlying convolution op. Conv1D.data_format Returns the data format. Conv1D.defun() Wraps this modules call method in a callable graph function. Conv1D.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Conv1D.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1D.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Conv1D.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1D.graph Returns the Graph instance which the module is connected to, or None. Conv1D.has_bias Returns True if bias Variable is present in the module. Conv1D.initializers Returns the initializers dictionary. Conv1D.input_channels Returns the number of input channels. Conv1D.input_shape Returns the input shape. Conv1D.is_connected Returns true iff the Module been connected to the Graph at least once. Conv1D.kernel_shape Returns the kernel shape. Conv1D.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1D.mask Returns the mask. Conv1D.module_name Returns the name of the Module. Conv1D.name_scopes Returns a tuple of all name_scopes generated by this module. Conv1D.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1D.output_channels Returns the number of output channels. Conv1D.padding Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension. Returns: The padding algorithm used, if this is the same for all dimensions. Raises: ValueError : If different padding algorithms are used for different dimensions. Conv1D.paddings Returns a tuple with the padding algorithm used for each dimension. Conv1D.partitioners Returns the partitioners dictionary. Conv1D.rate Returns the dilation rate. Conv1D.regularizers Returns the regularizers dictionary. Conv1D.scope_name Returns the full name of the Module's variable scope. Conv1D.stride Returns the stride. Conv1D.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1D.transpose(name=None) Returns matching Conv1DTranspose module. Args: name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.name . Returns: Conv1DTranspose module. Raises: base.NotSupportedError: If rate in any dimension 1. Conv1D.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Conv1D.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1D.w Returns the Variable containing the weight matrix. class Conv1DLSTM 1D convolutional LSTM. Conv1DLSTM.__init__(name='conv_1d_lstm', **kwargs) Construct Conv1DLSTM. See snt.ConvLSTM for more details. Conv1DLSTM.__call__(inputs, state) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). Conv1DLSTM.connected_subgraphs Returns the subgraphs created by this module so far. Conv1DLSTM.convolutions Conv1DLSTM.defun() Wraps this modules call method in a callable graph function. Conv1DLSTM.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Conv1DLSTM.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DLSTM.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Conv1DLSTM.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DLSTM.graph Returns the Graph instance which the module is connected to, or None. Conv1DLSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. Conv1DLSTM.is_connected Returns true iff the Module been connected to the Graph at least once. Conv1DLSTM.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DLSTM.module_name Returns the name of the Module. Conv1DLSTM.name_scopes Returns a tuple of all name_scopes generated by this module. Conv1DLSTM.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DLSTM.output_size tf.TensorShape indicating the size of the core output. Conv1DLSTM.scope_name Returns the full name of the Module's variable scope. Conv1DLSTM.state_size Tuple of tf.TensorShape s indicating the size of state tensors. Conv1DLSTM.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DLSTM.use_layer_norm Boolean indicating whether layer norm is enabled. Conv1DLSTM.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DLSTM.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DLSTM.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class Conv1DTranspose 1D transposed / reverse / up 1D convolution module, including bias. This performs a 1D transpose convolution by lightly wrapping the TensorFlow op tf.nn.conv2d_transpose , setting the size of the height dimension of the image to 1. Conv1DTranspose.__init__(output_channels, output_shape=None, kernel_shape=None, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NWC', custom_getter=None, name='conv_1d_transpose') Constructs a Conv1DTranspose module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution Args: output_channels : Number of output channels. Can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure output_channels can be called, returning an integer, when build is called. output_shape : Output shape of transpose convolution. Can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_shape can be called, returning an iterable of format (out_length) when build is called. If a None value is given, a default shape is automatically calculated (see docstring of _default_transpose_size function for more details). kernel_shape : Sequence of kernel sizes (of size 1), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 1), or integer that is used to define stride in all dimensions. padding : Padding algorithm, either snt.SAME or snt.VALID . use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NWC), or the second dimension (NCW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two or four integers. ValueError : If the given padding is not snt.VALID or snt.SAME . ValueError : If the given kernel_shape is None . KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_1D_DATA_FORMATS ). Conv1DTranspose.__call__(inputs) Connects the _ConvNDTranspose module into the graph. If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same final N dimensions, in order for the existing variables to be the correct size for the multiplication. The batch size may differ for each connection. Args: inputs : A Tensor of shape data_format and of type tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Returns: A Tensor of shape data_format and of type tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Raises: ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If output_shape is an iterable and is not in the format (out_height, out_width) . TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Conv1DTranspose.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias. Conv1DTranspose.connected_subgraphs Returns the subgraphs created by this module so far. Conv1DTranspose.conv_op_padding Returns the padding algorithm used for the underlying convolution op. Conv1DTranspose.defun() Wraps this modules call method in a callable graph function. Conv1DTranspose.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Conv1DTranspose.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DTranspose.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Conv1DTranspose.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DTranspose.graph Returns the Graph instance which the module is connected to, or None. Conv1DTranspose.has_bias Returns True if bias Variable is present in the module. Conv1DTranspose.initializers Returns the initializers dictionary. Conv1DTranspose.input_channels Returns the number of input channels. Conv1DTranspose.input_shape Returns the input shape. Conv1DTranspose.is_connected Returns true iff the Module been connected to the Graph at least once. Conv1DTranspose.kernel_shape Returns the kernel shape. Conv1DTranspose.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DTranspose.module_name Returns the name of the Module. Conv1DTranspose.name_scopes Returns a tuple of all name_scopes generated by this module. Conv1DTranspose.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DTranspose.output_channels Returns the number of output channels. Conv1DTranspose.output_shape Returns the output shape. Conv1DTranspose.padding Returns the padding algorithm. Conv1DTranspose.partitioners Returns the partitioners dictionary. Conv1DTranspose.regularizers Returns the regularizers dictionary. Conv1DTranspose.scope_name Returns the full name of the Module's variable scope. Conv1DTranspose.stride Returns the stride. Conv1DTranspose.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DTranspose.transpose(name=None) Returns matching Conv1D module. Args: name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.name . Returns: Conv1D module. Conv1DTranspose.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DTranspose.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv1DTranspose.w Returns the Variable containing the weight matrix. class Conv2D Spatial convolution and dilated convolution module, including bias. This acts as a light wrapper around the class _ConvND . Conv2D.__init__(output_channels, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, mask=None, data_format='NHWC', custom_getter=None, name='conv_2d') Constructs a Conv2D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution Args: output_channels : Number of output channels. output_channels can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_channels can be called, returning an integer, when build is called. kernel_shape : Sequence of kernel sizes (of size 2), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 2), or integer that is used to define stride in all dimensions. rate : Sequence of dilation rates (of size 2), or integer that is used to define dilation rate in all dimensions. 1 corresponds to standard 2D convolution, rate 1 corresponds to dilated convolution. Cannot be 1 if any of stride is also 1. padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 2. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializer for the weights is a truncated normal initializer, which is commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . mask : A convertible to a 4D tensor which is multiplied component-wise with the weights (Optional). data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NHWC), or the second dimension (NCHW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. base.IncompatibleShapeError: If the given rate is not an integer; or if the given rate is not a sequence of two integers. base.IncompatibleShapeError: If a mask is given and its rank is neither 2 nor 4, or if it is a TensorFlow Tensor with a not fully defined shape. base.NotSupportedError: If rate in any dimension and the stride in any dimension are simultaneously 1. ValueError : If the given padding is not snt.VALID , snt.SAME , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a sequence of these. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. TypeError : If mask is given and it is not convertible to a Tensor. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_2D_DATA_FORMATS ). Conv2D.__call__(inputs) Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection. Args: inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Returns: A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels]. Raises: ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Conv2D.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias. Conv2D.clone(name=None) Returns a cloned _ConvND module. Args: name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name . Returns: A copy of the current class. Conv2D.connected_subgraphs Returns the subgraphs created by this module so far. Conv2D.conv_op_padding Returns the padding algorithm used for the underlying convolution op. Conv2D.data_format Returns the data format. Conv2D.defun() Wraps this modules call method in a callable graph function. Conv2D.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Conv2D.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2D.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Conv2D.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2D.graph Returns the Graph instance which the module is connected to, or None. Conv2D.has_bias Returns True if bias Variable is present in the module. Conv2D.initializers Returns the initializers dictionary. Conv2D.input_channels Returns the number of input channels. Conv2D.input_shape Returns the input shape. Conv2D.is_connected Returns true iff the Module been connected to the Graph at least once. Conv2D.kernel_shape Returns the kernel shape. Conv2D.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2D.mask Returns the mask. Conv2D.module_name Returns the name of the Module. Conv2D.name_scopes Returns a tuple of all name_scopes generated by this module. Conv2D.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2D.output_channels Returns the number of output channels. Conv2D.padding Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension. Returns: The padding algorithm used, if this is the same for all dimensions. Raises: ValueError : If different padding algorithms are used for different dimensions. Conv2D.paddings Returns a tuple with the padding algorithm used for each dimension. Conv2D.partitioners Returns the partitioners dictionary. Conv2D.rate Returns the dilation rate. Conv2D.regularizers Returns the regularizers dictionary. Conv2D.scope_name Returns the full name of the Module's variable scope. Conv2D.stride Returns the stride. Conv2D.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2D.transpose(name=None) Returns matching Conv2DTranspose module. Args: name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.name . Returns: Conv2DTranspose module. Raises: base.NotSupportedError: If rate in any dimension 1. Conv2D.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Conv2D.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2D.w Returns the Variable containing the weight matrix. class Conv2DLSTM 2D convolutional LSTM. Conv2DLSTM.__init__(name='conv_2d_lstm', **kwargs) Construct Conv2DLSTM. See snt.ConvLSTM for more details. Conv2DLSTM.__call__(inputs, state) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). Conv2DLSTM.connected_subgraphs Returns the subgraphs created by this module so far. Conv2DLSTM.convolutions Conv2DLSTM.defun() Wraps this modules call method in a callable graph function. Conv2DLSTM.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Conv2DLSTM.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DLSTM.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Conv2DLSTM.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DLSTM.graph Returns the Graph instance which the module is connected to, or None. Conv2DLSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. Conv2DLSTM.is_connected Returns true iff the Module been connected to the Graph at least once. Conv2DLSTM.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DLSTM.module_name Returns the name of the Module. Conv2DLSTM.name_scopes Returns a tuple of all name_scopes generated by this module. Conv2DLSTM.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DLSTM.output_size tf.TensorShape indicating the size of the core output. Conv2DLSTM.scope_name Returns the full name of the Module's variable scope. Conv2DLSTM.state_size Tuple of tf.TensorShape s indicating the size of state tensors. Conv2DLSTM.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DLSTM.use_layer_norm Boolean indicating whether layer norm is enabled. Conv2DLSTM.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DLSTM.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DLSTM.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class Conv2DTranspose Spatial transposed / reverse / up 2D convolution module, including bias. This acts as a light wrapper around the TensorFlow op tf.nn.conv2d_transpose abstracting away variable creation and sharing. Conv2DTranspose.__init__(output_channels, output_shape=None, kernel_shape=None, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NHWC', custom_getter=None, name='conv_2d_transpose') Constructs a Conv2DTranspose module . See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution Args: output_channels : Number of output channels. Can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure output_channels can be called, returning an integer, when build is called. output_shape : Output shape of transpose convolution. Can be either an iterable of integers or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_shape can be called, returning an iterable of format (out_height, out_width) when build is called. Note that output_shape defines the size of output signal domain, as opposed to the shape of the output Tensor . If a None value is given, a default shape is automatically calculated (see docstring of _default_transpose_size function for more details). kernel_shape : Sequence of kernel sizes (of size 2), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 2), or integer that is used to define stride in all dimensions. padding : Padding algorithm, either snt.SAME or snt.VALID . use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NHWC), or the second dimension (\"NCHW\"). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: base.IncompatibleShapeError: If the given kernel shape is neither an integer nor a sequence of two integers. base.IncompatibleShapeError: If the given stride is neither an integer nor a sequence of two or four integers. ValueError : If the given padding is not snt.VALID or snt.SAME . ValueError : If the given kernel_shape is None . KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_2D_DATA_FORMATS ). Conv2DTranspose.__call__(inputs) Connects the _ConvNDTranspose module into the graph. If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same final N dimensions, in order for the existing variables to be the correct size for the multiplication. The batch size may differ for each connection. Args: inputs : A Tensor of shape data_format and of type tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Returns: A Tensor of shape data_format and of type tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Raises: ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If output_shape is an iterable and is not in the format (out_height, out_width) . TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Conv2DTranspose.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias. Conv2DTranspose.connected_subgraphs Returns the subgraphs created by this module so far. Conv2DTranspose.conv_op_padding Returns the padding algorithm used for the underlying convolution op. Conv2DTranspose.defun() Wraps this modules call method in a callable graph function. Conv2DTranspose.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Conv2DTranspose.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DTranspose.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Conv2DTranspose.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DTranspose.graph Returns the Graph instance which the module is connected to, or None. Conv2DTranspose.has_bias Returns True if bias Variable is present in the module. Conv2DTranspose.initializers Returns the initializers dictionary. Conv2DTranspose.input_channels Returns the number of input channels. Conv2DTranspose.input_shape Returns the input shape. Conv2DTranspose.is_connected Returns true iff the Module been connected to the Graph at least once. Conv2DTranspose.kernel_shape Returns the kernel shape. Conv2DTranspose.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DTranspose.module_name Returns the name of the Module. Conv2DTranspose.name_scopes Returns a tuple of all name_scopes generated by this module. Conv2DTranspose.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DTranspose.output_channels Returns the number of output channels. Conv2DTranspose.output_shape Returns the output shape. Conv2DTranspose.padding Returns the padding algorithm. Conv2DTranspose.partitioners Returns the partitioners dictionary. Conv2DTranspose.regularizers Returns the regularizers dictionary. Conv2DTranspose.scope_name Returns the full name of the Module's variable scope. Conv2DTranspose.stride Returns the stride. Conv2DTranspose.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DTranspose.transpose(name=None) Returns matching Conv2D module. Args: name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.name . Returns: Conv2D module. Conv2DTranspose.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DTranspose.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv2DTranspose.w Returns the Variable containing the weight matrix. class Conv3D Volumetric convolution module, including optional bias. This acts as a light wrapper around the class _ConvND . Conv3D.__init__(output_channels, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, mask=None, data_format='NDHWC', custom_getter=None, name='conv_3d') Constructs a Conv3D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution Args: output_channels : Number of output channels. output_channels can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_channels can be called, returning an integer, when build is called. kernel_shape : Sequence of kernel sizes (of size 3), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 3), or integer that is used to define stride in all dimensions. rate : Sequence of dilation rates (of size 3), or integer that is used to define dilation rate in all dimensions. 1 corresponds to standard 3D convolution, rate 1 corresponds to dilated convolution. Cannot be 1 if any of stride is also 1. padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 3. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializer for the weights is a truncated normal initializer, which is commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . mask : An object convertible to a 5D tensor which is multiplied component-wise with the weights (Optional). data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NDHWC), or the second dimension (NCDHW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two or four integers. base.IncompatibleShapeError: If the given rate is not an integer; or if the given rate is not a sequence of two integers. base.NotSupportedError: If rate in any dimension and the stride in any dimension are simultaneously 1. ValueError : If the given padding is not snt.VALID , snt.SAME , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a sequence of these. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_3D_DATA_FORMATS ). Conv3D.__call__(inputs) Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection. Args: inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Returns: A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels]. Raises: ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Conv3D.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias. Conv3D.clone(name=None) Returns a cloned _ConvND module. Args: name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name . Returns: A copy of the current class. Conv3D.connected_subgraphs Returns the subgraphs created by this module so far. Conv3D.conv_op_padding Returns the padding algorithm used for the underlying convolution op. Conv3D.data_format Returns the data format. Conv3D.defun() Wraps this modules call method in a callable graph function. Conv3D.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Conv3D.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv3D.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Conv3D.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv3D.graph Returns the Graph instance which the module is connected to, or None. Conv3D.has_bias Returns True if bias Variable is present in the module. Conv3D.initializers Returns the initializers dictionary. Conv3D.input_channels Returns the number of input channels. Conv3D.input_shape Returns the input shape. Conv3D.is_connected Returns true iff the Module been connected to the Graph at least once. Conv3D.kernel_shape Returns the kernel shape. Conv3D.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Conv3D.mask Returns the mask. Conv3D.module_name Returns the name of the Module. Conv3D.name_scopes Returns a tuple of all name_scopes generated by this module. Conv3D.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv3D.output_channels Returns the number of output channels. Conv3D.padding Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension. Returns: The padding algorithm used, if this is the same for all dimensions. Raises: ValueError : If different padding algorithms are used for different dimensions. Conv3D.paddings Returns a tuple with the padding algorithm used for each dimension. Conv3D.partitioners Returns the partitioners dictionary. Conv3D.rate Returns the dilation rate. Conv3D.regularizers Returns the regularizers dictionary. Conv3D.scope_name Returns the full name of the Module's variable scope. Conv3D.stride Returns the stride. Conv3D.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv3D.transpose(name=None) Returns matching Conv3DTranspose module. Args: name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.name . Returns: Conv3DTranspose module. Raises: base.NotSupportedError: If rate in any dimension 1. Conv3D.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Conv3D.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv3D.w Returns the Variable containing the weight matrix. class Conv3DTranspose Volumetric transposed / reverse / up 3D convolution module, including bias. This acts as a light wrapper around the TensorFlow op tf.nn.conv3d_transpose abstracting away variable creation and sharing. Conv3DTranspose.__init__(output_channels, output_shape=None, kernel_shape=None, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NDHWC', custom_getter=None, name='conv_3d_transpose') Constructs a Conv3DTranspose module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution Args: output_channels : Number of output channels. output_channels can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure output_channels can be called, returning an integer, when build is called. output_shape : Output shape of transpose convolution. Can be either an iterable of integers or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_shape can be called, returning an iterable of format (out_depth, out_height, out_width) when build is called. Note that output_shape defines the size of output signal domain, as opposed to the shape of the output Tensor . If a None value is given, a default shape is automatically calculated (see docstring of _default_transpose_size function for more details). kernel_shape : Sequence of kernel sizes (of size 3), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 3), or integer that is used to define stride in all dimensions. padding : Padding algorithm, either snt.SAME or snt.VALID . use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NDHWC), or the second dimension (NCDHW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: module.IncompatibleShapeError: If the given kernel shape is neither an integer nor a sequence of three integers. module.IncompatibleShapeError: If the given stride is neither an integer nor a sequence of three or five integers. ValueError : If the given padding is not snt.VALID or snt.SAME . ValueError : If the given kernel_shape is None . KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_3D_DATA_FORMATS ). Conv3DTranspose.__call__(inputs) Connects the _ConvNDTranspose module into the graph. If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same final N dimensions, in order for the existing variables to be the correct size for the multiplication. The batch size may differ for each connection. Args: inputs : A Tensor of shape data_format and of type tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Returns: A Tensor of shape data_format and of type tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Raises: ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If output_shape is an iterable and is not in the format (out_height, out_width) . TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Conv3DTranspose.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias. Conv3DTranspose.connected_subgraphs Returns the subgraphs created by this module so far. Conv3DTranspose.conv_op_padding Returns the padding algorithm used for the underlying convolution op. Conv3DTranspose.defun() Wraps this modules call method in a callable graph function. Conv3DTranspose.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Conv3DTranspose.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv3DTranspose.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Conv3DTranspose.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv3DTranspose.graph Returns the Graph instance which the module is connected to, or None. Conv3DTranspose.has_bias Returns True if bias Variable is present in the module. Conv3DTranspose.initializers Returns the initializers dictionary. Conv3DTranspose.input_channels Returns the number of input channels. Conv3DTranspose.input_shape Returns the input shape. Conv3DTranspose.is_connected Returns true iff the Module been connected to the Graph at least once. Conv3DTranspose.kernel_shape Returns the kernel shape. Conv3DTranspose.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Conv3DTranspose.module_name Returns the name of the Module. Conv3DTranspose.name_scopes Returns a tuple of all name_scopes generated by this module. Conv3DTranspose.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv3DTranspose.output_channels Returns the number of output channels. Conv3DTranspose.output_shape Returns the output shape. Conv3DTranspose.padding Returns the padding algorithm. Conv3DTranspose.partitioners Returns the partitioners dictionary. Conv3DTranspose.regularizers Returns the regularizers dictionary. Conv3DTranspose.scope_name Returns the full name of the Module's variable scope. Conv3DTranspose.stride Returns the stride. Conv3DTranspose.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv3DTranspose.transpose(name=None) Returns transposed Conv3DTranspose module, i.e. a Conv3D module. Conv3DTranspose.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Conv3DTranspose.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Conv3DTranspose.w Returns the Variable containing the weight matrix. class DeepRNN RNN core that passes data through a number of internal modules or ops. This module is constructed by passing an iterable of externally constructed modules or ops. The DeepRNN takes (input, prev_state) as input and passes the input through each internal module in the order they were presented, using elements from prev_state as necessary for internal recurrent cores. The output is (output, next_state) in common with other RNN cores. By default, skip connections from the input to all internal modules and from each intermediate output to the final output are used. E.g.: lstm1 = snt.LSTM(hidden_size=256) lstm2 = snt.LSTM(hidden_size=256) deep_rnn = snt.DeepRNN([lstm1, lstm2]) output, next_state = deep_rnn(input, prev_state) The computation set up inside the DeepRNN has the same effect as: prev_state1, prev_state2 = prev_state lstm1_output, next_state1 = lstm1(input, prev_state1) lstm2_output, next_state2 = lstm2( tf.concat([input, lstm1_output], 1), prev_state2) next_state = (next_state1, next_state2) output = tf.concat([lstm1_output, lstm2_output], 1) Every internal module receives the preceding module's output and the entire core's input. The output is created by concatenating each internal module's output. In the case of internal recurrent elements, corresponding elements of the state are used such that state[i] is passed to the i 'th internal recurrent element. Note that the state of a DeepRNN is always a tuple, which will contain the same number of elements as there are internal recurrent cores. If no internal modules are recurrent, the state of the DeepRNN as a whole is the empty tuple. Wrapping non-recurrent modules into a DeepRNN can be useful to produce something API compatible with a \"real\" recurrent module, simplifying code that handles the cores. Without skip connections the previous example would become the following (note the only difference is the addition of skip_connections=False ): # ... declare other modules as above deep_rnn = snt.DeepRNN([lin, tanh, lstm], skip_connections=False) output, next_state = deep_rnn(input, prev_state) which is equivalent to: lin_output = lin(input) tanh_output = tanh(lin_output) lstm_output, lstm_next_state = lstm(tanh_output, prev_state[0]) next_state = (lstm_next_state,) output = lstm_output Note: when using skip connections, all the cores should be recurrent. DeepRNN.__init__(cores, skip_connections=True, concat_final_output_if_skip=True, name='deep_rnn') Construct a Deep RNN core. Args: cores : iterable of modules or ops. skip_connections : a boolean that indicates whether to use skip connections. This means that the input is fed to all the layers, after being concatenated on the last dimension with the output of the previous layer. The output of the module will be the concatenation of all the outputs of the internal modules. concat_final_output_if_skip : A boolean that indicates whether the outputs of intermediate layers should be concatenated into the timestep-wise output of the core. By default this is True. If this is set to False, then the core output is that of the final layer, i.e. that of cores[-1] . name : name of the module. Raises: ValueError : if cores is not an iterable, or if skip_connections is True and not all the modules are recurrent. DeepRNN.__call__(inputs, prev_state, **kwargs) Connects the DeepRNN module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided as input_ and state must have the same final dimension, in order for the existing variables to be the correct size for their corresponding multiplications. The batch size may differ for each connection. Args: inputs : a nested tuple of Tensors of arbitrary dimensionality, with at least an initial batch dimension. prev_state : a tuple of prev_state s that corresponds to the state of each one of the cores of the DeepCore . **kwargs : optional kwargs to be passed to the _build of all sub-modules. E.g. is_training=True. Note all sub-modules must accept the given kwarg. Returns: output : a nested tuple of Tensors of arbitrary dimensionality, with at least an initial batch dimension. next_state : a tuple of next_state s that corresponds to the updated state of each one of the cores of the DeepCore . Raises: ValueError : if connecting the module into the graph any time after the first time, and the inferred size of the inputs does not match previous invocations. This may happen if one connects a module any time after the first time that does not have the configuration of skip connections as the first time. DeepRNN.connected_subgraphs Returns the subgraphs created by this module so far. DeepRNN.defun() Wraps this modules call method in a callable graph function. DeepRNN.defun_wrapped Returns boolean indicating whether this module is defun wrapped. DeepRNN.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. DeepRNN.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. DeepRNN.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. DeepRNN.graph Returns the Graph instance which the module is connected to, or None. DeepRNN.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None) Builds the default start state for a DeepRNN. Args: batch_size : An int, float or scalar Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the number of passed initializers is not the same as the number of recurrent cores. DeepRNN.is_connected Returns true iff the Module been connected to the Graph at least once. DeepRNN.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. DeepRNN.module_name Returns the name of the Module. DeepRNN.name_scopes Returns a tuple of all name_scopes generated by this module. DeepRNN.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. DeepRNN.output_size Integer or TensorShape: size of outputs produced by this cell. DeepRNN.scope_name Returns the full name of the Module's variable scope. DeepRNN.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. DeepRNN.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. DeepRNN.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. DeepRNN.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. DeepRNN.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class DepthwiseConv2D Spatial depthwise 2D convolution module, including bias. This acts as a light wrapper around the TensorFlow ops tf.nn.depthwise_conv2d , abstracting away variable creation and sharing. DepthwiseConv2D.__init__(channel_multiplier, kernel_shape, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NHWC', custom_getter=None, name='conv_2d_depthwise') Constructs a DepthwiseConv2D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution Args: channel_multiplier : Number of channels to expand convolution to. Must be an integer. Must be 0. When channel_multiplier is set to 1, apply a different filter to each input channel producing one output channel per input channel. Numbers larger than 1 cause multiple different filters to be applied to each input channel, with their outputs being concatenated together, producing channel_multiplier * input_channels output channels. kernel_shape : Iterable with 2 elements in the following layout: [filter_height, filter_width] or integer that is used to define the list in all dimensions. stride : Iterable with 2 or 4 elements of kernel strides, or integer that is used to define stride in all dimensions. Layout of list: In case of 4 elements: [1, stride_height, stride_widith, 1] In case of 2 elements: [stride_height, stride_width] . padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 2. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners for the filters (with key 'w') and the biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NHWC), or the second dimension (\"NCHW\"). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: ValueError : If channel_multiplier isn't of type ( numbers.Integral or tf.Dimension ). ValueError : If channel_multiplier is less than 1. ValueError : If the given data_format is not a supported format (see SUPPORTED_2D_DATA_FORMATS ). base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. ValueError : If the given padding is not snt.VALID , snt.SAME , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a sequence of these. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If the passed-in data_format doesn't have a channel dimension. DepthwiseConv2D.__call__(inputs) Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection. Args: inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Returns: A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels]. Raises: ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . DepthwiseConv2D.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias. DepthwiseConv2D.channel_multiplier Returns the channel multiplier argument. DepthwiseConv2D.clone(name=None) Returns a cloned _ConvND module. Args: name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name . Returns: A copy of the current class. DepthwiseConv2D.connected_subgraphs Returns the subgraphs created by this module so far. DepthwiseConv2D.conv_op_padding Returns the padding algorithm used for the underlying convolution op. DepthwiseConv2D.data_format Returns the data format. DepthwiseConv2D.defun() Wraps this modules call method in a callable graph function. DepthwiseConv2D.defun_wrapped Returns boolean indicating whether this module is defun wrapped. DepthwiseConv2D.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. DepthwiseConv2D.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. DepthwiseConv2D.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. DepthwiseConv2D.graph Returns the Graph instance which the module is connected to, or None. DepthwiseConv2D.has_bias Returns True if bias Variable is present in the module. DepthwiseConv2D.initializers Returns the initializers dictionary. DepthwiseConv2D.input_channels Returns the number of input channels. DepthwiseConv2D.input_shape Returns the input shape. DepthwiseConv2D.is_connected Returns true iff the Module been connected to the Graph at least once. DepthwiseConv2D.kernel_shape Returns the kernel shape. DepthwiseConv2D.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. DepthwiseConv2D.mask Returns the mask. DepthwiseConv2D.module_name Returns the name of the Module. DepthwiseConv2D.name_scopes Returns a tuple of all name_scopes generated by this module. DepthwiseConv2D.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. DepthwiseConv2D.output_channels Returns the number of output channels. DepthwiseConv2D.padding Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension. Returns: The padding algorithm used, if this is the same for all dimensions. Raises: ValueError : If different padding algorithms are used for different dimensions. DepthwiseConv2D.paddings Returns a tuple with the padding algorithm used for each dimension. DepthwiseConv2D.partitioners Returns the partitioners dictionary. DepthwiseConv2D.rate Returns the dilation rate. DepthwiseConv2D.regularizers Returns the regularizers dictionary. DepthwiseConv2D.scope_name Returns the full name of the Module's variable scope. DepthwiseConv2D.stride Returns the stride. DepthwiseConv2D.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. DepthwiseConv2D.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. DepthwiseConv2D.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. DepthwiseConv2D.w Returns the Variable containing the weight matrix. class DifferentGraphError Error raised when trying to connect a Sonnet module to multiple Graphs. class Embed Module for embedding tokens in a low-dimensional space. Embed.__init__(vocab_size=None, embed_dim=None, existing_vocab=None, densify_gradients=False, initializers=None, partitioners=None, regularizers=None, trainable=True, custom_getter=None, name='embed') Constructs an Embed module. Args: vocab_size : int. Number of unique tokens to embed. If not provided, an existing vocabulary matrix from which vocab_size can be inferred must be provided as existing_vocab. embed_dim : int or None. Number of dimensions to assign to each embedding. If not specified, a sensible default is chosen based on vocab_size . If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. existing_vocab : a [vocab_size, embed_dim] vocabulary matrix. Will be converted to a tf.float32 tensor. If provided, neither or vocab_size or embed_dim should be provided as they are inferred. densify_gradients : if True, we convert the embedding gradient from an indexed-slices to a regular tensor before sending it back to the parameter server. This avoids excess computation on the parameter server. Use this option for moderately sized embeddings, e.g., a vocabulary size on the order of up to thousands. For embeddings larger than these, e.g. a vocabulary size on the order of tens or hundreds of thousands, set this to False. initializers : Optional dict containing initializers for embeddings (with key 'embeddings'). As a default, embeddings are initialized via a truncated normal distribution. partitioners : Optional dict containing partitioners for embeddings (with key 'embeddings'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for embeddings (with key 'embeddings'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . trainable : if True, the embeddings will be updated during training. If False, they are fixed to their initial values. If trainable=False and a regularizer is given, the resulting loss stays constant. custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : string. Name for this module. Raises: ValueError : if neither one of vocab_size or existing_vocab is provided, or if existing_vocab is provided along with vocab_size, embedding_dim, initializers, partitioners or regularizers (as these should be inferred). Embed.__call__(ids) Lookup embeddings. Looks up an embedding vector for each value in ids . All ids must be within [0, vocab_size), else an InvalidArgumentError is raised at runtime. Args: ids : Tensor of dtype int64. Returns: Tensor of tf.shape(ids) + [embedding_dim] and dtype float32. Embed.connected_subgraphs Returns the subgraphs created by this module so far. Embed.defun() Wraps this modules call method in a callable graph function. Embed.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Embed.embed_dim Size of embedding vectors. Embed.embeddings Returns the Variable containing embeddings. Returns: A 2D Variable containing one embedding vector per row, constructed in the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. Embed.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Embed.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Embed.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Embed.graph Returns the Graph instance which the module is connected to, or None. Embed.is_connected Returns true iff the Module been connected to the Graph at least once. Embed.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Embed.module_name Returns the name of the Module. Embed.name_scopes Returns a tuple of all name_scopes generated by this module. Embed.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Embed.scope_name Returns the full name of the Module's variable scope. Embed.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Embed.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Embed.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Embed.vocab_size Size of input vocabulary. class Error Base class for all errors from snt. This is thrown to indicate a Neural Network specific problem, e.g. wrong module arity, module is not connected to the graph when it should be, tried to wire together incompatible modules, etc. class FlattenTrailingDimensions Flattens trailing dimensions of a Tensor. FlattenTrailingDimensions.__init__(dim_from, name='batch_dim_from') Constructs a FlattenTrailingDimensions module. For example, given an input Tensor with shape [B, H, W, C] : dim_from=1 will return a Tensor with shape [B, H*W*C] . dim_from=2 will return a Tensor with shape [B, H, W*C] . dim_from=3 will return the input itself. dim_from=4 will return a Tensor with shape [B, H, W, C, 1] . dim_from =5 will generate a ValueError when building the module. The preserved dimensions can be unknown at building time. Equivalent to BatchFlatten(preserve_dims=dim_from, name=name). Args: dim_from : All dimensions after and including dim_from will be flattened into a single dimension. name : Name of the module. Raises: ValueError : If dim_from = 0 . FlattenTrailingDimensions.__call__(inputs) Connects the module into the graph, with input Tensor inputs . Args: inputs : A Tensor of shape [b_1, b_2, ..., b_preserve_dims, b_preserve_dims+1, ...]. Returns: A Tensor of shape [b_1, b_2, ..., b_preserve_dims, b_reshape_1, b_reshape_2, ...], with reshaping defined by the constructor shape parameter. Raises: ValueError : If output shape is incompatible with input shape; or if shape array contains non numeric entries; or if shape array contains more than 1 wildcard -1; or if the input array contains unknown, non-preserved dimensions (except when the unknown dimension is the only non-preserved dimension and doesn't actually need reshaping). FlattenTrailingDimensions.connected_subgraphs Returns the subgraphs created by this module so far. FlattenTrailingDimensions.defun() Wraps this modules call method in a callable graph function. FlattenTrailingDimensions.defun_wrapped Returns boolean indicating whether this module is defun wrapped. FlattenTrailingDimensions.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. FlattenTrailingDimensions.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. FlattenTrailingDimensions.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. FlattenTrailingDimensions.graph Returns the Graph instance which the module is connected to, or None. FlattenTrailingDimensions.input_shape Returns shape of input Tensor passed at last call to build . FlattenTrailingDimensions.is_connected Returns true iff the Module been connected to the Graph at least once. FlattenTrailingDimensions.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. FlattenTrailingDimensions.module_name Returns the name of the Module. FlattenTrailingDimensions.name_scopes Returns a tuple of all name_scopes generated by this module. FlattenTrailingDimensions.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. FlattenTrailingDimensions.scope_name Returns the full name of the Module's variable scope. FlattenTrailingDimensions.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. FlattenTrailingDimensions.transpose(name=None) Returns transpose batch reshape. FlattenTrailingDimensions.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. FlattenTrailingDimensions.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class GRU GRU recurrent network cell. The implementation is based on: https://arxiv.org/pdf/1412.3555v1.pdf. Attributes: state_size: Integer indicating the size of state tensor. output_size: Integer indicating the size of the core output. GRU.__init__(hidden_size, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='gru') Construct GRU. Args: hidden_size : (int) Hidden size dimensionality. initializers : Dict containing ops to initialize the weights. This dict may contain any of the keys returned by GRU.get_possible_initializer_keys . partitioners : Optional dict containing partitioners to partition the weights and biases. As a default, no partitioners are used. This dict may contain any of the keys returned by GRU.get_possible_initializer_keys regularizers : Optional dict containing regularizers for the weights and biases. As a default, no regularizers are used. This dict may contain any of the keys returned by GRU.get_possible_initializer_keys custom_getter : Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. See the tf.get_variable documentation for more details. name : Name of the module. Raises: KeyError : if initializers contains any keys not returned by GRU.get_possible_initializer_keys . KeyError : if partitioners contains any keys not returned by GRU.get_possible_initializer_keys . KeyError : if regularizers contains any keys not returned by GRU.get_possible_initializer_keys . GRU.__call__(inputs, prev_state) Connects the GRU module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided as inputs and state must have the same final dimension, in order for the existing variables to be the correct size for their corresponding multiplications. The batch size may differ for each connection. Args: inputs : Tensor of size [batch_size, input_size] . prev_state : Tensor of size [batch_size, hidden_size] . Returns: A tuple (output, next_state) where output is a Tensor of size [batch_size, hidden_size] and next_state is a Tensor of size [batch_size, hidden_size] . Raises: ValueError : If connecting the module into the graph any time after the first time, and the inferred size of the inputs does not match previous invocations. GRU.connected_subgraphs Returns the subgraphs created by this module so far. GRU.defun() Wraps this modules call method in a callable graph function. GRU.defun_wrapped Returns boolean indicating whether this module is defun wrapped. GRU.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. GRU.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. The set of all possible initializer keys are: wz : weight for input - update cell uz : weight for prev_state - update cell bz : bias for update_cell wr : weight for input - reset cell ur : weight for prev_state - reset cell br : bias for reset cell wh : weight for input - candidate activation uh : weight for prev_state - candidate activation bh : bias for candidate activation Returns: Set with strings corresponding to the strings that may be passed to the constructor. GRU.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. GRU.graph Returns the Graph instance which the module is connected to, or None. GRU.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. GRU.is_connected Returns true iff the Module been connected to the Graph at least once. GRU.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. GRU.module_name Returns the name of the Module. GRU.name_scopes Returns a tuple of all name_scopes generated by this module. GRU.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. GRU.output_size Integer or TensorShape: size of outputs produced by this cell. GRU.scope_name Returns the full name of the Module's variable scope. GRU.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. GRU.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. GRU.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. GRU.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. GRU.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class GridWarper Grid warper interface class. An object implementing the GridWarper interface generates a reference grid of feature points at construction time, and warps it via a parametric transformation model, specified at run time by an input parameter Tensor. Grid warpers must then implement a create_features function used to generate the reference grid to be warped in the forward pass (according to a determined warping model). GridWarper.__init__(source_shape, output_shape, num_coeff, name, **kwargs) Constructs a GridWarper module and initializes the source grid params. source_shape and output_shape are used to define the size of the source and output signal domains, as opposed to the shape of the respective Tensors. For example, for an image of size width=W and height=H , {source,output}_shape=[H, W] ; for a volume of size width=W , height=H and depth=D , {source,output}_shape=[H, W, D] . Args: source_shape : Iterable of integers determining the size of the source signal domain. output_shape : Iterable of integers determining the size of the destination resampled signal domain. num_coeff : Number of coefficients parametrizing the grid warp. For example, a 2D affine transformation will be defined by the 6 parameters populating the corresponding 2x3 affine matrix. name : Name of Module. **kwargs : Extra kwargs to be forwarded to the create_features function, instantiating the source grid parameters. Raises: Error : If len(output_shape) len(source_shape) . TypeError : If output_shape and source_shape are not both iterable. GridWarper.__call__(*args, **kwargs) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). GridWarper.connected_subgraphs Returns the subgraphs created by this module so far. GridWarper.defun() Wraps this modules call method in a callable graph function. GridWarper.defun_wrapped Returns boolean indicating whether this module is defun wrapped. GridWarper.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. GridWarper.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. GridWarper.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. GridWarper.graph Returns the Graph instance which the module is connected to, or None. GridWarper.is_connected Returns true iff the Module been connected to the Graph at least once. GridWarper.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. GridWarper.module_name Returns the name of the Module. GridWarper.n_coeff Returns number of coefficients of warping function. GridWarper.name_scopes Returns a tuple of all name_scopes generated by this module. GridWarper.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. GridWarper.output_shape Returns a tuple containing the shape of the output grid. GridWarper.psi Returns a list of features used to compute the grid warp. GridWarper.scope_name Returns the full name of the Module's variable scope. GridWarper.source_shape Returns a tuple containing the shape of the source signal. GridWarper.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. GridWarper.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. GridWarper.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class HighwayCore Recurrent Highway Network cell. The implementation is based on: https://arxiv.org/pdf/1607.03474v5.pdf As per the first lines of section 5 of the reference paper, 1 - T is used instead of a dedicated C gate. Attributes: state_size: Integer indicating the size of state tensor. output_size: Integer indicating the size of the core output. HighwayCore.__init__(hidden_size, num_layers, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='highwaycore') Construct a new Recurrent Highway core. Args: hidden_size : (int) Hidden size dimensionality. num_layers : (int) Number of highway layers. initializers : Dict containing ops to initialize the weights. This dict may contain any of the keys returned by HighwayCore.get_possible_initializer_keys . partitioners : Optional dict containing partitioners to partition the weights and biases. As a default, no partitioners are used. This dict may contain any of the keys returned by HighwayCore.get_possible_initializer_keys . regularizers : Optional dict containing regularizers for the weights and biases. As a default, no regularizers are used. This dict may contain any of the keys returned by HighwayCore.get_possible_initializer_keys . custom_getter : Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. See the tf.get_variable documentation for more details. name : Name of the module. Raises: KeyError : if initializers contains any keys not returned by HighwayCore.get_possible_initializer_keys . KeyError : if partitioners contains any keys not returned by HighwayCore.get_possible_initializer_keys . KeyError : if regularizers contains any keys not returned by HighwayCore.get_possible_initializer_keys . HighwayCore.__call__(inputs, prev_state) Connects the highway core module into the graph. Args: inputs : Tensor of size [batch_size, input_size] . prev_state : Tensor of size [batch_size, hidden_size] . Returns: A tuple (output, next_state) where output is a Tensor of size [batch_size, hidden_size] and next_state is a Tensor of size [batch_size, hidden_size] . Raises: ValueError : If connecting the module into the graph any time after the first time, and the inferred size of the inputs does not match previous invocations. HighwayCore.connected_subgraphs Returns the subgraphs created by this module so far. HighwayCore.defun() Wraps this modules call method in a callable graph function. HighwayCore.defun_wrapped Returns boolean indicating whether this module is defun wrapped. HighwayCore.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. HighwayCore.get_possible_initializer_keys(num_layers) Returns the keys the dictionary of variable initializers may contain. The set of all possible initializer keys are: wt : weight for input - T gate wh : weight for input - H gate wtL : weight for prev state - T gate for layer L (indexed from 0) whL : weight for prev state - H gate for layer L (indexed from 0) btL : bias for prev state - T gate for layer L (indexed from 0) bhL : bias for prev state - H gate for layer L (indexed from 0) Args: num_layers : (int) Number of highway layers. Returns: Set with strings corresponding to the strings that may be passed to the constructor. HighwayCore.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. HighwayCore.graph Returns the Graph instance which the module is connected to, or None. HighwayCore.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. HighwayCore.is_connected Returns true iff the Module been connected to the Graph at least once. HighwayCore.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. HighwayCore.module_name Returns the name of the Module. HighwayCore.name_scopes Returns a tuple of all name_scopes generated by this module. HighwayCore.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. HighwayCore.output_size Integer or TensorShape: size of outputs produced by this cell. HighwayCore.scope_name Returns the full name of the Module's variable scope. HighwayCore.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. HighwayCore.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. HighwayCore.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. HighwayCore.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. HighwayCore.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class InPlaneConv2D Applies an in-plane convolution to each channel with tied filter weights. This acts as a light wrapper around the TensorFlow op tf.nn.depthwise_conv2d ; it differs from the DepthWiseConv2D module in that it has tied weights (i.e. the same filter) for all the in-out channel pairs. InPlaneConv2D.__init__(kernel_shape, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NHWC', custom_getter=None, name='in_plane_conv2d') Constructs an InPlaneConv2D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution Args: kernel_shape : Iterable with 2 elements in the layout [filter_height, filter_width]; or integer that is used to define the list in all dimensions. stride : Iterable with 2 or 4 elements of kernel strides, or integer that is used to define stride in all dimensions. padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 2. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition the filters (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NHWC), or the second dimension (NCHW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: ValueError : If the given data_format is not a supported format (see SUPPORTED_2D_DATA_FORMATS ). base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. ValueError : If the given padding is not snt.VALID , snt.SAME , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a sequence of these. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If the passed-in data_format doesn't have a channel dimension. InPlaneConv2D.__call__(inputs) Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection. Args: inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Returns: A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels]. Raises: ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . InPlaneConv2D.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias. InPlaneConv2D.clone(name=None) Returns a cloned _ConvND module. Args: name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name . Returns: A copy of the current class. InPlaneConv2D.connected_subgraphs Returns the subgraphs created by this module so far. InPlaneConv2D.conv_op_padding Returns the padding algorithm used for the underlying convolution op. InPlaneConv2D.data_format Returns the data format. InPlaneConv2D.defun() Wraps this modules call method in a callable graph function. InPlaneConv2D.defun_wrapped Returns boolean indicating whether this module is defun wrapped. InPlaneConv2D.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. InPlaneConv2D.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. InPlaneConv2D.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. InPlaneConv2D.graph Returns the Graph instance which the module is connected to, or None. InPlaneConv2D.has_bias Returns True if bias Variable is present in the module. InPlaneConv2D.initializers Returns the initializers dictionary. InPlaneConv2D.input_channels Returns the number of input channels. InPlaneConv2D.input_shape Returns the input shape. InPlaneConv2D.is_connected Returns true iff the Module been connected to the Graph at least once. InPlaneConv2D.kernel_shape Returns the kernel shape. InPlaneConv2D.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. InPlaneConv2D.mask Returns the mask. InPlaneConv2D.module_name Returns the name of the Module. InPlaneConv2D.name_scopes Returns a tuple of all name_scopes generated by this module. InPlaneConv2D.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. InPlaneConv2D.output_channels Returns the number of output channels. InPlaneConv2D.padding Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension. Returns: The padding algorithm used, if this is the same for all dimensions. Raises: ValueError : If different padding algorithms are used for different dimensions. InPlaneConv2D.paddings Returns a tuple with the padding algorithm used for each dimension. InPlaneConv2D.partitioners Returns the partitioners dictionary. InPlaneConv2D.rate Returns the dilation rate. InPlaneConv2D.regularizers Returns the regularizers dictionary. InPlaneConv2D.scope_name Returns the full name of the Module's variable scope. InPlaneConv2D.stride Returns the stride. InPlaneConv2D.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. InPlaneConv2D.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. InPlaneConv2D.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. InPlaneConv2D.w Returns the Variable containing the weight matrix. class IncompatibleShapeError Error raised when the shape of the input at build time is incompatible. class LSTM LSTM recurrent network cell with optional peepholes layer normalization. The implementation is based on: http://arxiv.org/abs/1409.2329. We add forget_bias (default: 1) to the biases of the forget gate in order to reduce the scale of forgetting in the beginning of the training. Layer normalization This is described in https://arxiv.org/pdf/1607.06450.pdf Peep-hole connections Peep-hole connections may optionally be used by specifying a flag in the constructor. These connections can aid increasing the precision of output timing, for more details see: https://research.google.com/pubs/archive/43905.pdf Recurrent projections Projection of the recurrent state, to reduce model parameters and speed up computation. For more details see: https://arxiv.org/abs/1402.1128 Attributes: state_size: Tuple of tf.TensorShape s indicating the size of state tensors. output_size: tf.TensorShape indicating the size of the core output. use_peepholes: Boolean indicating whether peephole connections are used. LSTM.__init__(hidden_size, forget_bias=1.0, initializers=None, partitioners=None, regularizers=None, use_peepholes=False, use_layer_norm=False, hidden_clip_value=None, projection_size=None, cell_clip_value=None, custom_getter=None, name='lstm') Construct LSTM. Args: hidden_size : (int) Hidden size dimensionality. forget_bias : (float) Bias for the forget activation. initializers : Dict containing ops to initialize the weights. This dictionary may contain any of the keys returned by LSTM.get_possible_initializer_keys . partitioners : Optional dict containing partitioners to partition the weights and biases. As a default, no partitioners are used. This dict may contain any of the keys returned by LSTM.get_possible_initializer_keys . regularizers : Optional dict containing regularizers for the weights and biases. As a default, no regularizers are used. This dict may contain any of the keys returned by LSTM.get_possible_initializer_keys . use_peepholes : Boolean that indicates whether peephole connections are used. use_layer_norm : Boolean that indicates whether to apply layer normalization. hidden_clip_value : Optional number; if set, then the LSTM hidden state vector is clipped by this value. projection_size : Optional number; if set, then the LSTM hidden state is projected to this size via a learnable projection matrix. cell_clip_value : Optional number; if set, then the LSTM cell vector is clipped by this value. custom_getter : Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. See the tf.get_variable documentation for more details. name : Name of the module. Raises: KeyError : if initializers contains any keys not returned by LSTM.get_possible_initializer_keys . KeyError : if partitioners contains any keys not returned by LSTM.get_possible_initializer_keys . KeyError : if regularizers contains any keys not returned by LSTM.get_possible_initializer_keys . ValueError : if a peephole initializer is passed in the initializer list, but use_peepholes is False. LSTM.__call__(inputs, prev_state) Connects the LSTM module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided as inputs and state must have the same final dimension, in order for the existing variables to be the correct size for their corresponding multiplications. The batch size may differ for each connection. Args: inputs : Tensor of size [batch_size, input_size] . prev_state : Tuple (prev_hidden, prev_cell). Returns: A tuple (output, next_state) where 'output' is a Tensor of size [batch_size, hidden_size] and 'next_state' is a LSTMState namedtuple (next_hidden, next_cell) where next_hidden and next_cell have size [batch_size, hidden_size] . If projection_size is specified, then next_hidden will have size [batch_size, projection_size] . Raises: ValueError : If connecting the module into the graph any time after the first time, and the inferred size of the inputs does not match previous invocations. LSTM.connected_subgraphs Returns the subgraphs created by this module so far. LSTM.defun() Wraps this modules call method in a callable graph function. LSTM.defun_wrapped Returns boolean indicating whether this module is defun wrapped. LSTM.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LSTM.get_possible_initializer_keys(use_peepholes=False, use_projection=False) Returns the keys the dictionary of variable initializers may contain. The set of all possible initializer keys are: w_gates : weight for gates b_gates : bias of gates w_f_diag : weight for prev_cell - forget gate peephole w_i_diag : weight for prev_cell - input gate peephole w_o_diag : weight for prev_cell - output gate peephole Args: cls:The class. use_peepholes : Boolean that indicates whether peephole connections are used. use_projection : Boolean that indicates whether a recurrent projection layer is used. Returns: Set with strings corresponding to the strings that may be passed to the constructor. LSTM.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LSTM.graph Returns the Graph instance which the module is connected to, or None. LSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. LSTM.is_connected Returns true iff the Module been connected to the Graph at least once. LSTM.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. LSTM.module_name Returns the name of the Module. LSTM.name_scopes Returns a tuple of all name_scopes generated by this module. LSTM.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LSTM.output_size tf.TensorShape indicating the size of the core output. LSTM.scope_name Returns the full name of the Module's variable scope. LSTM.state_size Tuple of tf.TensorShape s indicating the size of state tensors. LSTM.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LSTM.use_layer_norm Boolean indicating whether layer norm is enabled. LSTM.use_peepholes Boolean indicating whether peephole connections are used. LSTM.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. LSTM.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LSTM.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class LSTMBlockCell Wraps the TensorFlow LSTMBlockCell as a Sonnet RNNCore. LSTMBlockCell.__init__(num_units, forget_bias=1.0, cell_clip=None, use_peephole=False, dtype=None, reuse=None, name='lstm_cell') Initialize the basic LSTM cell. Args: num_units : int, The number of units in the LSTM cell. forget_bias : float, The bias added to forget gates (see above). cell_clip : An optional float . Defaults to -1 (no clipping). use_peephole : Whether to use peephole connections or not. dtype : the variable dtype of this layer. Default to tf.float32. reuse : (optional) boolean describing whether to reuse variables in an existing scope. If not True , and the existing scope already has the given variables, an error is raised. name : String, the name of the layer. Layers with the same name will share weights, but to avoid mistakes we require reuse=True in such cases. By default this is \"lstm_cell\", for variable-name compatibility with tf.compat.v1.nn.rnn_cell.LSTMCell . When restoring from CudnnLSTM-trained checkpoints, must use CudnnCompatibleLSTMBlockCell instead. LSTMBlockCell.__call__(inputs, prev_state) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). LSTMBlockCell.connected_subgraphs Returns the subgraphs created by this module so far. LSTMBlockCell.defun() Wraps this modules call method in a callable graph function. LSTMBlockCell.defun_wrapped Returns boolean indicating whether this module is defun wrapped. LSTMBlockCell.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LSTMBlockCell.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. LSTMBlockCell.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LSTMBlockCell.graph Returns the Graph instance which the module is connected to, or None. LSTMBlockCell.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. LSTMBlockCell.is_connected Returns true iff the Module been connected to the Graph at least once. LSTMBlockCell.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. LSTMBlockCell.module_name Returns the name of the Module. LSTMBlockCell.name_scopes Returns a tuple of all name_scopes generated by this module. LSTMBlockCell.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LSTMBlockCell.output_size Integer or TensorShape: size of outputs produced by this cell. LSTMBlockCell.scope_name Returns the full name of the Module's variable scope. LSTMBlockCell.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. LSTMBlockCell.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LSTMBlockCell.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. LSTMBlockCell.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LSTMBlockCell.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class LSTMState LSTMState(hidden, cell) LSTMState.cell Alias for field number 1 LSTMState.hidden Alias for field number 0 class LayerNorm Layer normalization module. Implementation based on: https://arxiv.org/abs/1607.06450 This module transforms input x into: outputs = gamma * (x - mu) / sigma + beta where mu and sigma are respectively the mean and standard deviation of x. Gamma and beta are trainable parameters for scaling and shifting respectively. Since the axes over which normalization is perfomed is configurable, this also subsumes instance normalization. LayerNorm.__init__(axis=None, offset=True, scale=True, eps=1e-05, initializers=None, partitioners=None, regularizers=None, name='layer_norm') Constructs a LayerNorm module. Args: axis : Optional dimension or iterable of indices of dimensions to normalize and reduce over. By default None and all dimensions except the first/batch dimension are reduced over. If the input tensor represents an image, summing over all except the batch and channel dimensions (e.g. for image format NHWC, axes=[1,2]), then this module corresponds to Instance Normalization (https://arxiv.org/abs/1607.08022). offset : Optional boolean to specify whether or not to apply a trained component-wise bias after the layer normalization and scaling. scale : Optional boolean to specify whether or not to apply a trained component-wise scale after the layer normalization. eps : small epsilon to avoid division by zero variance. Defaults to 1e-5 as used in the paper. initializers : Dict containing ops to initialize the scale (with key 'gamma') and bias (with key 'beta'). partitioners : Optional dict containing partitioners to partition the scale (with key 'gamma') and bias (with key 'beta'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the scale (with key 'gamma') and bias (with key 'beta').. As a default, no regularizers are used. name : name of the module. Raises: KeyError : If initializers , partitioners or regularizers contain any keys other than gamma or beta . TypeError : If any of the given initializers, partitioners or regularizers are not callable. LayerNorm.__call__(inputs) Connects the LayerNorm module into the graph. Args: inputs : a Tensor of dimensionality = 2. Returns: normalized : layer normalized outputs with same shape as inputs. Raises: base.NotSupportedError: If inputs has less than 2 dimensions. LayerNorm.beta LayerNorm.connected_subgraphs Returns the subgraphs created by this module so far. LayerNorm.defun() Wraps this modules call method in a callable graph function. LayerNorm.defun_wrapped Returns boolean indicating whether this module is defun wrapped. LayerNorm.gamma LayerNorm.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LayerNorm.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. LayerNorm.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LayerNorm.graph Returns the Graph instance which the module is connected to, or None. LayerNorm.initializers LayerNorm.is_connected Returns true iff the Module been connected to the Graph at least once. LayerNorm.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. LayerNorm.module_name Returns the name of the Module. LayerNorm.name_scopes Returns a tuple of all name_scopes generated by this module. LayerNorm.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LayerNorm.partitioners LayerNorm.regularizers LayerNorm.scope_name Returns the full name of the Module's variable scope. LayerNorm.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. LayerNorm.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. LayerNorm.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class Linear Linear module, optionally including bias. Linear.__init__(output_size, use_bias=True, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='linear') Constructs a Linear module. Args: output_size : Output dimensionality. output_size can be either an integer or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_size can be called, returning an integer, when build is called. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing initializers to initialize the weights (with key 'w') or biases (with key 'b'). The default initializer for the weights is a truncated normal initializer, which is commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the weights (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: KeyError : If initializers , partitioners or regularizers contains any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. Linear.__call__(inputs) Connects the Linear module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the Tensor provided here must have the same final dimension, in order for the existing variables to be the correct size for the multiplication. The batch size may differ for each connection. Args: inputs : A 2D Tensor of size [batch_size, input_size]. Returns: A 2D Tensor of size [batch_size, output_size]. Raises: base.IncompatibleShapeError: If the input is not a 2-D Tensor with the size of the second dimension specified. base.IncompatibleShapeError: If reconnecting an already connected module into the graph, and the shape of the input is not compatible with previous inputs. Linear.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias. Linear.clone(name=None) Returns a cloned Linear module. Args: name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name . Returns: Cloned Linear module. Linear.connected_subgraphs Returns the subgraphs created by this module so far. Linear.defun() Wraps this modules call method in a callable graph function. Linear.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Linear.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Linear.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Linear.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Linear.graph Returns the Graph instance which the module is connected to, or None. Linear.has_bias Returns True if bias Variable is present in the module. Linear.initializers Returns the initializers dictionary. Linear.input_shape Returns shape of input Tensor passed at last call to build . Linear.is_connected Returns true iff the Module been connected to the Graph at least once. Linear.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Linear.module_name Returns the name of the Module. Linear.name_scopes Returns a tuple of all name_scopes generated by this module. Linear.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Linear.output_size Returns the module output size. Linear.partitioners Returns the partitioners dictionary. Linear.regularizers Returns the regularizers dictionary. Linear.scope_name Returns the full name of the Module's variable scope. Linear.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Linear.transpose(name=None) Returns transposed Linear module. Args: name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.module_name . Returns: Transposed Linear module. Linear.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Linear.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Linear.w Returns the Variable containing the weight matrix. Returns: Variable object containing the weights, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. class MergeDims Merges a tensor or nested list of tensors along a range of dimensions. Tensors are reshaped by specifying the range of dimensions to merge. Hence, the reshape can be performed without knowing in advance the rank of the input tensor. For example, merging dimensions 1, 2 and 3 together can be performed by calling: output = MergeDims(start=1, size=3)(x) A nested list of tensors can be merged: x = [tf.random_uniform(shape=[5, 5]), [tf.random_uniform(shape=[3, 3, 3])]] output = MergeDims(start=0, size=2)(x) MergeDims.__init__(start, size, name='merge_dims') Constructs the MergeDims module. Args: start : Start of the range of dimensions to merge. size : Size the range of dimensions to merge. name : The name of the module. Raises: ValueError : If size is not strictly greater than 1. MergeDims.__call__(inputs) Connects the MergeDims module into the graph. Args: inputs : Tensor or a nested list of Tensors to merge. Its rank must be greater than or equal to start + size . Returns: The merged Tensor or a nested list of merged Tensors. Raises: ValueError : If any of the inputs tensors has insufficient rank. MergeDims.connected_subgraphs Returns the subgraphs created by this module so far. MergeDims.defun() Wraps this modules call method in a callable graph function. MergeDims.defun_wrapped Returns boolean indicating whether this module is defun wrapped. MergeDims.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. MergeDims.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. MergeDims.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. MergeDims.graph Returns the Graph instance which the module is connected to, or None. MergeDims.is_connected Returns true iff the Module been connected to the Graph at least once. MergeDims.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. MergeDims.module_name Returns the name of the Module. MergeDims.name_scopes Returns a tuple of all name_scopes generated by this module. MergeDims.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. MergeDims.scope_name Returns the full name of the Module's variable scope. MergeDims.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. MergeDims.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. MergeDims.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class ModelRNN RNNCore that ignores input and uses a model to compute its next state. ModelRNN.__init__(model, name='model_rnn') Construct a Basic RNN core. Args: model : callable that computes the next state. name : name of the module. Raises: TypeError : if model is not a callable object or if it is an RNNCore. AttributeError : if model does not have an output_size attribute. ModelRNN.__call__(inputs, prev_state) Connects the ModelRNN module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided as input_ and state must have the same final dimension, in order for the existing variables to be the correct size for their corresponding multiplications. The batch size may differ for each connection. Args: inputs : Tensor input to the ModelRNN (ignored). prev_state : Tensor of size model.output_size . Returns: output : Tensor of size model.output_size . next_state : Tensor of size model.output_size . ModelRNN.connected_subgraphs Returns the subgraphs created by this module so far. ModelRNN.defun() Wraps this modules call method in a callable graph function. ModelRNN.defun_wrapped Returns boolean indicating whether this module is defun wrapped. ModelRNN.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ModelRNN.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. ModelRNN.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ModelRNN.graph Returns the Graph instance which the module is connected to, or None. ModelRNN.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. ModelRNN.is_connected Returns true iff the Module been connected to the Graph at least once. ModelRNN.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. ModelRNN.module_name Returns the name of the Module. ModelRNN.name_scopes Returns a tuple of all name_scopes generated by this module. ModelRNN.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ModelRNN.output_size Integer or TensorShape: size of outputs produced by this cell. ModelRNN.scope_name Returns the full name of the Module's variable scope. ModelRNN.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. ModelRNN.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ModelRNN.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. ModelRNN.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ModelRNN.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class Module Module wrapping a function provided by the user. Module.__init__(build, custom_getter=None, name=None) Constructs a module with a given build function. The Module class can be used to wrap a function assembling a network into a module. For example, the following code implements a simple one-hidden-layer MLP model by defining a function called make_model and using a Module instance to wrap it. def make_model(inputs): lin1 = snt.Linear(name= lin1 , output_size=10)(inputs) relu1 = tf.nn.relu(lin1, name= relu1 ) lin2 = snt.Linear(name= lin2 , output_size=20)(relu1) return lin2 model = snt.Module(name='simple_mlp', build=make_model) outputs = model(inputs) The partial package from functools can be used to bake configuration parameters into the function at construction time, as shown in the following example. from functools import partial def make_model(inputs, output_sizes): lin1 = snt.Linear(name= lin1 , output_size=output_sizes[0])(inputs) relu1 = tf.nn.relu(lin1, name= relu1 ) lin2 = snt.Linear(name= lin2 , output_size=output_sizes[1])(relu1) return lin2 model = snt.Module(name='simple_mlp', build=partial(make_model, output_sizes=[10, 20]) outputs = model(inputs) Args: build : Callable to be invoked when connecting the module to the graph. The build function is invoked when the module is called, and its role is to specify how to add elements to the Graph, and how to compute output Tensors from input Tensors. The build function signature can include the following parameters: args - Input Tensors. *kwargs - Additional Python parameters controlling connection. custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Module name. If set to None (the default), the name will be set to that of the build callable converted to snake_case . If build has no name, the name will be 'module'. Raises: TypeError : If build is not callable. TypeError : If a given custom_getter is not callable. Module.__call__(*args, **kwargs) Forwards call to the passed-in build function. Module.connected_subgraphs Returns the subgraphs created by this module so far. Module.defun() Wraps this modules call method in a callable graph function. Module.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Module.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Module.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Module.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Module.graph Returns the Graph instance which the module is connected to, or None. Module.is_connected Returns true iff the Module been connected to the Graph at least once. Module.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Module.module_name Returns the name of the Module. Module.name_scopes Returns a tuple of all name_scopes generated by this module. Module.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Module.scope_name Returns the full name of the Module's variable scope. Module.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Module.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Module.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class ModuleInfoError Error raised when Sonnet ModuleInfo cannot be serialized. class MovingAverage Calculates a differentiable decaying moving average. The moving average is kept in a variable that can either be local or global. The initial moving average value is set to the first value that is received by the module. The module lets gradients flow through the last element added to the moving average. MovingAverage.__init__(decay=0.99, local=False, name='moving_average') Constructor. Args: decay : float in range [0, 1], decay of the moving average. local : bool, specifies whether the variables are local or not. name : string, name of the Sonnet module. Default is 'moving_average'. Raises: ValueError : if decay is not in the valid range [0, 1]. MovingAverage.__call__(inputs) Returns the moving average of the values that went through inputs . Args: inputs : tensor. Returns: A moving average calculated as (1 - decay) * inputs + decay * average . MovingAverage.connected_subgraphs Returns the subgraphs created by this module so far. MovingAverage.defun() Wraps this modules call method in a callable graph function. MovingAverage.defun_wrapped Returns boolean indicating whether this module is defun wrapped. MovingAverage.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. MovingAverage.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. MovingAverage.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. MovingAverage.graph Returns the Graph instance which the module is connected to, or None. MovingAverage.is_connected Returns true iff the Module been connected to the Graph at least once. MovingAverage.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. MovingAverage.module_name Returns the name of the Module. MovingAverage.name_scopes Returns a tuple of all name_scopes generated by this module. MovingAverage.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. MovingAverage.reset() MovingAverage.scope_name Returns the full name of the Module's variable scope. MovingAverage.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. MovingAverage.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. MovingAverage.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class NotConnectedError Error raised when operating on a module that has not yet been connected. Some module properties / methods are valid to access before the module has been connected into the graph, but some are not. This Error is raised when the user attempts to do anything not valid before connection. class NotInitializedError Error raised when connecting an uninitialized Sonnet module. Before they can be connected, all Sonnet modules must call AbstractModule.__init__ (e.g. via a super call). class NotSupportedError Error raised when something that cannot be supported is requested. For example a Dilated Convolution module cannot be transposed. class OptimizationConstraints Container for optimization constraints. Users can add to an OptimizationConstraints instance multiple inequality constraints, either implicitly passing inequality ops, such as optimization_constraints.add(x y) , or explicitly specifying the constraint type, as in optimization_constraints.add_geq(x, y) . Users can finally add the constraints to the TensorFlow graph calling optimization_constraints() ; when doing so, Lagrange multipliers are automatically added to the graph, so that users can optimize them alongside other variables in the graph, using the same optimizer and minimize() . Example usage: regularization_loss = model.regularization_loss(data) reconstruction_error = model.reconstruction_error(data) avg_reconstruction_error = snt.MovingAverage()(reconstruction_error) constraints = snt.OptimizationConstraints() constraints.add(avg_reconstruction_error reconstruction_threshold) loss = regularization_loss + constraints() # The following call actually performs an update step for # min_{theta} max_{lambda} ( # regularization_loss(theta) + # lambda * (avg_reconstruction_error - reconstruction_threshold)) # where theta are the model parameters and lambda are the Lagrange # multipliers. update = optimizer.minimize(loss) OptimizationConstraints.__init__(rate=1.0, valid_range=None) Instantiates a container for optimization constraints. Args: rate : optional float, default 1.0. Default factor for Lagrange multiplier gradient scaling. Use there rate argument to scale the gradients of the Lagrange multipliers - note that this parameter has no effect when using optimisers such as Adam. This parameter can be overridden when adding constraints to the container. valid_range : optional tuple of length 2, default None. Default valid range for Lagrange multipliers. This parameter can be overridden when adding constraints to the container. OptimizationConstraints.add(expression, rate=None, valid_range=None, initializer=None) Add inequality constraint whose type depends on analysis of input op. Args: expression : op of type Greater , GreaterEqual , Less or LessEqual . Note that GreaterEqual and LessEqual are accepted only for convenience, and will result in the same behavior as Greater and Less respectively. rate : optional float, default None. Factor for Lagrange multiplier gradient scaling. Use there rate argument to scale the gradients of the Lagrange multipliers - note that this parameter has no effect when using optimisers such as Adam. This parameter overrides the defaults defined instantiating the container. valid_range : optional tuple of length 2, default None. Default valid range for Lagrange multipliers. This parameter overrides the defaults defined instantiating the container. initializer : optional tensorflow initializer, array or value to be used for the Lagrange multiplier initialization. By default Lagrange multiplier will be initialized to 1.0. Returns: Self. Raises: TypeError , when input expression op is not one of Greater , GreaterEqual , Less , LessEqual . OptimizationConstraints.add_geq(lhs, rhs=0.0, rate=None, valid_range=None, initializer=None) Add a 'greater than' inequality constraint. Args: lhs : left hand argument of inequality expression. rhs : reft hand argument of inequality expression, defaults to 0.0. rate : optional float, default None. Factor for Lagrange multiplier gradient scaling. Use there rate argument to scale the gradients of the Lagrange multipliers - note that this parameter has no effect when using optimisers such as Adam. This parameter overrides the defaults defined instantiating the container. valid_range : optional tuple of length 2, default None. Default valid range for Lagrange multipliers. This parameter overrides the defaults defined instantiating the container. initializer : optional tensorflow initializer, array or value to be used for the Lagrange multiplier initialization. By default Lagrange multiplier will be initialized to 1.0. Returns: Self. OptimizationConstraints.add_leq(lhs, rhs=0.0, rate=None, valid_range=None, initializer=None) Add a 'less than' inequality constraint. Args: lhs : left hand argument of inequality expression. rhs : reft hand argument of inequality expression, defaults to 0.0. rate : optional float, default None. Factor for Lagrange multiplier gradient scaling. Use there rate argument to scale the gradients of the Lagrange multipliers - note that this parameter has no effect when using optimisers such as Adam. This parameter overrides the defaults defined instantiating the container. valid_range : optional tuple of length 2, default None. Default valid range for Lagrange multipliers. This parameter overrides the defaults defined instantiating the container. initializer : optional tensorflow initializer, array or value to be used for the Lagrange multiplier initialization. By default Lagrange multiplier will be initialized to 1.0. Returns: Self. OptimizationConstraints.constraints OptimizationConstraints.lagrange_multipliers class ParentNotBuiltError Error raised when the parent of a module has not been built yet. For example, when making a transpose of modules that inherit from module.Transposable , the parent has to be connected to the graph before the child transpose to ensure that shape inference has already occurred. class RNNCellWrapper RNN core that delegates to a tf.contrib.rnn.RNNCell . RNNCellWrapper.__init__(cell_ctor, *args, **kwargs) Constructs the cell, within this module's variable scope. Args: cell_ctor : Callable that instantiates a tf.contrib.rnn.RNNCell . *args : Arguments to pass to cell_ctor . **kwargs : Keyword arguments to pass to cell_ctor . If name is provided, it is passed to RNNCore.__init__ as well. If custom_getter is provided, it is passed to RNNCore.__init__ but not to cell_ctor . RNNCellWrapper.__call__(inputs, prev_state) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). RNNCellWrapper.connected_subgraphs Returns the subgraphs created by this module so far. RNNCellWrapper.defun() Wraps this modules call method in a callable graph function. RNNCellWrapper.defun_wrapped Returns boolean indicating whether this module is defun wrapped. RNNCellWrapper.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RNNCellWrapper.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. RNNCellWrapper.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RNNCellWrapper.graph Returns the Graph instance which the module is connected to, or None. RNNCellWrapper.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. RNNCellWrapper.is_connected Returns true iff the Module been connected to the Graph at least once. RNNCellWrapper.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. RNNCellWrapper.module_name Returns the name of the Module. RNNCellWrapper.name_scopes Returns a tuple of all name_scopes generated by this module. RNNCellWrapper.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RNNCellWrapper.output_size Integer or TensorShape: size of outputs produced by this cell. RNNCellWrapper.scope_name Returns the full name of the Module's variable scope. RNNCellWrapper.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. RNNCellWrapper.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RNNCellWrapper.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. RNNCellWrapper.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RNNCellWrapper.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class RNNCore Superclass for Recurrent Neural Network Cores. This class defines the basic functionality that every core should implement, mainly the initial_state method which will return an example of their initial state. It also inherits from the interface snt.AbstractModule . As with any other snt.Module any subclass must implement a _build method that constructs the graph that corresponds to a core. Such a _build method should always have the same interface, which is the following: output, next_state = self._build(input, prev_state) where output, next_state, input, and prev_state are arbitrarily nested tensors. Such structures can be defined according to the following grammar: element = tuple(element*) | list(element*) | tf.Tensor This class is to be used with tensorflow containers such as rnn in tensorflow.python.ops.rnn. These containers only accept inputs which are compatible with the tf.contrib.rnn.RNNCell API, so that all the RNNCores should expose state_size and output_size properties. RNNCore.__init__(_sentinel=None, custom_getter=None, name=None) Performs the initialisation necessary for all AbstractModule instances. Every subclass of AbstractModule must begin their constructor with a call to this constructor, i.e. super(MySubModule, self).__init__(custom_getter=custom_getter, name=name) . If you instantiate sub-modules in init you must create them within the _enter_variable_scope context manager to ensure they are in the module's variable scope. Alternatively, instantiate sub-modules in _build . Args: _sentinel: Variable that only carries a non-None value if __init__ was called without named parameters. If this is the case, a deprecation warning is issued in form of a ValueError . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of this module. Used to construct the Templated build function. If None the module's class name is used (converted to snake case). Raises: TypeError : If name is not a string. TypeError : If a given custom_getter is not callable. ValueError : If __init__ was called without named arguments. RNNCore.__call__(*args, **kwargs) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). RNNCore.connected_subgraphs Returns the subgraphs created by this module so far. RNNCore.defun() Wraps this modules call method in a callable graph function. RNNCore.defun_wrapped Returns boolean indicating whether this module is defun wrapped. RNNCore.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RNNCore.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. RNNCore.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RNNCore.graph Returns the Graph instance which the module is connected to, or None. RNNCore.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. RNNCore.is_connected Returns true iff the Module been connected to the Graph at least once. RNNCore.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. RNNCore.module_name Returns the name of the Module. RNNCore.name_scopes Returns a tuple of all name_scopes generated by this module. RNNCore.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RNNCore.output_size Integer or TensorShape: size of outputs produced by this cell. RNNCore.scope_name Returns the full name of the Module's variable scope. RNNCore.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. RNNCore.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RNNCore.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. RNNCore.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RNNCore.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class RelationalMemory Relational Memory Core. RelationalMemory.__init__(mem_slots, head_size, num_heads=1, num_blocks=1, forget_bias=1.0, input_bias=0.0, gate_style='unit', attention_mlp_layers=2, key_size=None, name='relational_memory') Constructs a RelationalMemory object. Args: mem_slots : The total number of memory slots to use. head_size : The size of an attention head. num_heads : The number of attention heads to use. Defaults to 1. num_blocks : Number of times to compute attention per time step. Defaults to 1. forget_bias : Bias to use for the forget gate, assuming we are using some form of gating. Defaults to 1. input_bias : Bias to use for the input gate, assuming we are using some form of gating. Defaults to 0. gate_style : Whether to use per-element gating ('unit'), per-memory slot gating ('memory'), or no gating at all (None). Defaults to unit . attention_mlp_layers : Number of layers to use in the post-attention MLP. Defaults to 2. key_size : Size of vector to use for key query vectors in the attention computation. Defaults to None, in which case we use head_size . name : Name of the module. Raises: ValueError : gate_style not one of [None, 'memory', 'unit']. ValueError : num_blocks is 1. ValueError : attention_mlp_layers is 1. RelationalMemory.__call__(inputs, memory, treat_input_as_matrix=False) Adds relational memory to the TensorFlow graph. Args: inputs : Tensor input. memory : Memory output from the previous time step. treat_input_as_matrix : Optional, whether to treat input as a sequence of matrices. Defaulta to False, in which case the input is flattened into a vector. Returns: output : This time step's output. next_memory : The next version of memory to use. RelationalMemory.connected_subgraphs Returns the subgraphs created by this module so far. RelationalMemory.defun() Wraps this modules call method in a callable graph function. RelationalMemory.defun_wrapped Returns boolean indicating whether this module is defun wrapped. RelationalMemory.forget_gate Returns the forget gate Tensor. RelationalMemory.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RelationalMemory.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. RelationalMemory.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RelationalMemory.graph Returns the Graph instance which the module is connected to, or None. RelationalMemory.initial_state(batch_size, trainable=False) Creates the initial memory. We should ensure each row of the memory is initialized to be unique, so initialize the matrix to be the identity. We then pad or truncate as necessary so that init_state is of size (batch_size, self._mem_slots, self._mem_size). Args: batch_size : The size of the batch. trainable : Whether the initial state is trainable. This is always True. Returns: init_state : A truncated or padded matrix of size (batch_size, self._mem_slots, self._mem_size). RelationalMemory.input_gate Returns the input gate Tensor. RelationalMemory.is_connected Returns true iff the Module been connected to the Graph at least once. RelationalMemory.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. RelationalMemory.module_name Returns the name of the Module. RelationalMemory.name_scopes Returns a tuple of all name_scopes generated by this module. RelationalMemory.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RelationalMemory.output_size Integer or TensorShape: size of outputs produced by this cell. RelationalMemory.scope_name Returns the full name of the Module's variable scope. RelationalMemory.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. RelationalMemory.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RelationalMemory.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. RelationalMemory.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. RelationalMemory.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class Residual Adds a residual connection to a base module. This module wraps a module M, where if M with traditionally output M(X), Residual(M)(x) = M(x) + x. Residual.__init__(base_module, name='residual') Performs the initialisation necessary for all AbstractModule instances. Every subclass of AbstractModule must begin their constructor with a call to this constructor, i.e. super(MySubModule, self).__init__(custom_getter=custom_getter, name=name) . If you instantiate sub-modules in init you must create them within the _enter_variable_scope context manager to ensure they are in the module's variable scope. Alternatively, instantiate sub-modules in _build . Args: _sentinel: Variable that only carries a non-None value if __init__ was called without named parameters. If this is the case, a deprecation warning is issued in form of a ValueError . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of this module. Used to construct the Templated build function. If None the module's class name is used (converted to snake case). Raises: TypeError : If name is not a string. TypeError : If a given custom_getter is not callable. ValueError : If __init__ was called without named arguments. Residual.__call__(inputs, **kwargs) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). Residual.connected_subgraphs Returns the subgraphs created by this module so far. Residual.defun() Wraps this modules call method in a callable graph function. Residual.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Residual.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Residual.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Residual.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Residual.graph Returns the Graph instance which the module is connected to, or None. Residual.is_connected Returns true iff the Module been connected to the Graph at least once. Residual.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Residual.module_name Returns the name of the Module. Residual.name_scopes Returns a tuple of all name_scopes generated by this module. Residual.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Residual.scope_name Returns the full name of the Module's variable scope. Residual.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Residual.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Residual.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class ResidualCore Adds a residual connection to a base RNN core. This module wraps a module M, where if M with traditionally output M(X), Residual(M)(x) = M(x) + x. ResidualCore.__init__(base_core, name='residual_core') Performs the initialisation necessary for all AbstractModule instances. Every subclass of AbstractModule must begin their constructor with a call to this constructor, i.e. super(MySubModule, self).__init__(custom_getter=custom_getter, name=name) . If you instantiate sub-modules in init you must create them within the _enter_variable_scope context manager to ensure they are in the module's variable scope. Alternatively, instantiate sub-modules in _build . Args: _sentinel: Variable that only carries a non-None value if __init__ was called without named parameters. If this is the case, a deprecation warning is issued in form of a ValueError . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of this module. Used to construct the Templated build function. If None the module's class name is used (converted to snake case). Raises: TypeError : If name is not a string. TypeError : If a given custom_getter is not callable. ValueError : If __init__ was called without named arguments. ResidualCore.__call__(inputs, prev_state, **kwargs) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). ResidualCore.connected_subgraphs Returns the subgraphs created by this module so far. ResidualCore.defun() Wraps this modules call method in a callable graph function. ResidualCore.defun_wrapped Returns boolean indicating whether this module is defun wrapped. ResidualCore.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ResidualCore.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. ResidualCore.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ResidualCore.graph Returns the Graph instance which the module is connected to, or None. ResidualCore.initial_state(*args, **kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. ResidualCore.is_connected Returns true iff the Module been connected to the Graph at least once. ResidualCore.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. ResidualCore.module_name Returns the name of the Module. ResidualCore.name_scopes Returns a tuple of all name_scopes generated by this module. ResidualCore.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ResidualCore.output_size Integer or TensorShape: size of outputs produced by this cell. ResidualCore.scope_name Returns the full name of the Module's variable scope. ResidualCore.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. ResidualCore.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ResidualCore.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. ResidualCore.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. ResidualCore.zero_state(*args, **kwargs) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class SelectInput Returns a subset of its inputs in an arbitrarily nested configuration. This module can be used for multiple purposes. The basic usage is to select a tensor or a subset of tensors: output = snt.SelectInput(idx=0, name='select')(input0, input1) == input0 output = snt.SelectInput(idx=[0, 2], name='select')(input0, input1, input2) == (input0, input2) Another usage is to change the orders of the input tensors: output = snt.SelectInput(idx=[1, 0], name='select')(input0, input1) == (input1, input0) Another usage is to duplicate an input: output = snt.SelectInput(idx=[0, 0], name='select')(input0) == (input0, input0) Another usage is to add arbitrary nesting: output = snt.SelectInput( idx=[0, [1, [2]]], name='select')(input0, input1, input2) == (input0, (input1, (input2,))) SelectInput.__init__(idx, name='select_input') Module constructor. Args: idx : Indexes of the tensors to select. If idx is an integer, then a Tensor is returned. If idx is a (nested) list/tuple, then a (nested) tuple of Tensor is returned. name : Name of the module. Raises: TypeError : If idx is not an list, tuple or integer. SelectInput.__call__(*inputs) Connects the module into the graph. Args: *inputs : Tensor variables to select. Returns: Subset of inputs in an arbitrarily nested configuration. Raises: ValueError : If any entry of idx is out of bounds with respect to the size of inputs . SelectInput.connected_subgraphs Returns the subgraphs created by this module so far. SelectInput.defun() Wraps this modules call method in a callable graph function. SelectInput.defun_wrapped Returns boolean indicating whether this module is defun wrapped. SelectInput.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SelectInput.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. SelectInput.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SelectInput.graph Returns the Graph instance which the module is connected to, or None. SelectInput.is_connected Returns true iff the Module been connected to the Graph at least once. SelectInput.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. SelectInput.module_name Returns the name of the Module. SelectInput.name_scopes Returns a tuple of all name_scopes generated by this module. SelectInput.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SelectInput.scope_name Returns the full name of the Module's variable scope. SelectInput.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SelectInput.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. SelectInput.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class SeparableConv1D Performs an in-plane convolution to each channel independently. This acts as a light wrapper around the TensorFlow op tf.nn.separable_conv2d , abstracting away variable creation and sharing. SeparableConv1D.__init__(output_channels, channel_multiplier, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NWC', custom_getter=None, name='separable_conv1d') Constructs a SeparableConv1D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution Args: output_channels : Number of output channels. Must be an integer. channel_multiplier : Number of channels to expand pointwise (depthwise) convolution to. Must be an integer. Must be 0. When channel_multiplier is set to 1, applies a different filter to each input channel. Numbers larger than 1 cause the filter to be applied to channel_multiplier input channels. Outputs are concatenated together. kernel_shape : List with 2 elements in the following layout: [filter_height, filter_width] or integer that is used to define the list in all dimensions. stride : List with 4 elements of kernel strides, or integer that is used to define stride in all dimensions. Layout of list: [1, stride_y, stride_x, 1]. rate : Sequence of dilation rates (of size 1), or integer that is used to define dilation rate in all dimensions. 1 corresponds to standard 1D convolution, rate 1 corresponds to dilated convolution. Cannot be 1 if any of stride is also 1. padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 1. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with keys 'w_dw' for depthwise and 'w_pw' for pointwise) or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition the filters (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with keys 'w_dw' for depthwise and 'w_pw' for pointwise) and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NWC), or the second dimension (\"NCW\"). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: ValueError : If channel_multiplier isn't of type ( numbers.Integral or tf.Dimension ). ValueError : If channel_multiplier is less than 1. ValueError : If the given data_format is not a supported format (see SUPPORTED_1D_DATA_FORMATS ). base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of one integer. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. base.IncompatibleShapeError: If the given rate is not an integer; or if the given rate is not a sequence of two integers. base.IncompatibleShapeError: If a mask is a TensorFlow Tensor with a not fully defined shape. base.NotSupportedError: If rate in any dimension and the stride in any dimension are simultaneously 1. ValueError : If the given padding is not snt.VALID , snt.SAME , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a sequence of these. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w_dw', 'w_pw' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. TypeError : If mask is given and it is not convertible to a Tensor. ValueError : If the passed-in data_format doesn't have a channel dimension. SeparableConv1D.__call__(inputs) Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection. Args: inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Returns: A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels]. Raises: ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . SeparableConv1D.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias. SeparableConv1D.channel_multiplier Returns the channel multiplier argument. SeparableConv1D.clone(name=None) Returns a cloned _ConvND module. Args: name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name . Returns: A copy of the current class. SeparableConv1D.connected_subgraphs Returns the subgraphs created by this module so far. SeparableConv1D.conv_op_padding Returns the padding algorithm used for the underlying convolution op. SeparableConv1D.data_format Returns the data format. SeparableConv1D.defun() Wraps this modules call method in a callable graph function. SeparableConv1D.defun_wrapped Returns boolean indicating whether this module is defun wrapped. SeparableConv1D.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv1D.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. SeparableConv1D.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv1D.graph Returns the Graph instance which the module is connected to, or None. SeparableConv1D.has_bias Returns True if bias Variable is present in the module. SeparableConv1D.initializers Returns the initializers dictionary. SeparableConv1D.input_channels Returns the number of input channels. SeparableConv1D.input_shape Returns the input shape. SeparableConv1D.is_connected Returns true iff the Module been connected to the Graph at least once. SeparableConv1D.kernel_shape Returns the kernel shape. SeparableConv1D.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv1D.mask Returns the mask. SeparableConv1D.module_name Returns the name of the Module. SeparableConv1D.name_scopes Returns a tuple of all name_scopes generated by this module. SeparableConv1D.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv1D.output_channels Returns the number of output channels. SeparableConv1D.padding Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension. Returns: The padding algorithm used, if this is the same for all dimensions. Raises: ValueError : If different padding algorithms are used for different dimensions. SeparableConv1D.paddings Returns a tuple with the padding algorithm used for each dimension. SeparableConv1D.partitioners Returns the partitioners dictionary. SeparableConv1D.rate Returns the dilation rate. SeparableConv1D.regularizers Returns the regularizers dictionary. SeparableConv1D.scope_name Returns the full name of the Module's variable scope. SeparableConv1D.stride Returns the stride. SeparableConv1D.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv1D.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv1D.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv1D.w Returns the Variable containing the weight matrix. SeparableConv1D.w_dw Returns the Variable containing the depthwise weight matrix. SeparableConv1D.w_pw Returns the Variable containing the pointwise weight matrix. class SeparableConv2D Performs an in-plane convolution to each channel independently. This acts as a light wrapper around the TensorFlow op tf.nn.separable_conv2d , abstracting away variable creation and sharing. SeparableConv2D.__init__(output_channels, channel_multiplier, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NHWC', custom_getter=None, name='separable_conv2d') Constructs a SeparableConv2D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution Args: output_channels : Number of output channels. Must be an integer. channel_multiplier : Number of channels to expand pointwise (depthwise) convolution to. Must be an integer. Must be 0. When channel_multiplier is set to 1, applies a different filter to each input channel. Numbers larger than 1 cause the filter to be applied to channel_multiplier input channels. Outputs are concatenated together. kernel_shape : List with 2 elements in the following layout: [filter_height, filter_width] or integer that is used to define the list in all dimensions. stride : List with 4 elements of kernel strides, or integer that is used to define stride in all dimensions. Layout of list: [1, stride_y, stride_x, 1]. rate : Sequence of dilation rates (of size 2), or integer that is used to define dilation rate in all dimensions. 1 corresponds to standard 2D convolution, rate 1 corresponds to dilated convolution. Cannot be 1 if any of stride is also 1. padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 2. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with keys 'w_dw' for depthwise and 'w_pw' for pointwise) or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition the filters (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with keys 'w_dw' for depthwise and 'w_pw' for pointwise) and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NHWC), or the second dimension (\"NCHW\"). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: ValueError : If channel_multiplier isn't of type ( numbers.Integral or tf.Dimension ). ValueError : If channel_multiplier is less than 1. ValueError : If the given data_format is not a supported format (see SUPPORTED_2D_DATA_FORMATS ). base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. base.IncompatibleShapeError: If the given rate is not an integer; or if the given rate is not a sequence of two integers. base.IncompatibleShapeError: If a mask is a TensorFlow Tensor with a not fully defined shape. base.NotSupportedError: If rate in any dimension and the stride in any dimension are simultaneously 1. ValueError : If the given padding is not snt.VALID , snt.SAME , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a sequence of these. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w_dw', 'w_pw' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. TypeError : If mask is given and it is not convertible to a Tensor. ValueError : If the passed-in data_format doesn't have a channel dimension. SeparableConv2D.__call__(inputs) Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection. Args: inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . Returns: A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels]. Raises: ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 . SeparableConv2D.b Returns the Variable containing the bias. Returns: Variable object containing the bias, from the most recent call . Raises: base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias. SeparableConv2D.channel_multiplier Returns the channel multiplier argument. SeparableConv2D.clone(name=None) Returns a cloned _ConvND module. Args: name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name . Returns: A copy of the current class. SeparableConv2D.connected_subgraphs Returns the subgraphs created by this module so far. SeparableConv2D.conv_op_padding Returns the padding algorithm used for the underlying convolution op. SeparableConv2D.data_format Returns the data format. SeparableConv2D.defun() Wraps this modules call method in a callable graph function. SeparableConv2D.defun_wrapped Returns boolean indicating whether this module is defun wrapped. SeparableConv2D.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv2D.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. SeparableConv2D.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv2D.graph Returns the Graph instance which the module is connected to, or None. SeparableConv2D.has_bias Returns True if bias Variable is present in the module. SeparableConv2D.initializers Returns the initializers dictionary. SeparableConv2D.input_channels Returns the number of input channels. SeparableConv2D.input_shape Returns the input shape. SeparableConv2D.is_connected Returns true iff the Module been connected to the Graph at least once. SeparableConv2D.kernel_shape Returns the kernel shape. SeparableConv2D.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv2D.mask Returns the mask. SeparableConv2D.module_name Returns the name of the Module. SeparableConv2D.name_scopes Returns a tuple of all name_scopes generated by this module. SeparableConv2D.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv2D.output_channels Returns the number of output channels. SeparableConv2D.padding Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension. Returns: The padding algorithm used, if this is the same for all dimensions. Raises: ValueError : If different padding algorithms are used for different dimensions. SeparableConv2D.paddings Returns a tuple with the padding algorithm used for each dimension. SeparableConv2D.partitioners Returns the partitioners dictionary. SeparableConv2D.rate Returns the dilation rate. SeparableConv2D.regularizers Returns the regularizers dictionary. SeparableConv2D.scope_name Returns the full name of the Module's variable scope. SeparableConv2D.stride Returns the stride. SeparableConv2D.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv2D.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv2D.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SeparableConv2D.w Returns the Variable containing the weight matrix. SeparableConv2D.w_dw Returns the Variable containing the depthwise weight matrix. SeparableConv2D.w_pw Returns the Variable containing the pointwise weight matrix. class Sequential Builds a module out of a sequence of callables. Note that Sequential is limited in the range of possible architectures it can handle. This is a deliberate design decision; Sequential is only meant to be used for the simple case of fusing together modules/ops where the input of a particular module/op is the output of the previous one. Another restriction is that it is not possible to have extra arguments in the _build method that are passed to the constituents of the module - for example, if there is a BatchNorm module in Sequential and the user wishes to switch the is_training flag. If this is the desired use case, the recommended solution is to use snt.Module to wrap a custom function, as shown in the following example: https://github.com/deepmind/sonnet/blob/master/sonnet/examples/module_with_build_args.py Sequential.__init__(layers, name='sequential') Constructs a Sequential module. This feeds the output of each layer into the next and returns the output of the final layer. If a layer returns a tuple, it is assumed that this must be unpacked into the argument list of the next layer. If it is not a tuple, it is simply passed through to the next layer unchanged. Args: layers : Iterable of callables to stack together, which can be modules or ops. name : Name of the module. Raises: TypeError : If layers is None or contains any non-callable items. Sequential.__call__(*args) Connects the Sequential module into the graph. Args: *args : A tuple of inputs, to be unpacked as the arguments to the first layer. Returns: The output value of the last layer. Sequential.connected_subgraphs Returns the subgraphs created by this module so far. Sequential.defun() Wraps this modules call method in a callable graph function. Sequential.defun_wrapped Returns boolean indicating whether this module is defun wrapped. Sequential.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Sequential.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. Sequential.get_variables(*args, **kwargs) Provide a warning that get_variables on Sequential always returns (). Sequential.graph Returns the Graph instance which the module is connected to, or None. Sequential.is_connected Returns true iff the Module been connected to the Graph at least once. Sequential.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. Sequential.layers Sequential.module_name Returns the name of the Module. Sequential.name_scopes Returns a tuple of all name_scopes generated by this module. Sequential.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Sequential.scope_name Returns the full name of the Module's variable scope. Sequential.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. Sequential.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. Sequential.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class SkipConnectionCore Adds a skip connection to the base RNN core. The output of the wrapped core is the concatenation of the output of the base core with its input. The state of the wrapped core is the state of the base core. SkipConnectionCore.__init__(base_core, input_shape=None, name='skip_connection_core') Construct a SkipConnectionCore. Args: base_core : Base RNNCore to wrap. input_shape : Shape of the input as tuple, excluding the batch size. name : Name of the module. SkipConnectionCore.__call__(inputs, prev_state, **kwargs) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). SkipConnectionCore.connected_subgraphs Returns the subgraphs created by this module so far. SkipConnectionCore.defun() Wraps this modules call method in a callable graph function. SkipConnectionCore.defun_wrapped Returns boolean indicating whether this module is defun wrapped. SkipConnectionCore.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SkipConnectionCore.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. SkipConnectionCore.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SkipConnectionCore.graph Returns the Graph instance which the module is connected to, or None. SkipConnectionCore.initial_state(*args, **kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. SkipConnectionCore.is_connected Returns true iff the Module been connected to the Graph at least once. SkipConnectionCore.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. SkipConnectionCore.module_name Returns the name of the Module. SkipConnectionCore.name_scopes Returns a tuple of all name_scopes generated by this module. SkipConnectionCore.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SkipConnectionCore.output_size Integer or TensorShape: size of outputs produced by this cell. SkipConnectionCore.scope_name Returns the full name of the Module's variable scope. SkipConnectionCore.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. SkipConnectionCore.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SkipConnectionCore.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. SkipConnectionCore.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SkipConnectionCore.zero_state(*args, **kwargs) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class SliceByDim Slices a tensor along specific dimensions. The user can slice a tensor by specifying only the list of dimensions that they want to slice, together with the lists of integers containing the beginning indices of the slicing, and the size of the slices. Hence, with SliceByDim slicing can be performed without knowing in advance the rank of the input tensor. Tensorflow also offers a built-in op performing slicing, tf.slice . However, tf.slice requires all the slicing dimensions to be specified, using wildcards when no slicing is required. For example, with tf.slice , slicing half a 5D tensor along dimension 1 would be: output = tf.slice(inputs, begin=[0, 0, 0, 0, 0], size=[-1, inputs.get_shape()[1].value//2, -1, -1, -1]) The same operation using SliceByDim would be: output = SliceByDim(dims=[1], begin=[0], size=[x.get_shape()[1].value//2])(x) SliceByDim can be used to specify multiple slicing dimensions, for example: output = SliceByDim(dims=[1, 3], begin=[0, 0], size=[12, 24])(x) SliceByDim.__init__(dims, begin, size, name='slice_by_dim') Constructs the SliceByDim module. Args: dims : The dimensions to slice along, as a list of unique integers. Negative integers index from the final dimension backwards, as in python arrays. begin : The beginning indices of the slicing, as a list of integers. Must be the same length as the dims list. size : The size of the slices, as a list of integers. Must be the same length as the dims list. name : The name of the module. Raises: ValueError : If dims has non-unique integers, or if the size of begin is different from the size of dims , or if the size of size is different from the size of dims . SliceByDim.__call__(inputs) Connects the SliceByDim module into the graph. Args: inputs : Tensor to slice. Its rank must be greater than the maximum dimension specified in dims (plus one as python is 0 indexed). Returns: The sliced tensor. Raises: ValueError : If inputs tensor has insufficient rank. SliceByDim.connected_subgraphs Returns the subgraphs created by this module so far. SliceByDim.defun() Wraps this modules call method in a callable graph function. SliceByDim.defun_wrapped Returns boolean indicating whether this module is defun wrapped. SliceByDim.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SliceByDim.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. SliceByDim.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SliceByDim.graph Returns the Graph instance which the module is connected to, or None. SliceByDim.is_connected Returns true iff the Module been connected to the Graph at least once. SliceByDim.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. SliceByDim.module_name Returns the name of the Module. SliceByDim.name_scopes Returns a tuple of all name_scopes generated by this module. SliceByDim.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SliceByDim.scope_name Returns the full name of the Module's variable scope. SliceByDim.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. SliceByDim.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. SliceByDim.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class TileByDim Tile a tensor along specific dimensions. The user can tile a tensor by specifying only the list of dimensions that they want to tile, together with the lists of integers containing the multiples of the tiling. Hence, with TileByDim tiling can be performed without knowing in advance the rank of the input tensor. Tensorflow also offers a built-in op performing tiling, tf.tile . However, tf.tile requires all the tiling dimensions to be specified, using 1 when no tiling is required. For example, with tf.tiling, tiling a 5D tensor along dimension 1 , by 2 would be: output = tf.tile(inputs, multiples=[1, 2, 1, 1, 1]) The same operation using TileByDim would be: output = TileByDim(dims=[1], multiples=[2])(x) TileByDim can be used to specify multiple tiling dimensions, for example: output = TileByDim(dims=[1, 3], multiples=[2, 4])(x) TileByDim.__init__(dims, multiples, name='tile_by_dim') Constructs the TileByDim module. Args: dims : The dimensions to tile along, as a list of unique integers. multiples : The multiple of the tiling, as a list of integers. Must be the same length as the dims list. name : The name of the module. Raises: ValueError : If dims has non-unique integers, or if the size of multiples is different from the size of dims . TileByDim.__call__(inputs) Connects the TileByDim module into the graph. Args: inputs : Tensor to tile. Returns: The tiled tensor. TileByDim.connected_subgraphs Returns the subgraphs created by this module so far. TileByDim.defun() Wraps this modules call method in a callable graph function. TileByDim.defun_wrapped Returns boolean indicating whether this module is defun wrapped. TileByDim.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TileByDim.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. TileByDim.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TileByDim.graph Returns the Graph instance which the module is connected to, or None. TileByDim.is_connected Returns true iff the Module been connected to the Graph at least once. TileByDim.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. TileByDim.module_name Returns the name of the Module. TileByDim.name_scopes Returns a tuple of all name_scopes generated by this module. TileByDim.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TileByDim.scope_name Returns the full name of the Module's variable scope. TileByDim.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TileByDim.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. TileByDim.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class TrainableInitialState Helper Module that creates a learnable initial state for an RNNCore. This class receives an example (possibly nested) initial state of an RNNCore, and returns a state that has the same shape, structure, and values, but is trainable. Additionally, the user may specify a boolean mask that indicates which parts of the initial state should be trainable. This allows users to train an unrolled RNNCore with a learnable initial state in the following way: core = ... # Any RNNCore module object. initial_state = core.initial_state(batch_size, dtype) trainable_initial_state = snt.TrainableInitialState(initial_state)() output, final_state = tf.nn.dynamic_rnn( core, input_sequence, initial_state=trainable_initial_state) TrainableInitialState.__init__(initial_state, mask=None, name='trainable_initial_state') Constructs the Module that introduces a trainable state in the graph. It receives an initial state that will be used as the initial values for the trainable variables that the module contains, and optionally a mask that indicates the parts of the initial state that should be learnable. Args: initial_state : tensor or arbitrarily nested iterables of tensors. mask : optional boolean mask. It should have the same nested structure as the given initial_state. name : module name. Raises: TypeError : if mask is not a list of booleans or None. TrainableInitialState.__call__() Connects the module to the graph. Returns: The learnable state, which has the same type, structure and shape as the initial_state passed to the constructor. TrainableInitialState.connected_subgraphs Returns the subgraphs created by this module so far. TrainableInitialState.defun() Wraps this modules call method in a callable graph function. TrainableInitialState.defun_wrapped Returns boolean indicating whether this module is defun wrapped. TrainableInitialState.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TrainableInitialState.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. TrainableInitialState.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TrainableInitialState.graph Returns the Graph instance which the module is connected to, or None. TrainableInitialState.is_connected Returns true iff the Module been connected to the Graph at least once. TrainableInitialState.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. TrainableInitialState.module_name Returns the name of the Module. TrainableInitialState.name_scopes Returns a tuple of all name_scopes generated by this module. TrainableInitialState.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TrainableInitialState.scope_name Returns the full name of the Module's variable scope. TrainableInitialState.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TrainableInitialState.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. TrainableInitialState.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class TrainableVariable Provides learnable parameter Tensor. TrainableVariable.__init__(shape, dtype=tf.float32, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='trainable_variable') Constructs a TrainableVariable module. Args: shape : Tensor shape. dtype : Tensor data type. initializers : Optional dictionary containing ops to initialize the weight Tensor, with key 'w'. partitioners : Optional dict containing a partitioner to partition the weight (with key 'w'). As a default, no partitioner is used. regularizers : Optional dict containing regularizers for the weights (with key 'w'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . custom_getter : Optional callable or dictionary of callables to use as custom_getter for the module. name : Name of the module. Raises: KeyError : If initializers contains any keys other than 'w'. KeyError : If partitioners contains any keys other than 'w'. KeyError : If regularizers contains any keys other than 'w'. TypeError : If any of the given initializers are not callable. TypeError : If any of the given partitioners are not callable. TypeError : If any of the given regularizers are not callable. TrainableVariable.__call__() Connects the TrainableTensor module into the graph. Returns: A Tensor of shape as determined in the constructor. TrainableVariable.connected_subgraphs Returns the subgraphs created by this module so far. TrainableVariable.defun() Wraps this modules call method in a callable graph function. TrainableVariable.defun_wrapped Returns boolean indicating whether this module is defun wrapped. TrainableVariable.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TrainableVariable.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. TrainableVariable.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TrainableVariable.graph Returns the Graph instance which the module is connected to, or None. TrainableVariable.is_connected Returns true iff the Module been connected to the Graph at least once. TrainableVariable.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. TrainableVariable.module_name Returns the name of the Module. TrainableVariable.name_scopes Returns a tuple of all name_scopes generated by this module. TrainableVariable.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TrainableVariable.scope_name Returns the full name of the Module's variable scope. TrainableVariable.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TrainableVariable.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. TrainableVariable.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. TrainableVariable.w Returns the Variable containing the weights Tensor. Returns: Variable object containing the weights, from the most recent call . Raises: base.Error: If the module has not been connected to the graph yet, meaning the variables do not exist. class Transposable Transposable module interface. The Transposable interface requires that transposable modules implement a method called transpose , returning a module that is the transposed version of the one the method is called on. Calling the method twice should return a module with the same specifications as the original module. When implementing a transposable module, special care is required to make sure that parameters needed to instantiate the module are provided as functions whose invocation is deferred to graph construction time. For example, in Linear we might want to call: linear = snt.Linear(name= linear , output_size=output_size) linear_transpose = linear.transpose() where the output_size for linear_transpose is not known yet, as linear is not yet connected to the graph: output_size is passed to linear_transpose's constructor as a lambda returning linear.input_size. The lambda will return the correct value once linear is given an input. Notice that linear_transpose's output_size value does not need to be defined until the module is connected to the graph. Transposable.input_shape() Returns shape of input Tensor passed at last call to build . Transposable.transpose(name=None, **kwargs) Builds and returns transposed version of module. Args: name : Name of the transposed module. **kwargs : Additional Python flags controlling transposition. Returns: Transposed version of the module. class UnderspecifiedError Error raised when too little information is available. This does not typically mean the user is trying to do something that doesn't work (in which case IncompatibleShapeError should be used), just that some more information needs to be provided in order to build the Graph. class VanillaRNN Basic fully connected vanilla RNN core. VanillaRNN.__init__(hidden_size, activation= function tanh at 0x7fcc5893e048 , initializers=None, partitioners=None, regularizers=None, name='vanilla_rnn') Construct a Basic RNN core. Args: hidden_size : hidden size dimensionality. activation : activation function to use. initializers : optional dict containing ops to initialize the weights. This dictionary may contain the keys 'in_to_hidden' and/or 'hidden_to_hidden'. partitioners : optional dict containing ops to partition the weights. This dictionary may contain the keys 'in_to_hidden' and/or 'hidden_to_hidden'. regularizers : optional dict containing ops to regularize the weights. This dictionary may contain the keys 'in_to_hidden' and/or 'hidden_to_hidden'. name : name of the module. Raises: KeyError : if initializers contains any keys other than 'in_to_hidden' or 'hidden_to_hidden'. KeyError : if partitioners contains any keys other than 'in_to_hidden' or 'hidden_to_hidden'. KeyError : if regularizers contains any keys other than 'in_to_hidden' or 'hidden_to_hidden'. TypeError : If any of the given initializers are not callable. TypeError : If any of the given partitioners are not callable. TypeError : If any of the given regularizers are not callable. VanillaRNN.__call__(input_, prev_state) Connects the VanillaRNN module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided as input_ and state must have the same final dimension, in order for the existing variables to be the correct size for their corresponding multiplications. The batch size may differ for each connection. Args: input_ : a 2D Tensor of size [batch_size, input_size]. prev_state : a 2D Tensor of size [batch_size, hidden_size]. Returns: output : a 2D Tensor of size [batch_size, hidden_size]. next_state : a Tensor of size [batch_size, hidden_size]. Raises: ValueError : if connecting the module into the graph any time after the first time, and the inferred size of the inputs does not match previous invocations. VanillaRNN.connected_subgraphs Returns the subgraphs created by this module so far. VanillaRNN.defun() Wraps this modules call method in a callable graph function. VanillaRNN.defun_wrapped Returns boolean indicating whether this module is defun wrapped. VanillaRNN.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. VanillaRNN.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. VanillaRNN.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. VanillaRNN.graph Returns the Graph instance which the module is connected to, or None. VanillaRNN.hidden_to_hidden_linear VanillaRNN.hidden_to_hidden_variables VanillaRNN.in_to_hidden_linear VanillaRNN.in_to_hidden_variables VanillaRNN.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. VanillaRNN.is_connected Returns true iff the Module been connected to the Graph at least once. VanillaRNN.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. VanillaRNN.module_name Returns the name of the Module. VanillaRNN.name_scopes Returns a tuple of all name_scopes generated by this module. VanillaRNN.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. VanillaRNN.output_size Integer or TensorShape: size of outputs produced by this cell. VanillaRNN.scope_name Returns the full name of the Module's variable scope. VanillaRNN.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. VanillaRNN.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. VanillaRNN.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. VanillaRNN.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. VanillaRNN.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . check_initializers(initializers, keys) Checks the given initializers. This checks that initializers is a dictionary that only contains keys in keys , and furthermore the entries in initializers are functions or further dictionaries (the latter used, for example, in passing initializers to modules inside modules) that must satisfy the same constraints. Args: initializers : Dictionary of initializers (allowing nested dictionaries) or None. keys : Iterable of valid keys for initializers . Returns: Copy of checked dictionary of initializers. If initializers=None , an empty dictionary will be returned. Raises: KeyError : If an initializer is provided for a key not in keys . TypeError : If a provided initializer is not a callable function, or initializers is not a Mapping. check_partitioners(partitioners, keys) Checks the given partitioners. This checks that partitioners is a dictionary that only contains keys in keys , and furthermore the entries in partitioners are functions or further dictionaries (the latter used, for example, in passing partitioners to modules inside modules) that must satisfy the same constraints. Args: partitioners : Dictionary of partitioners (allowing nested dictionaries) or None. keys : Iterable of valid keys for partitioners . Returns: Checked dictionary of partitioners. If partitioners=None , an empty dictionary will be returned. Raises: KeyError : If an partitioner is provided for a key not in keys . TypeError : If a provided partitioner is not a callable function, or partitioners is not a Mapping. check_regularizers(regularizers, keys) Checks the given regularizers. This checks that regularizers is a dictionary that only contains keys in keys , and furthermore the entries in regularizers are functions or further dictionaries (the latter used, for example, in passing regularizers to modules inside modules) that must satisfy the same constraints. Args: regularizers : Dictionary of regularizers (allowing nested dictionaries) or None. keys : Iterable of valid keys for regularizers . Returns: Copy of checked dictionary of regularizers. If regularizers=None , an empty dictionary will be returned. Raises: KeyError : If an regularizers is provided for a key not in keys . TypeError : If a provided regularizer is not a callable function, or regularizers is not a Mapping. clip_gradient(net, clip_value_min, clip_value_max, name=None) Clips respective gradients of a given tensor. Acts as identity for the forward pass, but clips gradient tensor element-wise by value during the backward pass. Any gradient values less than clip_value_min or greater than clip_values_max are set to the respective limit values. Args: net : A tf.Tensor . clip_value_min : A 0-D Tensor or scalar. The minimum value to clip by. clip_value_max : A 0-D Tensor or scalar. The maximum value to clip by. name : A name for the operation (optional, default 'clip_gradient'). Returns: A tf.Tensor with the same type as the input tensor. Raises: ValueError : If net dtype is non-float. count_variables_by_type(variables=None) Returns a dict mapping dtypes to number of variables and scalars. Args: variables : iterable of tf.Variable s, or None. If None is passed, then all global and local variables in the current graph are used. Returns: A dict mapping tf.dtype keys to a dict containing the keys 'num_scalars' and 'num_variables'. custom_getter_router(custom_getter_map, name_fn) Creates a custom getter than matches requests to dict of custom getters. Custom getters are callables which implement the [custom getter API] (https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/get_variable). The returned custom getter dispatches calls based on pattern matching the name of the requested variable to the keys of custom_getter_map. For example, { \".*/w\": snt.custom_getters.stop_gradient, } will match all variables named with the suffix \"/w\". The name_fn is provided to allow processing of the name, such as stripping off a scope prefix before matching. Args: custom_getter_map : Mapping of regular expressions to custom getter functions. name_fn : Callable to map variable name through before matching to regular expressions. This might, for example, strip off a scope prefix. Returns: A custom getter. Raises: TypeError : If an entry in custom_getter_map is not a callable function. deprecation_warning(deprecation_message) Log a warning message the user is using deprecated functionality. format_variable_map(variable_map, join_lines=True) Takes a key-to-variable map and formats it as a table. format_variables(variables, join_lines=True) Takes a collection of variables and formats it as a table. get_lagrange_multiplier(shape=(), rate=1.0, initializer=1.0, maximize=True, valid_range=None, name='lagrange_multiplier') Lagrange multiplier factory. This factory returns ops that help setting up constrained optimization problems in Tensorflow. Given a constraint function op (either scalar or vectorial), use this function to instantiate a Lagrange multiplier op, then dot product the two and add them to the loss that is being optimized over. There is no need to instantiate a second optimizer to solve the minmax problem, as the Lagrange Multiplier op is setup to manipulate its own gradients so that a single optmizer can be used to update all the variables correctly. Args: shape : Lagrange multipliers can be used with both scalar and vector constraint functions; when using vector constraints use the shape kwarg to pass in shape information and instantiate variables of the correct shape. rate : Scalar used to scale the magnitude of gradients of the Lagrange multipliers, defaulting to 1e-2. Using the default value will make the Lagrange multipliers updates slower compared to the ones for the model's parameters. initializer : Initializer for the Lagrange multipliers. Note that when using inequality constraints the initial value of the multiplier will be transformed via the parametrization function. maximize : Boolean, True if we want to maximize the loss w.r.t. the Lagrange multipliers, False otherwise. valid_range : tuple, or list. of values used to clip the value of the (possibly reparametrized) Lagrange multipliers. name : Name of the Lagrange multiplier op. Returns: An op to be inserted in the graph, by multipling it with a constraint op and adding the resulting op to a loss. The Lagrange multiplier gradients are modified to that by calling minimize on the loss the optimizer will actually minimize w.r.t. to the model's parameters and maximize w.r.t. the Lagrande multipliers, hence enforcing the constraints. Raises: ValueError : If the Lagrange multiplier is set to enforce an equality constraint and a parametrization function is also provided. get_normalized_variable_map(scope_or_module, collection='variables', context=None, group_sliced_variables=True) Builds map of tf.Variable s in scope or module with normalized names. The names of the variables are normalized to remove the scope prefix. Args: scope_or_module : Scope or module to build map from. collection : Collection to restrict query to. By default this is tf.Graphkeys.GLOBAL_VARIABLES , which includes non-trainable variables such as moving averages. context : Scope or module, identical to or parent of scope . If given, this will be used as the stripped prefix. By default None , which means context=scope . group_sliced_variables : Boolean, if set to True, sliced variables are grouped together in the returned map; if set to False, each partition of a sliced variable is a separate (key, value) pair. Returns: Dictionary mapping normalized variable name to tf.Variable , or a list of tf.Variables if the variable is a sliced (partitioned) variable. Raises: ValueError : If context is given but is not a proper prefix of scope . get_saver(scope, collections=('variables',), context=None, **kwargs) Builds a tf.train.Saver for the scope or module, with normalized names. The names of the variables are normalized to remove the scope prefix. This allows the same variables to be restored into another similar scope or module using a complementary tf.train.Saver object. Args: scope : Scope or module. Variables within will be saved or restored. collections : Sequence of collections of variables to restrict tf.train.Saver to. By default this is tf.GraphKeys.GLOBAL_VARIABLES which includes moving averages variables as well as trainable variables. context : Scope or module, identical to or parent of scope . If given, this will be used as the stripped prefix. **kwargs : Extra keyword arguments to pass to tf.train.Saver. Returns: A `tf.train.Saver` object for Variables in the scope or module. get_variables_in_module(module, collection='trainable_variables') Returns tuple of tf.Variable s declared inside an snt.Module . Note that this operates by searching the variable scope a module contains, and so does not know about any modules which were constructed elsewhere but used inside this module. Args: module : snt.Module instance to query the scope of. collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. get_variables_in_scope(scope, collection='trainable_variables') Returns a tuple tf.Variable s in a scope for a given collection. Args: scope : tf.VariableScope or string to retrieve variables from. collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. has_variable_scope(obj) Determines whether the given object has a variable scope. highway_core_with_recurrent_dropout(hidden_size, num_layers, keep_prob=0.5, **kwargs) Highway core with recurrent dropout. Args: hidden_size : (int) Hidden size dimensionality. num_layers : (int) Number of highway layers. keep_prob : the probability to keep an entry when applying dropout. **kwargs : Extra keyword arguments to pass to the highway core. Returns: A tuple (train_core, test_core) where train_core is a higway core with recurrent dropout enabled to be used for training and test_core is the same highway core without recurrent dropout. log_variables(variables=None) Logs variable information. This function logs the name, shape, type, collections, and device for either all variables or a given iterable of variables. In the \"Device\" columns, the nature of the variable (legacy or resource (for ResourceVariables)) is also specified in parenthesis. Args: variables : iterable of variables; if not provided, then all variables (in the default graph) are logged. lstm_with_recurrent_dropout(hidden_size, keep_prob=0.5, **kwargs) LSTM with recurrent dropout. Args: hidden_size : the LSTM hidden size. keep_prob : the probability to keep an entry when applying dropout. **kwargs : Extra keyword arguments to pass to the LSTM. Returns: A tuple (train_lstm, test_lstm) where train_lstm is an LSTM with recurrent dropout enabled to be used for training and test_lstm is the same LSTM without recurrent dropout. lstm_with_zoneout(hidden_size, keep_prob_c=0.5, keep_prob_h=0.95, **kwargs) LSTM with recurrent dropout. Args: hidden_size : the LSTM hidden size. keep_prob_c : the probability to use the new value of the cell state rather than freezing it. keep_prob_h : the probability to use the new value of the hidden state rather than freezing it. **kwargs : Extra keyword arguments to pass to the LSTM. Returns: A tuple (train_lstm, test_lstm) where train_lstm is an LSTM with recurrent dropout enabled to be used for training and test_lstm is the same LSTM without zoneout. merge_leading_dims(array_or_tensor, n_dims=2) Merge the first dimensions of a tensor. Args: array_or_tensor : Tensor to have its first dimensions merged. Can also be an array or numerical value, which will be converted to a tensor for batch application, if needed. n_dims : Number of dimensions to merge. Returns: Either the input value converted to a Tensor, with the requested dimensions merged, or the unmodified input value if the input has less than n_dims dimensions. Raises: ValueError : If the rank of array_or_tensor is not well-defined. observe_connections(observer) Notifies the observer whenever any Sonnet module is connected to the graph. If a module contains nested modules, the observer is notified once for each nested module, followed by the containing module. For example: def logging_observer(connected_subgraph): logging.info(connected_subgraph.module.module_name) with snt.observe_connections(logging_observer): output = imagenet_module(input_tensor) Args: observer : Callable accepting a single argument. Will be called with a ConnectedSubGraph each time a module is connected to the graph. Yields: None : just yields control to the inner context. parse_string_to_constructor(ctor_string) Returns a callable which corresponds to the constructor string. Various modules (eg, ConvNet2D) take constructor arguments which are callables, indicating a submodule to build. These can be passed as actual constructors, eg snt.LayerNorm , however that makes the config for that module not trivially serializable. This function tries to map a string representation to the underlying callable, allowing configs to remain serializable where necessary. Args: ctor_string : string representing some module in Sonnet. If the string is provided with no dots, we assume it is a member of Sonnet available at top level, i.e. \"LayerNorm\" maps to snt.LayerNorm . Raises: ValueError : if no matching constructor can be found. Returns: Callable constructor which corresponds to ctor_string . remove_unsupported_kwargs(module_or_fn, all_kwargs_dict) Removes any kwargs not supported by module_or_fn from all_kwargs_dict . A new dict is return with shallow copies of keys values from all_kwargs_dict , as long as the key is accepted by module_or_fn. The returned dict can then be used to connect module_or_fn (along with some other inputs, ie non-keyword arguments, in general). snt.supports_kwargs is used to tell whether a given kwarg is supported. Note that this method may give false negatives, which would lead to extraneous removals in the result of this function. Please read the docstring for snt.supports_kwargs for details, and manually inspect the results from this function if in doubt. Args: module_or_fn : some callable which can be interrogated by snt.supports_kwargs . Generally a Sonnet module or a method (wrapped in @reuse_variables ) of a Sonnet module. all_kwargs_dict : a dict containing strings as keys, or None. Raises: ValueError : if all_kwargs_dict is not a dict. Returns: A dict containing some subset of the keys and values in all_kwargs_dict . This subset may be empty. If all_kwargs_dict is None, this will be an empty dict. reuse_variables(method) Wraps an arbitrary method so it does variable sharing. This decorator creates variables the first time it calls method , and reuses them for subsequent calls. The object that calls method provides a tf.VariableScope , either as a variable_scope attribute or as the return value of an _enter_variable_scope() method. The first time the wrapped method is invoked, it enters the caller's tf.VariableScope with reuse=False . On all subsequent calls it enters the same variable scope with reuse=True . Variables are created in the context of the tf.VariableScope provided by the caller object. Ops are created with an additional tf.name_scope() , which adds a scope for the wrapped method name. For example: class MyClass(object): def __init__(self, name): with tf.variable_scope(None, default_name=name) as variable_scope: self.variable_scope = variable_scope @snt.reuse_variables def add_x(self, tensor): x = tf.get_variable( x , shape=tensor.get_shape()) return tensor + x module = MyClass( my_module_name ) input_tensor = tf.zeros(shape=(5,)) # This creates the variable my_module_name/x # and op my_module_name/add_x/add output = module.add_x(input_tensor) For performance when executing eagerly it may be desirable to additionally annotate these methods using defun , such that they are encapsulated as graph functions. This is not recommended if your method returns a variable since the output of defun would be an op that returned the variable's value when evaluated (rather than the variable instance). class FooModule(snt.AbstractModule): def _build(self, inputs): return complex_math(inputs) @tfe.defun @snt.reuse_variables def more_complex_stuff(self, inputs): return more_complex_math(inputs) Args: method : The method to wrap. Returns: The wrapped method. scale_gradient(net, scale, name='scale_gradient') Scales gradients for the backwards pass. This might be used to, for example, allow one part of a model to learn at a lower rate than the rest. WARNING: Think carefully about how your optimizer works. If, for example, you use rmsprop, the gradient is always rescaled (with some additional epsilon) towards unity. This means scale_gradient won't have the effect of lowering the learning rate. If scale is 0.0 , this op reduces to tf.stop_gradient . If scale is 1.0 , this op reduces to tf.identity . Args: net : A tf.Tensor or in eager mode a callable that produces a tf.Tensor . scale : The scale factor for the gradient on the backwards pass. name : A name for the operation (optional). Returns: In graph mode returns a tf.Tensor with the same type as the input tensor. In eager mode returns a callable wrapping net whose gradients are scaled. Raises: ValueError : If net dtype is non-float and scale is not zero or one. split_leading_dim(tensor, inputs, n_dims=2) Split the first dimension of a tensor. Args: tensor : Tensor to have its first dimension split. inputs : Original reference input to look the dimensions of. n_dims : Number of dimensions to split. Returns: The input tensor, with its first dimension split. summarize_variables(variables=None) Logs a summary of variable information. This function groups Variables by dtype and prints out the number of Variables and the total number of scalar values for each datatype, as well as the total memory consumed. For Variables of type tf.string, the memory usage cannot be accurately calculated from the Graph as the memory requirements change based on what strings are actually stored, which can only be determined inside a session. In this case, the amount of memory used to stored the pointers to the strings is logged, along with a warning. Args: variables : iterable of variables; if not provided, then all variables (in the default graph) are summarized. supports_kwargs(module_or_fn, kwargs_list) Determines whether the provided callable supports all the kwargs. This is useful when you have a module that might or might not support a kwarg such as is_training . Rather than calling the module and catching the error, risking the potential modification of underlying state, this function introspects the module to see what kwargs are actually supported, using the python inspect module. Note that many TF functions do not export a valid argspec object, rather they have a generic args, *kwargs signature due to various layers of wrapping (deprecation decorators, etc). In those circumstances we return MAYBE_SUPPORTED, and users will have to use another method to tell whether the kwargs are supported (e.g. by just calling the function). Args: module_or_fn : some callable, generally an object or a method of some object. If an object is provided, we check wither module_or_fn.__call__ supports the provided kwargs, which for a Sonnet module will automatically check the signature of _build. If module_or_fn is a function/method, then we check its signature directly, so non-Sonnet functions can be used. kwargs_list : string or iterable of strings of keyword arg names to test for. If an empty iterable is provided this function will always return True. Raises: ValueError : if a non-string is provided in kwargs_list . Returns: a string, one of 'supported', 'not_supported' or 'maybe_supported'. trainable_initial_state(batch_size, state_size, dtype, initializers=None, regularizers=None, name=None) Creates an initial state consisting of trainable variables. The trainable variables are created with the same shapes as the elements of state_size and are tiled to produce an initial state. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. state_size : A TensorShape or nested tuple of TensorShape s to use for the shape of the trainable variables. dtype : The data type used to create the variables and thus initial state. initializers : An optional container of the same structure as state_size containing initializers for the variables. regularizers : An optional container of the same structure as state_size containing regularizers for the variables. name : optional string used to prefix the initial state variable names. Returns: A Tensor or nested tuple of Tensor s with the same size and structure as state_size , where each Tensor is a tiled trainable Variable . Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. variable_map_items(variable_map) Yields an iterator over (string, variable) pairs in the variable map. In general, variable maps map variable names to either a tf.Variable , or list of tf.Variable s (in case of sliced variables). Args: variable_map : dict, variable map over which to iterate. Yields: (string, tf.Variable) pairs. wrap_with_spectral_norm(module_class, sn_kwargs=None, pow_iter_collection=None) Returns a constructor for the inner class with spectral normalization. This function accepts a Sonnet AbstractModule class as argument (the class, not an instance of that class) alongside an optional dictionary of keyword arguments for the spectral_norm function, and returns a constructor which can be treated identically to the constructor of the input class, but with spectral normalization applied to the weights created by the class. Internally, this is just a partially evaluated SpectralNormWrapper module. pow_iter_collection , if not None, is treated as the name of a TensorFlow global collection. Each time the module's weight matrix is accessed ops are built for performing one step of power iteration to approximate that weight's first singular follow and ops are created for saving this new approximation in an internal variable. At build-time the resulting object takes a special boolean 'enable_power_iteration' keyword argument. If this is True (the default), a control dependency on the operation for updating this internal variable is attached to the returned weight. Otherwise, the update is not attached as a control dependency, but an op is placed into the pow_iter_collection global collection which causes the internal variable to be updated. It is then up to the user to choose whether to run this update. Args: module_class : A constructor/class reference for a Sonnet module you would like to wrap and automatically apply spectral normalization. sn_kwargs : Keyword arguments to be passed to the spectral_norm function in addition to the weight tensor. pow_iter_collection : The name of a global collection for potentially storing ops for updating internal variables. Returns: An snt.AbstractModule class representing the original with spectral norm. class custom_getters.Context Contextually switching a custom getter on. Example usage, using snt.custom_getters.stop_gradient with Context to selectively disable gradients flowing to variables for particular connections of the module: custom_getter = snt.custom_getters.Context(snt.custom_getters.stop_gradient) lin = snt.Linear(10, custom_getter=custom_getter) lin(net1) # custom getter not used, gradients on with custom_getter: lin(net2) # custom getter used, gradients off Warning: If the custom getter affects the way the variable is created, then switching it on or off after the variable has been created will have no effect. For example, it is not possible to selectively switch off trainability using custom_getters.non_trainable , since this is a creation-time attribute. It is however possible to selectively switch off gradients using custom_getters.stop_gradient , since this applies an operation to the variable. custom_getters.Context.__init__(getter, verbose=False, default_getter=None) Initializes a contextual switch for a custom getter. Args: getter : The custom getter which we may want to switch on. verbose : Log out every time a variable is fetched, and whether or not getter is used. default_getter : The custom getter to use when this context is not active. If None, the default custom getter is used. Returns: A custom getter which can also be used as a context manager. Entering the context enables the custom getter. custom_getters.non_trainable(getter, *args, **kwargs) Custom getter which makes a variable non-trainable. Usage like: with tf.variable_scope(\"\", custom_getter=snt.custom_getters.non_trainable): net = snt.Linear(num_hidden)(net) or, using the custom_getter constructor argument, linear = snt.Linear(num_hidden, custom_getter=snt.custom_getters.non_trainable) net = linear(net) will result in the variables inside the linear having trainable=False , i.e. won't be added to tf.trainable_variables() and thus won't be optimized. Warning: If reuse=True and the variable has previously been created in the same graph with trainable=True , this custom getter will do nothing. Similarly if the variable is reused after being created by this custom getter it will still be non-trainable, even if trainable=True . When used with a Sonnet module, the module must be constructed inside the variable scope with the custom getter. Just building the module inside said variable scope will not use the custom getter. Args: getter : The true getter to call. *args : Arguments, in the same format as tf.get_variable. **kwargs : Keyword arguments, in the same format as tf.get_variable. Returns: The return value of getter(*args, **kwargs) except with trainable=False enforced. custom_getters.override_args(**kwargs) Creates a custom getter that applies specified named arguments. Args: **kwargs : Overriding arguments for the custom getter to use in preference the named arguments it's called with. Returns: Custom getter. custom_getters.override_default_args(**kwargs) Creates a custom getter that applies specified named arguments. The returned custom getter treats the specified named arguments as revised defaults, and does not override any non- None argument values supplied by the original get_variable call (or by a nested scope's custom getter). Args: **kwargs : Overriding arguments for the custom getter to use in preference the named arguments it's called with. Returns: Custom getter. custom_getters.restore_initializer(filename, name_fn=None, collection='variables') Custom getter to restore all variables with snt.restore_initializer . Args: filename : The filename of the checkpoint. name_fn : A function which can map the name of the variable requested. This allows restoring variables with values having different names in the checkpoint. collection : Only set the restore initializer for variables in this collection. If None , it will attempt to restore all variables. By default tf.compat.v1.GraphKeys.GLOBAL_VARIABLES . Returns: A restore_initializer custom getter, which is a function taking arguments (getter, name, args, *kwargs). custom_getters.stop_gradient(getter, *args, **kwargs) Custom getter which prevents variables being optimized. Usage like: with tf.variable_scope(\"\", custom_getter=snt.custom_getters.stop_gradient): net = snt.Linear(num_hidden)(net) or, using the custom_getter constructor argument, linear = snt.Linear(num_hidden, custom_getter=snt.custom_getters.stop_gradient) net = linear(net) will result in the gradient with respect to the variables in the linear module being None . By default, the variables will still be in the trainable variables collection. When used with a Sonnet module, the module must be constructed inside the variable scope with the custom getter. Just building the module inside said variable scope will not use the custom getter. Args: getter : The true getter to call. *args : Arguments, in the same format as tf.get_variable. **kwargs : Keyword arguments, in the same format as tf.get_variable. Returns: The return value of getter(*args, **kwargs) with a tf.stop_gradient. class custom_getters.bayes_by_backprop.EstimatorModes class custom_getters.bayes_by_backprop._VariableMetadata VariableMetadata(raw_variable_name, raw_variable_shape, scope_name, posterior, posterior_estimate, prior, kl_cost, prior_vars, posterior_vars) custom_getters.bayes_by_backprop._VariableMetadata.kl_cost Alias for field number 6 custom_getters.bayes_by_backprop._VariableMetadata.posterior Alias for field number 3 custom_getters.bayes_by_backprop._VariableMetadata.posterior_estimate Alias for field number 4 custom_getters.bayes_by_backprop._VariableMetadata.posterior_vars Alias for field number 8 custom_getters.bayes_by_backprop._VariableMetadata.prior Alias for field number 5 custom_getters.bayes_by_backprop._VariableMetadata.prior_vars Alias for field number 7 custom_getters.bayes_by_backprop._VariableMetadata.raw_variable_name Alias for field number 0 custom_getters.bayes_by_backprop._VariableMetadata.raw_variable_shape Alias for field number 1 custom_getters.bayes_by_backprop._VariableMetadata.scope_name Alias for field number 2 custom_getters.bayes_by_backprop.adaptive_gaussian_prior_builder(getter, name, *args, **kwargs) A pre-canned builder for adaptive scalar gaussian prior distributions. Given a true getter function and arguments forwarded from tf.get_variable , return a distribution object for a scalar-valued adaptive gaussian prior which will be broadcast over a variable of the requisite shape. This prior's parameters (e.g loc and scale for a gaussian) will consist of a single learned scalar for the entire tf.Variable for which it serves as the prior, regardless of that tf.Variable 's shape. Args: getter : The getter passed to a custom_getter . Please see the documentation for tf.get_variable . name : The name argument passed to tf.get_variable . *args : See positional arguments passed to tf.get_variable . **kwargs : See keyword arguments passed to tf.get_variable . Returns: An instance of tfp.distributions.Normal representing the prior distribution over the variable in question. custom_getters.bayes_by_backprop.analytic_kl_builder(posterior, prior, sample) A pre-canned builder for the analytic kl divergence. custom_getters.bayes_by_backprop.bayes_by_backprop_getter(posterior_builder= function diagonal_gaussian_posterior_builder at 0x7fcc549372f0 , prior_builder= function fixed_gaussian_prior_builder at 0x7fcc54937620 , kl_builder= function stochastic_kl_builder at 0x7fcc54937730 , sampling_mode_tensor=None, fresh_noise_per_connection=True, keep_control_dependencies=False) Creates a custom getter which does Bayes by Backprop. Please see tf.get_variable for general documentation on custom getters. All arguments are optional. If nothing is configued, then a diagonal gaussian posterior will be used, and a fixed N(0, 0.01) prior will be used. Please see the default posterior_builder and prior_builder for a more detailed understanding of the default settings. Args: posterior_builder : A builder function which constructs an instance of tfp.distributions.Distribution which shall serve as the posterior over the tf.Variable of interest. The builder receives the getter and the arguments forwarded from tf.get_variable . Suppose one wrote tf.get_variable( 'weights', shape=(3,), initializer=tf.zeros_initializer, dtype=tf.float32) then the posterior_builder argument would receive the name , shape , initializer , and dtype arguments passed above. The builder must return a tfp.distributions.Distribution object. Please see the tf.get_variable for documentation on custom_getter and getter , and see bbb.diagonal_gaussian_posterior_builder (the default) for an example of using this builder API. prior_builder : A builder function which constructs an instance of tfp.distributions.Distribution which shall serve as the prior over the tf.Variable of interest. Identical API to posterior_builder . See bbb.fixed_gaussian_prior_builder (the default) for an example. kl_builder : A builder function which receives the posterior distribution, prior distribution, and a sample from the posterior. It returns a scalar-shaped tf.Tensor representing the total KL cost for the tf.Variable in question. See bbb.stochastic_kl_builder (default) and bbb.analytic_kl_builder for examples. sampling_mode_tensor : A tf.Tensor which determines how an estimate from the posterior is produced. It must be scalar-shaped and have a dtype of tf.string . Valid values for this tensor are bbb.EstimatorModes.sample (which is the default), bbb.EstimatorModes.mean , and bbb.EstimatorModes.last_sample . bbb.EstimatorModes.sample is appropriate for training, and bbb.EstimatorModes.mean can be used at test time. fresh_noise_per_connection : A boolean. Indicates that each time a stochastic variable is retrieved with this custom getter, new sampling noise should be used. This is True by default. If this argument is set to False , then the same noise is used for each connection. Note that this does not apply to connections within a tf.while_loop ; the same sampling noise is always used in different iterations of a tf.while_loop within one session.run() call. See the unit tests for details. keep_control_dependencies : A boolean. This argument should only be used by advanced users. Indicates that each time a stochastic variable is retrieved in the loop body of a tf.while_loop construct, new sampling noise should be used. The default behavior is False , so that RNNs use the same weights at each recurrent time step. This is done by removing the creation of the Variable from any existing control flow contexts. Notably, the Variables will be created outside the context of any tf.while_loop, making them fetchable. When this argument is True , any Variables used in the loop body of a tf.while_loop will be non-fetchable. If the KL cost needs to be evaluated, the Variable must first be used outside the loop body. This op using the Variable simply needs to be placed on the graph to get a stochastic estimate of the KL; it doesn't need to ever be used. Example: ``` def loop_body(i): logits = sonnet_module(queue) i = i + 1 with tf.variable_scope('bbb', custom_getter=bbb.bayes_by_backprop_getter( fresh_noise_per_connection=True, keep_control_dependencies=True)): unused_op = sonnet_module(queue) # Adds KL estimate to bbb Collection final_i = tf.while_loop(lambda i: i 5, loop_body, tf.constant(0.)) ``` Here when we add unused_op to the graph, we also add a number of tensors associated with the particular stochastic variable, including its contribution to the KL cost, to a graph-level registry. These are organized in a per-stochastic-variable data structure and be accessed with bbb.get_variable_metadata() . Without this line, these Tensors would instead be added the first time the Variable is used in the while_loop, which would make them non-fetchable. In all cases, the KL cost is only added once per Variable, which is the correct behavior, since if a variable is used multiple times in a model, the KL cost should remain unaffected. Returns: A custom_getter function which implements Bayes by Backprop. custom_getters.bayes_by_backprop.diagonal_gaussian_posterior_builder(getter, name, shape=None, *args, **kwargs) A pre-canned builder for diagonal gaussian posterior distributions. Given a true getter function and arguments forwarded from tf.get_variable , return a distribution object for a diagonal posterior over a variable of the requisite shape. Args: getter : The getter passed to a custom_getter . Please see the documentation for tf.get_variable . name : The name argument passed to tf.get_variable . shape : The shape argument passed to tf.get_variable . *args : See positional arguments passed to tf.get_variable . **kwargs : See keyword arguments passed to tf.get_variable . Returns: An instance of tfp.distributions.Normal representing the posterior distribution over the variable in question. custom_getters.bayes_by_backprop.fixed_gaussian_prior_builder(getter, name, dtype=None, *args, **kwargs) A pre-canned builder for fixed gaussian prior distributions. Given a true getter function and arguments forwarded from tf.get_variable , return a distribution object for a scalar-valued fixed gaussian prior which will be broadcast over a variable of the requisite shape. Args: getter : The getter passed to a custom_getter . Please see the documentation for tf.get_variable . name : The name argument passed to tf.get_variable . dtype : The dtype argument passed to tf.get_variable . *args : See positional arguments passed to tf.get_variable . **kwargs : See keyword arguments passed to tf.get_variable . Returns: An instance of tfp.distributions.Normal representing the prior distribution over the variable in question. custom_getters.bayes_by_backprop.get_total_kl_cost(name='total_kl_cost', filter_by_name_substring=None) Get the total cost for all (or a subset of) the stochastic variables. Args: name : A name for the tensor representing the total kl cost. filter_by_name_substring : A string used to filter which variables count toward the total KL cost. By default, this argument is None , and all variables trained using Bayes by Backprop are included. If this argument is provided, the variables whose KL costs are summed will be all those whose name contains filter_by_name_substring . An example use of this would be to select all variables within a particular scope. Returns: A tensor representing the total KL cost in the ELBO loss. custom_getters.bayes_by_backprop.get_variable_metadata(scope_name_substring=None) custom_getters.bayes_by_backprop.inverse_softplus(y) The inverse of the softplus function. Computes the inverse of softplus, a function which maps an unconstrained real number to the positive reals, e.g. to squash an unconstrained neural network activation to parameterize a variance. Args: y : A positive number. Returns: The number x such that softplus(x) = y. custom_getters.bayes_by_backprop.scale_variable_initializer(desired_scale) custom_getters.bayes_by_backprop.stochastic_kl_builder(posterior, prior, sample) A pre-canned builder for a ubiquitous stochastic KL estimator. nest.assert_same_structure(*args, **kwargs) nest.assert_shallow_structure(*args, **kwargs) nest.flatten(*args, **kwargs) nest.flatten_dict_items(*args, **kwargs) nest.flatten_iterable(*args, **kwargs) nest.flatten_up_to(*args, **kwargs) nest.is_iterable(*args, **kwargs) nest.is_sequence(*args, **kwargs) nest.map(*args, **kwargs) nest.map_up_to(*args, **kwargs) nest.pack_iterable_as(*args, **kwargs) nest.pack_sequence_as(*args, **kwargs) nest.with_deprecation_warning(fn, extra_message='') Wraps the function and prints a warn-once (per extra_message ) warning. class nets.AlexNet Implementation of AlexNet with full and mini versions. Based on: 'ImageNet Classification with Deep Convolutional Neural Networks' Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, NIPS 2012 http://papers.nips.cc/paper/4824-imagenet-classification-w nets.AlexNet.__init__(mode, use_batch_norm=False, batch_norm_config=None, initializers=None, partitioners=None, regularizers=None, bn_on_fc_layers=True, custom_getter=None, name='alex_net') Constructs AlexNet. Args: mode : Construction mode of network: AlexNet.FULL or AlexNet.MINI . use_batch_norm : Whether to use batch normalization between the output of a layer and the activation function. batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializers are truncated normal initializers, which are commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). partitioners : Optional dict containing partitioners for the filters (with key 'w') and the biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . bn_on_fc_layers : If use_batch_norm is True, add batch normalization to the fully-connected layers. This is deprecated. custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: base.Error: If the given mode is not one of AlexNet.FULL , or AlexNet.MINI . KeyError : If initializers , partitioners or regularizers contains any keys other than 'w' or 'b'. nets.AlexNet.__call__(inputs, keep_prob=None, is_training=None, test_local_stats=True) Connects the AlexNet module into the graph. The is_training flag only controls the batch norm settings, if False it does not force no dropout by overriding any input keep_prob . To avoid any confusion this may cause, if is_training=False and keep_prob would cause dropout to be applied, an error is thrown. Args: inputs : A Tensor of size [batch_size, input_height, input_width, input_channels], representing a batch of input images. keep_prob : A scalar Tensor representing the dropout keep probability. When is_training=False this must be None or 1 to give no dropout. is_training : Boolean to indicate if we are currently training. Must be specified if batch normalization or dropout is used. test_local_stats : Boolean to indicate to snt.BatchNorm if batch normalization should use local batch statistics at test time. By default True . Returns: A Tensor of size [batch_size, output_size], where output_size depends on the mode the network was constructed in. Raises: base.IncompatibleShapeError: If any of the input image dimensions (input_height, input_width) are too small for the given network mode. ValueError : If keep_prob is not None or 1 when is_training=False . ValueError : If is_training is not explicitly specified when using batch normalization. nets.AlexNet.connected_subgraphs Returns the subgraphs created by this module so far. nets.AlexNet.conv_modules Returns list containing convolutional modules of network. Returns: A list containing the Conv2D modules. nets.AlexNet.defun() Wraps this modules call method in a callable graph function. nets.AlexNet.defun_wrapped Returns boolean indicating whether this module is defun wrapped. nets.AlexNet.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNet.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. nets.AlexNet.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNet.graph Returns the Graph instance which the module is connected to, or None. nets.AlexNet.initializers nets.AlexNet.is_connected Returns true iff the Module been connected to the Graph at least once. nets.AlexNet.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNet.linear_modules Returns list containing linear modules of network. Returns: A list containing the Linear modules. nets.AlexNet.min_input_size Returns integer specifying the minimum width and height for the input. Note that the input can be non-square, but both the width and height must be = this number in size. Returns: The minimum size as an integer. nets.AlexNet.module_name Returns the name of the Module. nets.AlexNet.name_scopes Returns a tuple of all name_scopes generated by this module. nets.AlexNet.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNet.partitioners nets.AlexNet.regularizers nets.AlexNet.scope_name Returns the full name of the Module's variable scope. nets.AlexNet.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNet.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNet.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class nets.AlexNetFull AlexNet constructed in the 'FULL' mode. nets.AlexNetFull.__init__(use_batch_norm=False, batch_norm_config=None, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='alex_net_full') Constructs AlexNet. Args: use_batch_norm : Whether to use batch normalization between the output of a layer and the activation function. batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializers are truncated normal initializers, which are commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). partitioners : Optional dict containing partitioners for the filters (with key 'w') and the biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: KeyError : If initializers , partitioners or regularizers contains any keys other than 'w' or 'b'. nets.AlexNetFull.__call__(inputs, keep_prob=None, is_training=None, test_local_stats=True) Connects the AlexNet module into the graph. The is_training flag only controls the batch norm settings, if False it does not force no dropout by overriding any input keep_prob . To avoid any confusion this may cause, if is_training=False and keep_prob would cause dropout to be applied, an error is thrown. Args: inputs : A Tensor of size [batch_size, input_height, input_width, input_channels], representing a batch of input images. keep_prob : A scalar Tensor representing the dropout keep probability. When is_training=False this must be None or 1 to give no dropout. is_training : Boolean to indicate if we are currently training. Must be specified if batch normalization or dropout is used. test_local_stats : Boolean to indicate to snt.BatchNorm if batch normalization should use local batch statistics at test time. By default True . Returns: A Tensor of size [batch_size, output_size], where output_size depends on the mode the network was constructed in. Raises: base.IncompatibleShapeError: If any of the input image dimensions (input_height, input_width) are too small for the given network mode. ValueError : If keep_prob is not None or 1 when is_training=False . ValueError : If is_training is not explicitly specified when using batch normalization. nets.AlexNetFull.connected_subgraphs Returns the subgraphs created by this module so far. nets.AlexNetFull.conv_modules Returns list containing convolutional modules of network. Returns: A list containing the Conv2D modules. nets.AlexNetFull.defun() Wraps this modules call method in a callable graph function. nets.AlexNetFull.defun_wrapped Returns boolean indicating whether this module is defun wrapped. nets.AlexNetFull.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNetFull.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. nets.AlexNetFull.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNetFull.graph Returns the Graph instance which the module is connected to, or None. nets.AlexNetFull.initializers nets.AlexNetFull.is_connected Returns true iff the Module been connected to the Graph at least once. nets.AlexNetFull.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNetFull.linear_modules Returns list containing linear modules of network. Returns: A list containing the Linear modules. nets.AlexNetFull.min_input_size Returns integer specifying the minimum width and height for the input. Note that the input can be non-square, but both the width and height must be = this number in size. Returns: The minimum size as an integer. nets.AlexNetFull.module_name Returns the name of the Module. nets.AlexNetFull.name_scopes Returns a tuple of all name_scopes generated by this module. nets.AlexNetFull.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNetFull.partitioners nets.AlexNetFull.regularizers nets.AlexNetFull.scope_name Returns the full name of the Module's variable scope. nets.AlexNetFull.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNetFull.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNetFull.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class nets.AlexNetMini AlexNet constructed in the 'MINI' mode. nets.AlexNetMini.__init__(use_batch_norm=False, batch_norm_config=None, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='alex_net_mini') Constructs AlexNet. Args: use_batch_norm : Whether to use batch normalization between the output of a layer and the activation function. batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializers are truncated normal initializers, which are commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). partitioners : Optional dict containing partitioners for the filters (with key 'w') and the biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: KeyError : If initializers , partitioners or regularizers contains any keys other than 'w' or 'b'. nets.AlexNetMini.__call__(inputs, keep_prob=None, is_training=None, test_local_stats=True) Connects the AlexNet module into the graph. The is_training flag only controls the batch norm settings, if False it does not force no dropout by overriding any input keep_prob . To avoid any confusion this may cause, if is_training=False and keep_prob would cause dropout to be applied, an error is thrown. Args: inputs : A Tensor of size [batch_size, input_height, input_width, input_channels], representing a batch of input images. keep_prob : A scalar Tensor representing the dropout keep probability. When is_training=False this must be None or 1 to give no dropout. is_training : Boolean to indicate if we are currently training. Must be specified if batch normalization or dropout is used. test_local_stats : Boolean to indicate to snt.BatchNorm if batch normalization should use local batch statistics at test time. By default True . Returns: A Tensor of size [batch_size, output_size], where output_size depends on the mode the network was constructed in. Raises: base.IncompatibleShapeError: If any of the input image dimensions (input_height, input_width) are too small for the given network mode. ValueError : If keep_prob is not None or 1 when is_training=False . ValueError : If is_training is not explicitly specified when using batch normalization. nets.AlexNetMini.connected_subgraphs Returns the subgraphs created by this module so far. nets.AlexNetMini.conv_modules Returns list containing convolutional modules of network. Returns: A list containing the Conv2D modules. nets.AlexNetMini.defun() Wraps this modules call method in a callable graph function. nets.AlexNetMini.defun_wrapped Returns boolean indicating whether this module is defun wrapped. nets.AlexNetMini.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNetMini.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. nets.AlexNetMini.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNetMini.graph Returns the Graph instance which the module is connected to, or None. nets.AlexNetMini.initializers nets.AlexNetMini.is_connected Returns true iff the Module been connected to the Graph at least once. nets.AlexNetMini.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNetMini.linear_modules Returns list containing linear modules of network. Returns: A list containing the Linear modules. nets.AlexNetMini.min_input_size Returns integer specifying the minimum width and height for the input. Note that the input can be non-square, but both the width and height must be = this number in size. Returns: The minimum size as an integer. nets.AlexNetMini.module_name Returns the name of the Module. nets.AlexNetMini.name_scopes Returns a tuple of all name_scopes generated by this module. nets.AlexNetMini.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNetMini.partitioners nets.AlexNetMini.regularizers nets.AlexNetMini.scope_name Returns the full name of the Module's variable scope. nets.AlexNetMini.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNetMini.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. nets.AlexNetMini.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class nets.ConvNet2D A 2D Convolutional Network module. nets.ConvNet2D.__init__(output_channels, kernel_shapes, strides, paddings, rates=(1,), activation= function relu at 0x7fcc58553c80 , activate_final=False, normalization_ctor=None, normalization_kwargs=None, normalize_final=None, initializers=None, partitioners=None, regularizers=None, use_batch_norm=None, use_bias=True, batch_norm_config=None, data_format='NHWC', custom_getter=None, name='conv_net_2d') Constructs a ConvNet2D module. By default, neither batch normalization nor activation are applied to the output of the final layer. Args: output_channels : Iterable of output channels, as defined in conv.Conv2D . Output channels can be defined either as number or via a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that entries can be called when build is called. Each entry in the iterable defines properties in the corresponding convolutional layer. kernel_shapes : Iterable of kernel sizes as defined in conv.Conv2D ; if the list contains one element only, the same kernel shape is used in each layer of the network. strides : Iterable of kernel strides as defined in conv.Conv2D ; if the list contains one element only, the same stride is used in each layer of the network. paddings : Iterable of padding options as defined in conv.Conv2D . Each can be snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a pair of these to use for height and width. If the Iterable contains one element only, the same padding is used in each layer of the network. rates : Iterable of dilation rates as defined in conv.Conv2D ; if the list contains one element only, the same rate is used in each layer of the network. activation : An activation op. activate_final : Boolean determining if the activation and batch normalization, if turned on, are applied to the final layer. normalization_ctor : Constructor to return a callable which will perform normalization at each layer. Defaults to None / no normalization. Examples of what could go here: snt.BatchNormV2 , snt.LayerNorm . If a string is provided, importlib is used to convert the string to a callable, so either snt.LayerNorm or \"snt.LayerNorm\" can be provided. normalization_kwargs : kwargs to be provided to normalization_ctor when it is called. normalize_final : Whether to apply normalization after the final conv layer. Default is to take the value of activate_final. initializers : Optional dict containing ops to initialize the filters of the whole network (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters of the whole network (with key 'w') or biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . use_batch_norm : Boolean determining if batch normalization is applied after convolution. Deprecated, use normalization_ctor instead. use_bias : Boolean or iterable of booleans determining whether to include bias parameters in the convolutional layers. Default True . batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. Deprecated, use normalization_kwargs instead. data_format : A string, one of \"NCHW\" or \"NHWC\". Specifies whether the channel dimension of the input and output is the last dimension (default, \"NHWC\"), or the second dimension (\"NCHW\"). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. Note that this custom_getter will not be passed to the transpose method. If you want to use a custom getter with the transposed of this convolutional network, you should provide one to the transpose method instead. name : Name of the module. Raises: TypeError : If output_channels is not iterable; or if kernel_shapes is not iterable; or strides is not iterable; or paddings is not iterable; or if activation is not callable. ValueError : If output_channels is empty; or if kernel_shapes has not length 1 or len(output_channels) ; or if strides has not length 1 or len(output_channels) ; or if paddings has not length 1 or len(output_channels) ; or if rates has not length 1 or len(output_channels) ; or if the given data_format is not a supported format (\"NHWC\" or \"NCHW\"); or if normalization_ctor is provided but cannot be mapped to a callable. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. nets.ConvNet2D.__call__(inputs, **normalization_build_kwargs) Assembles the ConvNet2D and connects it to the graph. Args: inputs : A 4D Tensor of shape [batch_size, input_height, input_width, input_channels] . **normalization_build_kwargs : kwargs passed to the normalization module at _build time. Returns: A 4D Tensor of shape [batch_size, output_height, output_width, output_channels[-1]] . Raises: ValueError : If is_training is not explicitly specified when using batch normalization. nets.ConvNet2D.activate_final nets.ConvNet2D.activation nets.ConvNet2D.batch_norm_config nets.ConvNet2D.connected_subgraphs Returns the subgraphs created by this module so far. nets.ConvNet2D.defun() Wraps this modules call method in a callable graph function. nets.ConvNet2D.defun_wrapped Returns boolean indicating whether this module is defun wrapped. nets.ConvNet2D.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.ConvNet2D.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. nets.ConvNet2D.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.ConvNet2D.graph Returns the Graph instance which the module is connected to, or None. nets.ConvNet2D.initializers nets.ConvNet2D.input_shape Returns shape of input Tensor passed at last call to build . nets.ConvNet2D.is_connected Returns true iff the Module been connected to the Graph at least once. nets.ConvNet2D.kernel_shapes nets.ConvNet2D.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. nets.ConvNet2D.layers Returns a tuple containing the convolutional layers of the network. nets.ConvNet2D.module_name Returns the name of the Module. nets.ConvNet2D.name_scopes Returns a tuple of all name_scopes generated by this module. nets.ConvNet2D.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.ConvNet2D.normalization_ctor nets.ConvNet2D.normalization_kwargs nets.ConvNet2D.normalize_final nets.ConvNet2D.output_channels nets.ConvNet2D.paddings nets.ConvNet2D.partitioners nets.ConvNet2D.rates nets.ConvNet2D.regularizers nets.ConvNet2D.scope_name Returns the full name of the Module's variable scope. nets.ConvNet2D.strides nets.ConvNet2D.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.ConvNet2D.transpose(name=None, output_channels=None, kernel_shapes=None, strides=None, paddings=None, activation=None, activate_final=None, normalization_ctor=None, normalization_kwargs=None, normalize_final=None, initializers=None, partitioners=None, regularizers=None, use_batch_norm=None, use_bias=None, batch_norm_config=None, data_format=None, custom_getter=None) Returns transposed version of this network. Args: name : Optional string specifying the name of the transposed module. The default name is constructed by appending \"_transpose\" to self.module_name . output_channels : Optional iterable of numbers of output channels. kernel_shapes : Optional iterable of kernel sizes. The default value is constructed by reversing self.kernel_shapes . strides : Optional iterable of kernel strides. The default value is constructed by reversing self.strides . paddings : Optional iterable of padding options, either snt.SAME or snt.VALID ; The default value is constructed by reversing self.paddings . activation : Optional activation op. Default value is self.activation . activate_final : Optional boolean determining if the activation and batch normalization, if turned on, are applied to the final layer. normalization_ctor : Constructor to return a callable which will perform normalization at each layer. Defaults to None / no normalization. Examples of what could go here: snt.BatchNormV2 , snt.LayerNorm . If a string is provided, importlib is used to convert the string to a callable, so either snt.LayerNorm or \"snt.LayerNorm\" can be provided. normalization_kwargs : kwargs to be provided to normalization_ctor when it is called. normalize_final : Whether to apply normalization after the final conv layer. Default is to take the value of activate_final. initializers : Optional dict containing ops to initialize the filters of the whole network (with key 'w') or biases (with key 'b'). The default value is self.initializers . partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). The default value is self.partitioners . regularizers : Optional dict containing regularizers for the filters of the whole network (with key 'w') or biases (with key 'b'). The default is self.regularizers . use_batch_norm : Optional boolean determining if batch normalization is applied after convolution. The default value is self.use_batch_norm . use_bias : Optional boolean or iterable of booleans determining whether to include bias parameters in the convolutional layers. Default is constructed by reversing self.use_bias . batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. Default is self.batch_norm_config . data_format : Optional string, one of \"NCHW\" or \"NHWC\". Specifies whether the channel dimension of the input and output is the last dimension. Default is self._data_format . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. Returns: Matching ConvNet2DTranspose module. Raises: ValueError : If output_channels is specified and its length does not match the number of layers. ValueError : If the given data_format is not a supported format (\"NHWC\" or \"NCHW\"). NotImplementedError : If the convolutions are dilated. nets.ConvNet2D.use_batch_norm nets.ConvNet2D.use_bias nets.ConvNet2D.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. nets.ConvNet2D.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class nets.ConvNet2DTranspose A 2D Transpose-Convolutional Network module. nets.ConvNet2DTranspose.__init__(output_channels, output_shapes, kernel_shapes, strides, paddings, activation= function relu at 0x7fcc58553c80 , activate_final=False, normalization_ctor=None, normalization_kwargs=None, normalize_final=None, initializers=None, partitioners=None, regularizers=None, use_batch_norm=False, use_bias=True, batch_norm_config=None, data_format='NHWC', custom_getter=None, name='conv_net_2d_transpose') Constructs a ConvNetTranspose2D module. output_{shapes,channels} can be defined either as iterable of {iterables,integers} or via a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that entries can be called returning meaningful values when build is called. Each entry in the iterable defines properties in the corresponding convolutional layer. By default, neither batch normalization nor activation are applied to the output of the final layer. Args: output_channels : Iterable of numbers of output channels. output_shapes : Iterable of output shapes as defined in conv.conv2DTranpose ; if the iterable contains one element only, the same shape is used in each layer of the network. kernel_shapes : Iterable of kernel sizes as defined in conv.Conv2D ; if the list contains one element only, the same kernel shape is used in each layer of the network. strides : Iterable of kernel strides as defined in conv.Conv2D ; if the list contains one element only, the same stride is used in each layer of the network. paddings : Iterable of padding options, either snt.SAME or snt.VALID ; if the Iterable contains one element only, the same padding is used in each layer of the network. activation : An activation op. activate_final : Boolean determining if the activation and batch normalization, if turned on, are applied to the final layer. normalization_ctor : Constructor to return a callable which will perform normalization at each layer. Defaults to None / no normalization. Examples of what could go here: snt.BatchNormV2 , snt.LayerNorm . If a string is provided, importlib is used to convert the string to a callable, so either snt.LayerNorm or \"snt.LayerNorm\" can be provided. normalization_kwargs : kwargs to be provided to normalization_ctor when it is called. normalize_final : Whether to apply normalization after the final conv layer. Default is to take the value of activate_final. initializers : Optional dict containing ops to initialize the filters of the whole network (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters of the whole network (with key 'w') or biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . use_batch_norm : Boolean determining if batch normalization is applied after convolution. use_bias : Boolean or iterable of booleans determining whether to include bias parameters in the convolutional layers. Default True . batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. data_format : A string, one of \"NCHW\" or \"NHWC\". Specifies whether the channel dimension of the input and output is the last dimension (default, \"NHWC\"), or the second dimension (\"NCHW\"). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: TypeError : If output_channels is not iterable; or if output_shapes is not iterable; or if kernel_shapes is not iterable; or if strides is not iterable; or if paddings is not iterable; or if activation is not callable. ValueError : If output_channels is empty; or if kernel_shapes has not length 1 or len(output_channels) ; or if strides has not length 1 or len(output_channels) ; or if paddings has not length 1 or len(output_channels) . ValueError : If the given data_format is not a supported format (\"NHWC\" or \"NCHW\"). ValueError : If normalization_ctor is provided but cannot be converted to a callable. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. nets.ConvNet2DTranspose.__call__(inputs, **normalization_build_kwargs) Assembles the ConvNet2D and connects it to the graph. Args: inputs : A 4D Tensor of shape [batch_size, input_height, input_width, input_channels] . **normalization_build_kwargs : kwargs passed to the normalization module at _build time. Returns: A 4D Tensor of shape [batch_size, output_height, output_width, output_channels[-1]] . Raises: ValueError : If is_training is not explicitly specified when using batch normalization. nets.ConvNet2DTranspose.activate_final nets.ConvNet2DTranspose.activation nets.ConvNet2DTranspose.batch_norm_config nets.ConvNet2DTranspose.connected_subgraphs Returns the subgraphs created by this module so far. nets.ConvNet2DTranspose.defun() Wraps this modules call method in a callable graph function. nets.ConvNet2DTranspose.defun_wrapped Returns boolean indicating whether this module is defun wrapped. nets.ConvNet2DTranspose.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.ConvNet2DTranspose.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. nets.ConvNet2DTranspose.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.ConvNet2DTranspose.graph Returns the Graph instance which the module is connected to, or None. nets.ConvNet2DTranspose.initializers nets.ConvNet2DTranspose.input_shape Returns shape of input Tensor passed at last call to build . nets.ConvNet2DTranspose.is_connected Returns true iff the Module been connected to the Graph at least once. nets.ConvNet2DTranspose.kernel_shapes nets.ConvNet2DTranspose.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. nets.ConvNet2DTranspose.layers Returns a tuple containing the convolutional layers of the network. nets.ConvNet2DTranspose.module_name Returns the name of the Module. nets.ConvNet2DTranspose.name_scopes Returns a tuple of all name_scopes generated by this module. nets.ConvNet2DTranspose.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.ConvNet2DTranspose.normalization_ctor nets.ConvNet2DTranspose.normalization_kwargs nets.ConvNet2DTranspose.normalize_final nets.ConvNet2DTranspose.output_channels nets.ConvNet2DTranspose.output_shapes nets.ConvNet2DTranspose.paddings nets.ConvNet2DTranspose.partitioners nets.ConvNet2DTranspose.rates nets.ConvNet2DTranspose.regularizers nets.ConvNet2DTranspose.scope_name Returns the full name of the Module's variable scope. nets.ConvNet2DTranspose.strides nets.ConvNet2DTranspose.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.ConvNet2DTranspose.transpose(name=None, output_channels=None, kernel_shapes=None, strides=None, paddings=None, activation=None, activate_final=None, normalization_ctor=None, normalization_kwargs=None, normalize_final=None, initializers=None, partitioners=None, regularizers=None, use_batch_norm=None, use_bias=None, batch_norm_config=None, data_format=None, custom_getter=None) Returns transposed version of this network. Args: name : Optional string specifying the name of the transposed module. The default name is constructed by appending \"_transpose\" to self.module_name . output_channels : Optional iterable of numbers of output channels. kernel_shapes : Optional iterable of kernel sizes. The default value is constructed by reversing self.kernel_shapes . strides : Optional iterable of kernel strides. The default value is constructed by reversing self.strides . paddings : Optional iterable of padding options, either snt.SAME or snt.VALID ; The default value is constructed by reversing self.paddings . activation : Optional activation op. Default value is self.activation . activate_final : Optional boolean determining if the activation and batch normalization, if turned on, are applied to the final layer. normalization_ctor : Constructor to return a callable which will perform normalization at each layer. Defaults to None / no normalization. Examples of what could go here: snt.BatchNormV2 , snt.LayerNorm . If a string is provided, importlib is used to convert the string to a callable, so either snt.LayerNorm or \"snt.LayerNorm\" can be provided. normalization_kwargs : kwargs to be provided to normalization_ctor when it is called. normalize_final : Whether to apply normalization after the final conv layer. Default is to take the value of activate_final. initializers : Optional dict containing ops to initialize the filters of the whole network (with key 'w') or biases (with key 'b'). The default value is self.initializers . partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). The default value is self.partitioners . regularizers : Optional dict containing regularizers for the filters of the whole network (with key 'w') or biases (with key 'b'). The default is self.regularizers . use_batch_norm : Optional boolean determining if batch normalization is applied after convolution. The default value is self.use_batch_norm . use_bias : Optional boolean or iterable of booleans determining whether to include bias parameters in the convolutional layers. Default is constructed by reversing self.use_bias . batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. Default is self.batch_norm_config . data_format : Optional string, one of \"NCHW\" or \"NHWC\". Specifies whether the channel dimension of the input and output is the last dimension. Default is self._data_format . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. Returns: Matching ConvNet2D module. Raises: ValueError : If output_channels is specified and its length does not match the number of layers. nets.ConvNet2DTranspose.use_batch_norm nets.ConvNet2DTranspose.use_bias nets.ConvNet2DTranspose.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. nets.ConvNet2DTranspose.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class nets.Dilation A convolutional module for per-pixel classification. Consists of 8 convolutional layers, 4 of which are dilated. When applied to the output of a model like VGG-16 (before fully connected layers), can be used to make predictions on a per-pixel basis. Note that the default initializers for the 'basic' model size require that the number of input channels be equal to the number of output classes, and the initializers for the 'large' model require it be a multiple. Based on: 'Multi-Scale Context Aggregation by Dilated Convolutions' Fisher Yu, Vladlen Koltun, ICLR 2016 https://arxiv.org/abs/1511.07122 Properties: conv_modules: list of sonnet modules. The 8 convolution layers used in the Dilation module. nets.Dilation.__init__(num_output_classes, initializers=None, regularizers=None, model_size='basic', name='dilation') Creates a dilation module. Args: num_output_classes : Int. Number of output classes to predict for each pixel in an image. initializers : Optional dict containing ops to initialize filters (with key 'w') or biases (with key 'b'). The default initializer makes this module equivalent to the identity. regularizers : Optional dict containing regularizers for the weights (with key 'w') or biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . model_size : string. One of 'basic' or 'large'. name : string. Name of module. nets.Dilation.__call__(images) Build dilation module. Args: images : Tensor of shape [batch_size, height, width, depth] and dtype float32. Represents a set of images with an arbitrary depth. Note that when using the default initializer, depth must equal num_output_classes. Returns: Tensor of shape [batch_size, height, width, num_output_classes] and dtype float32. Represents, for each image and pixel, logits for per-class predictions. Raises: IncompatibleShapeError : If images is not rank 4. ValueError : If model_size is not one of 'basic' or 'large'. nets.Dilation.connected_subgraphs Returns the subgraphs created by this module so far. nets.Dilation.conv_modules nets.Dilation.defun() Wraps this modules call method in a callable graph function. nets.Dilation.defun_wrapped Returns boolean indicating whether this module is defun wrapped. nets.Dilation.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.Dilation.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. nets.Dilation.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.Dilation.graph Returns the Graph instance which the module is connected to, or None. nets.Dilation.is_connected Returns true iff the Module been connected to the Graph at least once. nets.Dilation.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. nets.Dilation.module_name Returns the name of the Module. nets.Dilation.name_scopes Returns a tuple of all name_scopes generated by this module. nets.Dilation.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.Dilation.scope_name Returns the full name of the Module's variable scope. nets.Dilation.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.Dilation.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. nets.Dilation.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class nets.MLP A Multi-Layer perceptron module. nets.MLP.__init__(output_sizes, activation= function relu at 0x7fcc58553c80 , activate_final=False, initializers=None, partitioners=None, regularizers=None, use_bias=True, use_dropout=False, custom_getter=None, name='mlp') Constructs an MLP module. Args: output_sizes : An iterable of output dimensionalities as defined in basic.Linear . Output size can be defined either as number or via a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that entries can be called when build is called. Each entry in the iterable defines properties in the corresponding linear layer. activation : An activation op. The activation is applied to intermediate layers, and optionally to the output of the final layer. activate_final : Boolean determining if the activation is applied to the output of the final layer. Default False . initializers : Optional dict containing ops to initialize the linear layers' weights (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition the linear layers' weights (with key 'w') or biases (with key 'b'). regularizers : Optional dict containing regularizers for the linear layers' weights (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . use_bias : Whether to include bias parameters in the linear layers. Default True . use_dropout : Whether to perform dropout on the linear layers. Default False . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module. Raises: KeyError : If initializers contains any keys other than 'w' or 'b'. KeyError : If regularizers contains any keys other than 'w' or 'b'. ValueError : If output_sizes is empty. TypeError : If activation is not callable; or if output_sizes is not iterable. nets.MLP.__call__(inputs, is_training=True, dropout_keep_prob=0.5) Assembles the MLP and connects it to the graph. Args: inputs : A 2D Tensor of size [batch_size, input_size] . is_training : A bool or tf.Bool Tensor. Indicates whether we are currently training. Defaults to True . dropout_keep_prob : The probability that each element is kept when both use_dropout and is_training are True. Defaults to 0.5. Returns: A 2D Tensor of size [batch_size, output_sizes[-1]] . nets.MLP.activate_final nets.MLP.activation nets.MLP.clone(name=None) Creates a new MLP with the same structure. Args: name : Optional string specifying the name of the new module. The default name is constructed by appending \"_clone\" to the original name. Returns: A cloned MLP module. nets.MLP.connected_subgraphs Returns the subgraphs created by this module so far. nets.MLP.defun() Wraps this modules call method in a callable graph function. nets.MLP.defun_wrapped Returns boolean indicating whether this module is defun wrapped. nets.MLP.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.MLP.get_possible_initializer_keys(use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. nets.MLP.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.MLP.graph Returns the Graph instance which the module is connected to, or None. nets.MLP.initializers Returns the intializers dictionary. nets.MLP.input_shape Returns shape of input Tensor passed at last call to build . nets.MLP.is_connected Returns true iff the Module been connected to the Graph at least once. nets.MLP.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. nets.MLP.layers Returns a tuple containing the linear layers of the MLP . nets.MLP.module_name Returns the name of the Module. nets.MLP.name_scopes Returns a tuple of all name_scopes generated by this module. nets.MLP.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.MLP.output_size Returns the size of the module output, not including the batch dimension. This allows the MLP to be used inside a DeepRNN. Returns: The scalar size of the module output. nets.MLP.output_sizes Returns a tuple of all output sizes of all the layers. nets.MLP.partitioners Returns the partitioners dictionary. nets.MLP.regularizers Returns the regularizers dictionary. nets.MLP.scope_name Returns the full name of the Module's variable scope. nets.MLP.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.MLP.transpose(name=None, activate_final=None) Returns transposed MLP . Args: name : Optional string specifying the name of the transposed module. The default name is constructed by appending \"_transpose\" to self.module_name . activate_final : Optional boolean determining if the activation and batch normalization, if turned on, are applied to the final layer. Returns: Matching transposed MLP module. nets.MLP.use_bias nets.MLP.use_dropout nets.MLP.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. nets.MLP.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class nets.VectorQuantizer Sonnet module representing the VQ-VAE layer. Implements the algorithm presented in 'Neural Discrete Representation Learning' by van den Oord et al. https://arxiv.org/abs/1711.00937 Input any tensor to be quantized. Last dimension will be used as space in which to quantize. All other dimensions will be flattened and will be seen as different examples to quantize. The output tensor will have the same shape as the input. For example a tensor with shape [16, 32, 32, 64] will be reshaped into [16384, 64] and all 16384 vectors (each of 64 dimensions) will be quantized independently. Args: embedding_dim: integer representing the dimensionality of the tensors in the quantized space. Inputs to the modules must be in this format as well. num_embeddings: integer, the number of vectors in the quantized space. commitment_cost: scalar which controls the weighting of the loss terms (see equation 4 in the paper - this variable is Beta). nets.VectorQuantizer.__init__(embedding_dim, num_embeddings, commitment_cost, name='vq_layer') Performs the initialisation necessary for all AbstractModule instances. Every subclass of AbstractModule must begin their constructor with a call to this constructor, i.e. super(MySubModule, self).__init__(custom_getter=custom_getter, name=name) . If you instantiate sub-modules in init you must create them within the _enter_variable_scope context manager to ensure they are in the module's variable scope. Alternatively, instantiate sub-modules in _build . Args: _sentinel: Variable that only carries a non-None value if __init__ was called without named parameters. If this is the case, a deprecation warning is issued in form of a ValueError . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of this module. Used to construct the Templated build function. If None the module's class name is used (converted to snake case). Raises: TypeError : If name is not a string. TypeError : If a given custom_getter is not callable. ValueError : If __init__ was called without named arguments. nets.VectorQuantizer.__call__(inputs, is_training) Connects the module to some inputs. Args: inputs : Tensor, final dimension must be equal to embedding_dim. All other leading dimensions will be flattened and treated as a large batch. is_training : boolean, whether this connection is to training data. Returns: dict containing the following keys and values: quantize : Tensor containing the quantized version of the input. loss : Tensor containing the loss to optimize. perplexity : Tensor containing the perplexity of the encodings. encodings : Tensor containing the discrete encodings, ie which element of the quantized space each input element was mapped to. encoding_indices : Tensor containing the discrete encoding indices, ie which element of the quantized space each input element was mapped to. nets.VectorQuantizer.connected_subgraphs Returns the subgraphs created by this module so far. nets.VectorQuantizer.defun() Wraps this modules call method in a callable graph function. nets.VectorQuantizer.defun_wrapped Returns boolean indicating whether this module is defun wrapped. nets.VectorQuantizer.embeddings nets.VectorQuantizer.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.VectorQuantizer.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. nets.VectorQuantizer.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.VectorQuantizer.graph Returns the Graph instance which the module is connected to, or None. nets.VectorQuantizer.is_connected Returns true iff the Module been connected to the Graph at least once. nets.VectorQuantizer.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. nets.VectorQuantizer.module_name Returns the name of the Module. nets.VectorQuantizer.name_scopes Returns a tuple of all name_scopes generated by this module. nets.VectorQuantizer.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.VectorQuantizer.quantize(encoding_indices) nets.VectorQuantizer.scope_name Returns the full name of the Module's variable scope. nets.VectorQuantizer.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.VectorQuantizer.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. nets.VectorQuantizer.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. class nets.VectorQuantizerEMA Sonnet module representing the VQ-VAE layer. Implements a slightly modified version of the algorithm presented in 'Neural Discrete Representation Learning' by van den Oord et al. https://arxiv.org/abs/1711.00937 The difference between VectorQuantizerEMA and VectorQuantizer is that this module uses exponential moving averages to update the embedding vectors instead of an auxiliary loss. This has the advantage that the embedding updates are independent of the choice of optimizer (SGD, RMSProp, Adam, K-Fac, ...) used for the encoder, decoder and other parts of the architecture. For most experiments the EMA version trains faster than the non-EMA version. Input any tensor to be quantized. Last dimension will be used as space in which to quantize. All other dimensions will be flattened and will be seen as different examples to quantize. The output tensor will have the same shape as the input. For example a tensor with shape [16, 32, 32, 64] will be reshaped into [16384, 64] and all 16384 vectors (each of 64 dimensions) will be quantized independently. Args: embedding_dim: integer representing the dimensionality of the tensors in the quantized space. Inputs to the modules must be in this format as well. num_embeddings: integer, the number of vectors in the quantized space. commitment_cost: scalar which controls the weighting of the loss terms (see equation 4 in the paper). decay: float, decay for the moving averages. epsilon: small float constant to avoid numerical instability. nets.VectorQuantizerEMA.__init__(embedding_dim, num_embeddings, commitment_cost, decay, epsilon=1e-05, name='VectorQuantizerEMA') Performs the initialisation necessary for all AbstractModule instances. Every subclass of AbstractModule must begin their constructor with a call to this constructor, i.e. super(MySubModule, self).__init__(custom_getter=custom_getter, name=name) . If you instantiate sub-modules in init you must create them within the _enter_variable_scope context manager to ensure they are in the module's variable scope. Alternatively, instantiate sub-modules in _build . Args: _sentinel: Variable that only carries a non-None value if __init__ was called without named parameters. If this is the case, a deprecation warning is issued in form of a ValueError . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of this module. Used to construct the Templated build function. If None the module's class name is used (converted to snake case). Raises: TypeError : If name is not a string. TypeError : If a given custom_getter is not callable. ValueError : If __init__ was called without named arguments. nets.VectorQuantizerEMA.__call__(inputs, is_training) Connects the module to some inputs. Args: inputs : Tensor, final dimension must be equal to embedding_dim. All other leading dimensions will be flattened and treated as a large batch. is_training : boolean, whether this connection is to training data. When this is set to False, the internal moving average statistics will not be updated. Returns: dict containing the following keys and values: quantize : Tensor containing the quantized version of the input. loss : Tensor containing the loss to optimize. perplexity : Tensor containing the perplexity of the encodings. encodings : Tensor containing the discrete encodings, ie which element of the quantized space each input element was mapped to. encoding_indices : Tensor containing the discrete encoding indices, ie which element of the quantized space each input element was mapped to. nets.VectorQuantizerEMA.connected_subgraphs Returns the subgraphs created by this module so far. nets.VectorQuantizerEMA.defun() Wraps this modules call method in a callable graph function. nets.VectorQuantizerEMA.defun_wrapped Returns boolean indicating whether this module is defun wrapped. nets.VectorQuantizerEMA.embeddings nets.VectorQuantizerEMA.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.VectorQuantizerEMA.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. nets.VectorQuantizerEMA.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.VectorQuantizerEMA.graph Returns the Graph instance which the module is connected to, or None. nets.VectorQuantizerEMA.is_connected Returns true iff the Module been connected to the Graph at least once. nets.VectorQuantizerEMA.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. nets.VectorQuantizerEMA.module_name Returns the name of the Module. nets.VectorQuantizerEMA.name_scopes Returns a tuple of all name_scopes generated by this module. nets.VectorQuantizerEMA.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.VectorQuantizerEMA.quantize(encoding_indices) nets.VectorQuantizerEMA.scope_name Returns the full name of the Module's variable scope. nets.VectorQuantizerEMA.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.VectorQuantizerEMA.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. nets.VectorQuantizerEMA.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. nets.identity_kernel_initializer(shape, dtype=tf.float32, partition_info=None) An initializer for constructing identity convolution kernels. Constructs a convolution kernel such that applying it is the same as an identity operation on the input. Formally, the kernel has entry [i, j, in, out] = 1 if in equals out and i and j are the middle of the kernel and 0 otherwise. Args: shape : List of integers. Represents shape of result. dtype : data type for values in result. partition_info : Partition information for initializer functions. Ignored. Returns: Tensor of desired shape and dtype such that applying it as a convolution kernel results in the identity operation. Raises: ValueError : If shape does not define a valid kernel. If filter width and height differ. If filter width and height are not odd numbers. If number of input and output channels differ. nets.noisy_identity_kernel_initializer(base_num_channels, stddev=1e-08) Build an initializer for constructing near-identity convolution kernels. Construct a convolution kernel where in_channels and out_channels are multiples of base_num_channels, but need not be equal. This initializer is essentially the same as identity_kernel_initializer, except that magnitude is \"spread out\" across multiple copies of the input. Args: base_num_channels : int. Number that divides both in_channels and out_channels. stddev : float. Standard deviation of truncated normal noise added to off-entries to break ties. Returns: Initializer function for building a noisy identity kernel. class protos.module_pb2.NestedData A ProtocolMessage class protos.module_pb2.SonnetModule A ProtocolMessage protos.module_pb2._b(x) class python.modules.attention.AttentionOutput AttentionOutput(read, weights, weight_logits) python.modules.attention.AttentionOutput.read Alias for field number 0 python.modules.attention.AttentionOutput.weight_logits Alias for field number 2 python.modules.attention.AttentionOutput.weights Alias for field number 1 python.modules.base.get_connection_stack() python.modules.base.get_module_stack() class python.modules.base_info.ConnectedSubGraph ConnectedSubGraph(module, name_scope, inputs, outputs) python.modules.base_info.ConnectedSubGraph.inputs Alias for field number 2 python.modules.base_info.ConnectedSubGraph.module Alias for field number 0 python.modules.base_info.ConnectedSubGraph.name_scope Alias for field number 1 python.modules.base_info.ConnectedSubGraph.outputs Alias for field number 3 class python.modules.base_info.ModuleInfo ModuleInfo(module_name, scope_name, class_name, connected_subgraphs) python.modules.base_info.ModuleInfo.class_name Alias for field number 2 python.modules.base_info.ModuleInfo.connected_subgraphs Alias for field number 3 python.modules.base_info.ModuleInfo.module_name Alias for field number 0 python.modules.base_info.ModuleInfo.scope_name Alias for field number 1 python.modules.basic.calculate_bias_shape(input_shape, bias_dims) Calculate bias_shape based on the input_shape and bias_dims . Args: input_shape : Shape of the input being passed into the module. The leading dimension is the minibatch size. bias_dims : The dimensions that bias should be applied over. The remaining dimensions will get broadcasted over. Returns: bias_shape : Tuple corresponding to the shape of bias Variable to create. Raises: ValueError : If the user attempts to add bias over the minibatch dimension, e.g. bias_dims=[0] . python.modules.basic.create_bias_initializer(unused_bias_shape, dtype=tf.float32) Returns a default initializer for the biases of a linear/AddBias module. python.modules.basic.create_linear_initializer(input_size, dtype=tf.float32) Returns a default initializer for weights of a linear module. python.modules.batch_norm.create_beta_initializer() Returns a default initializer for the beta in batch norm. python.modules.batch_norm.create_gamma_initializer() Returns a default initializer for the gamma in batch norm. python.modules.batch_norm.create_mean_initializer() Returns a default initializer for the moving_mean in batch norm. python.modules.batch_norm.create_variance_initializer() Returns a default initializer for the moving_variance in batch norm. python.modules.batch_norm_v2.create_beta_initializer() Returns a default initializer for the beta in batch norm. python.modules.batch_norm_v2.create_gamma_initializer() Returns a default initializer for the gamma in batch norm. python.modules.batch_norm_v2.create_mean_initializer() Returns a default initializer for the moving_mean in batch norm. python.modules.batch_norm_v2.create_variance_initializer() Returns a default initializer for the moving_variance in batch norm. python.modules.conv.create_bias_initializer(unused_bias_shape, dtype=tf.float32) Returns a default initializer for the biases of a convolutional module. python.modules.conv.create_weight_initializer(fan_in_shape, dtype=tf.float32) Returns a default initializer for the weights of a convolutional module. class python.modules.gated_rnn.ConvLSTM Convolutional LSTM. python.modules.gated_rnn.ConvLSTM.__init__(conv_ndims, input_shape, output_channels, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, legacy_bias_behaviour=True, forget_bias=1.0, initializers=None, partitioners=None, regularizers=None, use_layer_norm=False, custom_getter=None, name='conv_lstm') Construct ConvLSTM. Args: conv_ndims : Convolution dimensionality (1, 2 or 3). input_shape : Shape of the input as an iterable, excluding the batch size. output_channels : Number of output channels of the conv LSTM. kernel_shape : Sequence of kernel sizes (of size conv_ndims), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size conv_ndims), or integer that is used to define stride in all dimensions. rate : Sequence of dilation rates (of size conv_ndims), or integer that is used to define dilation rate in all dimensions. 1 corresponds to a standard convolution, while rate 1 corresponds to a dilated convolution. Cannot be 1 if any of stride is also 1. padding : Padding algorithm, either snt.SAME or snt.VALID . use_bias : Use bias in convolutions. legacy_bias_behaviour : If True, bias is applied to both input and hidden convolutions, creating a redundant bias variable. If False, bias is only applied to input convolution, removing the redundancy. forget_bias : Forget bias. initializers : Dict containing ops to initialize the convolutional weights. partitioners : Optional dict containing partitioners to partition the convolutional weights and biases. As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the convolutional weights and biases. As a default, no regularizers are used. use_layer_norm : Boolean that indicates whether to apply layer normalization. This is applied across the entire layer, normalizing over all non-batch dimensions. custom_getter : Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. See the tf.get_variable documentation for more details. name : Name of the module. Raises: ValueError : If skip_connection is True and stride is different from 1 or if input_shape is incompatible with conv_ndims . python.modules.gated_rnn.ConvLSTM.__call__(inputs, state) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). python.modules.gated_rnn.ConvLSTM.connected_subgraphs Returns the subgraphs created by this module so far. python.modules.gated_rnn.ConvLSTM.convolutions python.modules.gated_rnn.ConvLSTM.defun() Wraps this modules call method in a callable graph function. python.modules.gated_rnn.ConvLSTM.defun_wrapped Returns boolean indicating whether this module is defun wrapped. python.modules.gated_rnn.ConvLSTM.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ConvLSTM.get_possible_initializer_keys(conv_ndims, use_bias=True) Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. python.modules.gated_rnn.ConvLSTM.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ConvLSTM.graph Returns the Graph instance which the module is connected to, or None. python.modules.gated_rnn.ConvLSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs) Builds the default start state for an RNNCore. Args: batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module. Returns: A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core. Raises: ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions. python.modules.gated_rnn.ConvLSTM.is_connected Returns true iff the Module been connected to the Graph at least once. python.modules.gated_rnn.ConvLSTM.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ConvLSTM.module_name Returns the name of the Module. python.modules.gated_rnn.ConvLSTM.name_scopes Returns a tuple of all name_scopes generated by this module. python.modules.gated_rnn.ConvLSTM.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ConvLSTM.output_size tf.TensorShape indicating the size of the core output. python.modules.gated_rnn.ConvLSTM.scope_name Returns the full name of the Module's variable scope. python.modules.gated_rnn.ConvLSTM.state_size Tuple of tf.TensorShape s indicating the size of state tensors. python.modules.gated_rnn.ConvLSTM.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ConvLSTM.use_layer_norm Boolean indicating whether layer norm is enabled. python.modules.gated_rnn.ConvLSTM.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ConvLSTM.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ConvLSTM.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class python.modules.gated_rnn.RecurrentDropoutWrapper Wraps an RNNCore so that recurrent dropout can be applied. python.modules.gated_rnn.RecurrentDropoutWrapper.__init__(core, keep_probs) Builds a new wrapper around a given core. Args: core : the RNN core to be wrapped. keep_probs : the recurrent dropout keep probabilities to apply. This should have the same structure has core.init_state. No dropout is applied for leafs set to None. python.modules.gated_rnn.RecurrentDropoutWrapper.__call__(inputs, prev_state) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). python.modules.gated_rnn.RecurrentDropoutWrapper.connected_subgraphs Returns the subgraphs created by this module so far. python.modules.gated_rnn.RecurrentDropoutWrapper.defun() Wraps this modules call method in a callable graph function. python.modules.gated_rnn.RecurrentDropoutWrapper.defun_wrapped Returns boolean indicating whether this module is defun wrapped. python.modules.gated_rnn.RecurrentDropoutWrapper.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.RecurrentDropoutWrapper.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. python.modules.gated_rnn.RecurrentDropoutWrapper.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.RecurrentDropoutWrapper.graph Returns the Graph instance which the module is connected to, or None. python.modules.gated_rnn.RecurrentDropoutWrapper.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None) Builds the default start state tensor of zeros. python.modules.gated_rnn.RecurrentDropoutWrapper.is_connected Returns true iff the Module been connected to the Graph at least once. python.modules.gated_rnn.RecurrentDropoutWrapper.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.RecurrentDropoutWrapper.module_name Returns the name of the Module. python.modules.gated_rnn.RecurrentDropoutWrapper.name_scopes Returns a tuple of all name_scopes generated by this module. python.modules.gated_rnn.RecurrentDropoutWrapper.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.RecurrentDropoutWrapper.output_size Integer or TensorShape: size of outputs produced by this cell. python.modules.gated_rnn.RecurrentDropoutWrapper.scope_name Returns the full name of the Module's variable scope. python.modules.gated_rnn.RecurrentDropoutWrapper.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. python.modules.gated_rnn.RecurrentDropoutWrapper.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.RecurrentDropoutWrapper.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.RecurrentDropoutWrapper.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.RecurrentDropoutWrapper.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . class python.modules.gated_rnn.ZoneoutWrapper Wraps an RNNCore so that zoneout can be applied. Zoneout was introduced in https://arxiv.org/abs/1606.01305 It consists of randomly freezing some RNN state in the same way recurrent dropout would replace this state with zero. python.modules.gated_rnn.ZoneoutWrapper.__init__(core, keep_probs, is_training) Builds a new wrapper around a given core. Args: core : the RNN core to be wrapped. keep_probs : the probabilities to use the updated states rather than keeping the old state values. This is one minus the probability that zoneout gets applied. This should have the same structure has core.init_state. No zoneout is applied for leafs set to None. is_training : when set, apply some stochastic zoneout. Otherwise perform a linear combination of the previous state and the current state based on the zoneout probability. python.modules.gated_rnn.ZoneoutWrapper.__call__(inputs, prev_state) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). python.modules.gated_rnn.ZoneoutWrapper.connected_subgraphs Returns the subgraphs created by this module so far. python.modules.gated_rnn.ZoneoutWrapper.defun() Wraps this modules call method in a callable graph function. python.modules.gated_rnn.ZoneoutWrapper.defun_wrapped Returns boolean indicating whether this module is defun wrapped. python.modules.gated_rnn.ZoneoutWrapper.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ZoneoutWrapper.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. python.modules.gated_rnn.ZoneoutWrapper.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ZoneoutWrapper.graph Returns the Graph instance which the module is connected to, or None. python.modules.gated_rnn.ZoneoutWrapper.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None) Builds the default start state tensor of zeros. python.modules.gated_rnn.ZoneoutWrapper.is_connected Returns true iff the Module been connected to the Graph at least once. python.modules.gated_rnn.ZoneoutWrapper.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ZoneoutWrapper.module_name Returns the name of the Module. python.modules.gated_rnn.ZoneoutWrapper.name_scopes Returns a tuple of all name_scopes generated by this module. python.modules.gated_rnn.ZoneoutWrapper.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ZoneoutWrapper.output_size Integer or TensorShape: size of outputs produced by this cell. python.modules.gated_rnn.ZoneoutWrapper.scope_name Returns the full name of the Module's variable scope. python.modules.gated_rnn.ZoneoutWrapper.state_size size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes. python.modules.gated_rnn.ZoneoutWrapper.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ZoneoutWrapper.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ZoneoutWrapper.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.gated_rnn.ZoneoutWrapper.zero_state(batch_size, dtype) Return zero-filled state tensor(s). Args: batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state. Returns: If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size . python.modules.layer_norm.create_beta_initializer() Returns a default initializer for the beta in layer norm. python.modules.layer_norm.create_gamma_initializer() Returns a default initializer for the gamma in layer norm. python.modules.rnn_core.with_doc(fn_with_doc_to_copy) Returns a decorator to copy documentation from the given function. Docstring is copied, including args and *kwargs documentation. Args: fn_with_doc_to_copy : Function whose docstring, including args and *kwargs documentation, is to be copied. Returns: Decorated version of wrapper_init with documentation copied from fn_with_doc_to_copy . class python.modules.spectral_normalization.SpectralNormWrapper Wraps a Sonnet Module to selectively apply Spectral Normalization. python.modules.spectral_normalization.SpectralNormWrapper.__init__(module, sn_kwargs, pow_iter_collection, *args, **kwargs) Constructs a wrapped Sonnet module with Spectral Normalization. The module expects a first argument which should be a Sonnet AbstractModule and a second argument which is a dictionary which is passed to the inner spectral_norm function as kwargs. When connecting this module to the graph,the argument 'pow_iter_collection' is treated specially for this wrapper (rather than for the _build method of the inner module). If pow_iter_collection is None (the default), the approximate first singular value for weights will not be updated based on the inputs passed at the given _build call. However an op for updating the singular value will be placed into the pow_iter_collection global collection. If pow_iter_collection is None or not passed, a control dependency on the update op will be applied to the output of the _build function. Regardless, the kwarg is deleted from the list of keywords passed to the inner module. Args: module : A constructor/class reference for a Sonnet module you would like to construct. sn_kwargs : Keyword arguments to be passed to the spectral_norm function in addition to the weight tensor. pow_iter_collection : The name of a global collection for potentially storing ops for updating internal variables. *args : Construction-time arguments to the module. **kwargs : Construction-time keyword arguments to the module. python.modules.spectral_normalization.SpectralNormWrapper.__call__(*args, **kwargs) Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template. Args: *args : Input Tensors. **kwargs : Additional Python flags controlling connection. Returns: output Tensor(s). python.modules.spectral_normalization.SpectralNormWrapper.connected_subgraphs Returns the subgraphs created by this module so far. python.modules.spectral_normalization.SpectralNormWrapper.defun() Wraps this modules call method in a callable graph function. python.modules.spectral_normalization.SpectralNormWrapper.defun_wrapped Returns boolean indicating whether this module is defun wrapped. python.modules.spectral_normalization.SpectralNormWrapper.get_all_variables(collection='trainable_variables') Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.spectral_normalization.SpectralNormWrapper.get_possible_initializer_keys() Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided. Returns: Set with strings corresponding to the strings that may be passed to the constructor. python.modules.spectral_normalization.SpectralNormWrapper.get_variables(collection='trainable_variables') Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to. Args: collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages. Returns: A tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.spectral_normalization.SpectralNormWrapper.graph Returns the Graph instance which the module is connected to, or None. python.modules.spectral_normalization.SpectralNormWrapper.is_connected Returns true iff the Module been connected to the Graph at least once. python.modules.spectral_normalization.SpectralNormWrapper.last_connected_subgraph Returns the last subgraph created by this module. Returns: The last connected subgraph. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.spectral_normalization.SpectralNormWrapper.module_name Returns the name of the Module. python.modules.spectral_normalization.SpectralNormWrapper.name_scopes Returns a tuple of all name_scopes generated by this module. python.modules.spectral_normalization.SpectralNormWrapper.non_trainable_variables All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.spectral_normalization.SpectralNormWrapper.scope_name Returns the full name of the Module's variable scope. python.modules.spectral_normalization.SpectralNormWrapper.sn_getter(spectral_norm_kwargs) Returns a curried spectral normalization Custom Getter. python.modules.spectral_normalization.SpectralNormWrapper.trainable_variables All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.spectral_normalization.SpectralNormWrapper.variable_scope Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property. Returns: variable_scope : tf.VariableScope instance of the internal tf.Template . Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.spectral_normalization.SpectralNormWrapper.variables All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured. Returns: A sorted (by variable name) tuple of tf.Variable objects. Raises: NotConnectedError : If the module is not connected to the Graph. python.modules.spectral_normalization.spectral_norm(weight, num_iters=1, update_collection=None, eps=0.0001) Spectral Weight Normalization. Applies first-singular-value spectral normalization to weight and returns a tensor equivalent to weight with spectral normalization applies. By default, it also updates an inner variable for keeping track of the spectral values of this weight matrix. If update_collection is not None, however, this function does not update the variable automatically, instead placing an op for this update in the 'update_collection' global collection. Args: weight : The weight tensor which requires spectral normalization num_iters : Number of SN iterations. update_collection : The update collection for assigning persisted variable u. If None, the function will update u0 during the forward pass. Otherwise if the update_collection equals 'update_collection', it will put the assignment in a collection defined by the user. Then the user will need to run the assignment explicitly. eps : numerical stability constant 0. Returns: A dictionary of: w_bar : The normalized weight tensor sigma : The estimated singular value for the weight tensor. u0 : The internal persisted variable. python.modules.util.get_variable_scope_name(value) Returns the name of the variable scope indicated by the given value. Args: value : String, variable scope, or object with variable_scope attribute (e.g., Sonnet module). Returns: The name (a string) of the corresponding variable scope. Raises: ValueError : If value does not identify a variable scope. python.modules.util.name_for_callable(func) Returns a module name for a callable or None if no name can be found. python.modules.util.notify_about_new_variables(callback) Calls callback(var) for all newly created variables. Callback should not modify the variable passed in. Use cases that require variables to be modified should use variable_creator_scope directly and sit within the variable creator stack. variables = [] with notify_about_variables(variables.append): ... v = tf.Variable(1.0, name='v') ... w = tf.get_variable('w', []) assert variables == [v, w] Args: callback : a callable taking a single argument which is a tf.Variable. Yields: None - used for contextmanager API. python.modules.util.sort_by_name(variables) Returns a tuple of variables sorted ascending by name. python.modules.util.to_snake_case(camel_case) Returns a CamelCase string as a snake_case string.","title":"Module Reference"},{"location":"sonnet/#sonnet-module-reference","text":"This python module contains Neural Network Modules for TensorFlow. Each module is a Python object which conceptually \"owns\" any variables required in that part of the Neural Network. The __call__ function on the object is used to connect that Module into the Graph, and this may be called repeatedly with sharing automatically taking place. Everything public should be imported by this top level __init__.py so that the library can be used as follows: import sonnet as snt linear = snt.Linear(...)","title":"sonnet - module reference"},{"location":"sonnet/#other-functions-and-classes","text":"","title":"Other Functions and Classes"},{"location":"sonnet/#class-actcore","text":"Adaptive computation time core. Implementation of the model described in \"Adaptive Computation Time for Recurrent Neural Networks\" paper, https://arxiv.org/abs/1603.08983. The ACTCore incorporates the pondering RNN of ACT, with different computation times for each element in the mini batch. Each pondering step is performed by the core passed to the constructor of ACTCore . The output of the ACTCore is made of (act_out, (iteration, remainder) , where iteration counts the number of pondering step in each batch element; remainder is the remainder as defined in the ACT paper; act_out is the weighted average output of all pondering steps (see ACT paper for more info).","title":"class ACTCore"},{"location":"sonnet/#actcore__init__core-output_size-threshold-get_state_for_halting-max_steps0-nameact_core","text":"Constructor.","title":"ACTCore.__init__(core, output_size, threshold, get_state_for_halting, max_steps=0, name='act_core')"},{"location":"sonnet/#args","text":"core : A sonnet.RNNCore object. This should only take a single Tensor in input, and output only a single flat Tensor . output_size : An integer. The size of each output in the sequence. threshold : A float between 0 and 1. Probability to reach for ACT to stop pondering. get_state_for_halting : A callable that can take the core state and return the input to the halting function. max_steps : Integer = 0, that controls the maximum number of ponder steps. If equal to 0, then this disables control. name : A string. The name of this module.","title":"Args:"},{"location":"sonnet/#raises","text":"ValueError : if threshold is not between 0 and 1. ValueError : if core has either nested outputs or outputs that are not one dimensional.","title":"Raises:"},{"location":"sonnet/#actcore__call__x-prev_state","text":"Connects the core to the graph.","title":"ACTCore.__call__(x, prev_state)"},{"location":"sonnet/#args_1","text":"x : Input Tensor of shape (batch_size, input_size) . prev_state : Previous state. This could be a Tensor , or a tuple of Tensor s.","title":"Args:"},{"location":"sonnet/#returns","text":"The tuple (output, state) for this core.","title":"Returns:"},{"location":"sonnet/#raises_1","text":"ValueError : if the Tensor x does not have rank 2.","title":"Raises:"},{"location":"sonnet/#actcorebatch_size","text":"","title":"ACTCore.batch_size"},{"location":"sonnet/#actcoreconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"ACTCore.connected_subgraphs"},{"location":"sonnet/#actcoredefun","text":"Wraps this modules call method in a callable graph function.","title":"ACTCore.defun()"},{"location":"sonnet/#actcoredefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"ACTCore.defun_wrapped"},{"location":"sonnet/#actcoredtype","text":"","title":"ACTCore.dtype"},{"location":"sonnet/#actcoreget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"ACTCore.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_2","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_1","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_2","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#actcoreget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"ACTCore.get_possible_initializer_keys()"},{"location":"sonnet/#returns_2","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#actcoreget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"ACTCore.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_3","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_3","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_3","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#actcoregraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"ACTCore.graph"},{"location":"sonnet/#actcoreinitial_stateargs-kwargs","text":"Builds the default start state for an RNNCore.","title":"ACTCore.initial_state(*args, **kwargs)"},{"location":"sonnet/#args_4","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_4","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_4","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#actcoreis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"ACTCore.is_connected"},{"location":"sonnet/#actcorelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"ACTCore.last_connected_subgraph"},{"location":"sonnet/#returns_5","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_5","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#actcoremodule_name","text":"Returns the name of the Module.","title":"ACTCore.module_name"},{"location":"sonnet/#actcorename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"ACTCore.name_scopes"},{"location":"sonnet/#actcorenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"ACTCore.non_trainable_variables"},{"location":"sonnet/#returns_6","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_6","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#actcoreoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"ACTCore.output_size"},{"location":"sonnet/#actcorescope_name","text":"Returns the full name of the Module's variable scope.","title":"ACTCore.scope_name"},{"location":"sonnet/#actcorestate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"ACTCore.state_size"},{"location":"sonnet/#actcoretrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"ACTCore.trainable_variables"},{"location":"sonnet/#returns_7","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_7","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#actcorevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"ACTCore.variable_scope"},{"location":"sonnet/#returns_8","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_8","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#actcorevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"ACTCore.variables"},{"location":"sonnet/#returns_9","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_9","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#actcorezero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"ACTCore.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_5","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_10","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-abstractmodule","text":"Superclass for Sonnet Modules. This class defines the functionality that every module should implement, principally the build method which is wrapped using tf.make_template and called from __call__ . Every time the module is called it will be connected into the graph but using the same shared set of variables, thanks to the template. For this to work correctly, the build implementation in the derived class must access all variables using tf.get_variable , not tf.Variable . The same set of variables must be created each time, if this is not the case an Error will be raised. Every subclass must call this class' __init__ at the start of their __init__ , passing the relevant name. If this step is omitted variable sharing will not work.","title":"class AbstractModule"},{"location":"sonnet/#abstractmodule__init___sentinelnone-custom_getternone-namenone","text":"Performs the initialisation necessary for all AbstractModule instances. Every subclass of AbstractModule must begin their constructor with a call to this constructor, i.e. super(MySubModule, self).__init__(custom_getter=custom_getter, name=name) . If you instantiate sub-modules in init you must create them within the _enter_variable_scope context manager to ensure they are in the module's variable scope. Alternatively, instantiate sub-modules in _build .","title":"AbstractModule.__init__(_sentinel=None, custom_getter=None, name=None)"},{"location":"sonnet/#args_6","text":"_sentinel: Variable that only carries a non-None value if __init__ was called without named parameters. If this is the case, a deprecation warning is issued in form of a ValueError . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of this module. Used to construct the Templated build function. If None the module's class name is used (converted to snake case).","title":"Args:"},{"location":"sonnet/#raises_10","text":"TypeError : If name is not a string. TypeError : If a given custom_getter is not callable. ValueError : If __init__ was called without named arguments.","title":"Raises:"},{"location":"sonnet/#abstractmodule__call__args-kwargs","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"AbstractModule.__call__(*args, **kwargs)"},{"location":"sonnet/#args_7","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_11","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#abstractmoduleconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"AbstractModule.connected_subgraphs"},{"location":"sonnet/#abstractmoduledefun","text":"Wraps this modules call method in a callable graph function.","title":"AbstractModule.defun()"},{"location":"sonnet/#abstractmoduledefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"AbstractModule.defun_wrapped"},{"location":"sonnet/#abstractmoduleget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"AbstractModule.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_8","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_12","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_11","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#abstractmoduleget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"AbstractModule.get_possible_initializer_keys()"},{"location":"sonnet/#returns_13","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#abstractmoduleget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"AbstractModule.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_9","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_14","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_12","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#abstractmodulegraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"AbstractModule.graph"},{"location":"sonnet/#abstractmoduleis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"AbstractModule.is_connected"},{"location":"sonnet/#abstractmodulelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"AbstractModule.last_connected_subgraph"},{"location":"sonnet/#returns_15","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_13","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#abstractmodulemodule_name","text":"Returns the name of the Module.","title":"AbstractModule.module_name"},{"location":"sonnet/#abstractmodulename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"AbstractModule.name_scopes"},{"location":"sonnet/#abstractmodulenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"AbstractModule.non_trainable_variables"},{"location":"sonnet/#returns_16","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_14","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#abstractmodulescope_name","text":"Returns the full name of the Module's variable scope.","title":"AbstractModule.scope_name"},{"location":"sonnet/#abstractmoduletrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"AbstractModule.trainable_variables"},{"location":"sonnet/#returns_17","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_15","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#abstractmodulevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"AbstractModule.variable_scope"},{"location":"sonnet/#returns_18","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_16","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#abstractmodulevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"AbstractModule.variables"},{"location":"sonnet/#returns_19","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_17","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-addbias","text":"AddBias module.","title":"class AddBias"},{"location":"sonnet/#addbias__init__output_shapenone-bias_dimsnone-initializersnone-partitionersnone-regularizersnone-nameadd","text":"Constructs an AddBias module that supports broadcasting.","title":"AddBias.__init__(output_shape=None, bias_dims=None, initializers=None, partitioners=None, regularizers=None, name='add')"},{"location":"sonnet/#args_10","text":"output_shape : Output dimensionality. output_shape can be either None , a tuple , or a callable . In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_shape can be called, returning a tuple, when build is called. If output_shape is left as None , the size will be directly inferred by the input. bias_dims : List of which dimensions to retain from the input shape when constructing the bias. The remaining dimensions will get broadcasted over (given size of 1), and leading dimensions will be removed completely. For example, for an input of [batch_size, dim1_size, dim2_size, dim3_size] and bias_dims=[1, 3] , the resulting bias will have shape [dim1_size, 1, dim3_size]. The default is to retain all dimensions apart from the minibatch dimension. Trying to retain the bias shape over the minibatch dimension, e.g. bias_dims=[0] , will result in an error at build time. See the 'Example Usage' section below for more information. initializers : Optional dict containing ops to initialize the biases (with key 'b'). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing a partitioner to partition the bias (with key 'b'). As a default, no partitioner is used. regularizers : Optional dict containing regularizers of the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Name of the module. Example Usage: # Create a 4D input Tensor. input = tf.random_normal( shape=(batch_size, dim1_size, dim2_size, dim3_size))) # Create a scalar bias: scalar_bias = snt.AddBias(bias_dims=[]) scalar_bias_output = scalar_bias(input) scalar_bias.b.get_shape() # () # Create a bias over all non-minibatch dimensions: all_bias = snt.AddBias() # or snt.AddBias(bias_dims=None) all_bias_output = all_bias(input) all_bias.b.get_shape() # (dim1_size, dim2_size, dim3_size) # Create a bias over the last non-minibatch dimension: last_bias = snt.AddBias(bias_dims=[-1]) last_bias_output = last_bias(input) last_bias.b.get_shape() # (dim3_size) # Create a bias over the first non-minibatch dimension: first_bias = snt.AddBias(bias_dims=[1]) first_bias_output = first_bias(input) first_bias.b.get_shape() # (dim1_size, 1, 1) # Subtract and later add the same learned bias: bias = snt.AddBias() hidden1 = bias(input, multiplier=-1) # ... reconstructed_input = bias(hidden4)","title":"Args:"},{"location":"sonnet/#raises_18","text":"KeyError : If initializers contains any keys other than 'b'. KeyError : If partitioners contains any keys other than 'b'. KeyError : If regularizers contains any keys other than 'b'. TypeError : If any of the given initializers are not callable. TypeError : If any of the given partitioners are not callable. TypeError : If any of the given regularizers are not callable.","title":"Raises:"},{"location":"sonnet/#addbias__call__inputs-multiplier1","text":"Connects the Add module into the graph, with input Tensor inputs .","title":"AddBias.__call__(inputs, multiplier=1)"},{"location":"sonnet/#args_11","text":"inputs : A Tensor of size [batch_size, input_size1, ...] . multiplier : A scalar or Tensor which the bias term is multiplied by before adding it to inputs . Anything which works in the expression bias * multiplier is acceptable here. This may be useful if you want to add a bias in one place and subtract the same bias in another place via multiplier=-1 .","title":"Args:"},{"location":"sonnet/#returns_20","text":"A Tensor of size [batch_size, input_size1, ...] .","title":"Returns:"},{"location":"sonnet/#raises_19","text":"base.IncompatibleShapeError: If the input is not a = 2D Tensor . base.IncompatibleShapeError: If connecting the module into the graph any time after the first time, and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the output_shape has been specified but it does not match the input_shape`. base.ParentNotBuiltError: If the module is a transposed and the original untransposed module has not been built.","title":"Raises:"},{"location":"sonnet/#addbiasb","text":"Returns the Variable containing the bias.","title":"AddBias.b"},{"location":"sonnet/#returns_21","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_20","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist.","title":"Raises:"},{"location":"sonnet/#addbiasconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"AddBias.connected_subgraphs"},{"location":"sonnet/#addbiasdefun","text":"Wraps this modules call method in a callable graph function.","title":"AddBias.defun()"},{"location":"sonnet/#addbiasdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"AddBias.defun_wrapped"},{"location":"sonnet/#addbiasget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"AddBias.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_12","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_22","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_21","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#addbiasget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"AddBias.get_possible_initializer_keys()"},{"location":"sonnet/#returns_23","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#addbiasget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"AddBias.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_13","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_24","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_22","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#addbiasgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"AddBias.graph"},{"location":"sonnet/#addbiasinput_shape","text":"Returns shape of input Tensor passed at last call to build .","title":"AddBias.input_shape"},{"location":"sonnet/#addbiasis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"AddBias.is_connected"},{"location":"sonnet/#addbiaslast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"AddBias.last_connected_subgraph"},{"location":"sonnet/#returns_25","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_23","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#addbiasmodule_name","text":"Returns the name of the Module.","title":"AddBias.module_name"},{"location":"sonnet/#addbiasname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"AddBias.name_scopes"},{"location":"sonnet/#addbiasnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"AddBias.non_trainable_variables"},{"location":"sonnet/#returns_26","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_24","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#addbiasscope_name","text":"Returns the full name of the Module's variable scope.","title":"AddBias.scope_name"},{"location":"sonnet/#addbiastrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"AddBias.trainable_variables"},{"location":"sonnet/#returns_27","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_25","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#addbiastransposenamenone","text":"Returns transposed AddBias module.","title":"AddBias.transpose(name=None)"},{"location":"sonnet/#args_14","text":"name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.module_name .","title":"Args:"},{"location":"sonnet/#returns_28","text":"Transposed AddBias module.","title":"Returns:"},{"location":"sonnet/#addbiasvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"AddBias.variable_scope"},{"location":"sonnet/#returns_29","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_26","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#addbiasvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"AddBias.variables"},{"location":"sonnet/#returns_30","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_27","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-affinegridwarper","text":"Affine Grid Warper class. The affine grid warper generates a reference grid of n-dimensional points and warps it via an affine transormation model determined by an input parameter Tensor. Some of the transformation parameters can be fixed at construction time via an AffineWarpConstraints object.","title":"class AffineGridWarper"},{"location":"sonnet/#affinegridwarper__init__source_shape-output_shape-constraintsnone-nameaffine_grid_warper","text":"Constructs an AffineGridWarper. source_shape and output_shape are used to define the size of the source and output signal domains, as opposed to the shape of the respective Tensors. For example, for an image of size width=W and height=H , {source,output}_shape=[H, W] ; for a volume of size width=W , height=H and depth=D , {source,output}_shape=[H, W, D] .","title":"AffineGridWarper.__init__(source_shape, output_shape, constraints=None, name='affine_grid_warper')"},{"location":"sonnet/#args_15","text":"source_shape : Iterable of integers determining the size of the source signal domain. output_shape : Iterable of integers determining the size of the destination resampled signal domain. constraints : Either a double list of shape [N, N+1] defining constraints on the entries of a matrix defining an affine transformation in N dimensions, or an AffineWarpConstraints object. If the double list is passed, a numeric value bakes in a constraint on the corresponding entry in the transformation matrix, whereas None implies that the corresponding entry will be specified at run time. name : Name of module.","title":"Args:"},{"location":"sonnet/#raises_28","text":"Error : If constraints fully define the affine transformation; or if input grid shape and contraints have different dimensionality. TypeError : If output_shape and source_shape are not both iterable.","title":"Raises:"},{"location":"sonnet/#affinegridwarper__call__inputs","text":"Assembles the module network and adds it to the graph. The internal computation graph is assembled according to the set of constraints provided at construction time.","title":"AffineGridWarper.__call__(inputs)"},{"location":"sonnet/#args_16","text":"inputs : Tensor containing a batch of transformation parameters.","title":"Args:"},{"location":"sonnet/#returns_31","text":"A batch of warped grids.","title":"Returns:"},{"location":"sonnet/#raises_29","text":"Error : If the input tensor size is not consistent with the constraints passed at construction time.","title":"Raises:"},{"location":"sonnet/#affinegridwarperconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"AffineGridWarper.connected_subgraphs"},{"location":"sonnet/#affinegridwarperconstraints","text":"","title":"AffineGridWarper.constraints"},{"location":"sonnet/#affinegridwarperdefun","text":"Wraps this modules call method in a callable graph function.","title":"AffineGridWarper.defun()"},{"location":"sonnet/#affinegridwarperdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"AffineGridWarper.defun_wrapped"},{"location":"sonnet/#affinegridwarperget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"AffineGridWarper.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_17","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_32","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_30","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#affinegridwarperget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"AffineGridWarper.get_possible_initializer_keys()"},{"location":"sonnet/#returns_33","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#affinegridwarperget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"AffineGridWarper.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_18","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_34","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_31","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#affinegridwarpergraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"AffineGridWarper.graph"},{"location":"sonnet/#affinegridwarperinversenamenone","text":"Returns a sonnet module to compute inverse affine transforms. The function first assembles a network that given the constraints of the current AffineGridWarper and a set of input parameters, retrieves the coefficients of the corresponding inverse affine transform, then feeds its output into a new AffineGridWarper setup to correctly warp the output space into the source space.","title":"AffineGridWarper.inverse(name=None)"},{"location":"sonnet/#args_19","text":"name : Name of module implementing the inverse grid transformation.","title":"Args:"},{"location":"sonnet/#returns_35","text":"A sonnet module performing the inverse affine transform of a reference grid of points via an AffineGridWarper module.","title":"Returns:"},{"location":"sonnet/#raises_32","text":"tf.errors.UnimplementedError: If the function is called on a non 2D instance of AffineGridWarper.","title":"Raises:"},{"location":"sonnet/#affinegridwarperis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"AffineGridWarper.is_connected"},{"location":"sonnet/#affinegridwarperlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"AffineGridWarper.last_connected_subgraph"},{"location":"sonnet/#returns_36","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_33","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#affinegridwarpermodule_name","text":"Returns the name of the Module.","title":"AffineGridWarper.module_name"},{"location":"sonnet/#affinegridwarpern_coeff","text":"Returns number of coefficients of warping function.","title":"AffineGridWarper.n_coeff"},{"location":"sonnet/#affinegridwarpername_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"AffineGridWarper.name_scopes"},{"location":"sonnet/#affinegridwarpernon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"AffineGridWarper.non_trainable_variables"},{"location":"sonnet/#returns_37","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_34","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#affinegridwarperoutput_shape","text":"Returns a tuple containing the shape of the output grid.","title":"AffineGridWarper.output_shape"},{"location":"sonnet/#affinegridwarperpsi","text":"Returns a list of features used to compute the grid warp.","title":"AffineGridWarper.psi"},{"location":"sonnet/#affinegridwarperscope_name","text":"Returns the full name of the Module's variable scope.","title":"AffineGridWarper.scope_name"},{"location":"sonnet/#affinegridwarpersource_shape","text":"Returns a tuple containing the shape of the source signal.","title":"AffineGridWarper.source_shape"},{"location":"sonnet/#affinegridwarpertrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"AffineGridWarper.trainable_variables"},{"location":"sonnet/#returns_38","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_35","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#affinegridwarpervariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"AffineGridWarper.variable_scope"},{"location":"sonnet/#returns_39","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_36","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#affinegridwarpervariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"AffineGridWarper.variables"},{"location":"sonnet/#returns_40","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_37","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-affinewarpconstraints","text":"Affine warp contraints class. AffineWarpConstraints allow for very succinct definitions of constraints on the values of entries in affine transform matrices.","title":"class AffineWarpConstraints"},{"location":"sonnet/#affinewarpconstraints__init__constraintsnone-none-none-none-none-none","text":"Creates a constraint definition for an affine transformation.","title":"AffineWarpConstraints.__init__(constraints=((None, None, None), (None, None, None)))"},{"location":"sonnet/#args_20","text":"constraints : A doubly-nested iterable of shape [N, N+1] defining constraints on the entries of a matrix that represents an affine transformation in N dimensions. A numeric value bakes in a constraint on the corresponding entry in the transformation matrix, whereas None implies that the corresponding entry will be specified at run time.","title":"Args:"},{"location":"sonnet/#raises_38","text":"TypeError : If constraints is not a nested iterable. ValueError : If the double iterable constraints has inconsistent dimensions.","title":"Raises:"},{"location":"sonnet/#affinewarpconstraintscombine_withadditional_constraints","text":"Combines two sets of constraints into a coherent single set.","title":"AffineWarpConstraints.combine_with(additional_constraints)"},{"location":"sonnet/#affinewarpconstraintsconstraints","text":"","title":"AffineWarpConstraints.constraints"},{"location":"sonnet/#affinewarpconstraintsmask","text":"","title":"AffineWarpConstraints.mask"},{"location":"sonnet/#affinewarpconstraintsno_constraintsnum_dim2","text":"Empty set of constraints for a num_dim-ensional affine transform.","title":"AffineWarpConstraints.no_constraints(num_dim=2)"},{"location":"sonnet/#affinewarpconstraintsno_shear_2d","text":"","title":"AffineWarpConstraints.no_shear_2d()"},{"location":"sonnet/#affinewarpconstraintsno_shear_3d","text":"Assigns contraints on shear components of affine transform in 3d.","title":"AffineWarpConstraints.no_shear_3d()"},{"location":"sonnet/#affinewarpconstraintsnum_dim","text":"","title":"AffineWarpConstraints.num_dim"},{"location":"sonnet/#affinewarpconstraintsnum_free_params","text":"","title":"AffineWarpConstraints.num_free_params"},{"location":"sonnet/#affinewarpconstraintsscale_2dxnone-ynone","text":"Assigns contraints on scaling components of affine transform in 2d.","title":"AffineWarpConstraints.scale_2d(x=None, y=None)"},{"location":"sonnet/#affinewarpconstraintsscale_3dxnone-ynone-znone","text":"Assigns contraints on scaling components of affine transform in 3d.","title":"AffineWarpConstraints.scale_3d(x=None, y=None, z=None)"},{"location":"sonnet/#affinewarpconstraintsshear_2dxnone-ynone","text":"Assigns contraints on shear components of affine transform in 2d.","title":"AffineWarpConstraints.shear_2d(x=None, y=None)"},{"location":"sonnet/#affinewarpconstraintstranslation_2dxnone-ynone","text":"Assign contraints on translation components of affine transform in 2d.","title":"AffineWarpConstraints.translation_2d(x=None, y=None)"},{"location":"sonnet/#affinewarpconstraintstranslation_3dxnone-ynone-znone","text":"Assign contraints on translation components of affine transform in 3d.","title":"AffineWarpConstraints.translation_3d(x=None, y=None, z=None)"},{"location":"sonnet/#class-attentiveread","text":"A module for reading with attention. This module reads a weighted sum of embeddings from memory, where each memory slot's weight is based on the logit returned by an attention embedding module. A mask may be given to ignore some memory slots (e.g. when attending over variable-length sequences).","title":"class AttentiveRead"},{"location":"sonnet/#attentiveread__init__attention_logit_mod-nameattention","text":"Initialize AttentiveRead module.","title":"AttentiveRead.__init__(attention_logit_mod, name='attention')"},{"location":"sonnet/#args_21","text":"attention_logit_mod : Module that produces logit corresponding to a memory slot's compatibility. Must map a [batch_size * memory_size, memory_word_size + query_word_size]-shaped Tensor to a [batch_size * memory_size, 1] shape Tensor. name : string. Name for module.","title":"Args:"},{"location":"sonnet/#attentiveread__call__memory-query-memory_masknone","text":"Perform a differentiable read.","title":"AttentiveRead.__call__(memory, query, memory_mask=None)"},{"location":"sonnet/#args_22","text":"memory : [batch_size, memory_size, memory_word_size]-shaped Tensor of dtype float32. This represents, for each example and memory slot, a single embedding to attend over. query : [batch_size, query_word_size]-shaped Tensor of dtype float32. Represents, for each example, a single embedding representing a query. memory_mask : None or [batch_size, memory_size]-shaped Tensor of dtype bool. An entry of False indicates that a memory slot should not enter the resulting weighted sum. If None, all memory is used.","title":"Args:"},{"location":"sonnet/#returns_41","text":"An AttentionOutput instance containing: read : [batch_size, memory_word_size]-shaped Tensor of dtype float32. This represents, for each example, a weighted sum of the contents of the memory. weights : [batch_size, memory_size]-shaped Tensor of dtype float32. This represents, for each example and memory slot, the attention weights used to compute the read. weight_logits : [batch_size, memory_size]-shaped Tensor of dtype float32. This represents, for each example and memory slot, the logits of the attention weights, that is, weights is calculated by taking the softmax of the weight logits.","title":"Returns:"},{"location":"sonnet/#raises_39","text":"UnderspecifiedError : if memory_word_size or query_word_size can not be inferred. IncompatibleShapeError : if memory, query, memory_mask, or output of attention_logit_mod do not match expected shapes.","title":"Raises:"},{"location":"sonnet/#attentivereadconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"AttentiveRead.connected_subgraphs"},{"location":"sonnet/#attentivereaddefun","text":"Wraps this modules call method in a callable graph function.","title":"AttentiveRead.defun()"},{"location":"sonnet/#attentivereaddefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"AttentiveRead.defun_wrapped"},{"location":"sonnet/#attentivereadget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"AttentiveRead.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_23","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_42","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_40","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#attentivereadget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"AttentiveRead.get_possible_initializer_keys()"},{"location":"sonnet/#returns_43","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#attentivereadget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"AttentiveRead.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_24","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_44","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_41","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#attentivereadgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"AttentiveRead.graph"},{"location":"sonnet/#attentivereadis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"AttentiveRead.is_connected"},{"location":"sonnet/#attentivereadlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"AttentiveRead.last_connected_subgraph"},{"location":"sonnet/#returns_45","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_42","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#attentivereadmodule_name","text":"Returns the name of the Module.","title":"AttentiveRead.module_name"},{"location":"sonnet/#attentivereadname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"AttentiveRead.name_scopes"},{"location":"sonnet/#attentivereadnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"AttentiveRead.non_trainable_variables"},{"location":"sonnet/#returns_46","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_43","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#attentivereadscope_name","text":"Returns the full name of the Module's variable scope.","title":"AttentiveRead.scope_name"},{"location":"sonnet/#attentivereadtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"AttentiveRead.trainable_variables"},{"location":"sonnet/#returns_47","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_44","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#attentivereadvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"AttentiveRead.variable_scope"},{"location":"sonnet/#returns_48","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_45","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#attentivereadvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"AttentiveRead.variables"},{"location":"sonnet/#returns_49","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_46","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-batchapply","text":"Merges a number of leading dimensions of an input tensor to manipulate it. Merges a number of leading dimensions of a tensor into a single dimension, connects the provided module, then splits the leading dimension of the result to match the input. Input tensors whose rank is smaller than the number of dimensions to collapse (e.g. all scalar values, which are tensors of rank 0), are passed unaltered to the provided module. This is useful for applying some module to each timestep of a Time x Batch x N tensor. If a module is hard coded to only support 2D (Batch x N) then the full 3D Tensor cannot be provided. BatchApply will 'merge' the first two dimensions of the sequence tensor by reshaping to a (Time * Batch) x N Tensor, and then the internal module can be applied. The result of that operation is reshaped such that its first dimensions are split to match the leading dimensions of the input.","title":"class BatchApply"},{"location":"sonnet/#batchapply__init__module_or_op-n_dims2-input_example_index0-namebatch_apply","text":"Constructor of the module.","title":"BatchApply.__init__(module_or_op, n_dims=2, input_example_index=0, name='batch_apply')"},{"location":"sonnet/#args_25","text":"module_or_op : Module or tensorflow op to apply to an input tensor. n_dims : Number of dimensions to merge before using module on the input of BatchApply. input_example_index : Index of input that has same shape for the first n_dims dimensions as module_or_op output(s). This is used for unflattening the output(s) if static shape inference is not possible. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_47","text":"TypeError : If n_dims is not an integer. ValueError : If n_dims is not greater than zero.","title":"Raises:"},{"location":"sonnet/#batchapply__call__args-kwargs","text":"Connects the BatchApply module into the graph.","title":"BatchApply.__call__(*args, **kwargs)"},{"location":"sonnet/#args_26","text":"*args : a Tensor or a nested list or dictionary of Tensors. The input tensors will have their first dimensions merged, then an op or a module will be called on the input. The first dimension of the output tensor(s) will be split again based on the leading dimensions of the first input tensor. **kwargs : Dictionary of named arguments; used in the same way as *args .","title":"Args:"},{"location":"sonnet/#returns_50","text":"A Tensor or nested list or dictionary of Tensors as a result of applying the process above. (\"None\" return values are also supported.)","title":"Returns:"},{"location":"sonnet/#batchapplyconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"BatchApply.connected_subgraphs"},{"location":"sonnet/#batchapplydefun","text":"Wraps this modules call method in a callable graph function.","title":"BatchApply.defun()"},{"location":"sonnet/#batchapplydefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"BatchApply.defun_wrapped"},{"location":"sonnet/#batchapplyget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"BatchApply.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_27","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_51","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_48","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchapplyget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"BatchApply.get_possible_initializer_keys()"},{"location":"sonnet/#returns_52","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#batchapplyget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"BatchApply.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_28","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_53","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_49","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchapplygraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"BatchApply.graph"},{"location":"sonnet/#batchapplyis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"BatchApply.is_connected"},{"location":"sonnet/#batchapplylast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"BatchApply.last_connected_subgraph"},{"location":"sonnet/#returns_54","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_50","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchapplymodule_name","text":"Returns the name of the Module.","title":"BatchApply.module_name"},{"location":"sonnet/#batchapplyname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"BatchApply.name_scopes"},{"location":"sonnet/#batchapplynon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchApply.non_trainable_variables"},{"location":"sonnet/#returns_55","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_51","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchapplyscope_name","text":"Returns the full name of the Module's variable scope.","title":"BatchApply.scope_name"},{"location":"sonnet/#batchapplytrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchApply.trainable_variables"},{"location":"sonnet/#returns_56","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_52","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchapplyvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"BatchApply.variable_scope"},{"location":"sonnet/#returns_57","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_53","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchapplyvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchApply.variables"},{"location":"sonnet/#returns_58","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_54","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-batchflatten","text":"Flattens the input Tensor, preserving the batch dimension(s).","title":"class BatchFlatten"},{"location":"sonnet/#batchflatten__init__preserve_dims1-namebatch_flatten","text":"Constructs a BatchFlatten module.","title":"BatchFlatten.__init__(preserve_dims=1, name='batch_flatten')"},{"location":"sonnet/#args_29","text":"preserve_dims : Number of leading dimensions that will not be reshaped. For example, given an input Tensor with shape [B, H, W, C] : * preserve_dims=1 will return a Tensor with shape [B, H*W*C] . * preserve_dims=2 will return a Tensor with shape [B, H, W*C] . * preserve_dims=3 will return the input itself, shape [B, H, W, C] . * preserve_dims=4 will return a Tensor with shape [B, H, W, C, 1] . * preserve_dims =5 will throw an error on build. The preserved dimensions can be unknown at building time. name : Name of the module.","title":"Args:"},{"location":"sonnet/#batchflatten__call__inputs","text":"Connects the module into the graph, with input Tensor inputs .","title":"BatchFlatten.__call__(inputs)"},{"location":"sonnet/#args_30","text":"inputs : A Tensor of shape [b_1, b_2, ..., b_preserve_dims, b_preserve_dims+1, ...].","title":"Args:"},{"location":"sonnet/#returns_59","text":"A Tensor of shape [b_1, b_2, ..., b_preserve_dims, b_reshape_1, b_reshape_2, ...], with reshaping defined by the constructor shape parameter.","title":"Returns:"},{"location":"sonnet/#raises_55","text":"ValueError : If output shape is incompatible with input shape; or if shape array contains non numeric entries; or if shape array contains more than 1 wildcard -1; or if the input array contains unknown, non-preserved dimensions (except when the unknown dimension is the only non-preserved dimension and doesn't actually need reshaping).","title":"Raises:"},{"location":"sonnet/#batchflattenconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"BatchFlatten.connected_subgraphs"},{"location":"sonnet/#batchflattendefun","text":"Wraps this modules call method in a callable graph function.","title":"BatchFlatten.defun()"},{"location":"sonnet/#batchflattendefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"BatchFlatten.defun_wrapped"},{"location":"sonnet/#batchflattenget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"BatchFlatten.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_31","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_60","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_56","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchflattenget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"BatchFlatten.get_possible_initializer_keys()"},{"location":"sonnet/#returns_61","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#batchflattenget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"BatchFlatten.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_32","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_62","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_57","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchflattengraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"BatchFlatten.graph"},{"location":"sonnet/#batchflatteninput_shape","text":"Returns shape of input Tensor passed at last call to build .","title":"BatchFlatten.input_shape"},{"location":"sonnet/#batchflattenis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"BatchFlatten.is_connected"},{"location":"sonnet/#batchflattenlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"BatchFlatten.last_connected_subgraph"},{"location":"sonnet/#returns_63","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_58","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchflattenmodule_name","text":"Returns the name of the Module.","title":"BatchFlatten.module_name"},{"location":"sonnet/#batchflattenname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"BatchFlatten.name_scopes"},{"location":"sonnet/#batchflattennon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchFlatten.non_trainable_variables"},{"location":"sonnet/#returns_64","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_59","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchflattenscope_name","text":"Returns the full name of the Module's variable scope.","title":"BatchFlatten.scope_name"},{"location":"sonnet/#batchflattentrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchFlatten.trainable_variables"},{"location":"sonnet/#returns_65","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_60","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchflattentransposenamenone","text":"Returns transpose batch reshape.","title":"BatchFlatten.transpose(name=None)"},{"location":"sonnet/#batchflattenvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"BatchFlatten.variable_scope"},{"location":"sonnet/#returns_66","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_61","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchflattenvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchFlatten.variables"},{"location":"sonnet/#returns_67","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_62","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-batchnorm","text":"Batch normalization module, including optional affine transformation. This module maintains exponential moving averages of the mean and variance, which can be optionally used to normalize at test time. At training time, batch statistics (mean, variance) are not shared between separate connections. The moving averages are shared between separate connections. At both training and test time, the optional affine transformation ( * gamma + beta ) is shared between separate connections. This is also the case for distributed replica training, where the batch statistics are not aggregated across replicas, but the moving averages are shared globally. When connecting the module to the graph, is_training=True means that Update ops are created to update the moving averages with the current batch's statistics. Features are normalized using the current batch's statistics . The test_local_stats setting is ignored. The moving averages are not used. whereas is_training=False means that Update ops are not created. Features are normalized using either: The test batch statistics if test_local_stats=True (default). The moving averages if test_local_stats=False . Local batch statistics are used by default at test time, but the moving averages can be used by specifying a flag when connecting. One often wants to use local batch statistics at test time to track the progress while the model is trained as it would ensure that moving average updates do not affect the training curves. Once the training is finished, it's often advantageous to use moving average statistics, since it would make evaluation agnostic to the batch size, and might even lead to small improvements over the local batch statistics. You can either update the moving averages automatically by setting update_ops_collection=None or by running the ops in the given collection, by default tf.GraphKeys.UPDATE_OPS. For example, to run the updates automatically: bn = BatchNorm(update_ops_collection=None) train_net = bn(train_inputs, is_training=True) this does, however, have the effect of blocking the forwards pass of the network until the update ops have been run and may have a small performance penalty. For example, to run the updates manually: bn = BatchNorm() train_net = bn(train_inputs, is_training=True) ... update_ops = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS)) train_op = tf.group(train_op, update_ops) Then, whenever train_op is run so also are the moving average update ops. Some batch normalization caveats: Batch normalization will remove the effect of adding a bias, so e.g. use_bias=False should be used for an immediately preceding snt.Linear module. If your data batches aren't i.i.d. then batch normalization can allow your network to 'cheat' by using the batch statistics to peek at the rest of the batch. This can exhibit itself as a higher test score with test_local_stats=True than test_local_stats=False .","title":"class BatchNorm"},{"location":"sonnet/#batchnorm__init__axisnone-offsettrue-scalefalse-decay_rate0999-eps0001-initializersnone-partitionersnone-regularizersnone-update_ops_collectionupdate_ops-fusedfalse-namebatch_norm","text":"Constructs a BatchNorm module. By default reduces over all input tensor dimensions apart from the final dimension. This has the effect of treating pixels in 1D/2D/3D images as additional elements of the minibatch. If this is not the desired behaviour, the user can specify the tensor indices to reduce over with axis .","title":"BatchNorm.__init__(axis=None, offset=True, scale=False, decay_rate=0.999, eps=0.001, initializers=None, partitioners=None, regularizers=None, update_ops_collection='update_ops', fused=False, name='batch_norm')"},{"location":"sonnet/#args_33","text":"axis : Optional iterable of indices of dimensions to reduce over. By default None and all dimensions except the last are reduced over. offset : Optional boolean to specify whether or not to apply a trained component-wise bias after the batch normalization and scaling. scale : Optional boolean to specify whether or not to apply a trained component-wise scale after the batch normalization. decay_rate : Decay rate of the exponential moving averages of the mean and variance. eps : Small number to avoid dividing by zero when diving by the standard deviation. initializers : Optional dict containing ops to initialize the weights of the affine transform ( gamma and beta ). partitioners : Optional dict containing partitioners to partition the weights of the affine transform ( gamma and beta ). regularizers : Optional dict containing regularizers for the weights of the affine transform ('gamma' and 'beta'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . update_ops_collection : Name of TensorFlow variable collection to add the moving average update ops to. If None , we instead add the update ops as control dependencies of the output of the module. This may result in some slowdown, as the feed-forward of the network is now blocked. By default, tf.GraphKeys.UPDATE_OPS . fused : Use nn.fused_batch_norm if True, nn.batch_normalization otherwise. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_63","text":"KeyError : If initializers contains any keys other than gamma , beta , moving_mean or moving_variance . KeyError : If partitioners or regularizers contains any keys other than gamma or beta . TypeError : If any of the given initializers, partitioners or regularizers are not callable.","title":"Raises:"},{"location":"sonnet/#batchnorm__call__input_batch-is_training-test_local_statstrue","text":"Connects the BatchNorm module into the graph.","title":"BatchNorm.__call__(input_batch, is_training, test_local_stats=True)"},{"location":"sonnet/#args_34","text":"input_batch : A Tensor of arbitrary dimension. By default, the final dimension is not reduced over when computing the minibatch statistics. is_training : A boolean to indicate if the module should be connected in training mode, meaning the moving averages are updated. Can be a Tensor. test_local_stats : A boolean to indicate if local batch statistics should be used when is_training=False . If not, moving averages are used. By default True . Can be a Tensor.","title":"Args:"},{"location":"sonnet/#returns_68","text":"A tensor with the same shape as input_batch .","title":"Returns:"},{"location":"sonnet/#raises_64","text":"base.IncompatibleShapeError: If axis is not valid for the input shape or has negative entries. base.NotSupportedError: If input_batch has data type of tf.bfloat16 .","title":"Raises:"},{"location":"sonnet/#batchnormbeta","text":"","title":"BatchNorm.beta"},{"location":"sonnet/#batchnormconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"BatchNorm.connected_subgraphs"},{"location":"sonnet/#batchnormdefun","text":"Wraps this modules call method in a callable graph function.","title":"BatchNorm.defun()"},{"location":"sonnet/#batchnormdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"BatchNorm.defun_wrapped"},{"location":"sonnet/#batchnormgamma","text":"","title":"BatchNorm.gamma"},{"location":"sonnet/#batchnormget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"BatchNorm.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_35","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_69","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_65","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"BatchNorm.get_possible_initializer_keys()"},{"location":"sonnet/#returns_70","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#batchnormget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"BatchNorm.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_36","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_71","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_66","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"BatchNorm.graph"},{"location":"sonnet/#batchnorminitializers","text":"","title":"BatchNorm.initializers"},{"location":"sonnet/#batchnormis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"BatchNorm.is_connected"},{"location":"sonnet/#batchnormlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"BatchNorm.last_connected_subgraph"},{"location":"sonnet/#returns_72","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_67","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormmodule_name","text":"Returns the name of the Module.","title":"BatchNorm.module_name"},{"location":"sonnet/#batchnormmoving_mean","text":"","title":"BatchNorm.moving_mean"},{"location":"sonnet/#batchnormmoving_variance","text":"","title":"BatchNorm.moving_variance"},{"location":"sonnet/#batchnormname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"BatchNorm.name_scopes"},{"location":"sonnet/#batchnormnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchNorm.non_trainable_variables"},{"location":"sonnet/#returns_73","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_68","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormpartitioners","text":"","title":"BatchNorm.partitioners"},{"location":"sonnet/#batchnormregularizers","text":"","title":"BatchNorm.regularizers"},{"location":"sonnet/#batchnormscope_name","text":"Returns the full name of the Module's variable scope.","title":"BatchNorm.scope_name"},{"location":"sonnet/#batchnormtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchNorm.trainable_variables"},{"location":"sonnet/#returns_74","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_69","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"BatchNorm.variable_scope"},{"location":"sonnet/#returns_75","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_70","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchNorm.variables"},{"location":"sonnet/#returns_76","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_71","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-batchnormlstm","text":"LSTM recurrent network cell with optional peepholes, batch normalization. The base implementation is based on: http://arxiv.org/abs/1409.2329. We add forget_bias (default: 1) to the biases of the forget gate in order to reduce the scale of forgetting in the beginning of the training.","title":"class BatchNormLSTM"},{"location":"sonnet/#peep-hole-connections","text":"Peep-hole connections may optionally be used by specifying a flag in the constructor. These connections can aid increasing the precision of output timing, for more details see: https://research.google.com/pubs/archive/43905.pdf","title":"Peep-hole connections"},{"location":"sonnet/#batch-normalization","text":"The batch norm transformation (in training mode) is batchnorm(x) = gamma * (x - mean(x)) / stddev(x) + beta, where gamma is a learnt scaling factor and beta is a learnt offset. Batch normalization may optionally be used at different places in the LSTM by specifying flag(s) in the constructor. These are applied when calculating the gate activations and cell-to-hidden transformation. The set-up is based on https://arxiv.org/pdf/1603.09025.pdf","title":"Batch normalization"},{"location":"sonnet/#batch-normalization-where-to-apply","text":"Batch norm can be applied in three different places in the LSTM: (h) To the W_h h_{t-1} contribution to the gates from the previous hiddens. (x) To the W_x x_t contribution to the gates from the current input. (c) To the cell value c_t when calculating the output h_t from the cell. (The notation here is consistent with the Recurrent Batch Normalization paper). Each of these can be controlled individually, because batch norm is expensive, and not all are necessary. The paper doesn't mention the relative effects of these different batch norms; however, experimentation with a shallow LSTM for the permuted_mnist sequence task suggests that (h) is the most important and the other two can be left off. For other tasks or deeper (stacked) LSTMs, other batch norm combinations may be more effective.","title":"Batch normalization: where to apply?"},{"location":"sonnet/#batch-normalization-collecting-stats-training-vs-test","text":"When switching to testing (see LSTM.with_batch_norm_control ), we can use a mean and stddev learnt from the training data instead of using the statistics from the test data. (This both increases test accuracy because the statistics have less variance, and if the test data does not have the same distribution as the training data then we must use the training statistics to ensure the effective network does not change when switching to testing anyhow.) This does however introduces a slight subtlety. The first few time steps of the RNN tend to have varying statistics (mean and variance) before settling down to a steady value. Therefore in general, better performance is obtained by using separate statistics for the first few time steps, and then using the final set of statistics for all subsequent time steps. This is controlled by the parameter max_unique_stats . (We can't have an unbounded number of distinct statistics for both technical reasons and also for the case where test sequences are longer than anything seen in training.) You may be fine leaving it at its default value of 1. Small values (like 10) may achieve better performance on some tasks when testing with cached statistics. Attributes: state_size: Tuple of tf.TensorShape s indicating the size of state tensors. output_size: tf.TensorShape indicating the size of the core output. use_peepholes: Boolean indicating whether peephole connections are used. use_batch_norm_h: Boolean indicating whether batch norm (h) is enabled. use_batch_norm_x: Boolean indicating whether batch norm (x) is enabled. use_batch_norm_c: Boolean indicating whether batch norm (c) is enabled.","title":"Batch normalization: collecting stats (training vs test)"},{"location":"sonnet/#batchnormlstm__init__hidden_size-forget_bias10-initializersnone-partitionersnone-regularizersnone-use_peepholesfalse-use_batch_norm_htrue-use_batch_norm_xfalse-use_batch_norm_cfalse-max_unique_stats1-hidden_clip_valuenone-cell_clip_valuenone-custom_getternone-namebatch_norm_lstm","text":"Construct BatchNormLSTM .","title":"BatchNormLSTM.__init__(hidden_size, forget_bias=1.0, initializers=None, partitioners=None, regularizers=None, use_peepholes=False, use_batch_norm_h=True, use_batch_norm_x=False, use_batch_norm_c=False, max_unique_stats=1, hidden_clip_value=None, cell_clip_value=None, custom_getter=None, name='batch_norm_lstm')"},{"location":"sonnet/#args_37","text":"hidden_size : (int) Hidden size dimensionality. forget_bias : (float) Bias for the forget activation. initializers : Dict containing ops to initialize the weights. This dictionary may contain any of the keys returned by BatchNormLSTM.get_possible_initializer_keys . The gamma and beta variables control batch normalization values for different batch norm transformations inside the cell; see the paper for details. partitioners : Optional dict containing partitioners to partition the weights and biases. As a default, no partitioners are used. This dict may contain any of the keys returned by BatchNormLSTM.get_possible_initializer_keys . regularizers : Optional dict containing regularizers for the weights and biases. As a default, no regularizers are used. This dict may contain any of the keys returned by BatchNormLSTM.get_possible_initializer_keys . use_peepholes : Boolean that indicates whether peephole connections are used. use_batch_norm_h : Boolean that indicates whether to apply batch normalization at the previous_hidden - gates contribution. If you are experimenting with batch norm then this may be the most effective to use, and is enabled by default. use_batch_norm_x : Boolean that indicates whether to apply batch normalization at the input - gates contribution. use_batch_norm_c : Boolean that indicates whether to apply batch normalization at the cell - output contribution. max_unique_stats : The maximum number of steps to use unique batch norm statistics for. (See module description above for more details.) hidden_clip_value : Optional number; if set, then the LSTM hidden state vector is clipped by this value. cell_clip_value : Optional number; if set, then the LSTM cell vector is clipped by this value. custom_getter : Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. See the tf.get_variable documentation for more details. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_72","text":"KeyError : if initializers contains any keys not returned by BatchNormLSTM.get_possible_initializer_keys . KeyError : if partitioners contains any keys not returned by BatchNormLSTM.get_possible_initializer_keys . KeyError : if regularizers contains any keys not returned by BatchNormLSTM.get_possible_initializer_keys . ValueError : if a peephole initializer is passed in the initializer list, but use_peepholes is False. ValueError : if a batch norm initializer is passed in the initializer list, but batch norm is disabled. ValueError : if none of the use_batch_norm_* options are True. ValueError : if max_unique_stats is 1.","title":"Raises:"},{"location":"sonnet/#batchnormlstm__call__inputs-prev_state-is_trainingnone-test_local_statstrue","text":"Connects the LSTM module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided as inputs and state must have the same final dimension, in order for the existing variables to be the correct size for their corresponding multiplications. The batch size may differ for each connection.","title":"BatchNormLSTM.__call__(inputs, prev_state, is_training=None, test_local_stats=True)"},{"location":"sonnet/#args_38","text":"inputs : Tensor of size [batch_size, input_size] . prev_state : Tuple (prev_hidden, prev_cell), or if batch norm is enabled and max_unique_stats 1 , then (prev_hidden, prev_cell, time_step). Here, prev_hidden and prev_cell are tensors of size [batch_size, hidden_size] , and time_step is used to indicate the current RNN step. is_training : Boolean indicating whether we are in training mode (as opposed to testing mode), passed to the batch norm modules. Note to use this you must wrap the cell via the with_batch_norm_control function. test_local_stats : Boolean indicating whether to use local batch statistics in test mode. See the BatchNorm documentation for more on this.","title":"Args:"},{"location":"sonnet/#returns_77","text":"A tuple (output, next_state) where 'output' is a Tensor of size [batch_size, hidden_size] and 'next_state' is a tuple (next_hidden, next_cell) or (next_hidden, next_cell, time_step + 1), where next_hidden and next_cell have size [batch_size, hidden_size] .","title":"Returns:"},{"location":"sonnet/#raises_73","text":"ValueError : If connecting the module into the graph any time after the first time, and the inferred size of the inputs does not match previous invocations.","title":"Raises:"},{"location":"sonnet/#batchnormlstmconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"BatchNormLSTM.connected_subgraphs"},{"location":"sonnet/#batchnormlstmdefun","text":"Wraps this modules call method in a callable graph function.","title":"BatchNormLSTM.defun()"},{"location":"sonnet/#batchnormlstmdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"BatchNormLSTM.defun_wrapped"},{"location":"sonnet/#batchnormlstmget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"BatchNormLSTM.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_39","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_78","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_74","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormlstmget_possible_initializer_keysuse_peepholesfalse-use_batch_norm_htrue-use_batch_norm_xfalse-use_batch_norm_cfalse","text":"Returns the keys the dictionary of variable initializers may contain.","title":"BatchNormLSTM.get_possible_initializer_keys(use_peepholes=False, use_batch_norm_h=True, use_batch_norm_x=False, use_batch_norm_c=False)"},{"location":"sonnet/#the-set-of-all-possible-initializer-keys-are","text":"w_gates : weight for gates b_gates : bias of gates w_f_diag : weight for prev_cell - forget gate peephole w_i_diag : weight for prev_cell - input gate peephole w_o_diag : weight for prev_cell - output gate peephole gamma_h : batch norm scaling for previous_hidden - gates gamma_x : batch norm scaling for input - gates gamma_c : batch norm scaling for cell - output beta_c : batch norm bias for cell - output","title":"The set of all possible initializer keys are:"},{"location":"sonnet/#args_40","text":"cls:The class. use_peepholes : Boolean that indicates whether peephole connections are used. use_batch_norm_h : Boolean that indicates whether to apply batch normalization at the previous_hidden - gates contribution. If you are experimenting with batch norm then this may be the most effective to turn on. use_batch_norm_x : Boolean that indicates whether to apply batch normalization at the input - gates contribution. use_batch_norm_c : Boolean that indicates whether to apply batch normalization at the cell - output contribution.","title":"Args:"},{"location":"sonnet/#returns_79","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#batchnormlstmget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"BatchNormLSTM.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_41","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_80","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_75","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormlstmgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"BatchNormLSTM.graph"},{"location":"sonnet/#batchnormlstminitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone","text":"Builds the default start state tensor of zeros.","title":"BatchNormLSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None)"},{"location":"sonnet/#args_42","text":"batch_size : An int, float or scalar Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. trainable_initializers : An optional pair of initializers for the initial hidden state and cell state. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_81","text":"A tensor tuple ([batch_size, state_size], [batch_size, state_size], ?) filled with zeros, with the third entry present when batch norm is enabled with max_unique_stats 1', with value 0` (representing the time step).","title":"Returns:"},{"location":"sonnet/#batchnormlstmis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"BatchNormLSTM.is_connected"},{"location":"sonnet/#batchnormlstmlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"BatchNormLSTM.last_connected_subgraph"},{"location":"sonnet/#returns_82","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_76","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormlstmmodule_name","text":"Returns the name of the Module.","title":"BatchNormLSTM.module_name"},{"location":"sonnet/#batchnormlstmname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"BatchNormLSTM.name_scopes"},{"location":"sonnet/#batchnormlstmnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchNormLSTM.non_trainable_variables"},{"location":"sonnet/#returns_83","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_77","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormlstmoutput_size","text":"tf.TensorShape indicating the size of the core output.","title":"BatchNormLSTM.output_size"},{"location":"sonnet/#batchnormlstmscope_name","text":"Returns the full name of the Module's variable scope.","title":"BatchNormLSTM.scope_name"},{"location":"sonnet/#batchnormlstmstate_size","text":"Tuple of tf.TensorShape s indicating the size of state tensors.","title":"BatchNormLSTM.state_size"},{"location":"sonnet/#batchnormlstmtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchNormLSTM.trainable_variables"},{"location":"sonnet/#returns_84","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_78","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormlstmuse_batch_norm_c","text":"Boolean indicating whether batch norm for cell - output is enabled.","title":"BatchNormLSTM.use_batch_norm_c"},{"location":"sonnet/#batchnormlstmuse_batch_norm_h","text":"Boolean indicating whether batch norm for hidden - gates is enabled.","title":"BatchNormLSTM.use_batch_norm_h"},{"location":"sonnet/#batchnormlstmuse_batch_norm_x","text":"Boolean indicating whether batch norm for input - gates is enabled.","title":"BatchNormLSTM.use_batch_norm_x"},{"location":"sonnet/#batchnormlstmuse_peepholes","text":"Boolean indicating whether peephole connections are used.","title":"BatchNormLSTM.use_peepholes"},{"location":"sonnet/#batchnormlstmvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"BatchNormLSTM.variable_scope"},{"location":"sonnet/#returns_85","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_79","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormlstmvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchNormLSTM.variables"},{"location":"sonnet/#returns_86","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_80","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormlstmwith_batch_norm_controlis_training-test_local_statstrue","text":"Wraps this RNNCore with the additional control input to the BatchNorm s. Example usage: lstm = snt.BatchNormLSTM(4) is_training = tf.placeholder(tf.bool) rnn_input = ... my_rnn = rnn.rnn(lstm.with_batch_norm_control(is_training), rnn_input)","title":"BatchNormLSTM.with_batch_norm_control(is_training, test_local_stats=True)"},{"location":"sonnet/#args_43","text":"is_training : Boolean that indicates whether we are in training mode or testing mode. When in training mode, the batch norm statistics are taken from the given batch, and moving statistics are updated. When in testing mode, the moving statistics are not updated, and in addition if test_local_stats is False then the moving statistics are used for the batch statistics. See the BatchNorm module for more details. test_local_stats : Boolean scalar indicated whether to use local batch statistics in test mode.","title":"Args:"},{"location":"sonnet/#returns_87","text":"snt.RNNCore wrapping this class with the extra input(s) added.","title":"Returns:"},{"location":"sonnet/#batchnormlstmzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"BatchNormLSTM.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_44","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_88","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-batchnormv2","text":"Batch normalization module, including optional affine transformation. This module maintains exponential moving averages of the mean and variance, which can be optionally used to normalize at test time. At training time, batch statistics (mean, variance) are not shared between separate connections. The moving averages are shared between separate connections. At both training and test time, the optional affine transformation ( * gamma + beta ) is shared between separate connections. This is also the case for distributed replica training, where the batch statistics are not aggregated across replicas, but the moving averages are shared globally. When connecting the module to the graph, is_training=True means that Update ops are created to update the moving averages with the current batch's statistics. Features are normalized using the current batch's statistics . The test_local_stats setting is ignored. The moving averages are not used. whereas is_training=False means that Update ops are not created. Features are normalized using either: The moving averages if test_local_stats=False (default). The test batch statistics if test_local_stats=True . The moving averages are used by default at test time, but local batch statistics can be used by specifying a flag when connecting. One often wants to use local batch statistics at test time to track the progress while the model is trained as it would ensure that moving average updates do not affect the training curves. Once the training is finished, it's often advantageous to use moving average statistics, since it would make evaluation agnostic to the batch size, and might even lead to small improvements over the local batch statistics. The moving averages will be updated automatically by default, but not if update_ops_collection is provided: in that case they will only be updated when the ops in that collection are run. For example, to run the updates automatically: bn = BatchNormV2() train_net = bn(train_inputs, is_training=True) this does, however, have the effect of blocking the forwards pass of the network until the update ops have been run and may have a small performance penalty. For example, to run the updates manually: bn = BatchNormV2(update_ops_collection=tf.GraphKeys.UPDATE_OPS) train_net = bn(train_inputs, is_training=True) ... update_ops = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS)) train_op = tf.group(train_op, update_ops) Then, whenever train_op is run so also are the moving average update ops. Some batch normalization caveats: Batch normalization will remove the effect of adding a bias, so e.g. use_bias=False should be used for an immediately preceding snt.Linear module. If your data batches aren't i.i.d. then batch normalization can allow your network to 'cheat' by using the batch statistics to peek at the rest of the batch. This can exhibit itself as a higher test score with test_local_stats=True than test_local_stats=False .","title":"class BatchNormV2"},{"location":"sonnet/#batchnormv2__init__data_formatnone-offsettrue-scalefalse-decay_rate0999-eps0001-initializersnone-partitionersnone-regularizersnone-update_ops_collectionnone-fusedtrue-namebatch_norm","text":"Constructs a BatchNormV2 module. Reduces over all input tensor dimensions apart from the channel dimension. This has the effect of treating pixels in 1D/2D/3D images as additional elements of the minibatch.","title":"BatchNormV2.__init__(data_format=None, offset=True, scale=False, decay_rate=0.999, eps=0.001, initializers=None, partitioners=None, regularizers=None, update_ops_collection=None, fused=True, name='batch_norm')"},{"location":"sonnet/#args_45","text":"data_format : The data format. Can be \"NC\", \"NWC\", \"NCW\", \"NHWC\", \"NCHW\", \"NDHWC\", or \"NCDHW\". If not provided we assume the channel dimension is last. offset : Optional boolean to specify whether or not to apply a trained component-wise bias after the batch normalization and scaling. scale : Optional boolean to specify whether or not to apply a trained component-wise scale after the batch normalization. decay_rate : Decay rate of the exponential moving averages of the mean and variance. eps : Small number to avoid dividing by zero when diving by the standard deviation. initializers : Optional dict containing ops to initialize the weights of the affine transform ( gamma and beta ). partitioners : Optional dict containing partitioners to partition the weights of the affine transform ( gamma and beta ). regularizers : Optional dict containing regularizers for the weights of the affine transform (\"gamma\" and \"beta\"). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . update_ops_collection : Optional name of TensorFlow variable collection to add the moving average update ops to. If not provided, we instead add the update ops as control dependencies of the output of the module. This may result in some slowdown, as the feed-forward of the network is now blocked. fused : Use nn.fused_batch_norm if True, nn.batch_normalization otherwise. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_81","text":"KeyError : If initializers contains any keys other than gamma , beta , moving_mean or moving_variance . KeyError : If partitioners or regularizers contains any keys other than gamma or beta . TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If data_format is invalid.","title":"Raises:"},{"location":"sonnet/#batchnormv2__call__input_batch-is_training-test_local_statsfalse","text":"Connects the BatchNormV2 module into the graph.","title":"BatchNormV2.__call__(input_batch, is_training, test_local_stats=False)"},{"location":"sonnet/#args_46","text":"input_batch : A Tensor of the same dimension as len(data_format) . is_training : A boolean to indicate if the module should be connected in training mode, meaning the moving averages are updated. Can be a Tensor. test_local_stats : A boolean to indicate if local batch statistics should be used when is_training=False . If not, moving averages are used. By default False . Can be a Tensor.","title":"Args:"},{"location":"sonnet/#returns_89","text":"A tensor with the same shape as input_batch .","title":"Returns:"},{"location":"sonnet/#raises_82","text":"base.IncompatibleShapeError: If data_format is not valid for the input shape. base.NotSupportedError: If input_batch has data type of tf.bfloat16 .","title":"Raises:"},{"location":"sonnet/#batchnormv2beta","text":"","title":"BatchNormV2.beta"},{"location":"sonnet/#batchnormv2connected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"BatchNormV2.connected_subgraphs"},{"location":"sonnet/#batchnormv2defun","text":"Wraps this modules call method in a callable graph function.","title":"BatchNormV2.defun()"},{"location":"sonnet/#batchnormv2defun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"BatchNormV2.defun_wrapped"},{"location":"sonnet/#batchnormv2gamma","text":"","title":"BatchNormV2.gamma"},{"location":"sonnet/#batchnormv2get_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"BatchNormV2.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_47","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_90","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_83","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormv2get_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"BatchNormV2.get_possible_initializer_keys()"},{"location":"sonnet/#returns_91","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#batchnormv2get_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"BatchNormV2.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_48","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_92","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_84","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormv2graph","text":"Returns the Graph instance which the module is connected to, or None.","title":"BatchNormV2.graph"},{"location":"sonnet/#batchnormv2initializers","text":"","title":"BatchNormV2.initializers"},{"location":"sonnet/#batchnormv2is_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"BatchNormV2.is_connected"},{"location":"sonnet/#batchnormv2last_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"BatchNormV2.last_connected_subgraph"},{"location":"sonnet/#returns_93","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_85","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormv2module_name","text":"Returns the name of the Module.","title":"BatchNormV2.module_name"},{"location":"sonnet/#batchnormv2moving_mean","text":"","title":"BatchNormV2.moving_mean"},{"location":"sonnet/#batchnormv2moving_variance","text":"","title":"BatchNormV2.moving_variance"},{"location":"sonnet/#batchnormv2name_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"BatchNormV2.name_scopes"},{"location":"sonnet/#batchnormv2non_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchNormV2.non_trainable_variables"},{"location":"sonnet/#returns_94","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_86","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormv2partitioners","text":"","title":"BatchNormV2.partitioners"},{"location":"sonnet/#batchnormv2regularizers","text":"","title":"BatchNormV2.regularizers"},{"location":"sonnet/#batchnormv2scope_name","text":"Returns the full name of the Module's variable scope.","title":"BatchNormV2.scope_name"},{"location":"sonnet/#batchnormv2trainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchNormV2.trainable_variables"},{"location":"sonnet/#returns_95","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_87","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormv2variable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"BatchNormV2.variable_scope"},{"location":"sonnet/#returns_96","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_88","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchnormv2variables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchNormV2.variables"},{"location":"sonnet/#returns_97","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_89","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-batchreshape","text":"Reshapes input Tensor, preserving the batch dimension.","title":"class BatchReshape"},{"location":"sonnet/#batchreshape__init__shape-preserve_dims1-namebatch_reshape","text":"Constructs a BatchReshape module.","title":"BatchReshape.__init__(shape, preserve_dims=1, name='batch_reshape')"},{"location":"sonnet/#args_49","text":"shape : Shape to reshape the input Tensor to while preserving its first preserve_dims dimensions; shape can be either a tuple/list, or a callable that returns the actual shape. The callable does not need to be ready to return something meaningful at construction time, but it will be required to be able to do so when the module is connected to the graph. When the special value -1 appears in shape the corresponding size is automatically inferred. Note that -1 can only appear once in shape . To flatten all non-batch dimensions, the snt.BatchFlatten module can also be used. preserve_dims : Number of leading dimensions that will not be reshaped. For example, given an input Tensor with shape [B, H, W, C, D] , and argument shape equal to (-1, D) : * preserve_dims=1 will return a Tensor with shape [B, H*W*C, D] . * preserve_dims=2 will return a Tensor with shape [B, H, W*C, D] . * preserve_dims=3 will return a Tensor with shape [B, H, W, C, D] . * preserve_dims=4 will return a Tensor with shape [B, H, W, C, 1, D] . * preserve_dims =5 will throw an error on build unless D=1. The preserved dimensions can be unknown at building time. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_90","text":"ValueError : If preserve_dims = 0 .","title":"Raises:"},{"location":"sonnet/#batchreshape__call__inputs","text":"Connects the module into the graph, with input Tensor inputs .","title":"BatchReshape.__call__(inputs)"},{"location":"sonnet/#args_50","text":"inputs : A Tensor of shape [b_1, b_2, ..., b_preserve_dims, b_preserve_dims+1, ...].","title":"Args:"},{"location":"sonnet/#returns_98","text":"A Tensor of shape [b_1, b_2, ..., b_preserve_dims, b_reshape_1, b_reshape_2, ...], with reshaping defined by the constructor shape parameter.","title":"Returns:"},{"location":"sonnet/#raises_91","text":"ValueError : If output shape is incompatible with input shape; or if shape array contains non numeric entries; or if shape array contains more than 1 wildcard -1; or if the input array contains unknown, non-preserved dimensions (except when the unknown dimension is the only non-preserved dimension and doesn't actually need reshaping).","title":"Raises:"},{"location":"sonnet/#batchreshapeconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"BatchReshape.connected_subgraphs"},{"location":"sonnet/#batchreshapedefun","text":"Wraps this modules call method in a callable graph function.","title":"BatchReshape.defun()"},{"location":"sonnet/#batchreshapedefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"BatchReshape.defun_wrapped"},{"location":"sonnet/#batchreshapeget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"BatchReshape.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_51","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_99","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_92","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchreshapeget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"BatchReshape.get_possible_initializer_keys()"},{"location":"sonnet/#returns_100","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#batchreshapeget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"BatchReshape.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_52","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_101","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_93","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchreshapegraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"BatchReshape.graph"},{"location":"sonnet/#batchreshapeinput_shape","text":"Returns shape of input Tensor passed at last call to build .","title":"BatchReshape.input_shape"},{"location":"sonnet/#batchreshapeis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"BatchReshape.is_connected"},{"location":"sonnet/#batchreshapelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"BatchReshape.last_connected_subgraph"},{"location":"sonnet/#returns_102","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_94","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchreshapemodule_name","text":"Returns the name of the Module.","title":"BatchReshape.module_name"},{"location":"sonnet/#batchreshapename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"BatchReshape.name_scopes"},{"location":"sonnet/#batchreshapenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchReshape.non_trainable_variables"},{"location":"sonnet/#returns_103","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_95","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchreshapescope_name","text":"Returns the full name of the Module's variable scope.","title":"BatchReshape.scope_name"},{"location":"sonnet/#batchreshapetrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchReshape.trainable_variables"},{"location":"sonnet/#returns_104","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_96","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchreshapetransposenamenone","text":"Returns transpose batch reshape.","title":"BatchReshape.transpose(name=None)"},{"location":"sonnet/#batchreshapevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"BatchReshape.variable_scope"},{"location":"sonnet/#returns_105","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_97","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#batchreshapevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BatchReshape.variables"},{"location":"sonnet/#returns_106","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_98","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-bidirectionalrnn","text":"Bidirectional RNNCore that processes the sequence forwards and backwards. Based upon the encoder implementation in: https://arxiv.org/abs/1409.0473 This interface of this module is different than the typical ones found in the RNNCore family. The primary difference is that it is pre-conditioned on the full input sequence in order to produce a full sequence of outputs and states concatenated along the feature dimension among the forward and backward cores.","title":"class BidirectionalRNN"},{"location":"sonnet/#bidirectionalrnn__init__forward_core-backward_core-namebidir_rnn","text":"Construct a Bidirectional RNN core.","title":"BidirectionalRNN.__init__(forward_core, backward_core, name='bidir_rnn')"},{"location":"sonnet/#args_53","text":"forward_core : callable RNNCore module that computes forward states. backward_core : callable RNNCore module that computes backward states. name : name of the module.","title":"Args:"},{"location":"sonnet/#raises_99","text":"ValueError : if not all the modules are recurrent.","title":"Raises:"},{"location":"sonnet/#bidirectionalrnn__call__input_sequence-state","text":"Connects the BidirectionalRNN module into the graph.","title":"BidirectionalRNN.__call__(input_sequence, state)"},{"location":"sonnet/#args_54","text":"input_sequence : tensor (time, batch, [feature_1, ..]). It must be time_major. state : tuple of states for the forward and backward cores.","title":"Args:"},{"location":"sonnet/#returns_107","text":"A dict with forward/backard states and output sequences: \"outputs\":{ \"forward\": ..., \"backward\": ...}, \"state\": { \"forward\": ..., \"backward\": ...}","title":"Returns:"},{"location":"sonnet/#raises_100","text":"ValueError : in case time dimension is not statically known.","title":"Raises:"},{"location":"sonnet/#bidirectionalrnnconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"BidirectionalRNN.connected_subgraphs"},{"location":"sonnet/#bidirectionalrnndefun","text":"Wraps this modules call method in a callable graph function.","title":"BidirectionalRNN.defun()"},{"location":"sonnet/#bidirectionalrnndefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"BidirectionalRNN.defun_wrapped"},{"location":"sonnet/#bidirectionalrnnget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"BidirectionalRNN.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_55","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_108","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_101","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#bidirectionalrnnget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"BidirectionalRNN.get_possible_initializer_keys()"},{"location":"sonnet/#returns_109","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#bidirectionalrnnget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"BidirectionalRNN.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_56","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_110","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_102","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#bidirectionalrnngraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"BidirectionalRNN.graph"},{"location":"sonnet/#bidirectionalrnninitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone","text":"Builds the default start state for a BidirectionalRNN. The Bidirectional RNN flattens the states of its forward and backward cores and concatentates them.","title":"BidirectionalRNN.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None)"},{"location":"sonnet/#args_57","text":"batch_size : An int, float or scalar Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_111","text":"Tuple of initial states from forward and backward RNNs.","title":"Returns:"},{"location":"sonnet/#bidirectionalrnnis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"BidirectionalRNN.is_connected"},{"location":"sonnet/#bidirectionalrnnlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"BidirectionalRNN.last_connected_subgraph"},{"location":"sonnet/#returns_112","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_103","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#bidirectionalrnnmodule_name","text":"Returns the name of the Module.","title":"BidirectionalRNN.module_name"},{"location":"sonnet/#bidirectionalrnnname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"BidirectionalRNN.name_scopes"},{"location":"sonnet/#bidirectionalrnnnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BidirectionalRNN.non_trainable_variables"},{"location":"sonnet/#returns_113","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_104","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#bidirectionalrnnoutput_size","text":"Flattened output size of cores.","title":"BidirectionalRNN.output_size"},{"location":"sonnet/#bidirectionalrnnscope_name","text":"Returns the full name of the Module's variable scope.","title":"BidirectionalRNN.scope_name"},{"location":"sonnet/#bidirectionalrnnstate_size","text":"Flattened state size of cores.","title":"BidirectionalRNN.state_size"},{"location":"sonnet/#bidirectionalrnntrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BidirectionalRNN.trainable_variables"},{"location":"sonnet/#returns_114","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_105","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#bidirectionalrnnvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"BidirectionalRNN.variable_scope"},{"location":"sonnet/#returns_115","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_106","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#bidirectionalrnnvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"BidirectionalRNN.variables"},{"location":"sonnet/#returns_116","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_107","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-causalconv1d","text":"1D convolution module, including optional bias. This is deprecated, please use the padding=CAUSAL argument to Conv1D. This acts as a light wrapper around _ConvND ensuring that the outputs at index i only depend on indices smaller than i (also known as a causal convolution). For further details on the theoretical background, refer to: https://arxiv.org/abs/1610.10099","title":"class CausalConv1D"},{"location":"sonnet/#causalconv1d__init__output_channels-kernel_shape-stride1-rate1-use_biastrue-initializersnone-partitionersnone-regularizersnone-masknone-paddingcausal-data_formatnwc-custom_getternone-namecausal_conv_1d","text":"Constructs a CausalConv1D module. This is deprecated, please use the padding=CAUSAL argument to Conv1D.","title":"CausalConv1D.__init__(output_channels, kernel_shape, stride=1, rate=1, use_bias=True, initializers=None, partitioners=None, regularizers=None, mask=None, padding='CAUSAL', data_format='NWC', custom_getter=None, name='causal_conv_1d')"},{"location":"sonnet/#args_58","text":"output_channels : Number of output channels. output_channels can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_channels can be called, returning an integer, when build is called. kernel_shape : Sequence of kernel sizes (of size 1), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 1), or integer that is used to define stride in all dimensions. rate : Sequence of dilation rates (of size 1), or integer that is used to define dilation rate in all dimensions. 1 corresponds to standard convolution, rate 1 corresponds to dilated convolution. Cannot be 1 if any of stride is also 1. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializer for the weights is a truncated normal initializer, which is commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . mask : A convertible to a 3D tensor which is multiplied component-wise with the weights (Optional). padding : Padding algorithm. Should be snt.CAUSAL . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NWC), or the second dimension (NCW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_108","text":"base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. base.IncompatibleShapeError: If the given rate is not an integer; or if the given rate is not a sequence of two integers. base.IncompatibleShapeError: If a mask is a TensorFlow Tensor with a not fully defined shape. base.NotSupportedError: If rate in any dimension and the stride in any dimension are simultaneously 1. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. TypeError : If mask is given and it is not convertible to a Tensor. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_1D_DATA_FORMATS ).","title":"Raises:"},{"location":"sonnet/#causalconv1d__call__inputs","text":"Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection.","title":"CausalConv1D.__call__(inputs)"},{"location":"sonnet/#args_59","text":"inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Args:"},{"location":"sonnet/#returns_117","text":"A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels].","title":"Returns:"},{"location":"sonnet/#raises_109","text":"ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Raises:"},{"location":"sonnet/#causalconv1db","text":"Returns the Variable containing the bias.","title":"CausalConv1D.b"},{"location":"sonnet/#returns_118","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_110","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias.","title":"Raises:"},{"location":"sonnet/#causalconv1dclonenamenone","text":"Returns a cloned _ConvND module.","title":"CausalConv1D.clone(name=None)"},{"location":"sonnet/#args_60","text":"name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name .","title":"Args:"},{"location":"sonnet/#returns_119","text":"A copy of the current class.","title":"Returns:"},{"location":"sonnet/#causalconv1dconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"CausalConv1D.connected_subgraphs"},{"location":"sonnet/#causalconv1dconv_op_padding","text":"Returns the padding algorithm used for the underlying convolution op.","title":"CausalConv1D.conv_op_padding"},{"location":"sonnet/#causalconv1ddata_format","text":"Returns the data format.","title":"CausalConv1D.data_format"},{"location":"sonnet/#causalconv1ddefun","text":"Wraps this modules call method in a callable graph function.","title":"CausalConv1D.defun()"},{"location":"sonnet/#causalconv1ddefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"CausalConv1D.defun_wrapped"},{"location":"sonnet/#causalconv1dget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"CausalConv1D.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_61","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_120","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_111","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#causalconv1dget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"CausalConv1D.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_121","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#causalconv1dget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"CausalConv1D.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_62","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_122","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_112","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#causalconv1dgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"CausalConv1D.graph"},{"location":"sonnet/#causalconv1dhas_bias","text":"Returns True if bias Variable is present in the module.","title":"CausalConv1D.has_bias"},{"location":"sonnet/#causalconv1dinitializers","text":"Returns the initializers dictionary.","title":"CausalConv1D.initializers"},{"location":"sonnet/#causalconv1dinput_channels","text":"Returns the number of input channels.","title":"CausalConv1D.input_channels"},{"location":"sonnet/#causalconv1dinput_shape","text":"Returns the input shape.","title":"CausalConv1D.input_shape"},{"location":"sonnet/#causalconv1dis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"CausalConv1D.is_connected"},{"location":"sonnet/#causalconv1dkernel_shape","text":"Returns the kernel shape.","title":"CausalConv1D.kernel_shape"},{"location":"sonnet/#causalconv1dlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"CausalConv1D.last_connected_subgraph"},{"location":"sonnet/#returns_123","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_113","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#causalconv1dmask","text":"Returns the mask.","title":"CausalConv1D.mask"},{"location":"sonnet/#causalconv1dmodule_name","text":"Returns the name of the Module.","title":"CausalConv1D.module_name"},{"location":"sonnet/#causalconv1dname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"CausalConv1D.name_scopes"},{"location":"sonnet/#causalconv1dnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"CausalConv1D.non_trainable_variables"},{"location":"sonnet/#returns_124","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_114","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#causalconv1doutput_channels","text":"Returns the number of output channels.","title":"CausalConv1D.output_channels"},{"location":"sonnet/#causalconv1dpadding","text":"Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension.","title":"CausalConv1D.padding"},{"location":"sonnet/#returns_125","text":"The padding algorithm used, if this is the same for all dimensions.","title":"Returns:"},{"location":"sonnet/#raises_115","text":"ValueError : If different padding algorithms are used for different dimensions.","title":"Raises:"},{"location":"sonnet/#causalconv1dpaddings","text":"Returns a tuple with the padding algorithm used for each dimension.","title":"CausalConv1D.paddings"},{"location":"sonnet/#causalconv1dpartitioners","text":"Returns the partitioners dictionary.","title":"CausalConv1D.partitioners"},{"location":"sonnet/#causalconv1drate","text":"Returns the dilation rate.","title":"CausalConv1D.rate"},{"location":"sonnet/#causalconv1dregularizers","text":"Returns the regularizers dictionary.","title":"CausalConv1D.regularizers"},{"location":"sonnet/#causalconv1dscope_name","text":"Returns the full name of the Module's variable scope.","title":"CausalConv1D.scope_name"},{"location":"sonnet/#causalconv1dstride","text":"Returns the stride.","title":"CausalConv1D.stride"},{"location":"sonnet/#causalconv1dtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"CausalConv1D.trainable_variables"},{"location":"sonnet/#returns_126","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_116","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#causalconv1dvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"CausalConv1D.variable_scope"},{"location":"sonnet/#returns_127","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_117","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#causalconv1dvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"CausalConv1D.variables"},{"location":"sonnet/#returns_128","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_118","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#causalconv1dw","text":"Returns the Variable containing the weight matrix.","title":"CausalConv1D.w"},{"location":"sonnet/#class-concatlinear","text":"Linear transformation of a number of concatenated inputs. This class ensures that at initialisation, the relative importance of all inputs are similar even if they have very different sizes. This assumes that all inputs have roughly the same range of values. For example, the following code also concatenates a list of inputs and applies a linear transform: inp = tf.concat(input_list, axis=-1) return snt.Linear(output_size)(inp) The issue with the above code is that if input_list is made of two Tensors of very different shapes such as [batch_size, 1] and [batch_size, 128] , then almost no signal will be received from the first Tensor. This class works around this problem by using a weight matrix with relatively larger coefficients for the first Tensor than for the second one.","title":"class ConcatLinear"},{"location":"sonnet/#concatlinear__init__output_size-use_biastrue-initializersnone-partitionersnone-regularizersnone-custom_getternone-nameconcat_linear","text":"Constructs a ConcatLinear module.","title":"ConcatLinear.__init__(output_size, use_bias=True, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='concat_linear')"},{"location":"sonnet/#args_63","text":"output_size : Output dimensionality. output_size can be either an integer or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_size can be called, returning an integer, when build is called. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing initializers to initialize the weights (with key 'w') or biases (with key 'b'). The default initializer for the weights is a truncated normal initializer, which is commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the weights (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#concatlinear__call__inputs_list","text":"Connects the module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided here must have the same final dimensions as when called the first time, in order for the existing variables to be the correct size for the multiplication. The batch size may differ for each connection.","title":"ConcatLinear.__call__(inputs_list)"},{"location":"sonnet/#args_64","text":"inputs_list : A list of 2D Tensors of rank 2, with leading batch dimension.","title":"Args:"},{"location":"sonnet/#returns_129","text":"A 2D Tensor of size [batch_size, output_size].","title":"Returns:"},{"location":"sonnet/#concatlinearconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"ConcatLinear.connected_subgraphs"},{"location":"sonnet/#concatlineardefun","text":"Wraps this modules call method in a callable graph function.","title":"ConcatLinear.defun()"},{"location":"sonnet/#concatlineardefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"ConcatLinear.defun_wrapped"},{"location":"sonnet/#concatlinearget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"ConcatLinear.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_65","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_130","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_119","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#concatlinearget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"ConcatLinear.get_possible_initializer_keys()"},{"location":"sonnet/#returns_131","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#concatlinearget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"ConcatLinear.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_66","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_132","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_120","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#concatlineargraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"ConcatLinear.graph"},{"location":"sonnet/#concatlinearis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"ConcatLinear.is_connected"},{"location":"sonnet/#concatlinearlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"ConcatLinear.last_connected_subgraph"},{"location":"sonnet/#returns_133","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_121","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#concatlinearmodule_name","text":"Returns the name of the Module.","title":"ConcatLinear.module_name"},{"location":"sonnet/#concatlinearname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"ConcatLinear.name_scopes"},{"location":"sonnet/#concatlinearnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"ConcatLinear.non_trainable_variables"},{"location":"sonnet/#returns_134","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_122","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#concatlinearscope_name","text":"Returns the full name of the Module's variable scope.","title":"ConcatLinear.scope_name"},{"location":"sonnet/#concatlineartrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"ConcatLinear.trainable_variables"},{"location":"sonnet/#returns_135","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_123","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#concatlinearvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"ConcatLinear.variable_scope"},{"location":"sonnet/#returns_136","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_124","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#concatlinearvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"ConcatLinear.variables"},{"location":"sonnet/#returns_137","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_125","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-conv1d","text":"1D convolution module, including optional bias. This acts as a light wrapper around the class _ConvND .","title":"class Conv1D"},{"location":"sonnet/#conv1d__init__output_channels-kernel_shape-stride1-rate1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-masknone-data_formatnwc-custom_getternone-nameconv_1d","text":"Constructs a Conv1D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution","title":"Conv1D.__init__(output_channels, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, mask=None, data_format='NWC', custom_getter=None, name='conv_1d')"},{"location":"sonnet/#args_67","text":"output_channels : Number of output channels. output_channels can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_channels can be called, returning an integer, when build is called. kernel_shape : Sequence of kernel sizes (of size 1), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 1), or integer that is used to define stride in all dimensions. rate : Sequence of dilation rates (of size 1), or integer that is used to define dilation rate in all dimensions. 1 corresponds to standard convolution, rate 1 corresponds to dilated convolution. Cannot be 1 if any of stride is also 1. padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 1. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializer for the weights is a truncated normal initializer, which is commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . mask : A convertible to a 3D tensor which is multiplied component-wise with the weights (Optional). data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NWC), or the second dimension (NCW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_126","text":"base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. base.IncompatibleShapeError: If the given rate is not an integer; or if the given rate is not a sequence of two integers. base.IncompatibleShapeError: If a mask is a TensorFlow Tensor with a not fully defined shape. base.NotSupportedError: If rate in any dimension and the stride in any dimension are simultaneously 1. ValueError : If the given padding is not snt.VALID or snt.SAME . KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. TypeError : If mask is given and it is not convertible to a Tensor. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_1D_DATA_FORMATS ).","title":"Raises:"},{"location":"sonnet/#conv1d__call__inputs","text":"Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection.","title":"Conv1D.__call__(inputs)"},{"location":"sonnet/#args_68","text":"inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Args:"},{"location":"sonnet/#returns_138","text":"A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels].","title":"Returns:"},{"location":"sonnet/#raises_127","text":"ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Raises:"},{"location":"sonnet/#conv1db","text":"Returns the Variable containing the bias.","title":"Conv1D.b"},{"location":"sonnet/#returns_139","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_128","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias.","title":"Raises:"},{"location":"sonnet/#conv1dclonenamenone","text":"Returns a cloned _ConvND module.","title":"Conv1D.clone(name=None)"},{"location":"sonnet/#args_69","text":"name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name .","title":"Args:"},{"location":"sonnet/#returns_140","text":"A copy of the current class.","title":"Returns:"},{"location":"sonnet/#conv1dconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Conv1D.connected_subgraphs"},{"location":"sonnet/#conv1dconv_op_padding","text":"Returns the padding algorithm used for the underlying convolution op.","title":"Conv1D.conv_op_padding"},{"location":"sonnet/#conv1ddata_format","text":"Returns the data format.","title":"Conv1D.data_format"},{"location":"sonnet/#conv1ddefun","text":"Wraps this modules call method in a callable graph function.","title":"Conv1D.defun()"},{"location":"sonnet/#conv1ddefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Conv1D.defun_wrapped"},{"location":"sonnet/#conv1dget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Conv1D.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_70","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_141","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_129","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Conv1D.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_142","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#conv1dget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"Conv1D.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_71","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_143","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_130","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Conv1D.graph"},{"location":"sonnet/#conv1dhas_bias","text":"Returns True if bias Variable is present in the module.","title":"Conv1D.has_bias"},{"location":"sonnet/#conv1dinitializers","text":"Returns the initializers dictionary.","title":"Conv1D.initializers"},{"location":"sonnet/#conv1dinput_channels","text":"Returns the number of input channels.","title":"Conv1D.input_channels"},{"location":"sonnet/#conv1dinput_shape","text":"Returns the input shape.","title":"Conv1D.input_shape"},{"location":"sonnet/#conv1dis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Conv1D.is_connected"},{"location":"sonnet/#conv1dkernel_shape","text":"Returns the kernel shape.","title":"Conv1D.kernel_shape"},{"location":"sonnet/#conv1dlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Conv1D.last_connected_subgraph"},{"location":"sonnet/#returns_144","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_131","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dmask","text":"Returns the mask.","title":"Conv1D.mask"},{"location":"sonnet/#conv1dmodule_name","text":"Returns the name of the Module.","title":"Conv1D.module_name"},{"location":"sonnet/#conv1dname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Conv1D.name_scopes"},{"location":"sonnet/#conv1dnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv1D.non_trainable_variables"},{"location":"sonnet/#returns_145","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_132","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1doutput_channels","text":"Returns the number of output channels.","title":"Conv1D.output_channels"},{"location":"sonnet/#conv1dpadding","text":"Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension.","title":"Conv1D.padding"},{"location":"sonnet/#returns_146","text":"The padding algorithm used, if this is the same for all dimensions.","title":"Returns:"},{"location":"sonnet/#raises_133","text":"ValueError : If different padding algorithms are used for different dimensions.","title":"Raises:"},{"location":"sonnet/#conv1dpaddings","text":"Returns a tuple with the padding algorithm used for each dimension.","title":"Conv1D.paddings"},{"location":"sonnet/#conv1dpartitioners","text":"Returns the partitioners dictionary.","title":"Conv1D.partitioners"},{"location":"sonnet/#conv1drate","text":"Returns the dilation rate.","title":"Conv1D.rate"},{"location":"sonnet/#conv1dregularizers","text":"Returns the regularizers dictionary.","title":"Conv1D.regularizers"},{"location":"sonnet/#conv1dscope_name","text":"Returns the full name of the Module's variable scope.","title":"Conv1D.scope_name"},{"location":"sonnet/#conv1dstride","text":"Returns the stride.","title":"Conv1D.stride"},{"location":"sonnet/#conv1dtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv1D.trainable_variables"},{"location":"sonnet/#returns_147","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_134","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dtransposenamenone","text":"Returns matching Conv1DTranspose module.","title":"Conv1D.transpose(name=None)"},{"location":"sonnet/#args_72","text":"name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.name .","title":"Args:"},{"location":"sonnet/#returns_148","text":"Conv1DTranspose module.","title":"Returns:"},{"location":"sonnet/#raises_135","text":"base.NotSupportedError: If rate in any dimension 1.","title":"Raises:"},{"location":"sonnet/#conv1dvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Conv1D.variable_scope"},{"location":"sonnet/#returns_149","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_136","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv1D.variables"},{"location":"sonnet/#returns_150","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_137","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dw","text":"Returns the Variable containing the weight matrix.","title":"Conv1D.w"},{"location":"sonnet/#class-conv1dlstm","text":"1D convolutional LSTM.","title":"class Conv1DLSTM"},{"location":"sonnet/#conv1dlstm__init__nameconv_1d_lstm-kwargs","text":"Construct Conv1DLSTM. See snt.ConvLSTM for more details.","title":"Conv1DLSTM.__init__(name='conv_1d_lstm', **kwargs)"},{"location":"sonnet/#conv1dlstm__call__inputs-state","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"Conv1DLSTM.__call__(inputs, state)"},{"location":"sonnet/#args_73","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_151","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#conv1dlstmconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Conv1DLSTM.connected_subgraphs"},{"location":"sonnet/#conv1dlstmconvolutions","text":"","title":"Conv1DLSTM.convolutions"},{"location":"sonnet/#conv1dlstmdefun","text":"Wraps this modules call method in a callable graph function.","title":"Conv1DLSTM.defun()"},{"location":"sonnet/#conv1dlstmdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Conv1DLSTM.defun_wrapped"},{"location":"sonnet/#conv1dlstmget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Conv1DLSTM.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_74","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_152","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_138","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dlstmget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Conv1DLSTM.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_153","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#conv1dlstmget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"Conv1DLSTM.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_75","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_154","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_139","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dlstmgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Conv1DLSTM.graph"},{"location":"sonnet/#conv1dlstminitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs","text":"Builds the default start state for an RNNCore.","title":"Conv1DLSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)"},{"location":"sonnet/#args_76","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_155","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_140","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#conv1dlstmis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Conv1DLSTM.is_connected"},{"location":"sonnet/#conv1dlstmlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Conv1DLSTM.last_connected_subgraph"},{"location":"sonnet/#returns_156","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_141","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dlstmmodule_name","text":"Returns the name of the Module.","title":"Conv1DLSTM.module_name"},{"location":"sonnet/#conv1dlstmname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Conv1DLSTM.name_scopes"},{"location":"sonnet/#conv1dlstmnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv1DLSTM.non_trainable_variables"},{"location":"sonnet/#returns_157","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_142","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dlstmoutput_size","text":"tf.TensorShape indicating the size of the core output.","title":"Conv1DLSTM.output_size"},{"location":"sonnet/#conv1dlstmscope_name","text":"Returns the full name of the Module's variable scope.","title":"Conv1DLSTM.scope_name"},{"location":"sonnet/#conv1dlstmstate_size","text":"Tuple of tf.TensorShape s indicating the size of state tensors.","title":"Conv1DLSTM.state_size"},{"location":"sonnet/#conv1dlstmtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv1DLSTM.trainable_variables"},{"location":"sonnet/#returns_158","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_143","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dlstmuse_layer_norm","text":"Boolean indicating whether layer norm is enabled.","title":"Conv1DLSTM.use_layer_norm"},{"location":"sonnet/#conv1dlstmvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Conv1DLSTM.variable_scope"},{"location":"sonnet/#returns_159","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_144","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dlstmvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv1DLSTM.variables"},{"location":"sonnet/#returns_160","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_145","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dlstmzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"Conv1DLSTM.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_77","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_161","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-conv1dtranspose","text":"1D transposed / reverse / up 1D convolution module, including bias. This performs a 1D transpose convolution by lightly wrapping the TensorFlow op tf.nn.conv2d_transpose , setting the size of the height dimension of the image to 1.","title":"class Conv1DTranspose"},{"location":"sonnet/#conv1dtranspose__init__output_channels-output_shapenone-kernel_shapenone-stride1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatnwc-custom_getternone-nameconv_1d_transpose","text":"Constructs a Conv1DTranspose module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution","title":"Conv1DTranspose.__init__(output_channels, output_shape=None, kernel_shape=None, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NWC', custom_getter=None, name='conv_1d_transpose')"},{"location":"sonnet/#args_78","text":"output_channels : Number of output channels. Can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure output_channels can be called, returning an integer, when build is called. output_shape : Output shape of transpose convolution. Can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_shape can be called, returning an iterable of format (out_length) when build is called. If a None value is given, a default shape is automatically calculated (see docstring of _default_transpose_size function for more details). kernel_shape : Sequence of kernel sizes (of size 1), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 1), or integer that is used to define stride in all dimensions. padding : Padding algorithm, either snt.SAME or snt.VALID . use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NWC), or the second dimension (NCW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_146","text":"base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two or four integers. ValueError : If the given padding is not snt.VALID or snt.SAME . ValueError : If the given kernel_shape is None . KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_1D_DATA_FORMATS ).","title":"Raises:"},{"location":"sonnet/#conv1dtranspose__call__inputs","text":"Connects the _ConvNDTranspose module into the graph. If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same final N dimensions, in order for the existing variables to be the correct size for the multiplication. The batch size may differ for each connection.","title":"Conv1DTranspose.__call__(inputs)"},{"location":"sonnet/#args_79","text":"inputs : A Tensor of shape data_format and of type tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Args:"},{"location":"sonnet/#returns_162","text":"A Tensor of shape data_format and of type tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Returns:"},{"location":"sonnet/#raises_147","text":"ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If output_shape is an iterable and is not in the format (out_height, out_width) . TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Raises:"},{"location":"sonnet/#conv1dtransposeb","text":"Returns the Variable containing the bias.","title":"Conv1DTranspose.b"},{"location":"sonnet/#returns_163","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_148","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias.","title":"Raises:"},{"location":"sonnet/#conv1dtransposeconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Conv1DTranspose.connected_subgraphs"},{"location":"sonnet/#conv1dtransposeconv_op_padding","text":"Returns the padding algorithm used for the underlying convolution op.","title":"Conv1DTranspose.conv_op_padding"},{"location":"sonnet/#conv1dtransposedefun","text":"Wraps this modules call method in a callable graph function.","title":"Conv1DTranspose.defun()"},{"location":"sonnet/#conv1dtransposedefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Conv1DTranspose.defun_wrapped"},{"location":"sonnet/#conv1dtransposeget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Conv1DTranspose.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_80","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_164","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_149","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dtransposeget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Conv1DTranspose.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_165","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#conv1dtransposeget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"Conv1DTranspose.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_81","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_166","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_150","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dtransposegraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Conv1DTranspose.graph"},{"location":"sonnet/#conv1dtransposehas_bias","text":"Returns True if bias Variable is present in the module.","title":"Conv1DTranspose.has_bias"},{"location":"sonnet/#conv1dtransposeinitializers","text":"Returns the initializers dictionary.","title":"Conv1DTranspose.initializers"},{"location":"sonnet/#conv1dtransposeinput_channels","text":"Returns the number of input channels.","title":"Conv1DTranspose.input_channels"},{"location":"sonnet/#conv1dtransposeinput_shape","text":"Returns the input shape.","title":"Conv1DTranspose.input_shape"},{"location":"sonnet/#conv1dtransposeis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Conv1DTranspose.is_connected"},{"location":"sonnet/#conv1dtransposekernel_shape","text":"Returns the kernel shape.","title":"Conv1DTranspose.kernel_shape"},{"location":"sonnet/#conv1dtransposelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Conv1DTranspose.last_connected_subgraph"},{"location":"sonnet/#returns_167","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_151","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dtransposemodule_name","text":"Returns the name of the Module.","title":"Conv1DTranspose.module_name"},{"location":"sonnet/#conv1dtransposename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Conv1DTranspose.name_scopes"},{"location":"sonnet/#conv1dtransposenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv1DTranspose.non_trainable_variables"},{"location":"sonnet/#returns_168","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_152","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dtransposeoutput_channels","text":"Returns the number of output channels.","title":"Conv1DTranspose.output_channels"},{"location":"sonnet/#conv1dtransposeoutput_shape","text":"Returns the output shape.","title":"Conv1DTranspose.output_shape"},{"location":"sonnet/#conv1dtransposepadding","text":"Returns the padding algorithm.","title":"Conv1DTranspose.padding"},{"location":"sonnet/#conv1dtransposepartitioners","text":"Returns the partitioners dictionary.","title":"Conv1DTranspose.partitioners"},{"location":"sonnet/#conv1dtransposeregularizers","text":"Returns the regularizers dictionary.","title":"Conv1DTranspose.regularizers"},{"location":"sonnet/#conv1dtransposescope_name","text":"Returns the full name of the Module's variable scope.","title":"Conv1DTranspose.scope_name"},{"location":"sonnet/#conv1dtransposestride","text":"Returns the stride.","title":"Conv1DTranspose.stride"},{"location":"sonnet/#conv1dtransposetrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv1DTranspose.trainable_variables"},{"location":"sonnet/#returns_169","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_153","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dtransposetransposenamenone","text":"Returns matching Conv1D module.","title":"Conv1DTranspose.transpose(name=None)"},{"location":"sonnet/#args_82","text":"name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.name .","title":"Args:"},{"location":"sonnet/#returns_170","text":"Conv1D module.","title":"Returns:"},{"location":"sonnet/#conv1dtransposevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Conv1DTranspose.variable_scope"},{"location":"sonnet/#returns_171","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_154","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dtransposevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv1DTranspose.variables"},{"location":"sonnet/#returns_172","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_155","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv1dtransposew","text":"Returns the Variable containing the weight matrix.","title":"Conv1DTranspose.w"},{"location":"sonnet/#class-conv2d","text":"Spatial convolution and dilated convolution module, including bias. This acts as a light wrapper around the class _ConvND .","title":"class Conv2D"},{"location":"sonnet/#conv2d__init__output_channels-kernel_shape-stride1-rate1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-masknone-data_formatnhwc-custom_getternone-nameconv_2d","text":"Constructs a Conv2D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution","title":"Conv2D.__init__(output_channels, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, mask=None, data_format='NHWC', custom_getter=None, name='conv_2d')"},{"location":"sonnet/#args_83","text":"output_channels : Number of output channels. output_channels can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_channels can be called, returning an integer, when build is called. kernel_shape : Sequence of kernel sizes (of size 2), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 2), or integer that is used to define stride in all dimensions. rate : Sequence of dilation rates (of size 2), or integer that is used to define dilation rate in all dimensions. 1 corresponds to standard 2D convolution, rate 1 corresponds to dilated convolution. Cannot be 1 if any of stride is also 1. padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 2. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializer for the weights is a truncated normal initializer, which is commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . mask : A convertible to a 4D tensor which is multiplied component-wise with the weights (Optional). data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NHWC), or the second dimension (NCHW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_156","text":"base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. base.IncompatibleShapeError: If the given rate is not an integer; or if the given rate is not a sequence of two integers. base.IncompatibleShapeError: If a mask is given and its rank is neither 2 nor 4, or if it is a TensorFlow Tensor with a not fully defined shape. base.NotSupportedError: If rate in any dimension and the stride in any dimension are simultaneously 1. ValueError : If the given padding is not snt.VALID , snt.SAME , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a sequence of these. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. TypeError : If mask is given and it is not convertible to a Tensor. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_2D_DATA_FORMATS ).","title":"Raises:"},{"location":"sonnet/#conv2d__call__inputs","text":"Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection.","title":"Conv2D.__call__(inputs)"},{"location":"sonnet/#args_84","text":"inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Args:"},{"location":"sonnet/#returns_173","text":"A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels].","title":"Returns:"},{"location":"sonnet/#raises_157","text":"ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Raises:"},{"location":"sonnet/#conv2db","text":"Returns the Variable containing the bias.","title":"Conv2D.b"},{"location":"sonnet/#returns_174","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_158","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias.","title":"Raises:"},{"location":"sonnet/#conv2dclonenamenone","text":"Returns a cloned _ConvND module.","title":"Conv2D.clone(name=None)"},{"location":"sonnet/#args_85","text":"name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name .","title":"Args:"},{"location":"sonnet/#returns_175","text":"A copy of the current class.","title":"Returns:"},{"location":"sonnet/#conv2dconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Conv2D.connected_subgraphs"},{"location":"sonnet/#conv2dconv_op_padding","text":"Returns the padding algorithm used for the underlying convolution op.","title":"Conv2D.conv_op_padding"},{"location":"sonnet/#conv2ddata_format","text":"Returns the data format.","title":"Conv2D.data_format"},{"location":"sonnet/#conv2ddefun","text":"Wraps this modules call method in a callable graph function.","title":"Conv2D.defun()"},{"location":"sonnet/#conv2ddefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Conv2D.defun_wrapped"},{"location":"sonnet/#conv2dget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Conv2D.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_86","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_176","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_159","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Conv2D.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_177","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#conv2dget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"Conv2D.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_87","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_178","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_160","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Conv2D.graph"},{"location":"sonnet/#conv2dhas_bias","text":"Returns True if bias Variable is present in the module.","title":"Conv2D.has_bias"},{"location":"sonnet/#conv2dinitializers","text":"Returns the initializers dictionary.","title":"Conv2D.initializers"},{"location":"sonnet/#conv2dinput_channels","text":"Returns the number of input channels.","title":"Conv2D.input_channels"},{"location":"sonnet/#conv2dinput_shape","text":"Returns the input shape.","title":"Conv2D.input_shape"},{"location":"sonnet/#conv2dis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Conv2D.is_connected"},{"location":"sonnet/#conv2dkernel_shape","text":"Returns the kernel shape.","title":"Conv2D.kernel_shape"},{"location":"sonnet/#conv2dlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Conv2D.last_connected_subgraph"},{"location":"sonnet/#returns_179","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_161","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dmask","text":"Returns the mask.","title":"Conv2D.mask"},{"location":"sonnet/#conv2dmodule_name","text":"Returns the name of the Module.","title":"Conv2D.module_name"},{"location":"sonnet/#conv2dname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Conv2D.name_scopes"},{"location":"sonnet/#conv2dnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv2D.non_trainable_variables"},{"location":"sonnet/#returns_180","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_162","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2doutput_channels","text":"Returns the number of output channels.","title":"Conv2D.output_channels"},{"location":"sonnet/#conv2dpadding","text":"Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension.","title":"Conv2D.padding"},{"location":"sonnet/#returns_181","text":"The padding algorithm used, if this is the same for all dimensions.","title":"Returns:"},{"location":"sonnet/#raises_163","text":"ValueError : If different padding algorithms are used for different dimensions.","title":"Raises:"},{"location":"sonnet/#conv2dpaddings","text":"Returns a tuple with the padding algorithm used for each dimension.","title":"Conv2D.paddings"},{"location":"sonnet/#conv2dpartitioners","text":"Returns the partitioners dictionary.","title":"Conv2D.partitioners"},{"location":"sonnet/#conv2drate","text":"Returns the dilation rate.","title":"Conv2D.rate"},{"location":"sonnet/#conv2dregularizers","text":"Returns the regularizers dictionary.","title":"Conv2D.regularizers"},{"location":"sonnet/#conv2dscope_name","text":"Returns the full name of the Module's variable scope.","title":"Conv2D.scope_name"},{"location":"sonnet/#conv2dstride","text":"Returns the stride.","title":"Conv2D.stride"},{"location":"sonnet/#conv2dtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv2D.trainable_variables"},{"location":"sonnet/#returns_182","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_164","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dtransposenamenone","text":"Returns matching Conv2DTranspose module.","title":"Conv2D.transpose(name=None)"},{"location":"sonnet/#args_88","text":"name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.name .","title":"Args:"},{"location":"sonnet/#returns_183","text":"Conv2DTranspose module.","title":"Returns:"},{"location":"sonnet/#raises_165","text":"base.NotSupportedError: If rate in any dimension 1.","title":"Raises:"},{"location":"sonnet/#conv2dvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Conv2D.variable_scope"},{"location":"sonnet/#returns_184","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_166","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv2D.variables"},{"location":"sonnet/#returns_185","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_167","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dw","text":"Returns the Variable containing the weight matrix.","title":"Conv2D.w"},{"location":"sonnet/#class-conv2dlstm","text":"2D convolutional LSTM.","title":"class Conv2DLSTM"},{"location":"sonnet/#conv2dlstm__init__nameconv_2d_lstm-kwargs","text":"Construct Conv2DLSTM. See snt.ConvLSTM for more details.","title":"Conv2DLSTM.__init__(name='conv_2d_lstm', **kwargs)"},{"location":"sonnet/#conv2dlstm__call__inputs-state","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"Conv2DLSTM.__call__(inputs, state)"},{"location":"sonnet/#args_89","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_186","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#conv2dlstmconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Conv2DLSTM.connected_subgraphs"},{"location":"sonnet/#conv2dlstmconvolutions","text":"","title":"Conv2DLSTM.convolutions"},{"location":"sonnet/#conv2dlstmdefun","text":"Wraps this modules call method in a callable graph function.","title":"Conv2DLSTM.defun()"},{"location":"sonnet/#conv2dlstmdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Conv2DLSTM.defun_wrapped"},{"location":"sonnet/#conv2dlstmget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Conv2DLSTM.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_90","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_187","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_168","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dlstmget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Conv2DLSTM.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_188","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#conv2dlstmget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"Conv2DLSTM.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_91","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_189","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_169","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dlstmgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Conv2DLSTM.graph"},{"location":"sonnet/#conv2dlstminitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs","text":"Builds the default start state for an RNNCore.","title":"Conv2DLSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)"},{"location":"sonnet/#args_92","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_190","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_170","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#conv2dlstmis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Conv2DLSTM.is_connected"},{"location":"sonnet/#conv2dlstmlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Conv2DLSTM.last_connected_subgraph"},{"location":"sonnet/#returns_191","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_171","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dlstmmodule_name","text":"Returns the name of the Module.","title":"Conv2DLSTM.module_name"},{"location":"sonnet/#conv2dlstmname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Conv2DLSTM.name_scopes"},{"location":"sonnet/#conv2dlstmnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv2DLSTM.non_trainable_variables"},{"location":"sonnet/#returns_192","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_172","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dlstmoutput_size","text":"tf.TensorShape indicating the size of the core output.","title":"Conv2DLSTM.output_size"},{"location":"sonnet/#conv2dlstmscope_name","text":"Returns the full name of the Module's variable scope.","title":"Conv2DLSTM.scope_name"},{"location":"sonnet/#conv2dlstmstate_size","text":"Tuple of tf.TensorShape s indicating the size of state tensors.","title":"Conv2DLSTM.state_size"},{"location":"sonnet/#conv2dlstmtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv2DLSTM.trainable_variables"},{"location":"sonnet/#returns_193","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_173","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dlstmuse_layer_norm","text":"Boolean indicating whether layer norm is enabled.","title":"Conv2DLSTM.use_layer_norm"},{"location":"sonnet/#conv2dlstmvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Conv2DLSTM.variable_scope"},{"location":"sonnet/#returns_194","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_174","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dlstmvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv2DLSTM.variables"},{"location":"sonnet/#returns_195","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_175","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dlstmzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"Conv2DLSTM.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_93","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_196","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-conv2dtranspose","text":"Spatial transposed / reverse / up 2D convolution module, including bias. This acts as a light wrapper around the TensorFlow op tf.nn.conv2d_transpose abstracting away variable creation and sharing.","title":"class Conv2DTranspose"},{"location":"sonnet/#conv2dtranspose__init__output_channels-output_shapenone-kernel_shapenone-stride1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatnhwc-custom_getternone-nameconv_2d_transpose","text":"Constructs a Conv2DTranspose module . See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution","title":"Conv2DTranspose.__init__(output_channels, output_shape=None, kernel_shape=None, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NHWC', custom_getter=None, name='conv_2d_transpose')"},{"location":"sonnet/#args_94","text":"output_channels : Number of output channels. Can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure output_channels can be called, returning an integer, when build is called. output_shape : Output shape of transpose convolution. Can be either an iterable of integers or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_shape can be called, returning an iterable of format (out_height, out_width) when build is called. Note that output_shape defines the size of output signal domain, as opposed to the shape of the output Tensor . If a None value is given, a default shape is automatically calculated (see docstring of _default_transpose_size function for more details). kernel_shape : Sequence of kernel sizes (of size 2), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 2), or integer that is used to define stride in all dimensions. padding : Padding algorithm, either snt.SAME or snt.VALID . use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NHWC), or the second dimension (\"NCHW\"). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_176","text":"base.IncompatibleShapeError: If the given kernel shape is neither an integer nor a sequence of two integers. base.IncompatibleShapeError: If the given stride is neither an integer nor a sequence of two or four integers. ValueError : If the given padding is not snt.VALID or snt.SAME . ValueError : If the given kernel_shape is None . KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_2D_DATA_FORMATS ).","title":"Raises:"},{"location":"sonnet/#conv2dtranspose__call__inputs","text":"Connects the _ConvNDTranspose module into the graph. If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same final N dimensions, in order for the existing variables to be the correct size for the multiplication. The batch size may differ for each connection.","title":"Conv2DTranspose.__call__(inputs)"},{"location":"sonnet/#args_95","text":"inputs : A Tensor of shape data_format and of type tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Args:"},{"location":"sonnet/#returns_197","text":"A Tensor of shape data_format and of type tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Returns:"},{"location":"sonnet/#raises_177","text":"ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If output_shape is an iterable and is not in the format (out_height, out_width) . TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Raises:"},{"location":"sonnet/#conv2dtransposeb","text":"Returns the Variable containing the bias.","title":"Conv2DTranspose.b"},{"location":"sonnet/#returns_198","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_178","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias.","title":"Raises:"},{"location":"sonnet/#conv2dtransposeconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Conv2DTranspose.connected_subgraphs"},{"location":"sonnet/#conv2dtransposeconv_op_padding","text":"Returns the padding algorithm used for the underlying convolution op.","title":"Conv2DTranspose.conv_op_padding"},{"location":"sonnet/#conv2dtransposedefun","text":"Wraps this modules call method in a callable graph function.","title":"Conv2DTranspose.defun()"},{"location":"sonnet/#conv2dtransposedefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Conv2DTranspose.defun_wrapped"},{"location":"sonnet/#conv2dtransposeget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Conv2DTranspose.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_96","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_199","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_179","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dtransposeget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Conv2DTranspose.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_200","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#conv2dtransposeget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"Conv2DTranspose.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_97","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_201","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_180","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dtransposegraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Conv2DTranspose.graph"},{"location":"sonnet/#conv2dtransposehas_bias","text":"Returns True if bias Variable is present in the module.","title":"Conv2DTranspose.has_bias"},{"location":"sonnet/#conv2dtransposeinitializers","text":"Returns the initializers dictionary.","title":"Conv2DTranspose.initializers"},{"location":"sonnet/#conv2dtransposeinput_channels","text":"Returns the number of input channels.","title":"Conv2DTranspose.input_channels"},{"location":"sonnet/#conv2dtransposeinput_shape","text":"Returns the input shape.","title":"Conv2DTranspose.input_shape"},{"location":"sonnet/#conv2dtransposeis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Conv2DTranspose.is_connected"},{"location":"sonnet/#conv2dtransposekernel_shape","text":"Returns the kernel shape.","title":"Conv2DTranspose.kernel_shape"},{"location":"sonnet/#conv2dtransposelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Conv2DTranspose.last_connected_subgraph"},{"location":"sonnet/#returns_202","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_181","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dtransposemodule_name","text":"Returns the name of the Module.","title":"Conv2DTranspose.module_name"},{"location":"sonnet/#conv2dtransposename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Conv2DTranspose.name_scopes"},{"location":"sonnet/#conv2dtransposenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv2DTranspose.non_trainable_variables"},{"location":"sonnet/#returns_203","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_182","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dtransposeoutput_channels","text":"Returns the number of output channels.","title":"Conv2DTranspose.output_channels"},{"location":"sonnet/#conv2dtransposeoutput_shape","text":"Returns the output shape.","title":"Conv2DTranspose.output_shape"},{"location":"sonnet/#conv2dtransposepadding","text":"Returns the padding algorithm.","title":"Conv2DTranspose.padding"},{"location":"sonnet/#conv2dtransposepartitioners","text":"Returns the partitioners dictionary.","title":"Conv2DTranspose.partitioners"},{"location":"sonnet/#conv2dtransposeregularizers","text":"Returns the regularizers dictionary.","title":"Conv2DTranspose.regularizers"},{"location":"sonnet/#conv2dtransposescope_name","text":"Returns the full name of the Module's variable scope.","title":"Conv2DTranspose.scope_name"},{"location":"sonnet/#conv2dtransposestride","text":"Returns the stride.","title":"Conv2DTranspose.stride"},{"location":"sonnet/#conv2dtransposetrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv2DTranspose.trainable_variables"},{"location":"sonnet/#returns_204","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_183","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dtransposetransposenamenone","text":"Returns matching Conv2D module.","title":"Conv2DTranspose.transpose(name=None)"},{"location":"sonnet/#args_98","text":"name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.name .","title":"Args:"},{"location":"sonnet/#returns_205","text":"Conv2D module.","title":"Returns:"},{"location":"sonnet/#conv2dtransposevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Conv2DTranspose.variable_scope"},{"location":"sonnet/#returns_206","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_184","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dtransposevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv2DTranspose.variables"},{"location":"sonnet/#returns_207","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_185","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv2dtransposew","text":"Returns the Variable containing the weight matrix.","title":"Conv2DTranspose.w"},{"location":"sonnet/#class-conv3d","text":"Volumetric convolution module, including optional bias. This acts as a light wrapper around the class _ConvND .","title":"class Conv3D"},{"location":"sonnet/#conv3d__init__output_channels-kernel_shape-stride1-rate1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-masknone-data_formatndhwc-custom_getternone-nameconv_3d","text":"Constructs a Conv3D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution","title":"Conv3D.__init__(output_channels, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, mask=None, data_format='NDHWC', custom_getter=None, name='conv_3d')"},{"location":"sonnet/#args_99","text":"output_channels : Number of output channels. output_channels can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_channels can be called, returning an integer, when build is called. kernel_shape : Sequence of kernel sizes (of size 3), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 3), or integer that is used to define stride in all dimensions. rate : Sequence of dilation rates (of size 3), or integer that is used to define dilation rate in all dimensions. 1 corresponds to standard 3D convolution, rate 1 corresponds to dilated convolution. Cannot be 1 if any of stride is also 1. padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 3. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializer for the weights is a truncated normal initializer, which is commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . mask : An object convertible to a 5D tensor which is multiplied component-wise with the weights (Optional). data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NDHWC), or the second dimension (NCDHW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_186","text":"base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two or four integers. base.IncompatibleShapeError: If the given rate is not an integer; or if the given rate is not a sequence of two integers. base.NotSupportedError: If rate in any dimension and the stride in any dimension are simultaneously 1. ValueError : If the given padding is not snt.VALID , snt.SAME , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a sequence of these. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_3D_DATA_FORMATS ).","title":"Raises:"},{"location":"sonnet/#conv3d__call__inputs","text":"Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection.","title":"Conv3D.__call__(inputs)"},{"location":"sonnet/#args_100","text":"inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Args:"},{"location":"sonnet/#returns_208","text":"A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels].","title":"Returns:"},{"location":"sonnet/#raises_187","text":"ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Raises:"},{"location":"sonnet/#conv3db","text":"Returns the Variable containing the bias.","title":"Conv3D.b"},{"location":"sonnet/#returns_209","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_188","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias.","title":"Raises:"},{"location":"sonnet/#conv3dclonenamenone","text":"Returns a cloned _ConvND module.","title":"Conv3D.clone(name=None)"},{"location":"sonnet/#args_101","text":"name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name .","title":"Args:"},{"location":"sonnet/#returns_210","text":"A copy of the current class.","title":"Returns:"},{"location":"sonnet/#conv3dconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Conv3D.connected_subgraphs"},{"location":"sonnet/#conv3dconv_op_padding","text":"Returns the padding algorithm used for the underlying convolution op.","title":"Conv3D.conv_op_padding"},{"location":"sonnet/#conv3ddata_format","text":"Returns the data format.","title":"Conv3D.data_format"},{"location":"sonnet/#conv3ddefun","text":"Wraps this modules call method in a callable graph function.","title":"Conv3D.defun()"},{"location":"sonnet/#conv3ddefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Conv3D.defun_wrapped"},{"location":"sonnet/#conv3dget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Conv3D.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_102","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_211","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_189","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Conv3D.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_212","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#conv3dget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"Conv3D.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_103","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_213","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_190","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Conv3D.graph"},{"location":"sonnet/#conv3dhas_bias","text":"Returns True if bias Variable is present in the module.","title":"Conv3D.has_bias"},{"location":"sonnet/#conv3dinitializers","text":"Returns the initializers dictionary.","title":"Conv3D.initializers"},{"location":"sonnet/#conv3dinput_channels","text":"Returns the number of input channels.","title":"Conv3D.input_channels"},{"location":"sonnet/#conv3dinput_shape","text":"Returns the input shape.","title":"Conv3D.input_shape"},{"location":"sonnet/#conv3dis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Conv3D.is_connected"},{"location":"sonnet/#conv3dkernel_shape","text":"Returns the kernel shape.","title":"Conv3D.kernel_shape"},{"location":"sonnet/#conv3dlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Conv3D.last_connected_subgraph"},{"location":"sonnet/#returns_214","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_191","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dmask","text":"Returns the mask.","title":"Conv3D.mask"},{"location":"sonnet/#conv3dmodule_name","text":"Returns the name of the Module.","title":"Conv3D.module_name"},{"location":"sonnet/#conv3dname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Conv3D.name_scopes"},{"location":"sonnet/#conv3dnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv3D.non_trainable_variables"},{"location":"sonnet/#returns_215","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_192","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3doutput_channels","text":"Returns the number of output channels.","title":"Conv3D.output_channels"},{"location":"sonnet/#conv3dpadding","text":"Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension.","title":"Conv3D.padding"},{"location":"sonnet/#returns_216","text":"The padding algorithm used, if this is the same for all dimensions.","title":"Returns:"},{"location":"sonnet/#raises_193","text":"ValueError : If different padding algorithms are used for different dimensions.","title":"Raises:"},{"location":"sonnet/#conv3dpaddings","text":"Returns a tuple with the padding algorithm used for each dimension.","title":"Conv3D.paddings"},{"location":"sonnet/#conv3dpartitioners","text":"Returns the partitioners dictionary.","title":"Conv3D.partitioners"},{"location":"sonnet/#conv3drate","text":"Returns the dilation rate.","title":"Conv3D.rate"},{"location":"sonnet/#conv3dregularizers","text":"Returns the regularizers dictionary.","title":"Conv3D.regularizers"},{"location":"sonnet/#conv3dscope_name","text":"Returns the full name of the Module's variable scope.","title":"Conv3D.scope_name"},{"location":"sonnet/#conv3dstride","text":"Returns the stride.","title":"Conv3D.stride"},{"location":"sonnet/#conv3dtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv3D.trainable_variables"},{"location":"sonnet/#returns_217","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_194","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dtransposenamenone","text":"Returns matching Conv3DTranspose module.","title":"Conv3D.transpose(name=None)"},{"location":"sonnet/#args_104","text":"name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.name .","title":"Args:"},{"location":"sonnet/#returns_218","text":"Conv3DTranspose module.","title":"Returns:"},{"location":"sonnet/#raises_195","text":"base.NotSupportedError: If rate in any dimension 1.","title":"Raises:"},{"location":"sonnet/#conv3dvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Conv3D.variable_scope"},{"location":"sonnet/#returns_219","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_196","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv3D.variables"},{"location":"sonnet/#returns_220","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_197","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dw","text":"Returns the Variable containing the weight matrix.","title":"Conv3D.w"},{"location":"sonnet/#class-conv3dtranspose","text":"Volumetric transposed / reverse / up 3D convolution module, including bias. This acts as a light wrapper around the TensorFlow op tf.nn.conv3d_transpose abstracting away variable creation and sharing.","title":"class Conv3DTranspose"},{"location":"sonnet/#conv3dtranspose__init__output_channels-output_shapenone-kernel_shapenone-stride1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatndhwc-custom_getternone-nameconv_3d_transpose","text":"Constructs a Conv3DTranspose module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution","title":"Conv3DTranspose.__init__(output_channels, output_shape=None, kernel_shape=None, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NDHWC', custom_getter=None, name='conv_3d_transpose')"},{"location":"sonnet/#args_105","text":"output_channels : Number of output channels. output_channels can be either a number or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure output_channels can be called, returning an integer, when build is called. output_shape : Output shape of transpose convolution. Can be either an iterable of integers or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_shape can be called, returning an iterable of format (out_depth, out_height, out_width) when build is called. Note that output_shape defines the size of output signal domain, as opposed to the shape of the output Tensor . If a None value is given, a default shape is automatically calculated (see docstring of _default_transpose_size function for more details). kernel_shape : Sequence of kernel sizes (of size 3), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size 3), or integer that is used to define stride in all dimensions. padding : Padding algorithm, either snt.SAME or snt.VALID . use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NDHWC), or the second dimension (NCDHW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_198","text":"module.IncompatibleShapeError: If the given kernel shape is neither an integer nor a sequence of three integers. module.IncompatibleShapeError: If the given stride is neither an integer nor a sequence of three or five integers. ValueError : If the given padding is not snt.VALID or snt.SAME . ValueError : If the given kernel_shape is None . KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If the passed-in data_format doesn't have a channel dimension. ValueError : If the given data_format is not a supported format (see SUPPORTED_3D_DATA_FORMATS ).","title":"Raises:"},{"location":"sonnet/#conv3dtranspose__call__inputs","text":"Connects the _ConvNDTranspose module into the graph. If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same final N dimensions, in order for the existing variables to be the correct size for the multiplication. The batch size may differ for each connection.","title":"Conv3DTranspose.__call__(inputs)"},{"location":"sonnet/#args_106","text":"inputs : A Tensor of shape data_format and of type tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Args:"},{"location":"sonnet/#returns_221","text":"A Tensor of shape data_format and of type tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Returns:"},{"location":"sonnet/#raises_199","text":"ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If output_shape is an iterable and is not in the format (out_height, out_width) . TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Raises:"},{"location":"sonnet/#conv3dtransposeb","text":"Returns the Variable containing the bias.","title":"Conv3DTranspose.b"},{"location":"sonnet/#returns_222","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_200","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias.","title":"Raises:"},{"location":"sonnet/#conv3dtransposeconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Conv3DTranspose.connected_subgraphs"},{"location":"sonnet/#conv3dtransposeconv_op_padding","text":"Returns the padding algorithm used for the underlying convolution op.","title":"Conv3DTranspose.conv_op_padding"},{"location":"sonnet/#conv3dtransposedefun","text":"Wraps this modules call method in a callable graph function.","title":"Conv3DTranspose.defun()"},{"location":"sonnet/#conv3dtransposedefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Conv3DTranspose.defun_wrapped"},{"location":"sonnet/#conv3dtransposeget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Conv3DTranspose.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_107","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_223","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_201","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dtransposeget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Conv3DTranspose.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_224","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#conv3dtransposeget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"Conv3DTranspose.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_108","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_225","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_202","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dtransposegraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Conv3DTranspose.graph"},{"location":"sonnet/#conv3dtransposehas_bias","text":"Returns True if bias Variable is present in the module.","title":"Conv3DTranspose.has_bias"},{"location":"sonnet/#conv3dtransposeinitializers","text":"Returns the initializers dictionary.","title":"Conv3DTranspose.initializers"},{"location":"sonnet/#conv3dtransposeinput_channels","text":"Returns the number of input channels.","title":"Conv3DTranspose.input_channels"},{"location":"sonnet/#conv3dtransposeinput_shape","text":"Returns the input shape.","title":"Conv3DTranspose.input_shape"},{"location":"sonnet/#conv3dtransposeis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Conv3DTranspose.is_connected"},{"location":"sonnet/#conv3dtransposekernel_shape","text":"Returns the kernel shape.","title":"Conv3DTranspose.kernel_shape"},{"location":"sonnet/#conv3dtransposelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Conv3DTranspose.last_connected_subgraph"},{"location":"sonnet/#returns_226","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_203","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dtransposemodule_name","text":"Returns the name of the Module.","title":"Conv3DTranspose.module_name"},{"location":"sonnet/#conv3dtransposename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Conv3DTranspose.name_scopes"},{"location":"sonnet/#conv3dtransposenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv3DTranspose.non_trainable_variables"},{"location":"sonnet/#returns_227","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_204","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dtransposeoutput_channels","text":"Returns the number of output channels.","title":"Conv3DTranspose.output_channels"},{"location":"sonnet/#conv3dtransposeoutput_shape","text":"Returns the output shape.","title":"Conv3DTranspose.output_shape"},{"location":"sonnet/#conv3dtransposepadding","text":"Returns the padding algorithm.","title":"Conv3DTranspose.padding"},{"location":"sonnet/#conv3dtransposepartitioners","text":"Returns the partitioners dictionary.","title":"Conv3DTranspose.partitioners"},{"location":"sonnet/#conv3dtransposeregularizers","text":"Returns the regularizers dictionary.","title":"Conv3DTranspose.regularizers"},{"location":"sonnet/#conv3dtransposescope_name","text":"Returns the full name of the Module's variable scope.","title":"Conv3DTranspose.scope_name"},{"location":"sonnet/#conv3dtransposestride","text":"Returns the stride.","title":"Conv3DTranspose.stride"},{"location":"sonnet/#conv3dtransposetrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv3DTranspose.trainable_variables"},{"location":"sonnet/#returns_228","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_205","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dtransposetransposenamenone","text":"Returns transposed Conv3DTranspose module, i.e. a Conv3D module.","title":"Conv3DTranspose.transpose(name=None)"},{"location":"sonnet/#conv3dtransposevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Conv3DTranspose.variable_scope"},{"location":"sonnet/#returns_229","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_206","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dtransposevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Conv3DTranspose.variables"},{"location":"sonnet/#returns_230","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_207","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#conv3dtransposew","text":"Returns the Variable containing the weight matrix.","title":"Conv3DTranspose.w"},{"location":"sonnet/#class-deeprnn","text":"RNN core that passes data through a number of internal modules or ops. This module is constructed by passing an iterable of externally constructed modules or ops. The DeepRNN takes (input, prev_state) as input and passes the input through each internal module in the order they were presented, using elements from prev_state as necessary for internal recurrent cores. The output is (output, next_state) in common with other RNN cores. By default, skip connections from the input to all internal modules and from each intermediate output to the final output are used. E.g.: lstm1 = snt.LSTM(hidden_size=256) lstm2 = snt.LSTM(hidden_size=256) deep_rnn = snt.DeepRNN([lstm1, lstm2]) output, next_state = deep_rnn(input, prev_state) The computation set up inside the DeepRNN has the same effect as: prev_state1, prev_state2 = prev_state lstm1_output, next_state1 = lstm1(input, prev_state1) lstm2_output, next_state2 = lstm2( tf.concat([input, lstm1_output], 1), prev_state2) next_state = (next_state1, next_state2) output = tf.concat([lstm1_output, lstm2_output], 1) Every internal module receives the preceding module's output and the entire core's input. The output is created by concatenating each internal module's output. In the case of internal recurrent elements, corresponding elements of the state are used such that state[i] is passed to the i 'th internal recurrent element. Note that the state of a DeepRNN is always a tuple, which will contain the same number of elements as there are internal recurrent cores. If no internal modules are recurrent, the state of the DeepRNN as a whole is the empty tuple. Wrapping non-recurrent modules into a DeepRNN can be useful to produce something API compatible with a \"real\" recurrent module, simplifying code that handles the cores. Without skip connections the previous example would become the following (note the only difference is the addition of skip_connections=False ): # ... declare other modules as above deep_rnn = snt.DeepRNN([lin, tanh, lstm], skip_connections=False) output, next_state = deep_rnn(input, prev_state) which is equivalent to: lin_output = lin(input) tanh_output = tanh(lin_output) lstm_output, lstm_next_state = lstm(tanh_output, prev_state[0]) next_state = (lstm_next_state,) output = lstm_output Note: when using skip connections, all the cores should be recurrent.","title":"class DeepRNN"},{"location":"sonnet/#deeprnn__init__cores-skip_connectionstrue-concat_final_output_if_skiptrue-namedeep_rnn","text":"Construct a Deep RNN core.","title":"DeepRNN.__init__(cores, skip_connections=True, concat_final_output_if_skip=True, name='deep_rnn')"},{"location":"sonnet/#args_109","text":"cores : iterable of modules or ops. skip_connections : a boolean that indicates whether to use skip connections. This means that the input is fed to all the layers, after being concatenated on the last dimension with the output of the previous layer. The output of the module will be the concatenation of all the outputs of the internal modules. concat_final_output_if_skip : A boolean that indicates whether the outputs of intermediate layers should be concatenated into the timestep-wise output of the core. By default this is True. If this is set to False, then the core output is that of the final layer, i.e. that of cores[-1] . name : name of the module.","title":"Args:"},{"location":"sonnet/#raises_208","text":"ValueError : if cores is not an iterable, or if skip_connections is True and not all the modules are recurrent.","title":"Raises:"},{"location":"sonnet/#deeprnn__call__inputs-prev_state-kwargs","text":"Connects the DeepRNN module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided as input_ and state must have the same final dimension, in order for the existing variables to be the correct size for their corresponding multiplications. The batch size may differ for each connection.","title":"DeepRNN.__call__(inputs, prev_state, **kwargs)"},{"location":"sonnet/#args_110","text":"inputs : a nested tuple of Tensors of arbitrary dimensionality, with at least an initial batch dimension. prev_state : a tuple of prev_state s that corresponds to the state of each one of the cores of the DeepCore . **kwargs : optional kwargs to be passed to the _build of all sub-modules. E.g. is_training=True. Note all sub-modules must accept the given kwarg.","title":"Args:"},{"location":"sonnet/#returns_231","text":"output : a nested tuple of Tensors of arbitrary dimensionality, with at least an initial batch dimension. next_state : a tuple of next_state s that corresponds to the updated state of each one of the cores of the DeepCore .","title":"Returns:"},{"location":"sonnet/#raises_209","text":"ValueError : if connecting the module into the graph any time after the first time, and the inferred size of the inputs does not match previous invocations. This may happen if one connects a module any time after the first time that does not have the configuration of skip connections as the first time.","title":"Raises:"},{"location":"sonnet/#deeprnnconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"DeepRNN.connected_subgraphs"},{"location":"sonnet/#deeprnndefun","text":"Wraps this modules call method in a callable graph function.","title":"DeepRNN.defun()"},{"location":"sonnet/#deeprnndefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"DeepRNN.defun_wrapped"},{"location":"sonnet/#deeprnnget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"DeepRNN.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_111","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_232","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_210","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#deeprnnget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"DeepRNN.get_possible_initializer_keys()"},{"location":"sonnet/#returns_233","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#deeprnnget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"DeepRNN.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_112","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_234","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_211","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#deeprnngraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"DeepRNN.graph"},{"location":"sonnet/#deeprnninitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone","text":"Builds the default start state for a DeepRNN.","title":"DeepRNN.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None)"},{"location":"sonnet/#args_113","text":"batch_size : An int, float or scalar Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_235","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_212","text":"ValueError : if the number of passed initializers is not the same as the number of recurrent cores.","title":"Raises:"},{"location":"sonnet/#deeprnnis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"DeepRNN.is_connected"},{"location":"sonnet/#deeprnnlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"DeepRNN.last_connected_subgraph"},{"location":"sonnet/#returns_236","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_213","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#deeprnnmodule_name","text":"Returns the name of the Module.","title":"DeepRNN.module_name"},{"location":"sonnet/#deeprnnname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"DeepRNN.name_scopes"},{"location":"sonnet/#deeprnnnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"DeepRNN.non_trainable_variables"},{"location":"sonnet/#returns_237","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_214","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#deeprnnoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"DeepRNN.output_size"},{"location":"sonnet/#deeprnnscope_name","text":"Returns the full name of the Module's variable scope.","title":"DeepRNN.scope_name"},{"location":"sonnet/#deeprnnstate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"DeepRNN.state_size"},{"location":"sonnet/#deeprnntrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"DeepRNN.trainable_variables"},{"location":"sonnet/#returns_238","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_215","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#deeprnnvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"DeepRNN.variable_scope"},{"location":"sonnet/#returns_239","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_216","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#deeprnnvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"DeepRNN.variables"},{"location":"sonnet/#returns_240","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_217","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#deeprnnzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"DeepRNN.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_114","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_241","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-depthwiseconv2d","text":"Spatial depthwise 2D convolution module, including bias. This acts as a light wrapper around the TensorFlow ops tf.nn.depthwise_conv2d , abstracting away variable creation and sharing.","title":"class DepthwiseConv2D"},{"location":"sonnet/#depthwiseconv2d__init__channel_multiplier-kernel_shape-stride1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatnhwc-custom_getternone-nameconv_2d_depthwise","text":"Constructs a DepthwiseConv2D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution","title":"DepthwiseConv2D.__init__(channel_multiplier, kernel_shape, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NHWC', custom_getter=None, name='conv_2d_depthwise')"},{"location":"sonnet/#args_115","text":"channel_multiplier : Number of channels to expand convolution to. Must be an integer. Must be 0. When channel_multiplier is set to 1, apply a different filter to each input channel producing one output channel per input channel. Numbers larger than 1 cause multiple different filters to be applied to each input channel, with their outputs being concatenated together, producing channel_multiplier * input_channels output channels. kernel_shape : Iterable with 2 elements in the following layout: [filter_height, filter_width] or integer that is used to define the list in all dimensions. stride : Iterable with 2 or 4 elements of kernel strides, or integer that is used to define stride in all dimensions. Layout of list: In case of 4 elements: [1, stride_height, stride_widith, 1] In case of 2 elements: [stride_height, stride_width] . padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 2. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners for the filters (with key 'w') and the biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NHWC), or the second dimension (\"NCHW\"). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_218","text":"ValueError : If channel_multiplier isn't of type ( numbers.Integral or tf.Dimension ). ValueError : If channel_multiplier is less than 1. ValueError : If the given data_format is not a supported format (see SUPPORTED_2D_DATA_FORMATS ). base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. ValueError : If the given padding is not snt.VALID , snt.SAME , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a sequence of these. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If the passed-in data_format doesn't have a channel dimension.","title":"Raises:"},{"location":"sonnet/#depthwiseconv2d__call__inputs","text":"Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection.","title":"DepthwiseConv2D.__call__(inputs)"},{"location":"sonnet/#args_116","text":"inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Args:"},{"location":"sonnet/#returns_242","text":"A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels].","title":"Returns:"},{"location":"sonnet/#raises_219","text":"ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Raises:"},{"location":"sonnet/#depthwiseconv2db","text":"Returns the Variable containing the bias.","title":"DepthwiseConv2D.b"},{"location":"sonnet/#returns_243","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_220","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias.","title":"Raises:"},{"location":"sonnet/#depthwiseconv2dchannel_multiplier","text":"Returns the channel multiplier argument.","title":"DepthwiseConv2D.channel_multiplier"},{"location":"sonnet/#depthwiseconv2dclonenamenone","text":"Returns a cloned _ConvND module.","title":"DepthwiseConv2D.clone(name=None)"},{"location":"sonnet/#args_117","text":"name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name .","title":"Args:"},{"location":"sonnet/#returns_244","text":"A copy of the current class.","title":"Returns:"},{"location":"sonnet/#depthwiseconv2dconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"DepthwiseConv2D.connected_subgraphs"},{"location":"sonnet/#depthwiseconv2dconv_op_padding","text":"Returns the padding algorithm used for the underlying convolution op.","title":"DepthwiseConv2D.conv_op_padding"},{"location":"sonnet/#depthwiseconv2ddata_format","text":"Returns the data format.","title":"DepthwiseConv2D.data_format"},{"location":"sonnet/#depthwiseconv2ddefun","text":"Wraps this modules call method in a callable graph function.","title":"DepthwiseConv2D.defun()"},{"location":"sonnet/#depthwiseconv2ddefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"DepthwiseConv2D.defun_wrapped"},{"location":"sonnet/#depthwiseconv2dget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"DepthwiseConv2D.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_118","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_245","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_221","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#depthwiseconv2dget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"DepthwiseConv2D.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_246","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#depthwiseconv2dget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"DepthwiseConv2D.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_119","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_247","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_222","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#depthwiseconv2dgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"DepthwiseConv2D.graph"},{"location":"sonnet/#depthwiseconv2dhas_bias","text":"Returns True if bias Variable is present in the module.","title":"DepthwiseConv2D.has_bias"},{"location":"sonnet/#depthwiseconv2dinitializers","text":"Returns the initializers dictionary.","title":"DepthwiseConv2D.initializers"},{"location":"sonnet/#depthwiseconv2dinput_channels","text":"Returns the number of input channels.","title":"DepthwiseConv2D.input_channels"},{"location":"sonnet/#depthwiseconv2dinput_shape","text":"Returns the input shape.","title":"DepthwiseConv2D.input_shape"},{"location":"sonnet/#depthwiseconv2dis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"DepthwiseConv2D.is_connected"},{"location":"sonnet/#depthwiseconv2dkernel_shape","text":"Returns the kernel shape.","title":"DepthwiseConv2D.kernel_shape"},{"location":"sonnet/#depthwiseconv2dlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"DepthwiseConv2D.last_connected_subgraph"},{"location":"sonnet/#returns_248","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_223","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#depthwiseconv2dmask","text":"Returns the mask.","title":"DepthwiseConv2D.mask"},{"location":"sonnet/#depthwiseconv2dmodule_name","text":"Returns the name of the Module.","title":"DepthwiseConv2D.module_name"},{"location":"sonnet/#depthwiseconv2dname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"DepthwiseConv2D.name_scopes"},{"location":"sonnet/#depthwiseconv2dnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"DepthwiseConv2D.non_trainable_variables"},{"location":"sonnet/#returns_249","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_224","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#depthwiseconv2doutput_channels","text":"Returns the number of output channels.","title":"DepthwiseConv2D.output_channels"},{"location":"sonnet/#depthwiseconv2dpadding","text":"Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension.","title":"DepthwiseConv2D.padding"},{"location":"sonnet/#returns_250","text":"The padding algorithm used, if this is the same for all dimensions.","title":"Returns:"},{"location":"sonnet/#raises_225","text":"ValueError : If different padding algorithms are used for different dimensions.","title":"Raises:"},{"location":"sonnet/#depthwiseconv2dpaddings","text":"Returns a tuple with the padding algorithm used for each dimension.","title":"DepthwiseConv2D.paddings"},{"location":"sonnet/#depthwiseconv2dpartitioners","text":"Returns the partitioners dictionary.","title":"DepthwiseConv2D.partitioners"},{"location":"sonnet/#depthwiseconv2drate","text":"Returns the dilation rate.","title":"DepthwiseConv2D.rate"},{"location":"sonnet/#depthwiseconv2dregularizers","text":"Returns the regularizers dictionary.","title":"DepthwiseConv2D.regularizers"},{"location":"sonnet/#depthwiseconv2dscope_name","text":"Returns the full name of the Module's variable scope.","title":"DepthwiseConv2D.scope_name"},{"location":"sonnet/#depthwiseconv2dstride","text":"Returns the stride.","title":"DepthwiseConv2D.stride"},{"location":"sonnet/#depthwiseconv2dtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"DepthwiseConv2D.trainable_variables"},{"location":"sonnet/#returns_251","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_226","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#depthwiseconv2dvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"DepthwiseConv2D.variable_scope"},{"location":"sonnet/#returns_252","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_227","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#depthwiseconv2dvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"DepthwiseConv2D.variables"},{"location":"sonnet/#returns_253","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_228","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#depthwiseconv2dw","text":"Returns the Variable containing the weight matrix.","title":"DepthwiseConv2D.w"},{"location":"sonnet/#class-differentgrapherror","text":"Error raised when trying to connect a Sonnet module to multiple Graphs.","title":"class DifferentGraphError"},{"location":"sonnet/#class-embed","text":"Module for embedding tokens in a low-dimensional space.","title":"class Embed"},{"location":"sonnet/#embed__init__vocab_sizenone-embed_dimnone-existing_vocabnone-densify_gradientsfalse-initializersnone-partitionersnone-regularizersnone-trainabletrue-custom_getternone-nameembed","text":"Constructs an Embed module.","title":"Embed.__init__(vocab_size=None, embed_dim=None, existing_vocab=None, densify_gradients=False, initializers=None, partitioners=None, regularizers=None, trainable=True, custom_getter=None, name='embed')"},{"location":"sonnet/#args_120","text":"vocab_size : int. Number of unique tokens to embed. If not provided, an existing vocabulary matrix from which vocab_size can be inferred must be provided as existing_vocab. embed_dim : int or None. Number of dimensions to assign to each embedding. If not specified, a sensible default is chosen based on vocab_size . If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. existing_vocab : a [vocab_size, embed_dim] vocabulary matrix. Will be converted to a tf.float32 tensor. If provided, neither or vocab_size or embed_dim should be provided as they are inferred. densify_gradients : if True, we convert the embedding gradient from an indexed-slices to a regular tensor before sending it back to the parameter server. This avoids excess computation on the parameter server. Use this option for moderately sized embeddings, e.g., a vocabulary size on the order of up to thousands. For embeddings larger than these, e.g. a vocabulary size on the order of tens or hundreds of thousands, set this to False. initializers : Optional dict containing initializers for embeddings (with key 'embeddings'). As a default, embeddings are initialized via a truncated normal distribution. partitioners : Optional dict containing partitioners for embeddings (with key 'embeddings'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for embeddings (with key 'embeddings'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . trainable : if True, the embeddings will be updated during training. If False, they are fixed to their initial values. If trainable=False and a regularizer is given, the resulting loss stays constant. custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : string. Name for this module.","title":"Args:"},{"location":"sonnet/#raises_229","text":"ValueError : if neither one of vocab_size or existing_vocab is provided, or if existing_vocab is provided along with vocab_size, embedding_dim, initializers, partitioners or regularizers (as these should be inferred).","title":"Raises:"},{"location":"sonnet/#embed__call__ids","text":"Lookup embeddings. Looks up an embedding vector for each value in ids . All ids must be within [0, vocab_size), else an InvalidArgumentError is raised at runtime.","title":"Embed.__call__(ids)"},{"location":"sonnet/#args_121","text":"ids : Tensor of dtype int64.","title":"Args:"},{"location":"sonnet/#returns_254","text":"Tensor of tf.shape(ids) + [embedding_dim] and dtype float32.","title":"Returns:"},{"location":"sonnet/#embedconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Embed.connected_subgraphs"},{"location":"sonnet/#embeddefun","text":"Wraps this modules call method in a callable graph function.","title":"Embed.defun()"},{"location":"sonnet/#embeddefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Embed.defun_wrapped"},{"location":"sonnet/#embedembed_dim","text":"Size of embedding vectors.","title":"Embed.embed_dim"},{"location":"sonnet/#embedembeddings","text":"Returns the Variable containing embeddings.","title":"Embed.embeddings"},{"location":"sonnet/#returns_255","text":"A 2D Variable containing one embedding vector per row, constructed in the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_230","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist.","title":"Raises:"},{"location":"sonnet/#embedget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Embed.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_122","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_256","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_231","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#embedget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Embed.get_possible_initializer_keys()"},{"location":"sonnet/#returns_257","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#embedget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"Embed.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_123","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_258","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_232","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#embedgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Embed.graph"},{"location":"sonnet/#embedis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Embed.is_connected"},{"location":"sonnet/#embedlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Embed.last_connected_subgraph"},{"location":"sonnet/#returns_259","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_233","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#embedmodule_name","text":"Returns the name of the Module.","title":"Embed.module_name"},{"location":"sonnet/#embedname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Embed.name_scopes"},{"location":"sonnet/#embednon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Embed.non_trainable_variables"},{"location":"sonnet/#returns_260","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_234","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#embedscope_name","text":"Returns the full name of the Module's variable scope.","title":"Embed.scope_name"},{"location":"sonnet/#embedtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Embed.trainable_variables"},{"location":"sonnet/#returns_261","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_235","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#embedvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Embed.variable_scope"},{"location":"sonnet/#returns_262","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_236","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#embedvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Embed.variables"},{"location":"sonnet/#returns_263","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_237","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#embedvocab_size","text":"Size of input vocabulary.","title":"Embed.vocab_size"},{"location":"sonnet/#class-error","text":"Base class for all errors from snt. This is thrown to indicate a Neural Network specific problem, e.g. wrong module arity, module is not connected to the graph when it should be, tried to wire together incompatible modules, etc.","title":"class Error"},{"location":"sonnet/#class-flattentrailingdimensions","text":"Flattens trailing dimensions of a Tensor.","title":"class FlattenTrailingDimensions"},{"location":"sonnet/#flattentrailingdimensions__init__dim_from-namebatch_dim_from","text":"Constructs a FlattenTrailingDimensions module. For example, given an input Tensor with shape [B, H, W, C] : dim_from=1 will return a Tensor with shape [B, H*W*C] . dim_from=2 will return a Tensor with shape [B, H, W*C] . dim_from=3 will return the input itself. dim_from=4 will return a Tensor with shape [B, H, W, C, 1] . dim_from =5 will generate a ValueError when building the module. The preserved dimensions can be unknown at building time. Equivalent to BatchFlatten(preserve_dims=dim_from, name=name).","title":"FlattenTrailingDimensions.__init__(dim_from, name='batch_dim_from')"},{"location":"sonnet/#args_124","text":"dim_from : All dimensions after and including dim_from will be flattened into a single dimension. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_238","text":"ValueError : If dim_from = 0 .","title":"Raises:"},{"location":"sonnet/#flattentrailingdimensions__call__inputs","text":"Connects the module into the graph, with input Tensor inputs .","title":"FlattenTrailingDimensions.__call__(inputs)"},{"location":"sonnet/#args_125","text":"inputs : A Tensor of shape [b_1, b_2, ..., b_preserve_dims, b_preserve_dims+1, ...].","title":"Args:"},{"location":"sonnet/#returns_264","text":"A Tensor of shape [b_1, b_2, ..., b_preserve_dims, b_reshape_1, b_reshape_2, ...], with reshaping defined by the constructor shape parameter.","title":"Returns:"},{"location":"sonnet/#raises_239","text":"ValueError : If output shape is incompatible with input shape; or if shape array contains non numeric entries; or if shape array contains more than 1 wildcard -1; or if the input array contains unknown, non-preserved dimensions (except when the unknown dimension is the only non-preserved dimension and doesn't actually need reshaping).","title":"Raises:"},{"location":"sonnet/#flattentrailingdimensionsconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"FlattenTrailingDimensions.connected_subgraphs"},{"location":"sonnet/#flattentrailingdimensionsdefun","text":"Wraps this modules call method in a callable graph function.","title":"FlattenTrailingDimensions.defun()"},{"location":"sonnet/#flattentrailingdimensionsdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"FlattenTrailingDimensions.defun_wrapped"},{"location":"sonnet/#flattentrailingdimensionsget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"FlattenTrailingDimensions.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_126","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_265","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_240","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#flattentrailingdimensionsget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"FlattenTrailingDimensions.get_possible_initializer_keys()"},{"location":"sonnet/#returns_266","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#flattentrailingdimensionsget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"FlattenTrailingDimensions.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_127","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_267","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_241","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#flattentrailingdimensionsgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"FlattenTrailingDimensions.graph"},{"location":"sonnet/#flattentrailingdimensionsinput_shape","text":"Returns shape of input Tensor passed at last call to build .","title":"FlattenTrailingDimensions.input_shape"},{"location":"sonnet/#flattentrailingdimensionsis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"FlattenTrailingDimensions.is_connected"},{"location":"sonnet/#flattentrailingdimensionslast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"FlattenTrailingDimensions.last_connected_subgraph"},{"location":"sonnet/#returns_268","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_242","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#flattentrailingdimensionsmodule_name","text":"Returns the name of the Module.","title":"FlattenTrailingDimensions.module_name"},{"location":"sonnet/#flattentrailingdimensionsname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"FlattenTrailingDimensions.name_scopes"},{"location":"sonnet/#flattentrailingdimensionsnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"FlattenTrailingDimensions.non_trainable_variables"},{"location":"sonnet/#returns_269","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_243","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#flattentrailingdimensionsscope_name","text":"Returns the full name of the Module's variable scope.","title":"FlattenTrailingDimensions.scope_name"},{"location":"sonnet/#flattentrailingdimensionstrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"FlattenTrailingDimensions.trainable_variables"},{"location":"sonnet/#returns_270","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_244","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#flattentrailingdimensionstransposenamenone","text":"Returns transpose batch reshape.","title":"FlattenTrailingDimensions.transpose(name=None)"},{"location":"sonnet/#flattentrailingdimensionsvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"FlattenTrailingDimensions.variable_scope"},{"location":"sonnet/#returns_271","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_245","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#flattentrailingdimensionsvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"FlattenTrailingDimensions.variables"},{"location":"sonnet/#returns_272","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_246","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-gru","text":"GRU recurrent network cell. The implementation is based on: https://arxiv.org/pdf/1412.3555v1.pdf. Attributes: state_size: Integer indicating the size of state tensor. output_size: Integer indicating the size of the core output.","title":"class GRU"},{"location":"sonnet/#gru__init__hidden_size-initializersnone-partitionersnone-regularizersnone-custom_getternone-namegru","text":"Construct GRU.","title":"GRU.__init__(hidden_size, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='gru')"},{"location":"sonnet/#args_128","text":"hidden_size : (int) Hidden size dimensionality. initializers : Dict containing ops to initialize the weights. This dict may contain any of the keys returned by GRU.get_possible_initializer_keys . partitioners : Optional dict containing partitioners to partition the weights and biases. As a default, no partitioners are used. This dict may contain any of the keys returned by GRU.get_possible_initializer_keys regularizers : Optional dict containing regularizers for the weights and biases. As a default, no regularizers are used. This dict may contain any of the keys returned by GRU.get_possible_initializer_keys custom_getter : Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. See the tf.get_variable documentation for more details. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_247","text":"KeyError : if initializers contains any keys not returned by GRU.get_possible_initializer_keys . KeyError : if partitioners contains any keys not returned by GRU.get_possible_initializer_keys . KeyError : if regularizers contains any keys not returned by GRU.get_possible_initializer_keys .","title":"Raises:"},{"location":"sonnet/#gru__call__inputs-prev_state","text":"Connects the GRU module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided as inputs and state must have the same final dimension, in order for the existing variables to be the correct size for their corresponding multiplications. The batch size may differ for each connection.","title":"GRU.__call__(inputs, prev_state)"},{"location":"sonnet/#args_129","text":"inputs : Tensor of size [batch_size, input_size] . prev_state : Tensor of size [batch_size, hidden_size] .","title":"Args:"},{"location":"sonnet/#returns_273","text":"A tuple (output, next_state) where output is a Tensor of size [batch_size, hidden_size] and next_state is a Tensor of size [batch_size, hidden_size] .","title":"Returns:"},{"location":"sonnet/#raises_248","text":"ValueError : If connecting the module into the graph any time after the first time, and the inferred size of the inputs does not match previous invocations.","title":"Raises:"},{"location":"sonnet/#gruconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"GRU.connected_subgraphs"},{"location":"sonnet/#grudefun","text":"Wraps this modules call method in a callable graph function.","title":"GRU.defun()"},{"location":"sonnet/#grudefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"GRU.defun_wrapped"},{"location":"sonnet/#gruget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"GRU.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_130","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_274","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_249","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#gruget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain.","title":"GRU.get_possible_initializer_keys()"},{"location":"sonnet/#the-set-of-all-possible-initializer-keys-are_1","text":"wz : weight for input - update cell uz : weight for prev_state - update cell bz : bias for update_cell wr : weight for input - reset cell ur : weight for prev_state - reset cell br : bias for reset cell wh : weight for input - candidate activation uh : weight for prev_state - candidate activation bh : bias for candidate activation","title":"The set of all possible initializer keys are:"},{"location":"sonnet/#returns_275","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#gruget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"GRU.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_131","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_276","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_250","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#grugraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"GRU.graph"},{"location":"sonnet/#gruinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs","text":"Builds the default start state for an RNNCore.","title":"GRU.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)"},{"location":"sonnet/#args_132","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_277","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_251","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#gruis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"GRU.is_connected"},{"location":"sonnet/#grulast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"GRU.last_connected_subgraph"},{"location":"sonnet/#returns_278","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_252","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#grumodule_name","text":"Returns the name of the Module.","title":"GRU.module_name"},{"location":"sonnet/#gruname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"GRU.name_scopes"},{"location":"sonnet/#grunon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"GRU.non_trainable_variables"},{"location":"sonnet/#returns_279","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_253","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#gruoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"GRU.output_size"},{"location":"sonnet/#gruscope_name","text":"Returns the full name of the Module's variable scope.","title":"GRU.scope_name"},{"location":"sonnet/#grustate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"GRU.state_size"},{"location":"sonnet/#grutrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"GRU.trainable_variables"},{"location":"sonnet/#returns_280","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_254","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#gruvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"GRU.variable_scope"},{"location":"sonnet/#returns_281","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_255","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#gruvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"GRU.variables"},{"location":"sonnet/#returns_282","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_256","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#gruzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"GRU.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_133","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_283","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-gridwarper","text":"Grid warper interface class. An object implementing the GridWarper interface generates a reference grid of feature points at construction time, and warps it via a parametric transformation model, specified at run time by an input parameter Tensor. Grid warpers must then implement a create_features function used to generate the reference grid to be warped in the forward pass (according to a determined warping model).","title":"class GridWarper"},{"location":"sonnet/#gridwarper__init__source_shape-output_shape-num_coeff-name-kwargs","text":"Constructs a GridWarper module and initializes the source grid params. source_shape and output_shape are used to define the size of the source and output signal domains, as opposed to the shape of the respective Tensors. For example, for an image of size width=W and height=H , {source,output}_shape=[H, W] ; for a volume of size width=W , height=H and depth=D , {source,output}_shape=[H, W, D] .","title":"GridWarper.__init__(source_shape, output_shape, num_coeff, name, **kwargs)"},{"location":"sonnet/#args_134","text":"source_shape : Iterable of integers determining the size of the source signal domain. output_shape : Iterable of integers determining the size of the destination resampled signal domain. num_coeff : Number of coefficients parametrizing the grid warp. For example, a 2D affine transformation will be defined by the 6 parameters populating the corresponding 2x3 affine matrix. name : Name of Module. **kwargs : Extra kwargs to be forwarded to the create_features function, instantiating the source grid parameters.","title":"Args:"},{"location":"sonnet/#raises_257","text":"Error : If len(output_shape) len(source_shape) . TypeError : If output_shape and source_shape are not both iterable.","title":"Raises:"},{"location":"sonnet/#gridwarper__call__args-kwargs","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"GridWarper.__call__(*args, **kwargs)"},{"location":"sonnet/#args_135","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_284","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#gridwarperconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"GridWarper.connected_subgraphs"},{"location":"sonnet/#gridwarperdefun","text":"Wraps this modules call method in a callable graph function.","title":"GridWarper.defun()"},{"location":"sonnet/#gridwarperdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"GridWarper.defun_wrapped"},{"location":"sonnet/#gridwarperget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"GridWarper.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_136","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_285","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_258","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#gridwarperget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"GridWarper.get_possible_initializer_keys()"},{"location":"sonnet/#returns_286","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#gridwarperget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"GridWarper.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_137","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_287","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_259","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#gridwarpergraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"GridWarper.graph"},{"location":"sonnet/#gridwarperis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"GridWarper.is_connected"},{"location":"sonnet/#gridwarperlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"GridWarper.last_connected_subgraph"},{"location":"sonnet/#returns_288","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_260","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#gridwarpermodule_name","text":"Returns the name of the Module.","title":"GridWarper.module_name"},{"location":"sonnet/#gridwarpern_coeff","text":"Returns number of coefficients of warping function.","title":"GridWarper.n_coeff"},{"location":"sonnet/#gridwarpername_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"GridWarper.name_scopes"},{"location":"sonnet/#gridwarpernon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"GridWarper.non_trainable_variables"},{"location":"sonnet/#returns_289","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_261","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#gridwarperoutput_shape","text":"Returns a tuple containing the shape of the output grid.","title":"GridWarper.output_shape"},{"location":"sonnet/#gridwarperpsi","text":"Returns a list of features used to compute the grid warp.","title":"GridWarper.psi"},{"location":"sonnet/#gridwarperscope_name","text":"Returns the full name of the Module's variable scope.","title":"GridWarper.scope_name"},{"location":"sonnet/#gridwarpersource_shape","text":"Returns a tuple containing the shape of the source signal.","title":"GridWarper.source_shape"},{"location":"sonnet/#gridwarpertrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"GridWarper.trainable_variables"},{"location":"sonnet/#returns_290","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_262","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#gridwarpervariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"GridWarper.variable_scope"},{"location":"sonnet/#returns_291","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_263","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#gridwarpervariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"GridWarper.variables"},{"location":"sonnet/#returns_292","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_264","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-highwaycore","text":"Recurrent Highway Network cell. The implementation is based on: https://arxiv.org/pdf/1607.03474v5.pdf As per the first lines of section 5 of the reference paper, 1 - T is used instead of a dedicated C gate. Attributes: state_size: Integer indicating the size of state tensor. output_size: Integer indicating the size of the core output.","title":"class HighwayCore"},{"location":"sonnet/#highwaycore__init__hidden_size-num_layers-initializersnone-partitionersnone-regularizersnone-custom_getternone-namehighwaycore","text":"Construct a new Recurrent Highway core.","title":"HighwayCore.__init__(hidden_size, num_layers, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='highwaycore')"},{"location":"sonnet/#args_138","text":"hidden_size : (int) Hidden size dimensionality. num_layers : (int) Number of highway layers. initializers : Dict containing ops to initialize the weights. This dict may contain any of the keys returned by HighwayCore.get_possible_initializer_keys . partitioners : Optional dict containing partitioners to partition the weights and biases. As a default, no partitioners are used. This dict may contain any of the keys returned by HighwayCore.get_possible_initializer_keys . regularizers : Optional dict containing regularizers for the weights and biases. As a default, no regularizers are used. This dict may contain any of the keys returned by HighwayCore.get_possible_initializer_keys . custom_getter : Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. See the tf.get_variable documentation for more details. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_265","text":"KeyError : if initializers contains any keys not returned by HighwayCore.get_possible_initializer_keys . KeyError : if partitioners contains any keys not returned by HighwayCore.get_possible_initializer_keys . KeyError : if regularizers contains any keys not returned by HighwayCore.get_possible_initializer_keys .","title":"Raises:"},{"location":"sonnet/#highwaycore__call__inputs-prev_state","text":"Connects the highway core module into the graph.","title":"HighwayCore.__call__(inputs, prev_state)"},{"location":"sonnet/#args_139","text":"inputs : Tensor of size [batch_size, input_size] . prev_state : Tensor of size [batch_size, hidden_size] .","title":"Args:"},{"location":"sonnet/#returns_293","text":"A tuple (output, next_state) where output is a Tensor of size [batch_size, hidden_size] and next_state is a Tensor of size [batch_size, hidden_size] .","title":"Returns:"},{"location":"sonnet/#raises_266","text":"ValueError : If connecting the module into the graph any time after the first time, and the inferred size of the inputs does not match previous invocations.","title":"Raises:"},{"location":"sonnet/#highwaycoreconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"HighwayCore.connected_subgraphs"},{"location":"sonnet/#highwaycoredefun","text":"Wraps this modules call method in a callable graph function.","title":"HighwayCore.defun()"},{"location":"sonnet/#highwaycoredefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"HighwayCore.defun_wrapped"},{"location":"sonnet/#highwaycoreget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"HighwayCore.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_140","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_294","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_267","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#highwaycoreget_possible_initializer_keysnum_layers","text":"Returns the keys the dictionary of variable initializers may contain.","title":"HighwayCore.get_possible_initializer_keys(num_layers)"},{"location":"sonnet/#the-set-of-all-possible-initializer-keys-are_2","text":"wt : weight for input - T gate wh : weight for input - H gate wtL : weight for prev state - T gate for layer L (indexed from 0) whL : weight for prev state - H gate for layer L (indexed from 0) btL : bias for prev state - T gate for layer L (indexed from 0) bhL : bias for prev state - H gate for layer L (indexed from 0)","title":"The set of all possible initializer keys are:"},{"location":"sonnet/#args_141","text":"num_layers : (int) Number of highway layers.","title":"Args:"},{"location":"sonnet/#returns_295","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#highwaycoreget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"HighwayCore.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_142","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_296","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_268","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#highwaycoregraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"HighwayCore.graph"},{"location":"sonnet/#highwaycoreinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs","text":"Builds the default start state for an RNNCore.","title":"HighwayCore.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)"},{"location":"sonnet/#args_143","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_297","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_269","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#highwaycoreis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"HighwayCore.is_connected"},{"location":"sonnet/#highwaycorelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"HighwayCore.last_connected_subgraph"},{"location":"sonnet/#returns_298","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_270","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#highwaycoremodule_name","text":"Returns the name of the Module.","title":"HighwayCore.module_name"},{"location":"sonnet/#highwaycorename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"HighwayCore.name_scopes"},{"location":"sonnet/#highwaycorenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"HighwayCore.non_trainable_variables"},{"location":"sonnet/#returns_299","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_271","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#highwaycoreoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"HighwayCore.output_size"},{"location":"sonnet/#highwaycorescope_name","text":"Returns the full name of the Module's variable scope.","title":"HighwayCore.scope_name"},{"location":"sonnet/#highwaycorestate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"HighwayCore.state_size"},{"location":"sonnet/#highwaycoretrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"HighwayCore.trainable_variables"},{"location":"sonnet/#returns_300","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_272","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#highwaycorevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"HighwayCore.variable_scope"},{"location":"sonnet/#returns_301","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_273","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#highwaycorevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"HighwayCore.variables"},{"location":"sonnet/#returns_302","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_274","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#highwaycorezero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"HighwayCore.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_144","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_303","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-inplaneconv2d","text":"Applies an in-plane convolution to each channel with tied filter weights. This acts as a light wrapper around the TensorFlow op tf.nn.depthwise_conv2d ; it differs from the DepthWiseConv2D module in that it has tied weights (i.e. the same filter) for all the in-out channel pairs.","title":"class InPlaneConv2D"},{"location":"sonnet/#inplaneconv2d__init__kernel_shape-stride1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatnhwc-custom_getternone-namein_plane_conv2d","text":"Constructs an InPlaneConv2D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution","title":"InPlaneConv2D.__init__(kernel_shape, stride=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NHWC', custom_getter=None, name='in_plane_conv2d')"},{"location":"sonnet/#args_145","text":"kernel_shape : Iterable with 2 elements in the layout [filter_height, filter_width]; or integer that is used to define the list in all dimensions. stride : Iterable with 2 or 4 elements of kernel strides, or integer that is used to define stride in all dimensions. padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 2. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition the filters (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NHWC), or the second dimension (NCHW). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_275","text":"ValueError : If the given data_format is not a supported format (see SUPPORTED_2D_DATA_FORMATS ). base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. ValueError : If the given padding is not snt.VALID , snt.SAME , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a sequence of these. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. ValueError : If the passed-in data_format doesn't have a channel dimension.","title":"Raises:"},{"location":"sonnet/#inplaneconv2d__call__inputs","text":"Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection.","title":"InPlaneConv2D.__call__(inputs)"},{"location":"sonnet/#args_146","text":"inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Args:"},{"location":"sonnet/#returns_304","text":"A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels].","title":"Returns:"},{"location":"sonnet/#raises_276","text":"ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Raises:"},{"location":"sonnet/#inplaneconv2db","text":"Returns the Variable containing the bias.","title":"InPlaneConv2D.b"},{"location":"sonnet/#returns_305","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_277","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias.","title":"Raises:"},{"location":"sonnet/#inplaneconv2dclonenamenone","text":"Returns a cloned _ConvND module.","title":"InPlaneConv2D.clone(name=None)"},{"location":"sonnet/#args_147","text":"name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name .","title":"Args:"},{"location":"sonnet/#returns_306","text":"A copy of the current class.","title":"Returns:"},{"location":"sonnet/#inplaneconv2dconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"InPlaneConv2D.connected_subgraphs"},{"location":"sonnet/#inplaneconv2dconv_op_padding","text":"Returns the padding algorithm used for the underlying convolution op.","title":"InPlaneConv2D.conv_op_padding"},{"location":"sonnet/#inplaneconv2ddata_format","text":"Returns the data format.","title":"InPlaneConv2D.data_format"},{"location":"sonnet/#inplaneconv2ddefun","text":"Wraps this modules call method in a callable graph function.","title":"InPlaneConv2D.defun()"},{"location":"sonnet/#inplaneconv2ddefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"InPlaneConv2D.defun_wrapped"},{"location":"sonnet/#inplaneconv2dget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"InPlaneConv2D.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_148","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_307","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_278","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#inplaneconv2dget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"InPlaneConv2D.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_308","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#inplaneconv2dget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"InPlaneConv2D.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_149","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_309","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_279","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#inplaneconv2dgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"InPlaneConv2D.graph"},{"location":"sonnet/#inplaneconv2dhas_bias","text":"Returns True if bias Variable is present in the module.","title":"InPlaneConv2D.has_bias"},{"location":"sonnet/#inplaneconv2dinitializers","text":"Returns the initializers dictionary.","title":"InPlaneConv2D.initializers"},{"location":"sonnet/#inplaneconv2dinput_channels","text":"Returns the number of input channels.","title":"InPlaneConv2D.input_channels"},{"location":"sonnet/#inplaneconv2dinput_shape","text":"Returns the input shape.","title":"InPlaneConv2D.input_shape"},{"location":"sonnet/#inplaneconv2dis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"InPlaneConv2D.is_connected"},{"location":"sonnet/#inplaneconv2dkernel_shape","text":"Returns the kernel shape.","title":"InPlaneConv2D.kernel_shape"},{"location":"sonnet/#inplaneconv2dlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"InPlaneConv2D.last_connected_subgraph"},{"location":"sonnet/#returns_310","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_280","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#inplaneconv2dmask","text":"Returns the mask.","title":"InPlaneConv2D.mask"},{"location":"sonnet/#inplaneconv2dmodule_name","text":"Returns the name of the Module.","title":"InPlaneConv2D.module_name"},{"location":"sonnet/#inplaneconv2dname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"InPlaneConv2D.name_scopes"},{"location":"sonnet/#inplaneconv2dnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"InPlaneConv2D.non_trainable_variables"},{"location":"sonnet/#returns_311","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_281","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#inplaneconv2doutput_channels","text":"Returns the number of output channels.","title":"InPlaneConv2D.output_channels"},{"location":"sonnet/#inplaneconv2dpadding","text":"Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension.","title":"InPlaneConv2D.padding"},{"location":"sonnet/#returns_312","text":"The padding algorithm used, if this is the same for all dimensions.","title":"Returns:"},{"location":"sonnet/#raises_282","text":"ValueError : If different padding algorithms are used for different dimensions.","title":"Raises:"},{"location":"sonnet/#inplaneconv2dpaddings","text":"Returns a tuple with the padding algorithm used for each dimension.","title":"InPlaneConv2D.paddings"},{"location":"sonnet/#inplaneconv2dpartitioners","text":"Returns the partitioners dictionary.","title":"InPlaneConv2D.partitioners"},{"location":"sonnet/#inplaneconv2drate","text":"Returns the dilation rate.","title":"InPlaneConv2D.rate"},{"location":"sonnet/#inplaneconv2dregularizers","text":"Returns the regularizers dictionary.","title":"InPlaneConv2D.regularizers"},{"location":"sonnet/#inplaneconv2dscope_name","text":"Returns the full name of the Module's variable scope.","title":"InPlaneConv2D.scope_name"},{"location":"sonnet/#inplaneconv2dstride","text":"Returns the stride.","title":"InPlaneConv2D.stride"},{"location":"sonnet/#inplaneconv2dtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"InPlaneConv2D.trainable_variables"},{"location":"sonnet/#returns_313","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_283","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#inplaneconv2dvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"InPlaneConv2D.variable_scope"},{"location":"sonnet/#returns_314","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_284","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#inplaneconv2dvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"InPlaneConv2D.variables"},{"location":"sonnet/#returns_315","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_285","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#inplaneconv2dw","text":"Returns the Variable containing the weight matrix.","title":"InPlaneConv2D.w"},{"location":"sonnet/#class-incompatibleshapeerror","text":"Error raised when the shape of the input at build time is incompatible.","title":"class IncompatibleShapeError"},{"location":"sonnet/#class-lstm","text":"LSTM recurrent network cell with optional peepholes layer normalization. The implementation is based on: http://arxiv.org/abs/1409.2329. We add forget_bias (default: 1) to the biases of the forget gate in order to reduce the scale of forgetting in the beginning of the training.","title":"class LSTM"},{"location":"sonnet/#layer-normalization","text":"This is described in https://arxiv.org/pdf/1607.06450.pdf","title":"Layer normalization"},{"location":"sonnet/#peep-hole-connections_1","text":"Peep-hole connections may optionally be used by specifying a flag in the constructor. These connections can aid increasing the precision of output timing, for more details see: https://research.google.com/pubs/archive/43905.pdf","title":"Peep-hole connections"},{"location":"sonnet/#recurrent-projections","text":"Projection of the recurrent state, to reduce model parameters and speed up computation. For more details see: https://arxiv.org/abs/1402.1128 Attributes: state_size: Tuple of tf.TensorShape s indicating the size of state tensors. output_size: tf.TensorShape indicating the size of the core output. use_peepholes: Boolean indicating whether peephole connections are used.","title":"Recurrent projections"},{"location":"sonnet/#lstm__init__hidden_size-forget_bias10-initializersnone-partitionersnone-regularizersnone-use_peepholesfalse-use_layer_normfalse-hidden_clip_valuenone-projection_sizenone-cell_clip_valuenone-custom_getternone-namelstm","text":"Construct LSTM.","title":"LSTM.__init__(hidden_size, forget_bias=1.0, initializers=None, partitioners=None, regularizers=None, use_peepholes=False, use_layer_norm=False, hidden_clip_value=None, projection_size=None, cell_clip_value=None, custom_getter=None, name='lstm')"},{"location":"sonnet/#args_150","text":"hidden_size : (int) Hidden size dimensionality. forget_bias : (float) Bias for the forget activation. initializers : Dict containing ops to initialize the weights. This dictionary may contain any of the keys returned by LSTM.get_possible_initializer_keys . partitioners : Optional dict containing partitioners to partition the weights and biases. As a default, no partitioners are used. This dict may contain any of the keys returned by LSTM.get_possible_initializer_keys . regularizers : Optional dict containing regularizers for the weights and biases. As a default, no regularizers are used. This dict may contain any of the keys returned by LSTM.get_possible_initializer_keys . use_peepholes : Boolean that indicates whether peephole connections are used. use_layer_norm : Boolean that indicates whether to apply layer normalization. hidden_clip_value : Optional number; if set, then the LSTM hidden state vector is clipped by this value. projection_size : Optional number; if set, then the LSTM hidden state is projected to this size via a learnable projection matrix. cell_clip_value : Optional number; if set, then the LSTM cell vector is clipped by this value. custom_getter : Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. See the tf.get_variable documentation for more details. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_286","text":"KeyError : if initializers contains any keys not returned by LSTM.get_possible_initializer_keys . KeyError : if partitioners contains any keys not returned by LSTM.get_possible_initializer_keys . KeyError : if regularizers contains any keys not returned by LSTM.get_possible_initializer_keys . ValueError : if a peephole initializer is passed in the initializer list, but use_peepholes is False.","title":"Raises:"},{"location":"sonnet/#lstm__call__inputs-prev_state","text":"Connects the LSTM module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided as inputs and state must have the same final dimension, in order for the existing variables to be the correct size for their corresponding multiplications. The batch size may differ for each connection.","title":"LSTM.__call__(inputs, prev_state)"},{"location":"sonnet/#args_151","text":"inputs : Tensor of size [batch_size, input_size] . prev_state : Tuple (prev_hidden, prev_cell).","title":"Args:"},{"location":"sonnet/#returns_316","text":"A tuple (output, next_state) where 'output' is a Tensor of size [batch_size, hidden_size] and 'next_state' is a LSTMState namedtuple (next_hidden, next_cell) where next_hidden and next_cell have size [batch_size, hidden_size] . If projection_size is specified, then next_hidden will have size [batch_size, projection_size] .","title":"Returns:"},{"location":"sonnet/#raises_287","text":"ValueError : If connecting the module into the graph any time after the first time, and the inferred size of the inputs does not match previous invocations.","title":"Raises:"},{"location":"sonnet/#lstmconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"LSTM.connected_subgraphs"},{"location":"sonnet/#lstmdefun","text":"Wraps this modules call method in a callable graph function.","title":"LSTM.defun()"},{"location":"sonnet/#lstmdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"LSTM.defun_wrapped"},{"location":"sonnet/#lstmget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"LSTM.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_152","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_317","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_288","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmget_possible_initializer_keysuse_peepholesfalse-use_projectionfalse","text":"Returns the keys the dictionary of variable initializers may contain.","title":"LSTM.get_possible_initializer_keys(use_peepholes=False, use_projection=False)"},{"location":"sonnet/#the-set-of-all-possible-initializer-keys-are_3","text":"w_gates : weight for gates b_gates : bias of gates w_f_diag : weight for prev_cell - forget gate peephole w_i_diag : weight for prev_cell - input gate peephole w_o_diag : weight for prev_cell - output gate peephole","title":"The set of all possible initializer keys are:"},{"location":"sonnet/#args_153","text":"cls:The class. use_peepholes : Boolean that indicates whether peephole connections are used. use_projection : Boolean that indicates whether a recurrent projection layer is used.","title":"Args:"},{"location":"sonnet/#returns_318","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#lstmget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"LSTM.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_154","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_319","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_289","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"LSTM.graph"},{"location":"sonnet/#lstminitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs","text":"Builds the default start state for an RNNCore.","title":"LSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)"},{"location":"sonnet/#args_155","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_320","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_290","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#lstmis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"LSTM.is_connected"},{"location":"sonnet/#lstmlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"LSTM.last_connected_subgraph"},{"location":"sonnet/#returns_321","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_291","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmmodule_name","text":"Returns the name of the Module.","title":"LSTM.module_name"},{"location":"sonnet/#lstmname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"LSTM.name_scopes"},{"location":"sonnet/#lstmnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"LSTM.non_trainable_variables"},{"location":"sonnet/#returns_322","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_292","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmoutput_size","text":"tf.TensorShape indicating the size of the core output.","title":"LSTM.output_size"},{"location":"sonnet/#lstmscope_name","text":"Returns the full name of the Module's variable scope.","title":"LSTM.scope_name"},{"location":"sonnet/#lstmstate_size","text":"Tuple of tf.TensorShape s indicating the size of state tensors.","title":"LSTM.state_size"},{"location":"sonnet/#lstmtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"LSTM.trainable_variables"},{"location":"sonnet/#returns_323","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_293","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmuse_layer_norm","text":"Boolean indicating whether layer norm is enabled.","title":"LSTM.use_layer_norm"},{"location":"sonnet/#lstmuse_peepholes","text":"Boolean indicating whether peephole connections are used.","title":"LSTM.use_peepholes"},{"location":"sonnet/#lstmvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"LSTM.variable_scope"},{"location":"sonnet/#returns_324","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_294","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"LSTM.variables"},{"location":"sonnet/#returns_325","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_295","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"LSTM.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_156","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_326","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-lstmblockcell","text":"Wraps the TensorFlow LSTMBlockCell as a Sonnet RNNCore.","title":"class LSTMBlockCell"},{"location":"sonnet/#lstmblockcell__init__num_units-forget_bias10-cell_clipnone-use_peepholefalse-dtypenone-reusenone-namelstm_cell","text":"Initialize the basic LSTM cell.","title":"LSTMBlockCell.__init__(num_units, forget_bias=1.0, cell_clip=None, use_peephole=False, dtype=None, reuse=None, name='lstm_cell')"},{"location":"sonnet/#args_157","text":"num_units : int, The number of units in the LSTM cell. forget_bias : float, The bias added to forget gates (see above). cell_clip : An optional float . Defaults to -1 (no clipping). use_peephole : Whether to use peephole connections or not. dtype : the variable dtype of this layer. Default to tf.float32. reuse : (optional) boolean describing whether to reuse variables in an existing scope. If not True , and the existing scope already has the given variables, an error is raised. name : String, the name of the layer. Layers with the same name will share weights, but to avoid mistakes we require reuse=True in such cases. By default this is \"lstm_cell\", for variable-name compatibility with tf.compat.v1.nn.rnn_cell.LSTMCell . When restoring from CudnnLSTM-trained checkpoints, must use CudnnCompatibleLSTMBlockCell instead.","title":"Args:"},{"location":"sonnet/#lstmblockcell__call__inputs-prev_state","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"LSTMBlockCell.__call__(inputs, prev_state)"},{"location":"sonnet/#args_158","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_327","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#lstmblockcellconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"LSTMBlockCell.connected_subgraphs"},{"location":"sonnet/#lstmblockcelldefun","text":"Wraps this modules call method in a callable graph function.","title":"LSTMBlockCell.defun()"},{"location":"sonnet/#lstmblockcelldefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"LSTMBlockCell.defun_wrapped"},{"location":"sonnet/#lstmblockcellget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"LSTMBlockCell.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_159","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_328","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_296","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmblockcellget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"LSTMBlockCell.get_possible_initializer_keys()"},{"location":"sonnet/#returns_329","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#lstmblockcellget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"LSTMBlockCell.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_160","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_330","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_297","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmblockcellgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"LSTMBlockCell.graph"},{"location":"sonnet/#lstmblockcellinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs","text":"Builds the default start state for an RNNCore.","title":"LSTMBlockCell.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)"},{"location":"sonnet/#args_161","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_331","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_298","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#lstmblockcellis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"LSTMBlockCell.is_connected"},{"location":"sonnet/#lstmblockcelllast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"LSTMBlockCell.last_connected_subgraph"},{"location":"sonnet/#returns_332","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_299","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmblockcellmodule_name","text":"Returns the name of the Module.","title":"LSTMBlockCell.module_name"},{"location":"sonnet/#lstmblockcellname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"LSTMBlockCell.name_scopes"},{"location":"sonnet/#lstmblockcellnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"LSTMBlockCell.non_trainable_variables"},{"location":"sonnet/#returns_333","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_300","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmblockcelloutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"LSTMBlockCell.output_size"},{"location":"sonnet/#lstmblockcellscope_name","text":"Returns the full name of the Module's variable scope.","title":"LSTMBlockCell.scope_name"},{"location":"sonnet/#lstmblockcellstate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"LSTMBlockCell.state_size"},{"location":"sonnet/#lstmblockcelltrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"LSTMBlockCell.trainable_variables"},{"location":"sonnet/#returns_334","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_301","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmblockcellvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"LSTMBlockCell.variable_scope"},{"location":"sonnet/#returns_335","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_302","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmblockcellvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"LSTMBlockCell.variables"},{"location":"sonnet/#returns_336","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_303","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lstmblockcellzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"LSTMBlockCell.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_162","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_337","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-lstmstate","text":"LSTMState(hidden, cell)","title":"class LSTMState"},{"location":"sonnet/#lstmstatecell","text":"Alias for field number 1","title":"LSTMState.cell"},{"location":"sonnet/#lstmstatehidden","text":"Alias for field number 0","title":"LSTMState.hidden"},{"location":"sonnet/#class-layernorm","text":"Layer normalization module. Implementation based on: https://arxiv.org/abs/1607.06450 This module transforms input x into: outputs = gamma * (x - mu) / sigma + beta where mu and sigma are respectively the mean and standard deviation of x. Gamma and beta are trainable parameters for scaling and shifting respectively. Since the axes over which normalization is perfomed is configurable, this also subsumes instance normalization.","title":"class LayerNorm"},{"location":"sonnet/#layernorm__init__axisnone-offsettrue-scaletrue-eps1e-05-initializersnone-partitionersnone-regularizersnone-namelayer_norm","text":"Constructs a LayerNorm module.","title":"LayerNorm.__init__(axis=None, offset=True, scale=True, eps=1e-05, initializers=None, partitioners=None, regularizers=None, name='layer_norm')"},{"location":"sonnet/#args_163","text":"axis : Optional dimension or iterable of indices of dimensions to normalize and reduce over. By default None and all dimensions except the first/batch dimension are reduced over. If the input tensor represents an image, summing over all except the batch and channel dimensions (e.g. for image format NHWC, axes=[1,2]), then this module corresponds to Instance Normalization (https://arxiv.org/abs/1607.08022). offset : Optional boolean to specify whether or not to apply a trained component-wise bias after the layer normalization and scaling. scale : Optional boolean to specify whether or not to apply a trained component-wise scale after the layer normalization. eps : small epsilon to avoid division by zero variance. Defaults to 1e-5 as used in the paper. initializers : Dict containing ops to initialize the scale (with key 'gamma') and bias (with key 'beta'). partitioners : Optional dict containing partitioners to partition the scale (with key 'gamma') and bias (with key 'beta'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the scale (with key 'gamma') and bias (with key 'beta').. As a default, no regularizers are used. name : name of the module.","title":"Args:"},{"location":"sonnet/#raises_304","text":"KeyError : If initializers , partitioners or regularizers contain any keys other than gamma or beta . TypeError : If any of the given initializers, partitioners or regularizers are not callable.","title":"Raises:"},{"location":"sonnet/#layernorm__call__inputs","text":"Connects the LayerNorm module into the graph.","title":"LayerNorm.__call__(inputs)"},{"location":"sonnet/#args_164","text":"inputs : a Tensor of dimensionality = 2.","title":"Args:"},{"location":"sonnet/#returns_338","text":"normalized : layer normalized outputs with same shape as inputs.","title":"Returns:"},{"location":"sonnet/#raises_305","text":"base.NotSupportedError: If inputs has less than 2 dimensions.","title":"Raises:"},{"location":"sonnet/#layernormbeta","text":"","title":"LayerNorm.beta"},{"location":"sonnet/#layernormconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"LayerNorm.connected_subgraphs"},{"location":"sonnet/#layernormdefun","text":"Wraps this modules call method in a callable graph function.","title":"LayerNorm.defun()"},{"location":"sonnet/#layernormdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"LayerNorm.defun_wrapped"},{"location":"sonnet/#layernormgamma","text":"","title":"LayerNorm.gamma"},{"location":"sonnet/#layernormget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"LayerNorm.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_165","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_339","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_306","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#layernormget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"LayerNorm.get_possible_initializer_keys()"},{"location":"sonnet/#returns_340","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#layernormget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"LayerNorm.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_166","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_341","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_307","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#layernormgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"LayerNorm.graph"},{"location":"sonnet/#layernorminitializers","text":"","title":"LayerNorm.initializers"},{"location":"sonnet/#layernormis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"LayerNorm.is_connected"},{"location":"sonnet/#layernormlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"LayerNorm.last_connected_subgraph"},{"location":"sonnet/#returns_342","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_308","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#layernormmodule_name","text":"Returns the name of the Module.","title":"LayerNorm.module_name"},{"location":"sonnet/#layernormname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"LayerNorm.name_scopes"},{"location":"sonnet/#layernormnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"LayerNorm.non_trainable_variables"},{"location":"sonnet/#returns_343","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_309","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#layernormpartitioners","text":"","title":"LayerNorm.partitioners"},{"location":"sonnet/#layernormregularizers","text":"","title":"LayerNorm.regularizers"},{"location":"sonnet/#layernormscope_name","text":"Returns the full name of the Module's variable scope.","title":"LayerNorm.scope_name"},{"location":"sonnet/#layernormtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"LayerNorm.trainable_variables"},{"location":"sonnet/#returns_344","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_310","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#layernormvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"LayerNorm.variable_scope"},{"location":"sonnet/#returns_345","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_311","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#layernormvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"LayerNorm.variables"},{"location":"sonnet/#returns_346","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_312","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-linear","text":"Linear module, optionally including bias.","title":"class Linear"},{"location":"sonnet/#linear__init__output_size-use_biastrue-initializersnone-partitionersnone-regularizersnone-custom_getternone-namelinear","text":"Constructs a Linear module.","title":"Linear.__init__(output_size, use_bias=True, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='linear')"},{"location":"sonnet/#args_167","text":"output_size : Output dimensionality. output_size can be either an integer or a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that output_size can be called, returning an integer, when build is called. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing initializers to initialize the weights (with key 'w') or biases (with key 'b'). The default initializer for the weights is a truncated normal initializer, which is commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). The default initializer for the bias is a zero initializer. partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the weights (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_313","text":"KeyError : If initializers , partitioners or regularizers contains any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable.","title":"Raises:"},{"location":"sonnet/#linear__call__inputs","text":"Connects the Linear module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the Tensor provided here must have the same final dimension, in order for the existing variables to be the correct size for the multiplication. The batch size may differ for each connection.","title":"Linear.__call__(inputs)"},{"location":"sonnet/#args_168","text":"inputs : A 2D Tensor of size [batch_size, input_size].","title":"Args:"},{"location":"sonnet/#returns_347","text":"A 2D Tensor of size [batch_size, output_size].","title":"Returns:"},{"location":"sonnet/#raises_314","text":"base.IncompatibleShapeError: If the input is not a 2-D Tensor with the size of the second dimension specified. base.IncompatibleShapeError: If reconnecting an already connected module into the graph, and the shape of the input is not compatible with previous inputs.","title":"Raises:"},{"location":"sonnet/#linearb","text":"Returns the Variable containing the bias.","title":"Linear.b"},{"location":"sonnet/#returns_348","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_315","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias.","title":"Raises:"},{"location":"sonnet/#linearclonenamenone","text":"Returns a cloned Linear module.","title":"Linear.clone(name=None)"},{"location":"sonnet/#args_169","text":"name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name .","title":"Args:"},{"location":"sonnet/#returns_349","text":"Cloned Linear module.","title":"Returns:"},{"location":"sonnet/#linearconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Linear.connected_subgraphs"},{"location":"sonnet/#lineardefun","text":"Wraps this modules call method in a callable graph function.","title":"Linear.defun()"},{"location":"sonnet/#lineardefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Linear.defun_wrapped"},{"location":"sonnet/#linearget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Linear.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_170","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_350","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_316","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#linearget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Linear.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_351","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#linearget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"Linear.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_171","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_352","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_317","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lineargraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Linear.graph"},{"location":"sonnet/#linearhas_bias","text":"Returns True if bias Variable is present in the module.","title":"Linear.has_bias"},{"location":"sonnet/#linearinitializers","text":"Returns the initializers dictionary.","title":"Linear.initializers"},{"location":"sonnet/#linearinput_shape","text":"Returns shape of input Tensor passed at last call to build .","title":"Linear.input_shape"},{"location":"sonnet/#linearis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Linear.is_connected"},{"location":"sonnet/#linearlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Linear.last_connected_subgraph"},{"location":"sonnet/#returns_353","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_318","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#linearmodule_name","text":"Returns the name of the Module.","title":"Linear.module_name"},{"location":"sonnet/#linearname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Linear.name_scopes"},{"location":"sonnet/#linearnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Linear.non_trainable_variables"},{"location":"sonnet/#returns_354","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_319","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#linearoutput_size","text":"Returns the module output size.","title":"Linear.output_size"},{"location":"sonnet/#linearpartitioners","text":"Returns the partitioners dictionary.","title":"Linear.partitioners"},{"location":"sonnet/#linearregularizers","text":"Returns the regularizers dictionary.","title":"Linear.regularizers"},{"location":"sonnet/#linearscope_name","text":"Returns the full name of the Module's variable scope.","title":"Linear.scope_name"},{"location":"sonnet/#lineartrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Linear.trainable_variables"},{"location":"sonnet/#returns_355","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_320","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#lineartransposenamenone","text":"Returns transposed Linear module.","title":"Linear.transpose(name=None)"},{"location":"sonnet/#args_172","text":"name : Optional string assigning name of transpose module. The default name is constructed by appending \"_transpose\" to self.module_name .","title":"Args:"},{"location":"sonnet/#returns_356","text":"Transposed Linear module.","title":"Returns:"},{"location":"sonnet/#linearvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Linear.variable_scope"},{"location":"sonnet/#returns_357","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_321","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#linearvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Linear.variables"},{"location":"sonnet/#returns_358","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_322","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#linearw","text":"Returns the Variable containing the weight matrix.","title":"Linear.w"},{"location":"sonnet/#returns_359","text":"Variable object containing the weights, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_323","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist.","title":"Raises:"},{"location":"sonnet/#class-mergedims","text":"Merges a tensor or nested list of tensors along a range of dimensions. Tensors are reshaped by specifying the range of dimensions to merge. Hence, the reshape can be performed without knowing in advance the rank of the input tensor. For example, merging dimensions 1, 2 and 3 together can be performed by calling: output = MergeDims(start=1, size=3)(x) A nested list of tensors can be merged: x = [tf.random_uniform(shape=[5, 5]), [tf.random_uniform(shape=[3, 3, 3])]] output = MergeDims(start=0, size=2)(x)","title":"class MergeDims"},{"location":"sonnet/#mergedims__init__start-size-namemerge_dims","text":"Constructs the MergeDims module.","title":"MergeDims.__init__(start, size, name='merge_dims')"},{"location":"sonnet/#args_173","text":"start : Start of the range of dimensions to merge. size : Size the range of dimensions to merge. name : The name of the module.","title":"Args:"},{"location":"sonnet/#raises_324","text":"ValueError : If size is not strictly greater than 1.","title":"Raises:"},{"location":"sonnet/#mergedims__call__inputs","text":"Connects the MergeDims module into the graph.","title":"MergeDims.__call__(inputs)"},{"location":"sonnet/#args_174","text":"inputs : Tensor or a nested list of Tensors to merge. Its rank must be greater than or equal to start + size .","title":"Args:"},{"location":"sonnet/#returns_360","text":"The merged Tensor or a nested list of merged Tensors.","title":"Returns:"},{"location":"sonnet/#raises_325","text":"ValueError : If any of the inputs tensors has insufficient rank.","title":"Raises:"},{"location":"sonnet/#mergedimsconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"MergeDims.connected_subgraphs"},{"location":"sonnet/#mergedimsdefun","text":"Wraps this modules call method in a callable graph function.","title":"MergeDims.defun()"},{"location":"sonnet/#mergedimsdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"MergeDims.defun_wrapped"},{"location":"sonnet/#mergedimsget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"MergeDims.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_175","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_361","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_326","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#mergedimsget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"MergeDims.get_possible_initializer_keys()"},{"location":"sonnet/#returns_362","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#mergedimsget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"MergeDims.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_176","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_363","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_327","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#mergedimsgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"MergeDims.graph"},{"location":"sonnet/#mergedimsis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"MergeDims.is_connected"},{"location":"sonnet/#mergedimslast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"MergeDims.last_connected_subgraph"},{"location":"sonnet/#returns_364","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_328","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#mergedimsmodule_name","text":"Returns the name of the Module.","title":"MergeDims.module_name"},{"location":"sonnet/#mergedimsname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"MergeDims.name_scopes"},{"location":"sonnet/#mergedimsnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"MergeDims.non_trainable_variables"},{"location":"sonnet/#returns_365","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_329","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#mergedimsscope_name","text":"Returns the full name of the Module's variable scope.","title":"MergeDims.scope_name"},{"location":"sonnet/#mergedimstrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"MergeDims.trainable_variables"},{"location":"sonnet/#returns_366","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_330","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#mergedimsvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"MergeDims.variable_scope"},{"location":"sonnet/#returns_367","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_331","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#mergedimsvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"MergeDims.variables"},{"location":"sonnet/#returns_368","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_332","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-modelrnn","text":"RNNCore that ignores input and uses a model to compute its next state.","title":"class ModelRNN"},{"location":"sonnet/#modelrnn__init__model-namemodel_rnn","text":"Construct a Basic RNN core.","title":"ModelRNN.__init__(model, name='model_rnn')"},{"location":"sonnet/#args_177","text":"model : callable that computes the next state. name : name of the module.","title":"Args:"},{"location":"sonnet/#raises_333","text":"TypeError : if model is not a callable object or if it is an RNNCore. AttributeError : if model does not have an output_size attribute.","title":"Raises:"},{"location":"sonnet/#modelrnn__call__inputs-prev_state","text":"Connects the ModelRNN module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided as input_ and state must have the same final dimension, in order for the existing variables to be the correct size for their corresponding multiplications. The batch size may differ for each connection.","title":"ModelRNN.__call__(inputs, prev_state)"},{"location":"sonnet/#args_178","text":"inputs : Tensor input to the ModelRNN (ignored). prev_state : Tensor of size model.output_size .","title":"Args:"},{"location":"sonnet/#returns_369","text":"output : Tensor of size model.output_size . next_state : Tensor of size model.output_size .","title":"Returns:"},{"location":"sonnet/#modelrnnconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"ModelRNN.connected_subgraphs"},{"location":"sonnet/#modelrnndefun","text":"Wraps this modules call method in a callable graph function.","title":"ModelRNN.defun()"},{"location":"sonnet/#modelrnndefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"ModelRNN.defun_wrapped"},{"location":"sonnet/#modelrnnget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"ModelRNN.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_179","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_370","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_334","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#modelrnnget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"ModelRNN.get_possible_initializer_keys()"},{"location":"sonnet/#returns_371","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#modelrnnget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"ModelRNN.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_180","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_372","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_335","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#modelrnngraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"ModelRNN.graph"},{"location":"sonnet/#modelrnninitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs","text":"Builds the default start state for an RNNCore.","title":"ModelRNN.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)"},{"location":"sonnet/#args_181","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_373","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_336","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#modelrnnis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"ModelRNN.is_connected"},{"location":"sonnet/#modelrnnlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"ModelRNN.last_connected_subgraph"},{"location":"sonnet/#returns_374","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_337","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#modelrnnmodule_name","text":"Returns the name of the Module.","title":"ModelRNN.module_name"},{"location":"sonnet/#modelrnnname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"ModelRNN.name_scopes"},{"location":"sonnet/#modelrnnnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"ModelRNN.non_trainable_variables"},{"location":"sonnet/#returns_375","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_338","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#modelrnnoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"ModelRNN.output_size"},{"location":"sonnet/#modelrnnscope_name","text":"Returns the full name of the Module's variable scope.","title":"ModelRNN.scope_name"},{"location":"sonnet/#modelrnnstate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"ModelRNN.state_size"},{"location":"sonnet/#modelrnntrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"ModelRNN.trainable_variables"},{"location":"sonnet/#returns_376","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_339","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#modelrnnvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"ModelRNN.variable_scope"},{"location":"sonnet/#returns_377","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_340","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#modelrnnvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"ModelRNN.variables"},{"location":"sonnet/#returns_378","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_341","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#modelrnnzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"ModelRNN.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_182","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_379","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-module","text":"Module wrapping a function provided by the user.","title":"class Module"},{"location":"sonnet/#module__init__build-custom_getternone-namenone","text":"Constructs a module with a given build function. The Module class can be used to wrap a function assembling a network into a module. For example, the following code implements a simple one-hidden-layer MLP model by defining a function called make_model and using a Module instance to wrap it. def make_model(inputs): lin1 = snt.Linear(name= lin1 , output_size=10)(inputs) relu1 = tf.nn.relu(lin1, name= relu1 ) lin2 = snt.Linear(name= lin2 , output_size=20)(relu1) return lin2 model = snt.Module(name='simple_mlp', build=make_model) outputs = model(inputs) The partial package from functools can be used to bake configuration parameters into the function at construction time, as shown in the following example. from functools import partial def make_model(inputs, output_sizes): lin1 = snt.Linear(name= lin1 , output_size=output_sizes[0])(inputs) relu1 = tf.nn.relu(lin1, name= relu1 ) lin2 = snt.Linear(name= lin2 , output_size=output_sizes[1])(relu1) return lin2 model = snt.Module(name='simple_mlp', build=partial(make_model, output_sizes=[10, 20]) outputs = model(inputs)","title":"Module.__init__(build, custom_getter=None, name=None)"},{"location":"sonnet/#args_183","text":"build : Callable to be invoked when connecting the module to the graph. The build function is invoked when the module is called, and its role is to specify how to add elements to the Graph, and how to compute output Tensors from input Tensors. The build function signature can include the following parameters: args - Input Tensors. *kwargs - Additional Python parameters controlling connection. custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Module name. If set to None (the default), the name will be set to that of the build callable converted to snake_case . If build has no name, the name will be 'module'.","title":"Args:"},{"location":"sonnet/#raises_342","text":"TypeError : If build is not callable. TypeError : If a given custom_getter is not callable.","title":"Raises:"},{"location":"sonnet/#module__call__args-kwargs","text":"Forwards call to the passed-in build function.","title":"Module.__call__(*args, **kwargs)"},{"location":"sonnet/#moduleconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Module.connected_subgraphs"},{"location":"sonnet/#moduledefun","text":"Wraps this modules call method in a callable graph function.","title":"Module.defun()"},{"location":"sonnet/#moduledefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Module.defun_wrapped"},{"location":"sonnet/#moduleget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Module.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_184","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_380","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_343","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#moduleget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Module.get_possible_initializer_keys()"},{"location":"sonnet/#returns_381","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#moduleget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"Module.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_185","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_382","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_344","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#modulegraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Module.graph"},{"location":"sonnet/#moduleis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Module.is_connected"},{"location":"sonnet/#modulelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Module.last_connected_subgraph"},{"location":"sonnet/#returns_383","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_345","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#modulemodule_name","text":"Returns the name of the Module.","title":"Module.module_name"},{"location":"sonnet/#modulename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Module.name_scopes"},{"location":"sonnet/#modulenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Module.non_trainable_variables"},{"location":"sonnet/#returns_384","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_346","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#modulescope_name","text":"Returns the full name of the Module's variable scope.","title":"Module.scope_name"},{"location":"sonnet/#moduletrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Module.trainable_variables"},{"location":"sonnet/#returns_385","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_347","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#modulevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Module.variable_scope"},{"location":"sonnet/#returns_386","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_348","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#modulevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Module.variables"},{"location":"sonnet/#returns_387","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_349","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-moduleinfoerror","text":"Error raised when Sonnet ModuleInfo cannot be serialized.","title":"class ModuleInfoError"},{"location":"sonnet/#class-movingaverage","text":"Calculates a differentiable decaying moving average. The moving average is kept in a variable that can either be local or global. The initial moving average value is set to the first value that is received by the module. The module lets gradients flow through the last element added to the moving average.","title":"class MovingAverage"},{"location":"sonnet/#movingaverage__init__decay099-localfalse-namemoving_average","text":"Constructor.","title":"MovingAverage.__init__(decay=0.99, local=False, name='moving_average')"},{"location":"sonnet/#args_186","text":"decay : float in range [0, 1], decay of the moving average. local : bool, specifies whether the variables are local or not. name : string, name of the Sonnet module. Default is 'moving_average'.","title":"Args:"},{"location":"sonnet/#raises_350","text":"ValueError : if decay is not in the valid range [0, 1].","title":"Raises:"},{"location":"sonnet/#movingaverage__call__inputs","text":"Returns the moving average of the values that went through inputs .","title":"MovingAverage.__call__(inputs)"},{"location":"sonnet/#args_187","text":"inputs : tensor.","title":"Args:"},{"location":"sonnet/#returns_388","text":"A moving average calculated as (1 - decay) * inputs + decay * average .","title":"Returns:"},{"location":"sonnet/#movingaverageconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"MovingAverage.connected_subgraphs"},{"location":"sonnet/#movingaveragedefun","text":"Wraps this modules call method in a callable graph function.","title":"MovingAverage.defun()"},{"location":"sonnet/#movingaveragedefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"MovingAverage.defun_wrapped"},{"location":"sonnet/#movingaverageget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"MovingAverage.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_188","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_389","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_351","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#movingaverageget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"MovingAverage.get_possible_initializer_keys()"},{"location":"sonnet/#returns_390","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#movingaverageget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"MovingAverage.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_189","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_391","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_352","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#movingaveragegraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"MovingAverage.graph"},{"location":"sonnet/#movingaverageis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"MovingAverage.is_connected"},{"location":"sonnet/#movingaveragelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"MovingAverage.last_connected_subgraph"},{"location":"sonnet/#returns_392","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_353","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#movingaveragemodule_name","text":"Returns the name of the Module.","title":"MovingAverage.module_name"},{"location":"sonnet/#movingaveragename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"MovingAverage.name_scopes"},{"location":"sonnet/#movingaveragenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"MovingAverage.non_trainable_variables"},{"location":"sonnet/#returns_393","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_354","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#movingaveragereset","text":"","title":"MovingAverage.reset()"},{"location":"sonnet/#movingaveragescope_name","text":"Returns the full name of the Module's variable scope.","title":"MovingAverage.scope_name"},{"location":"sonnet/#movingaveragetrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"MovingAverage.trainable_variables"},{"location":"sonnet/#returns_394","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_355","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#movingaveragevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"MovingAverage.variable_scope"},{"location":"sonnet/#returns_395","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_356","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#movingaveragevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"MovingAverage.variables"},{"location":"sonnet/#returns_396","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_357","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-notconnectederror","text":"Error raised when operating on a module that has not yet been connected. Some module properties / methods are valid to access before the module has been connected into the graph, but some are not. This Error is raised when the user attempts to do anything not valid before connection.","title":"class NotConnectedError"},{"location":"sonnet/#class-notinitializederror","text":"Error raised when connecting an uninitialized Sonnet module. Before they can be connected, all Sonnet modules must call AbstractModule.__init__ (e.g. via a super call).","title":"class NotInitializedError"},{"location":"sonnet/#class-notsupportederror","text":"Error raised when something that cannot be supported is requested. For example a Dilated Convolution module cannot be transposed.","title":"class NotSupportedError"},{"location":"sonnet/#class-optimizationconstraints","text":"Container for optimization constraints. Users can add to an OptimizationConstraints instance multiple inequality constraints, either implicitly passing inequality ops, such as optimization_constraints.add(x y) , or explicitly specifying the constraint type, as in optimization_constraints.add_geq(x, y) . Users can finally add the constraints to the TensorFlow graph calling optimization_constraints() ; when doing so, Lagrange multipliers are automatically added to the graph, so that users can optimize them alongside other variables in the graph, using the same optimizer and minimize() . Example usage: regularization_loss = model.regularization_loss(data) reconstruction_error = model.reconstruction_error(data) avg_reconstruction_error = snt.MovingAverage()(reconstruction_error) constraints = snt.OptimizationConstraints() constraints.add(avg_reconstruction_error reconstruction_threshold) loss = regularization_loss + constraints() # The following call actually performs an update step for # min_{theta} max_{lambda} ( # regularization_loss(theta) + # lambda * (avg_reconstruction_error - reconstruction_threshold)) # where theta are the model parameters and lambda are the Lagrange # multipliers. update = optimizer.minimize(loss)","title":"class OptimizationConstraints"},{"location":"sonnet/#optimizationconstraints__init__rate10-valid_rangenone","text":"Instantiates a container for optimization constraints.","title":"OptimizationConstraints.__init__(rate=1.0, valid_range=None)"},{"location":"sonnet/#args_190","text":"rate : optional float, default 1.0. Default factor for Lagrange multiplier gradient scaling. Use there rate argument to scale the gradients of the Lagrange multipliers - note that this parameter has no effect when using optimisers such as Adam. This parameter can be overridden when adding constraints to the container. valid_range : optional tuple of length 2, default None. Default valid range for Lagrange multipliers. This parameter can be overridden when adding constraints to the container.","title":"Args:"},{"location":"sonnet/#optimizationconstraintsaddexpression-ratenone-valid_rangenone-initializernone","text":"Add inequality constraint whose type depends on analysis of input op.","title":"OptimizationConstraints.add(expression, rate=None, valid_range=None, initializer=None)"},{"location":"sonnet/#args_191","text":"expression : op of type Greater , GreaterEqual , Less or LessEqual . Note that GreaterEqual and LessEqual are accepted only for convenience, and will result in the same behavior as Greater and Less respectively. rate : optional float, default None. Factor for Lagrange multiplier gradient scaling. Use there rate argument to scale the gradients of the Lagrange multipliers - note that this parameter has no effect when using optimisers such as Adam. This parameter overrides the defaults defined instantiating the container. valid_range : optional tuple of length 2, default None. Default valid range for Lagrange multipliers. This parameter overrides the defaults defined instantiating the container. initializer : optional tensorflow initializer, array or value to be used for the Lagrange multiplier initialization. By default Lagrange multiplier will be initialized to 1.0.","title":"Args:"},{"location":"sonnet/#returns_397","text":"Self.","title":"Returns:"},{"location":"sonnet/#raises_358","text":"TypeError , when input expression op is not one of Greater , GreaterEqual , Less , LessEqual .","title":"Raises:"},{"location":"sonnet/#optimizationconstraintsadd_geqlhs-rhs00-ratenone-valid_rangenone-initializernone","text":"Add a 'greater than' inequality constraint.","title":"OptimizationConstraints.add_geq(lhs, rhs=0.0, rate=None, valid_range=None, initializer=None)"},{"location":"sonnet/#args_192","text":"lhs : left hand argument of inequality expression. rhs : reft hand argument of inequality expression, defaults to 0.0. rate : optional float, default None. Factor for Lagrange multiplier gradient scaling. Use there rate argument to scale the gradients of the Lagrange multipliers - note that this parameter has no effect when using optimisers such as Adam. This parameter overrides the defaults defined instantiating the container. valid_range : optional tuple of length 2, default None. Default valid range for Lagrange multipliers. This parameter overrides the defaults defined instantiating the container. initializer : optional tensorflow initializer, array or value to be used for the Lagrange multiplier initialization. By default Lagrange multiplier will be initialized to 1.0.","title":"Args:"},{"location":"sonnet/#returns_398","text":"Self.","title":"Returns:"},{"location":"sonnet/#optimizationconstraintsadd_leqlhs-rhs00-ratenone-valid_rangenone-initializernone","text":"Add a 'less than' inequality constraint.","title":"OptimizationConstraints.add_leq(lhs, rhs=0.0, rate=None, valid_range=None, initializer=None)"},{"location":"sonnet/#args_193","text":"lhs : left hand argument of inequality expression. rhs : reft hand argument of inequality expression, defaults to 0.0. rate : optional float, default None. Factor for Lagrange multiplier gradient scaling. Use there rate argument to scale the gradients of the Lagrange multipliers - note that this parameter has no effect when using optimisers such as Adam. This parameter overrides the defaults defined instantiating the container. valid_range : optional tuple of length 2, default None. Default valid range for Lagrange multipliers. This parameter overrides the defaults defined instantiating the container. initializer : optional tensorflow initializer, array or value to be used for the Lagrange multiplier initialization. By default Lagrange multiplier will be initialized to 1.0.","title":"Args:"},{"location":"sonnet/#returns_399","text":"Self.","title":"Returns:"},{"location":"sonnet/#optimizationconstraintsconstraints","text":"","title":"OptimizationConstraints.constraints"},{"location":"sonnet/#optimizationconstraintslagrange_multipliers","text":"","title":"OptimizationConstraints.lagrange_multipliers"},{"location":"sonnet/#class-parentnotbuilterror","text":"Error raised when the parent of a module has not been built yet. For example, when making a transpose of modules that inherit from module.Transposable , the parent has to be connected to the graph before the child transpose to ensure that shape inference has already occurred.","title":"class ParentNotBuiltError"},{"location":"sonnet/#class-rnncellwrapper","text":"RNN core that delegates to a tf.contrib.rnn.RNNCell .","title":"class RNNCellWrapper"},{"location":"sonnet/#rnncellwrapper__init__cell_ctor-args-kwargs","text":"Constructs the cell, within this module's variable scope.","title":"RNNCellWrapper.__init__(cell_ctor, *args, **kwargs)"},{"location":"sonnet/#args_194","text":"cell_ctor : Callable that instantiates a tf.contrib.rnn.RNNCell . *args : Arguments to pass to cell_ctor . **kwargs : Keyword arguments to pass to cell_ctor . If name is provided, it is passed to RNNCore.__init__ as well. If custom_getter is provided, it is passed to RNNCore.__init__ but not to cell_ctor .","title":"Args:"},{"location":"sonnet/#rnncellwrapper__call__inputs-prev_state","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"RNNCellWrapper.__call__(inputs, prev_state)"},{"location":"sonnet/#args_195","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_400","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#rnncellwrapperconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"RNNCellWrapper.connected_subgraphs"},{"location":"sonnet/#rnncellwrapperdefun","text":"Wraps this modules call method in a callable graph function.","title":"RNNCellWrapper.defun()"},{"location":"sonnet/#rnncellwrapperdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"RNNCellWrapper.defun_wrapped"},{"location":"sonnet/#rnncellwrapperget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"RNNCellWrapper.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_196","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_401","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_359","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncellwrapperget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"RNNCellWrapper.get_possible_initializer_keys()"},{"location":"sonnet/#returns_402","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#rnncellwrapperget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"RNNCellWrapper.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_197","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_403","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_360","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncellwrappergraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"RNNCellWrapper.graph"},{"location":"sonnet/#rnncellwrapperinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs","text":"Builds the default start state for an RNNCore.","title":"RNNCellWrapper.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)"},{"location":"sonnet/#args_198","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_404","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_361","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#rnncellwrapperis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"RNNCellWrapper.is_connected"},{"location":"sonnet/#rnncellwrapperlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"RNNCellWrapper.last_connected_subgraph"},{"location":"sonnet/#returns_405","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_362","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncellwrappermodule_name","text":"Returns the name of the Module.","title":"RNNCellWrapper.module_name"},{"location":"sonnet/#rnncellwrappername_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"RNNCellWrapper.name_scopes"},{"location":"sonnet/#rnncellwrappernon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"RNNCellWrapper.non_trainable_variables"},{"location":"sonnet/#returns_406","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_363","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncellwrapperoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"RNNCellWrapper.output_size"},{"location":"sonnet/#rnncellwrapperscope_name","text":"Returns the full name of the Module's variable scope.","title":"RNNCellWrapper.scope_name"},{"location":"sonnet/#rnncellwrapperstate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"RNNCellWrapper.state_size"},{"location":"sonnet/#rnncellwrappertrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"RNNCellWrapper.trainable_variables"},{"location":"sonnet/#returns_407","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_364","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncellwrappervariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"RNNCellWrapper.variable_scope"},{"location":"sonnet/#returns_408","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_365","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncellwrappervariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"RNNCellWrapper.variables"},{"location":"sonnet/#returns_409","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_366","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncellwrapperzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"RNNCellWrapper.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_199","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_410","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-rnncore","text":"Superclass for Recurrent Neural Network Cores. This class defines the basic functionality that every core should implement, mainly the initial_state method which will return an example of their initial state. It also inherits from the interface snt.AbstractModule . As with any other snt.Module any subclass must implement a _build method that constructs the graph that corresponds to a core. Such a _build method should always have the same interface, which is the following: output, next_state = self._build(input, prev_state) where output, next_state, input, and prev_state are arbitrarily nested tensors. Such structures can be defined according to the following grammar: element = tuple(element*) | list(element*) | tf.Tensor This class is to be used with tensorflow containers such as rnn in tensorflow.python.ops.rnn. These containers only accept inputs which are compatible with the tf.contrib.rnn.RNNCell API, so that all the RNNCores should expose state_size and output_size properties.","title":"class RNNCore"},{"location":"sonnet/#rnncore__init___sentinelnone-custom_getternone-namenone","text":"Performs the initialisation necessary for all AbstractModule instances. Every subclass of AbstractModule must begin their constructor with a call to this constructor, i.e. super(MySubModule, self).__init__(custom_getter=custom_getter, name=name) . If you instantiate sub-modules in init you must create them within the _enter_variable_scope context manager to ensure they are in the module's variable scope. Alternatively, instantiate sub-modules in _build .","title":"RNNCore.__init__(_sentinel=None, custom_getter=None, name=None)"},{"location":"sonnet/#args_200","text":"_sentinel: Variable that only carries a non-None value if __init__ was called without named parameters. If this is the case, a deprecation warning is issued in form of a ValueError . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of this module. Used to construct the Templated build function. If None the module's class name is used (converted to snake case).","title":"Args:"},{"location":"sonnet/#raises_367","text":"TypeError : If name is not a string. TypeError : If a given custom_getter is not callable. ValueError : If __init__ was called without named arguments.","title":"Raises:"},{"location":"sonnet/#rnncore__call__args-kwargs","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"RNNCore.__call__(*args, **kwargs)"},{"location":"sonnet/#args_201","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_411","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#rnncoreconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"RNNCore.connected_subgraphs"},{"location":"sonnet/#rnncoredefun","text":"Wraps this modules call method in a callable graph function.","title":"RNNCore.defun()"},{"location":"sonnet/#rnncoredefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"RNNCore.defun_wrapped"},{"location":"sonnet/#rnncoreget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"RNNCore.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_202","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_412","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_368","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncoreget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"RNNCore.get_possible_initializer_keys()"},{"location":"sonnet/#returns_413","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#rnncoreget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"RNNCore.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_203","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_414","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_369","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncoregraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"RNNCore.graph"},{"location":"sonnet/#rnncoreinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs","text":"Builds the default start state for an RNNCore.","title":"RNNCore.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)"},{"location":"sonnet/#args_204","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_415","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_370","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#rnncoreis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"RNNCore.is_connected"},{"location":"sonnet/#rnncorelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"RNNCore.last_connected_subgraph"},{"location":"sonnet/#returns_416","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_371","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncoremodule_name","text":"Returns the name of the Module.","title":"RNNCore.module_name"},{"location":"sonnet/#rnncorename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"RNNCore.name_scopes"},{"location":"sonnet/#rnncorenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"RNNCore.non_trainable_variables"},{"location":"sonnet/#returns_417","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_372","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncoreoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"RNNCore.output_size"},{"location":"sonnet/#rnncorescope_name","text":"Returns the full name of the Module's variable scope.","title":"RNNCore.scope_name"},{"location":"sonnet/#rnncorestate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"RNNCore.state_size"},{"location":"sonnet/#rnncoretrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"RNNCore.trainable_variables"},{"location":"sonnet/#returns_418","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_373","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncorevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"RNNCore.variable_scope"},{"location":"sonnet/#returns_419","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_374","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncorevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"RNNCore.variables"},{"location":"sonnet/#returns_420","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_375","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#rnncorezero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"RNNCore.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_205","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_421","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-relationalmemory","text":"Relational Memory Core.","title":"class RelationalMemory"},{"location":"sonnet/#relationalmemory__init__mem_slots-head_size-num_heads1-num_blocks1-forget_bias10-input_bias00-gate_styleunit-attention_mlp_layers2-key_sizenone-namerelational_memory","text":"Constructs a RelationalMemory object.","title":"RelationalMemory.__init__(mem_slots, head_size, num_heads=1, num_blocks=1, forget_bias=1.0, input_bias=0.0, gate_style='unit', attention_mlp_layers=2, key_size=None, name='relational_memory')"},{"location":"sonnet/#args_206","text":"mem_slots : The total number of memory slots to use. head_size : The size of an attention head. num_heads : The number of attention heads to use. Defaults to 1. num_blocks : Number of times to compute attention per time step. Defaults to 1. forget_bias : Bias to use for the forget gate, assuming we are using some form of gating. Defaults to 1. input_bias : Bias to use for the input gate, assuming we are using some form of gating. Defaults to 0. gate_style : Whether to use per-element gating ('unit'), per-memory slot gating ('memory'), or no gating at all (None). Defaults to unit . attention_mlp_layers : Number of layers to use in the post-attention MLP. Defaults to 2. key_size : Size of vector to use for key query vectors in the attention computation. Defaults to None, in which case we use head_size . name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_376","text":"ValueError : gate_style not one of [None, 'memory', 'unit']. ValueError : num_blocks is 1. ValueError : attention_mlp_layers is 1.","title":"Raises:"},{"location":"sonnet/#relationalmemory__call__inputs-memory-treat_input_as_matrixfalse","text":"Adds relational memory to the TensorFlow graph.","title":"RelationalMemory.__call__(inputs, memory, treat_input_as_matrix=False)"},{"location":"sonnet/#args_207","text":"inputs : Tensor input. memory : Memory output from the previous time step. treat_input_as_matrix : Optional, whether to treat input as a sequence of matrices. Defaulta to False, in which case the input is flattened into a vector.","title":"Args:"},{"location":"sonnet/#returns_422","text":"output : This time step's output. next_memory : The next version of memory to use.","title":"Returns:"},{"location":"sonnet/#relationalmemoryconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"RelationalMemory.connected_subgraphs"},{"location":"sonnet/#relationalmemorydefun","text":"Wraps this modules call method in a callable graph function.","title":"RelationalMemory.defun()"},{"location":"sonnet/#relationalmemorydefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"RelationalMemory.defun_wrapped"},{"location":"sonnet/#relationalmemoryforget_gate","text":"Returns the forget gate Tensor.","title":"RelationalMemory.forget_gate"},{"location":"sonnet/#relationalmemoryget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"RelationalMemory.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_208","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_423","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_377","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#relationalmemoryget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"RelationalMemory.get_possible_initializer_keys()"},{"location":"sonnet/#returns_424","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#relationalmemoryget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"RelationalMemory.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_209","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_425","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_378","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#relationalmemorygraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"RelationalMemory.graph"},{"location":"sonnet/#relationalmemoryinitial_statebatch_size-trainablefalse","text":"Creates the initial memory. We should ensure each row of the memory is initialized to be unique, so initialize the matrix to be the identity. We then pad or truncate as necessary so that init_state is of size (batch_size, self._mem_slots, self._mem_size).","title":"RelationalMemory.initial_state(batch_size, trainable=False)"},{"location":"sonnet/#args_210","text":"batch_size : The size of the batch. trainable : Whether the initial state is trainable. This is always True.","title":"Args:"},{"location":"sonnet/#returns_426","text":"init_state : A truncated or padded matrix of size (batch_size, self._mem_slots, self._mem_size).","title":"Returns:"},{"location":"sonnet/#relationalmemoryinput_gate","text":"Returns the input gate Tensor.","title":"RelationalMemory.input_gate"},{"location":"sonnet/#relationalmemoryis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"RelationalMemory.is_connected"},{"location":"sonnet/#relationalmemorylast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"RelationalMemory.last_connected_subgraph"},{"location":"sonnet/#returns_427","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_379","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#relationalmemorymodule_name","text":"Returns the name of the Module.","title":"RelationalMemory.module_name"},{"location":"sonnet/#relationalmemoryname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"RelationalMemory.name_scopes"},{"location":"sonnet/#relationalmemorynon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"RelationalMemory.non_trainable_variables"},{"location":"sonnet/#returns_428","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_380","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#relationalmemoryoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"RelationalMemory.output_size"},{"location":"sonnet/#relationalmemoryscope_name","text":"Returns the full name of the Module's variable scope.","title":"RelationalMemory.scope_name"},{"location":"sonnet/#relationalmemorystate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"RelationalMemory.state_size"},{"location":"sonnet/#relationalmemorytrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"RelationalMemory.trainable_variables"},{"location":"sonnet/#returns_429","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_381","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#relationalmemoryvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"RelationalMemory.variable_scope"},{"location":"sonnet/#returns_430","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_382","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#relationalmemoryvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"RelationalMemory.variables"},{"location":"sonnet/#returns_431","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_383","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#relationalmemoryzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"RelationalMemory.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_211","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_432","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-residual","text":"Adds a residual connection to a base module. This module wraps a module M, where if M with traditionally output M(X), Residual(M)(x) = M(x) + x.","title":"class Residual"},{"location":"sonnet/#residual__init__base_module-nameresidual","text":"Performs the initialisation necessary for all AbstractModule instances. Every subclass of AbstractModule must begin their constructor with a call to this constructor, i.e. super(MySubModule, self).__init__(custom_getter=custom_getter, name=name) . If you instantiate sub-modules in init you must create them within the _enter_variable_scope context manager to ensure they are in the module's variable scope. Alternatively, instantiate sub-modules in _build .","title":"Residual.__init__(base_module, name='residual')"},{"location":"sonnet/#args_212","text":"_sentinel: Variable that only carries a non-None value if __init__ was called without named parameters. If this is the case, a deprecation warning is issued in form of a ValueError . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of this module. Used to construct the Templated build function. If None the module's class name is used (converted to snake case).","title":"Args:"},{"location":"sonnet/#raises_384","text":"TypeError : If name is not a string. TypeError : If a given custom_getter is not callable. ValueError : If __init__ was called without named arguments.","title":"Raises:"},{"location":"sonnet/#residual__call__inputs-kwargs","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"Residual.__call__(inputs, **kwargs)"},{"location":"sonnet/#args_213","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_433","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#residualconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Residual.connected_subgraphs"},{"location":"sonnet/#residualdefun","text":"Wraps this modules call method in a callable graph function.","title":"Residual.defun()"},{"location":"sonnet/#residualdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Residual.defun_wrapped"},{"location":"sonnet/#residualget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Residual.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_214","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_434","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_385","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Residual.get_possible_initializer_keys()"},{"location":"sonnet/#returns_435","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#residualget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"Residual.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_215","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_436","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_386","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Residual.graph"},{"location":"sonnet/#residualis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Residual.is_connected"},{"location":"sonnet/#residuallast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Residual.last_connected_subgraph"},{"location":"sonnet/#returns_437","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_387","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualmodule_name","text":"Returns the name of the Module.","title":"Residual.module_name"},{"location":"sonnet/#residualname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Residual.name_scopes"},{"location":"sonnet/#residualnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Residual.non_trainable_variables"},{"location":"sonnet/#returns_438","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_388","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualscope_name","text":"Returns the full name of the Module's variable scope.","title":"Residual.scope_name"},{"location":"sonnet/#residualtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Residual.trainable_variables"},{"location":"sonnet/#returns_439","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_389","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Residual.variable_scope"},{"location":"sonnet/#returns_440","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_390","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Residual.variables"},{"location":"sonnet/#returns_441","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_391","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-residualcore","text":"Adds a residual connection to a base RNN core. This module wraps a module M, where if M with traditionally output M(X), Residual(M)(x) = M(x) + x.","title":"class ResidualCore"},{"location":"sonnet/#residualcore__init__base_core-nameresidual_core","text":"Performs the initialisation necessary for all AbstractModule instances. Every subclass of AbstractModule must begin their constructor with a call to this constructor, i.e. super(MySubModule, self).__init__(custom_getter=custom_getter, name=name) . If you instantiate sub-modules in init you must create them within the _enter_variable_scope context manager to ensure they are in the module's variable scope. Alternatively, instantiate sub-modules in _build .","title":"ResidualCore.__init__(base_core, name='residual_core')"},{"location":"sonnet/#args_216","text":"_sentinel: Variable that only carries a non-None value if __init__ was called without named parameters. If this is the case, a deprecation warning is issued in form of a ValueError . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of this module. Used to construct the Templated build function. If None the module's class name is used (converted to snake case).","title":"Args:"},{"location":"sonnet/#raises_392","text":"TypeError : If name is not a string. TypeError : If a given custom_getter is not callable. ValueError : If __init__ was called without named arguments.","title":"Raises:"},{"location":"sonnet/#residualcore__call__inputs-prev_state-kwargs","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"ResidualCore.__call__(inputs, prev_state, **kwargs)"},{"location":"sonnet/#args_217","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_442","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#residualcoreconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"ResidualCore.connected_subgraphs"},{"location":"sonnet/#residualcoredefun","text":"Wraps this modules call method in a callable graph function.","title":"ResidualCore.defun()"},{"location":"sonnet/#residualcoredefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"ResidualCore.defun_wrapped"},{"location":"sonnet/#residualcoreget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"ResidualCore.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_218","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_443","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_393","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualcoreget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"ResidualCore.get_possible_initializer_keys()"},{"location":"sonnet/#returns_444","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#residualcoreget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"ResidualCore.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_219","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_445","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_394","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualcoregraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"ResidualCore.graph"},{"location":"sonnet/#residualcoreinitial_stateargs-kwargs","text":"Builds the default start state for an RNNCore.","title":"ResidualCore.initial_state(*args, **kwargs)"},{"location":"sonnet/#args_220","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_446","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_395","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#residualcoreis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"ResidualCore.is_connected"},{"location":"sonnet/#residualcorelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"ResidualCore.last_connected_subgraph"},{"location":"sonnet/#returns_447","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_396","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualcoremodule_name","text":"Returns the name of the Module.","title":"ResidualCore.module_name"},{"location":"sonnet/#residualcorename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"ResidualCore.name_scopes"},{"location":"sonnet/#residualcorenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"ResidualCore.non_trainable_variables"},{"location":"sonnet/#returns_448","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_397","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualcoreoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"ResidualCore.output_size"},{"location":"sonnet/#residualcorescope_name","text":"Returns the full name of the Module's variable scope.","title":"ResidualCore.scope_name"},{"location":"sonnet/#residualcorestate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"ResidualCore.state_size"},{"location":"sonnet/#residualcoretrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"ResidualCore.trainable_variables"},{"location":"sonnet/#returns_449","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_398","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualcorevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"ResidualCore.variable_scope"},{"location":"sonnet/#returns_450","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_399","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualcorevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"ResidualCore.variables"},{"location":"sonnet/#returns_451","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_400","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#residualcorezero_stateargs-kwargs","text":"Return zero-filled state tensor(s).","title":"ResidualCore.zero_state(*args, **kwargs)"},{"location":"sonnet/#args_221","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_452","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-selectinput","text":"Returns a subset of its inputs in an arbitrarily nested configuration. This module can be used for multiple purposes. The basic usage is to select a tensor or a subset of tensors: output = snt.SelectInput(idx=0, name='select')(input0, input1) == input0 output = snt.SelectInput(idx=[0, 2], name='select')(input0, input1, input2) == (input0, input2) Another usage is to change the orders of the input tensors: output = snt.SelectInput(idx=[1, 0], name='select')(input0, input1) == (input1, input0) Another usage is to duplicate an input: output = snt.SelectInput(idx=[0, 0], name='select')(input0) == (input0, input0) Another usage is to add arbitrary nesting: output = snt.SelectInput( idx=[0, [1, [2]]], name='select')(input0, input1, input2) == (input0, (input1, (input2,)))","title":"class SelectInput"},{"location":"sonnet/#selectinput__init__idx-nameselect_input","text":"Module constructor.","title":"SelectInput.__init__(idx, name='select_input')"},{"location":"sonnet/#args_222","text":"idx : Indexes of the tensors to select. If idx is an integer, then a Tensor is returned. If idx is a (nested) list/tuple, then a (nested) tuple of Tensor is returned. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_401","text":"TypeError : If idx is not an list, tuple or integer.","title":"Raises:"},{"location":"sonnet/#selectinput__call__inputs","text":"Connects the module into the graph.","title":"SelectInput.__call__(*inputs)"},{"location":"sonnet/#args_223","text":"*inputs : Tensor variables to select.","title":"Args:"},{"location":"sonnet/#returns_453","text":"Subset of inputs in an arbitrarily nested configuration.","title":"Returns:"},{"location":"sonnet/#raises_402","text":"ValueError : If any entry of idx is out of bounds with respect to the size of inputs .","title":"Raises:"},{"location":"sonnet/#selectinputconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"SelectInput.connected_subgraphs"},{"location":"sonnet/#selectinputdefun","text":"Wraps this modules call method in a callable graph function.","title":"SelectInput.defun()"},{"location":"sonnet/#selectinputdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"SelectInput.defun_wrapped"},{"location":"sonnet/#selectinputget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"SelectInput.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_224","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_454","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_403","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#selectinputget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"SelectInput.get_possible_initializer_keys()"},{"location":"sonnet/#returns_455","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#selectinputget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"SelectInput.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_225","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_456","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_404","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#selectinputgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"SelectInput.graph"},{"location":"sonnet/#selectinputis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"SelectInput.is_connected"},{"location":"sonnet/#selectinputlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"SelectInput.last_connected_subgraph"},{"location":"sonnet/#returns_457","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_405","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#selectinputmodule_name","text":"Returns the name of the Module.","title":"SelectInput.module_name"},{"location":"sonnet/#selectinputname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"SelectInput.name_scopes"},{"location":"sonnet/#selectinputnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SelectInput.non_trainable_variables"},{"location":"sonnet/#returns_458","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_406","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#selectinputscope_name","text":"Returns the full name of the Module's variable scope.","title":"SelectInput.scope_name"},{"location":"sonnet/#selectinputtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SelectInput.trainable_variables"},{"location":"sonnet/#returns_459","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_407","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#selectinputvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"SelectInput.variable_scope"},{"location":"sonnet/#returns_460","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_408","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#selectinputvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SelectInput.variables"},{"location":"sonnet/#returns_461","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_409","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-separableconv1d","text":"Performs an in-plane convolution to each channel independently. This acts as a light wrapper around the TensorFlow op tf.nn.separable_conv2d , abstracting away variable creation and sharing.","title":"class SeparableConv1D"},{"location":"sonnet/#separableconv1d__init__output_channels-channel_multiplier-kernel_shape-stride1-rate1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatnwc-custom_getternone-nameseparable_conv1d","text":"Constructs a SeparableConv1D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution","title":"SeparableConv1D.__init__(output_channels, channel_multiplier, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NWC', custom_getter=None, name='separable_conv1d')"},{"location":"sonnet/#args_226","text":"output_channels : Number of output channels. Must be an integer. channel_multiplier : Number of channels to expand pointwise (depthwise) convolution to. Must be an integer. Must be 0. When channel_multiplier is set to 1, applies a different filter to each input channel. Numbers larger than 1 cause the filter to be applied to channel_multiplier input channels. Outputs are concatenated together. kernel_shape : List with 2 elements in the following layout: [filter_height, filter_width] or integer that is used to define the list in all dimensions. stride : List with 4 elements of kernel strides, or integer that is used to define stride in all dimensions. Layout of list: [1, stride_y, stride_x, 1]. rate : Sequence of dilation rates (of size 1), or integer that is used to define dilation rate in all dimensions. 1 corresponds to standard 1D convolution, rate 1 corresponds to dilated convolution. Cannot be 1 if any of stride is also 1. padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 1. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with keys 'w_dw' for depthwise and 'w_pw' for pointwise) or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition the filters (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with keys 'w_dw' for depthwise and 'w_pw' for pointwise) and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NWC), or the second dimension (\"NCW\"). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_410","text":"ValueError : If channel_multiplier isn't of type ( numbers.Integral or tf.Dimension ). ValueError : If channel_multiplier is less than 1. ValueError : If the given data_format is not a supported format (see SUPPORTED_1D_DATA_FORMATS ). base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of one integer. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. base.IncompatibleShapeError: If the given rate is not an integer; or if the given rate is not a sequence of two integers. base.IncompatibleShapeError: If a mask is a TensorFlow Tensor with a not fully defined shape. base.NotSupportedError: If rate in any dimension and the stride in any dimension are simultaneously 1. ValueError : If the given padding is not snt.VALID , snt.SAME , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a sequence of these. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w_dw', 'w_pw' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. TypeError : If mask is given and it is not convertible to a Tensor. ValueError : If the passed-in data_format doesn't have a channel dimension.","title":"Raises:"},{"location":"sonnet/#separableconv1d__call__inputs","text":"Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection.","title":"SeparableConv1D.__call__(inputs)"},{"location":"sonnet/#args_227","text":"inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Args:"},{"location":"sonnet/#returns_462","text":"A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels].","title":"Returns:"},{"location":"sonnet/#raises_411","text":"ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Raises:"},{"location":"sonnet/#separableconv1db","text":"Returns the Variable containing the bias.","title":"SeparableConv1D.b"},{"location":"sonnet/#returns_463","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_412","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias.","title":"Raises:"},{"location":"sonnet/#separableconv1dchannel_multiplier","text":"Returns the channel multiplier argument.","title":"SeparableConv1D.channel_multiplier"},{"location":"sonnet/#separableconv1dclonenamenone","text":"Returns a cloned _ConvND module.","title":"SeparableConv1D.clone(name=None)"},{"location":"sonnet/#args_228","text":"name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name .","title":"Args:"},{"location":"sonnet/#returns_464","text":"A copy of the current class.","title":"Returns:"},{"location":"sonnet/#separableconv1dconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"SeparableConv1D.connected_subgraphs"},{"location":"sonnet/#separableconv1dconv_op_padding","text":"Returns the padding algorithm used for the underlying convolution op.","title":"SeparableConv1D.conv_op_padding"},{"location":"sonnet/#separableconv1ddata_format","text":"Returns the data format.","title":"SeparableConv1D.data_format"},{"location":"sonnet/#separableconv1ddefun","text":"Wraps this modules call method in a callable graph function.","title":"SeparableConv1D.defun()"},{"location":"sonnet/#separableconv1ddefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"SeparableConv1D.defun_wrapped"},{"location":"sonnet/#separableconv1dget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"SeparableConv1D.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_229","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_465","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_413","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv1dget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"SeparableConv1D.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_466","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#separableconv1dget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"SeparableConv1D.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_230","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_467","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_414","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv1dgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"SeparableConv1D.graph"},{"location":"sonnet/#separableconv1dhas_bias","text":"Returns True if bias Variable is present in the module.","title":"SeparableConv1D.has_bias"},{"location":"sonnet/#separableconv1dinitializers","text":"Returns the initializers dictionary.","title":"SeparableConv1D.initializers"},{"location":"sonnet/#separableconv1dinput_channels","text":"Returns the number of input channels.","title":"SeparableConv1D.input_channels"},{"location":"sonnet/#separableconv1dinput_shape","text":"Returns the input shape.","title":"SeparableConv1D.input_shape"},{"location":"sonnet/#separableconv1dis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"SeparableConv1D.is_connected"},{"location":"sonnet/#separableconv1dkernel_shape","text":"Returns the kernel shape.","title":"SeparableConv1D.kernel_shape"},{"location":"sonnet/#separableconv1dlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"SeparableConv1D.last_connected_subgraph"},{"location":"sonnet/#returns_468","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_415","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv1dmask","text":"Returns the mask.","title":"SeparableConv1D.mask"},{"location":"sonnet/#separableconv1dmodule_name","text":"Returns the name of the Module.","title":"SeparableConv1D.module_name"},{"location":"sonnet/#separableconv1dname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"SeparableConv1D.name_scopes"},{"location":"sonnet/#separableconv1dnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SeparableConv1D.non_trainable_variables"},{"location":"sonnet/#returns_469","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_416","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv1doutput_channels","text":"Returns the number of output channels.","title":"SeparableConv1D.output_channels"},{"location":"sonnet/#separableconv1dpadding","text":"Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension.","title":"SeparableConv1D.padding"},{"location":"sonnet/#returns_470","text":"The padding algorithm used, if this is the same for all dimensions.","title":"Returns:"},{"location":"sonnet/#raises_417","text":"ValueError : If different padding algorithms are used for different dimensions.","title":"Raises:"},{"location":"sonnet/#separableconv1dpaddings","text":"Returns a tuple with the padding algorithm used for each dimension.","title":"SeparableConv1D.paddings"},{"location":"sonnet/#separableconv1dpartitioners","text":"Returns the partitioners dictionary.","title":"SeparableConv1D.partitioners"},{"location":"sonnet/#separableconv1drate","text":"Returns the dilation rate.","title":"SeparableConv1D.rate"},{"location":"sonnet/#separableconv1dregularizers","text":"Returns the regularizers dictionary.","title":"SeparableConv1D.regularizers"},{"location":"sonnet/#separableconv1dscope_name","text":"Returns the full name of the Module's variable scope.","title":"SeparableConv1D.scope_name"},{"location":"sonnet/#separableconv1dstride","text":"Returns the stride.","title":"SeparableConv1D.stride"},{"location":"sonnet/#separableconv1dtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SeparableConv1D.trainable_variables"},{"location":"sonnet/#returns_471","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_418","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv1dvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"SeparableConv1D.variable_scope"},{"location":"sonnet/#returns_472","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_419","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv1dvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SeparableConv1D.variables"},{"location":"sonnet/#returns_473","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_420","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv1dw","text":"Returns the Variable containing the weight matrix.","title":"SeparableConv1D.w"},{"location":"sonnet/#separableconv1dw_dw","text":"Returns the Variable containing the depthwise weight matrix.","title":"SeparableConv1D.w_dw"},{"location":"sonnet/#separableconv1dw_pw","text":"Returns the Variable containing the pointwise weight matrix.","title":"SeparableConv1D.w_pw"},{"location":"sonnet/#class-separableconv2d","text":"Performs an in-plane convolution to each channel independently. This acts as a light wrapper around the TensorFlow op tf.nn.separable_conv2d , abstracting away variable creation and sharing.","title":"class SeparableConv2D"},{"location":"sonnet/#separableconv2d__init__output_channels-channel_multiplier-kernel_shape-stride1-rate1-paddingsame-use_biastrue-initializersnone-partitionersnone-regularizersnone-data_formatnhwc-custom_getternone-nameseparable_conv2d","text":"Constructs a SeparableConv2D module. See the following documentation for an explanation of VALID versus SAME padding modes: https://www.tensorflow.org/api_docs/python/tf/nn/convolution","title":"SeparableConv2D.__init__(output_channels, channel_multiplier, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, initializers=None, partitioners=None, regularizers=None, data_format='NHWC', custom_getter=None, name='separable_conv2d')"},{"location":"sonnet/#args_231","text":"output_channels : Number of output channels. Must be an integer. channel_multiplier : Number of channels to expand pointwise (depthwise) convolution to. Must be an integer. Must be 0. When channel_multiplier is set to 1, applies a different filter to each input channel. Numbers larger than 1 cause the filter to be applied to channel_multiplier input channels. Outputs are concatenated together. kernel_shape : List with 2 elements in the following layout: [filter_height, filter_width] or integer that is used to define the list in all dimensions. stride : List with 4 elements of kernel strides, or integer that is used to define stride in all dimensions. Layout of list: [1, stride_y, stride_x, 1]. rate : Sequence of dilation rates (of size 2), or integer that is used to define dilation rate in all dimensions. 1 corresponds to standard 2D convolution, rate 1 corresponds to dilated convolution. Cannot be 1 if any of stride is also 1. padding : Padding algorithm. Either snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL , or a sequence of these paddings of length 2. snt.SAME and snt.VALID are explained in the Tensorflow docs at https://www.tensorflow.org/api_docs/python/tf/nn/convolution. snt.FULL pre- and post-pads with the maximum padding which does not result in a convolution over just padded elements. snt.CAUSAL pre-pads to ensure that each output value only depends on input values at the same or preceding indices (\"no dependence on the future\"). snt.REVERSE_CAUSAL post-pads to ensure that each output value only depends on input values at the same or greater indices (\"no dependence on the past\"). If you use the same padding for all dimensions, and it is one of SAME or VALID, then this is supported directly by the underlying convolution op. In all other cases, the input data will be padded using tf.pad before calling the convolution op. use_bias : Whether to include bias parameters. Default True . initializers : Optional dict containing ops to initialize the filters (with keys 'w_dw' for depthwise and 'w_pw' for pointwise) or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition the filters (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with keys 'w_dw' for depthwise and 'w_pw' for pointwise) and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . data_format : A string. Specifies whether the channel dimension of the input and output is the last dimension (default, NHWC), or the second dimension (\"NCHW\"). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_421","text":"ValueError : If channel_multiplier isn't of type ( numbers.Integral or tf.Dimension ). ValueError : If channel_multiplier is less than 1. ValueError : If the given data_format is not a supported format (see SUPPORTED_2D_DATA_FORMATS ). base.IncompatibleShapeError: If the given kernel shape is not an integer; or if the given kernel shape is not a sequence of two integers. base.IncompatibleShapeError: If the given stride is not an integer; or if the given stride is not a sequence of two integers. base.IncompatibleShapeError: If the given rate is not an integer; or if the given rate is not a sequence of two integers. base.IncompatibleShapeError: If a mask is a TensorFlow Tensor with a not fully defined shape. base.NotSupportedError: If rate in any dimension and the stride in any dimension are simultaneously 1. ValueError : If the given padding is not snt.VALID , snt.SAME , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a sequence of these. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w_dw', 'w_pw' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable. TypeError : If mask is given and it is not convertible to a Tensor. ValueError : If the passed-in data_format doesn't have a channel dimension.","title":"Raises:"},{"location":"sonnet/#separableconv2d__call__inputs","text":"Connects the _ConvND module into the graph, with input Tensor inputs . If this is not the first time the module has been connected to the graph, the input Tensor provided here must have the same number of channels, in order for the existing variables to be the correct size for the multiplication; the batch size and input spatial dimensions may differ for each connection.","title":"SeparableConv2D.__call__(inputs)"},{"location":"sonnet/#args_232","text":"inputs : A ND Tensor of the same rank as data_format , and either of types tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Args:"},{"location":"sonnet/#returns_474","text":"A ND Tensor of shape [batch_size, output_dim_1, output_dim_2, ..., output_channels].","title":"Returns:"},{"location":"sonnet/#raises_422","text":"ValueError : If connecting the module into the graph any time after the first time and the inferred size of the input does not match previous invocations. base.IncompatibleShapeError: If the input tensor has the wrong number of dimensions. base.UnderspecifiedError: If the channel dimension of inputs isn't defined. base.IncompatibleShapeError: If a mask is present and its shape is incompatible with the shape of the weights. TypeError : If input Tensor dtype is not compatible with either tf.float16 , tf.bfloat16 , tf.float32 or tf.float64 .","title":"Raises:"},{"location":"sonnet/#separableconv2db","text":"Returns the Variable containing the bias.","title":"SeparableConv2D.b"},{"location":"sonnet/#returns_475","text":"Variable object containing the bias, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_423","text":"base.NotConnectedError: If the module has not been connected to the graph yet, meaning the variables do not exist. AttributeError : If the module does not use bias.","title":"Raises:"},{"location":"sonnet/#separableconv2dchannel_multiplier","text":"Returns the channel multiplier argument.","title":"SeparableConv2D.channel_multiplier"},{"location":"sonnet/#separableconv2dclonenamenone","text":"Returns a cloned _ConvND module.","title":"SeparableConv2D.clone(name=None)"},{"location":"sonnet/#args_233","text":"name : Optional string assigning name of cloned module. The default name is constructed by appending \"_clone\" to self.module_name .","title":"Args:"},{"location":"sonnet/#returns_476","text":"A copy of the current class.","title":"Returns:"},{"location":"sonnet/#separableconv2dconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"SeparableConv2D.connected_subgraphs"},{"location":"sonnet/#separableconv2dconv_op_padding","text":"Returns the padding algorithm used for the underlying convolution op.","title":"SeparableConv2D.conv_op_padding"},{"location":"sonnet/#separableconv2ddata_format","text":"Returns the data format.","title":"SeparableConv2D.data_format"},{"location":"sonnet/#separableconv2ddefun","text":"Wraps this modules call method in a callable graph function.","title":"SeparableConv2D.defun()"},{"location":"sonnet/#separableconv2ddefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"SeparableConv2D.defun_wrapped"},{"location":"sonnet/#separableconv2dget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"SeparableConv2D.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_234","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_477","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_424","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv2dget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"SeparableConv2D.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_478","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#separableconv2dget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"SeparableConv2D.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_235","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_479","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_425","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv2dgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"SeparableConv2D.graph"},{"location":"sonnet/#separableconv2dhas_bias","text":"Returns True if bias Variable is present in the module.","title":"SeparableConv2D.has_bias"},{"location":"sonnet/#separableconv2dinitializers","text":"Returns the initializers dictionary.","title":"SeparableConv2D.initializers"},{"location":"sonnet/#separableconv2dinput_channels","text":"Returns the number of input channels.","title":"SeparableConv2D.input_channels"},{"location":"sonnet/#separableconv2dinput_shape","text":"Returns the input shape.","title":"SeparableConv2D.input_shape"},{"location":"sonnet/#separableconv2dis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"SeparableConv2D.is_connected"},{"location":"sonnet/#separableconv2dkernel_shape","text":"Returns the kernel shape.","title":"SeparableConv2D.kernel_shape"},{"location":"sonnet/#separableconv2dlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"SeparableConv2D.last_connected_subgraph"},{"location":"sonnet/#returns_480","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_426","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv2dmask","text":"Returns the mask.","title":"SeparableConv2D.mask"},{"location":"sonnet/#separableconv2dmodule_name","text":"Returns the name of the Module.","title":"SeparableConv2D.module_name"},{"location":"sonnet/#separableconv2dname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"SeparableConv2D.name_scopes"},{"location":"sonnet/#separableconv2dnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SeparableConv2D.non_trainable_variables"},{"location":"sonnet/#returns_481","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_427","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv2doutput_channels","text":"Returns the number of output channels.","title":"SeparableConv2D.output_channels"},{"location":"sonnet/#separableconv2dpadding","text":"Returns the padding algorithm used, if this is the same for all dims. Use .paddings if you want a tuple with the padding algorithm used for each dimension.","title":"SeparableConv2D.padding"},{"location":"sonnet/#returns_482","text":"The padding algorithm used, if this is the same for all dimensions.","title":"Returns:"},{"location":"sonnet/#raises_428","text":"ValueError : If different padding algorithms are used for different dimensions.","title":"Raises:"},{"location":"sonnet/#separableconv2dpaddings","text":"Returns a tuple with the padding algorithm used for each dimension.","title":"SeparableConv2D.paddings"},{"location":"sonnet/#separableconv2dpartitioners","text":"Returns the partitioners dictionary.","title":"SeparableConv2D.partitioners"},{"location":"sonnet/#separableconv2drate","text":"Returns the dilation rate.","title":"SeparableConv2D.rate"},{"location":"sonnet/#separableconv2dregularizers","text":"Returns the regularizers dictionary.","title":"SeparableConv2D.regularizers"},{"location":"sonnet/#separableconv2dscope_name","text":"Returns the full name of the Module's variable scope.","title":"SeparableConv2D.scope_name"},{"location":"sonnet/#separableconv2dstride","text":"Returns the stride.","title":"SeparableConv2D.stride"},{"location":"sonnet/#separableconv2dtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SeparableConv2D.trainable_variables"},{"location":"sonnet/#returns_483","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_429","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv2dvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"SeparableConv2D.variable_scope"},{"location":"sonnet/#returns_484","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_430","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv2dvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SeparableConv2D.variables"},{"location":"sonnet/#returns_485","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_431","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#separableconv2dw","text":"Returns the Variable containing the weight matrix.","title":"SeparableConv2D.w"},{"location":"sonnet/#separableconv2dw_dw","text":"Returns the Variable containing the depthwise weight matrix.","title":"SeparableConv2D.w_dw"},{"location":"sonnet/#separableconv2dw_pw","text":"Returns the Variable containing the pointwise weight matrix.","title":"SeparableConv2D.w_pw"},{"location":"sonnet/#class-sequential","text":"Builds a module out of a sequence of callables. Note that Sequential is limited in the range of possible architectures it can handle. This is a deliberate design decision; Sequential is only meant to be used for the simple case of fusing together modules/ops where the input of a particular module/op is the output of the previous one. Another restriction is that it is not possible to have extra arguments in the _build method that are passed to the constituents of the module - for example, if there is a BatchNorm module in Sequential and the user wishes to switch the is_training flag. If this is the desired use case, the recommended solution is to use snt.Module to wrap a custom function, as shown in the following example: https://github.com/deepmind/sonnet/blob/master/sonnet/examples/module_with_build_args.py","title":"class Sequential"},{"location":"sonnet/#sequential__init__layers-namesequential","text":"Constructs a Sequential module. This feeds the output of each layer into the next and returns the output of the final layer. If a layer returns a tuple, it is assumed that this must be unpacked into the argument list of the next layer. If it is not a tuple, it is simply passed through to the next layer unchanged.","title":"Sequential.__init__(layers, name='sequential')"},{"location":"sonnet/#args_236","text":"layers : Iterable of callables to stack together, which can be modules or ops. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_432","text":"TypeError : If layers is None or contains any non-callable items.","title":"Raises:"},{"location":"sonnet/#sequential__call__args","text":"Connects the Sequential module into the graph.","title":"Sequential.__call__(*args)"},{"location":"sonnet/#args_237","text":"*args : A tuple of inputs, to be unpacked as the arguments to the first layer.","title":"Args:"},{"location":"sonnet/#returns_486","text":"The output value of the last layer.","title":"Returns:"},{"location":"sonnet/#sequentialconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"Sequential.connected_subgraphs"},{"location":"sonnet/#sequentialdefun","text":"Wraps this modules call method in a callable graph function.","title":"Sequential.defun()"},{"location":"sonnet/#sequentialdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"Sequential.defun_wrapped"},{"location":"sonnet/#sequentialget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"Sequential.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_238","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_487","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_433","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#sequentialget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"Sequential.get_possible_initializer_keys()"},{"location":"sonnet/#returns_488","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#sequentialget_variablesargs-kwargs","text":"Provide a warning that get_variables on Sequential always returns ().","title":"Sequential.get_variables(*args, **kwargs)"},{"location":"sonnet/#sequentialgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"Sequential.graph"},{"location":"sonnet/#sequentialis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"Sequential.is_connected"},{"location":"sonnet/#sequentiallast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"Sequential.last_connected_subgraph"},{"location":"sonnet/#returns_489","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_434","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#sequentiallayers","text":"","title":"Sequential.layers"},{"location":"sonnet/#sequentialmodule_name","text":"Returns the name of the Module.","title":"Sequential.module_name"},{"location":"sonnet/#sequentialname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"Sequential.name_scopes"},{"location":"sonnet/#sequentialnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Sequential.non_trainable_variables"},{"location":"sonnet/#returns_490","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_435","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#sequentialscope_name","text":"Returns the full name of the Module's variable scope.","title":"Sequential.scope_name"},{"location":"sonnet/#sequentialtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Sequential.trainable_variables"},{"location":"sonnet/#returns_491","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_436","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#sequentialvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"Sequential.variable_scope"},{"location":"sonnet/#returns_492","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_437","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#sequentialvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"Sequential.variables"},{"location":"sonnet/#returns_493","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_438","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-skipconnectioncore","text":"Adds a skip connection to the base RNN core. The output of the wrapped core is the concatenation of the output of the base core with its input. The state of the wrapped core is the state of the base core.","title":"class SkipConnectionCore"},{"location":"sonnet/#skipconnectioncore__init__base_core-input_shapenone-nameskip_connection_core","text":"Construct a SkipConnectionCore.","title":"SkipConnectionCore.__init__(base_core, input_shape=None, name='skip_connection_core')"},{"location":"sonnet/#args_239","text":"base_core : Base RNNCore to wrap. input_shape : Shape of the input as tuple, excluding the batch size. name : Name of the module.","title":"Args:"},{"location":"sonnet/#skipconnectioncore__call__inputs-prev_state-kwargs","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"SkipConnectionCore.__call__(inputs, prev_state, **kwargs)"},{"location":"sonnet/#args_240","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_494","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#skipconnectioncoreconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"SkipConnectionCore.connected_subgraphs"},{"location":"sonnet/#skipconnectioncoredefun","text":"Wraps this modules call method in a callable graph function.","title":"SkipConnectionCore.defun()"},{"location":"sonnet/#skipconnectioncoredefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"SkipConnectionCore.defun_wrapped"},{"location":"sonnet/#skipconnectioncoreget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"SkipConnectionCore.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_241","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_495","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_439","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#skipconnectioncoreget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"SkipConnectionCore.get_possible_initializer_keys()"},{"location":"sonnet/#returns_496","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#skipconnectioncoreget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"SkipConnectionCore.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_242","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_497","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_440","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#skipconnectioncoregraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"SkipConnectionCore.graph"},{"location":"sonnet/#skipconnectioncoreinitial_stateargs-kwargs","text":"Builds the default start state for an RNNCore.","title":"SkipConnectionCore.initial_state(*args, **kwargs)"},{"location":"sonnet/#args_243","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_498","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_441","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#skipconnectioncoreis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"SkipConnectionCore.is_connected"},{"location":"sonnet/#skipconnectioncorelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"SkipConnectionCore.last_connected_subgraph"},{"location":"sonnet/#returns_499","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_442","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#skipconnectioncoremodule_name","text":"Returns the name of the Module.","title":"SkipConnectionCore.module_name"},{"location":"sonnet/#skipconnectioncorename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"SkipConnectionCore.name_scopes"},{"location":"sonnet/#skipconnectioncorenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SkipConnectionCore.non_trainable_variables"},{"location":"sonnet/#returns_500","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_443","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#skipconnectioncoreoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"SkipConnectionCore.output_size"},{"location":"sonnet/#skipconnectioncorescope_name","text":"Returns the full name of the Module's variable scope.","title":"SkipConnectionCore.scope_name"},{"location":"sonnet/#skipconnectioncorestate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"SkipConnectionCore.state_size"},{"location":"sonnet/#skipconnectioncoretrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SkipConnectionCore.trainable_variables"},{"location":"sonnet/#returns_501","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_444","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#skipconnectioncorevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"SkipConnectionCore.variable_scope"},{"location":"sonnet/#returns_502","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_445","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#skipconnectioncorevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SkipConnectionCore.variables"},{"location":"sonnet/#returns_503","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_446","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#skipconnectioncorezero_stateargs-kwargs","text":"Return zero-filled state tensor(s).","title":"SkipConnectionCore.zero_state(*args, **kwargs)"},{"location":"sonnet/#args_244","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_504","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-slicebydim","text":"Slices a tensor along specific dimensions. The user can slice a tensor by specifying only the list of dimensions that they want to slice, together with the lists of integers containing the beginning indices of the slicing, and the size of the slices. Hence, with SliceByDim slicing can be performed without knowing in advance the rank of the input tensor. Tensorflow also offers a built-in op performing slicing, tf.slice . However, tf.slice requires all the slicing dimensions to be specified, using wildcards when no slicing is required. For example, with tf.slice , slicing half a 5D tensor along dimension 1 would be: output = tf.slice(inputs, begin=[0, 0, 0, 0, 0], size=[-1, inputs.get_shape()[1].value//2, -1, -1, -1]) The same operation using SliceByDim would be: output = SliceByDim(dims=[1], begin=[0], size=[x.get_shape()[1].value//2])(x) SliceByDim can be used to specify multiple slicing dimensions, for example: output = SliceByDim(dims=[1, 3], begin=[0, 0], size=[12, 24])(x)","title":"class SliceByDim"},{"location":"sonnet/#slicebydim__init__dims-begin-size-nameslice_by_dim","text":"Constructs the SliceByDim module.","title":"SliceByDim.__init__(dims, begin, size, name='slice_by_dim')"},{"location":"sonnet/#args_245","text":"dims : The dimensions to slice along, as a list of unique integers. Negative integers index from the final dimension backwards, as in python arrays. begin : The beginning indices of the slicing, as a list of integers. Must be the same length as the dims list. size : The size of the slices, as a list of integers. Must be the same length as the dims list. name : The name of the module.","title":"Args:"},{"location":"sonnet/#raises_447","text":"ValueError : If dims has non-unique integers, or if the size of begin is different from the size of dims , or if the size of size is different from the size of dims .","title":"Raises:"},{"location":"sonnet/#slicebydim__call__inputs","text":"Connects the SliceByDim module into the graph.","title":"SliceByDim.__call__(inputs)"},{"location":"sonnet/#args_246","text":"inputs : Tensor to slice. Its rank must be greater than the maximum dimension specified in dims (plus one as python is 0 indexed).","title":"Args:"},{"location":"sonnet/#returns_505","text":"The sliced tensor.","title":"Returns:"},{"location":"sonnet/#raises_448","text":"ValueError : If inputs tensor has insufficient rank.","title":"Raises:"},{"location":"sonnet/#slicebydimconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"SliceByDim.connected_subgraphs"},{"location":"sonnet/#slicebydimdefun","text":"Wraps this modules call method in a callable graph function.","title":"SliceByDim.defun()"},{"location":"sonnet/#slicebydimdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"SliceByDim.defun_wrapped"},{"location":"sonnet/#slicebydimget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"SliceByDim.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_247","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_506","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_449","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#slicebydimget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"SliceByDim.get_possible_initializer_keys()"},{"location":"sonnet/#returns_507","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#slicebydimget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"SliceByDim.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_248","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_508","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_450","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#slicebydimgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"SliceByDim.graph"},{"location":"sonnet/#slicebydimis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"SliceByDim.is_connected"},{"location":"sonnet/#slicebydimlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"SliceByDim.last_connected_subgraph"},{"location":"sonnet/#returns_509","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_451","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#slicebydimmodule_name","text":"Returns the name of the Module.","title":"SliceByDim.module_name"},{"location":"sonnet/#slicebydimname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"SliceByDim.name_scopes"},{"location":"sonnet/#slicebydimnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SliceByDim.non_trainable_variables"},{"location":"sonnet/#returns_510","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_452","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#slicebydimscope_name","text":"Returns the full name of the Module's variable scope.","title":"SliceByDim.scope_name"},{"location":"sonnet/#slicebydimtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SliceByDim.trainable_variables"},{"location":"sonnet/#returns_511","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_453","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#slicebydimvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"SliceByDim.variable_scope"},{"location":"sonnet/#returns_512","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_454","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#slicebydimvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"SliceByDim.variables"},{"location":"sonnet/#returns_513","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_455","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-tilebydim","text":"Tile a tensor along specific dimensions. The user can tile a tensor by specifying only the list of dimensions that they want to tile, together with the lists of integers containing the multiples of the tiling. Hence, with TileByDim tiling can be performed without knowing in advance the rank of the input tensor. Tensorflow also offers a built-in op performing tiling, tf.tile . However, tf.tile requires all the tiling dimensions to be specified, using 1 when no tiling is required. For example, with tf.tiling, tiling a 5D tensor along dimension 1 , by 2 would be: output = tf.tile(inputs, multiples=[1, 2, 1, 1, 1]) The same operation using TileByDim would be: output = TileByDim(dims=[1], multiples=[2])(x) TileByDim can be used to specify multiple tiling dimensions, for example: output = TileByDim(dims=[1, 3], multiples=[2, 4])(x)","title":"class TileByDim"},{"location":"sonnet/#tilebydim__init__dims-multiples-nametile_by_dim","text":"Constructs the TileByDim module.","title":"TileByDim.__init__(dims, multiples, name='tile_by_dim')"},{"location":"sonnet/#args_249","text":"dims : The dimensions to tile along, as a list of unique integers. multiples : The multiple of the tiling, as a list of integers. Must be the same length as the dims list. name : The name of the module.","title":"Args:"},{"location":"sonnet/#raises_456","text":"ValueError : If dims has non-unique integers, or if the size of multiples is different from the size of dims .","title":"Raises:"},{"location":"sonnet/#tilebydim__call__inputs","text":"Connects the TileByDim module into the graph.","title":"TileByDim.__call__(inputs)"},{"location":"sonnet/#args_250","text":"inputs : Tensor to tile.","title":"Args:"},{"location":"sonnet/#returns_514","text":"The tiled tensor.","title":"Returns:"},{"location":"sonnet/#tilebydimconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"TileByDim.connected_subgraphs"},{"location":"sonnet/#tilebydimdefun","text":"Wraps this modules call method in a callable graph function.","title":"TileByDim.defun()"},{"location":"sonnet/#tilebydimdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"TileByDim.defun_wrapped"},{"location":"sonnet/#tilebydimget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"TileByDim.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_251","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_515","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_457","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#tilebydimget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"TileByDim.get_possible_initializer_keys()"},{"location":"sonnet/#returns_516","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#tilebydimget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"TileByDim.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_252","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_517","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_458","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#tilebydimgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"TileByDim.graph"},{"location":"sonnet/#tilebydimis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"TileByDim.is_connected"},{"location":"sonnet/#tilebydimlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"TileByDim.last_connected_subgraph"},{"location":"sonnet/#returns_518","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_459","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#tilebydimmodule_name","text":"Returns the name of the Module.","title":"TileByDim.module_name"},{"location":"sonnet/#tilebydimname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"TileByDim.name_scopes"},{"location":"sonnet/#tilebydimnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"TileByDim.non_trainable_variables"},{"location":"sonnet/#returns_519","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_460","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#tilebydimscope_name","text":"Returns the full name of the Module's variable scope.","title":"TileByDim.scope_name"},{"location":"sonnet/#tilebydimtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"TileByDim.trainable_variables"},{"location":"sonnet/#returns_520","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_461","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#tilebydimvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"TileByDim.variable_scope"},{"location":"sonnet/#returns_521","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_462","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#tilebydimvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"TileByDim.variables"},{"location":"sonnet/#returns_522","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_463","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-trainableinitialstate","text":"Helper Module that creates a learnable initial state for an RNNCore. This class receives an example (possibly nested) initial state of an RNNCore, and returns a state that has the same shape, structure, and values, but is trainable. Additionally, the user may specify a boolean mask that indicates which parts of the initial state should be trainable. This allows users to train an unrolled RNNCore with a learnable initial state in the following way: core = ... # Any RNNCore module object. initial_state = core.initial_state(batch_size, dtype) trainable_initial_state = snt.TrainableInitialState(initial_state)() output, final_state = tf.nn.dynamic_rnn( core, input_sequence, initial_state=trainable_initial_state)","title":"class TrainableInitialState"},{"location":"sonnet/#trainableinitialstate__init__initial_state-masknone-nametrainable_initial_state","text":"Constructs the Module that introduces a trainable state in the graph. It receives an initial state that will be used as the initial values for the trainable variables that the module contains, and optionally a mask that indicates the parts of the initial state that should be learnable.","title":"TrainableInitialState.__init__(initial_state, mask=None, name='trainable_initial_state')"},{"location":"sonnet/#args_253","text":"initial_state : tensor or arbitrarily nested iterables of tensors. mask : optional boolean mask. It should have the same nested structure as the given initial_state. name : module name.","title":"Args:"},{"location":"sonnet/#raises_464","text":"TypeError : if mask is not a list of booleans or None.","title":"Raises:"},{"location":"sonnet/#trainableinitialstate__call__","text":"Connects the module to the graph.","title":"TrainableInitialState.__call__()"},{"location":"sonnet/#returns_523","text":"The learnable state, which has the same type, structure and shape as the initial_state passed to the constructor.","title":"Returns:"},{"location":"sonnet/#trainableinitialstateconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"TrainableInitialState.connected_subgraphs"},{"location":"sonnet/#trainableinitialstatedefun","text":"Wraps this modules call method in a callable graph function.","title":"TrainableInitialState.defun()"},{"location":"sonnet/#trainableinitialstatedefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"TrainableInitialState.defun_wrapped"},{"location":"sonnet/#trainableinitialstateget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"TrainableInitialState.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_254","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_524","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_465","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainableinitialstateget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"TrainableInitialState.get_possible_initializer_keys()"},{"location":"sonnet/#returns_525","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#trainableinitialstateget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"TrainableInitialState.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_255","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_526","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_466","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainableinitialstategraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"TrainableInitialState.graph"},{"location":"sonnet/#trainableinitialstateis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"TrainableInitialState.is_connected"},{"location":"sonnet/#trainableinitialstatelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"TrainableInitialState.last_connected_subgraph"},{"location":"sonnet/#returns_527","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_467","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainableinitialstatemodule_name","text":"Returns the name of the Module.","title":"TrainableInitialState.module_name"},{"location":"sonnet/#trainableinitialstatename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"TrainableInitialState.name_scopes"},{"location":"sonnet/#trainableinitialstatenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"TrainableInitialState.non_trainable_variables"},{"location":"sonnet/#returns_528","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_468","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainableinitialstatescope_name","text":"Returns the full name of the Module's variable scope.","title":"TrainableInitialState.scope_name"},{"location":"sonnet/#trainableinitialstatetrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"TrainableInitialState.trainable_variables"},{"location":"sonnet/#returns_529","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_469","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainableinitialstatevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"TrainableInitialState.variable_scope"},{"location":"sonnet/#returns_530","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_470","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainableinitialstatevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"TrainableInitialState.variables"},{"location":"sonnet/#returns_531","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_471","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-trainablevariable","text":"Provides learnable parameter Tensor.","title":"class TrainableVariable"},{"location":"sonnet/#trainablevariable__init__shape-dtypetffloat32-initializersnone-partitionersnone-regularizersnone-custom_getternone-nametrainable_variable","text":"Constructs a TrainableVariable module.","title":"TrainableVariable.__init__(shape, dtype=tf.float32, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='trainable_variable')"},{"location":"sonnet/#args_256","text":"shape : Tensor shape. dtype : Tensor data type. initializers : Optional dictionary containing ops to initialize the weight Tensor, with key 'w'. partitioners : Optional dict containing a partitioner to partition the weight (with key 'w'). As a default, no partitioner is used. regularizers : Optional dict containing regularizers for the weights (with key 'w'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . custom_getter : Optional callable or dictionary of callables to use as custom_getter for the module. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_472","text":"KeyError : If initializers contains any keys other than 'w'. KeyError : If partitioners contains any keys other than 'w'. KeyError : If regularizers contains any keys other than 'w'. TypeError : If any of the given initializers are not callable. TypeError : If any of the given partitioners are not callable. TypeError : If any of the given regularizers are not callable.","title":"Raises:"},{"location":"sonnet/#trainablevariable__call__","text":"Connects the TrainableTensor module into the graph.","title":"TrainableVariable.__call__()"},{"location":"sonnet/#returns_532","text":"A Tensor of shape as determined in the constructor.","title":"Returns:"},{"location":"sonnet/#trainablevariableconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"TrainableVariable.connected_subgraphs"},{"location":"sonnet/#trainablevariabledefun","text":"Wraps this modules call method in a callable graph function.","title":"TrainableVariable.defun()"},{"location":"sonnet/#trainablevariabledefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"TrainableVariable.defun_wrapped"},{"location":"sonnet/#trainablevariableget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"TrainableVariable.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_257","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_533","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_473","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainablevariableget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"TrainableVariable.get_possible_initializer_keys()"},{"location":"sonnet/#returns_534","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#trainablevariableget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"TrainableVariable.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_258","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_535","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_474","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainablevariablegraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"TrainableVariable.graph"},{"location":"sonnet/#trainablevariableis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"TrainableVariable.is_connected"},{"location":"sonnet/#trainablevariablelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"TrainableVariable.last_connected_subgraph"},{"location":"sonnet/#returns_536","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_475","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainablevariablemodule_name","text":"Returns the name of the Module.","title":"TrainableVariable.module_name"},{"location":"sonnet/#trainablevariablename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"TrainableVariable.name_scopes"},{"location":"sonnet/#trainablevariablenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"TrainableVariable.non_trainable_variables"},{"location":"sonnet/#returns_537","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_476","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainablevariablescope_name","text":"Returns the full name of the Module's variable scope.","title":"TrainableVariable.scope_name"},{"location":"sonnet/#trainablevariabletrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"TrainableVariable.trainable_variables"},{"location":"sonnet/#returns_538","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_477","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainablevariablevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"TrainableVariable.variable_scope"},{"location":"sonnet/#returns_539","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_478","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainablevariablevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"TrainableVariable.variables"},{"location":"sonnet/#returns_540","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_479","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#trainablevariablew","text":"Returns the Variable containing the weights Tensor.","title":"TrainableVariable.w"},{"location":"sonnet/#returns_541","text":"Variable object containing the weights, from the most recent call .","title":"Returns:"},{"location":"sonnet/#raises_480","text":"base.Error: If the module has not been connected to the graph yet, meaning the variables do not exist.","title":"Raises:"},{"location":"sonnet/#class-transposable","text":"Transposable module interface. The Transposable interface requires that transposable modules implement a method called transpose , returning a module that is the transposed version of the one the method is called on. Calling the method twice should return a module with the same specifications as the original module. When implementing a transposable module, special care is required to make sure that parameters needed to instantiate the module are provided as functions whose invocation is deferred to graph construction time. For example, in Linear we might want to call: linear = snt.Linear(name= linear , output_size=output_size) linear_transpose = linear.transpose() where the output_size for linear_transpose is not known yet, as linear is not yet connected to the graph: output_size is passed to linear_transpose's constructor as a lambda returning linear.input_size. The lambda will return the correct value once linear is given an input. Notice that linear_transpose's output_size value does not need to be defined until the module is connected to the graph.","title":"class Transposable"},{"location":"sonnet/#transposableinput_shape","text":"Returns shape of input Tensor passed at last call to build .","title":"Transposable.input_shape()"},{"location":"sonnet/#transposabletransposenamenone-kwargs","text":"Builds and returns transposed version of module.","title":"Transposable.transpose(name=None, **kwargs)"},{"location":"sonnet/#args_259","text":"name : Name of the transposed module. **kwargs : Additional Python flags controlling transposition.","title":"Args:"},{"location":"sonnet/#returns_542","text":"Transposed version of the module.","title":"Returns:"},{"location":"sonnet/#class-underspecifiederror","text":"Error raised when too little information is available. This does not typically mean the user is trying to do something that doesn't work (in which case IncompatibleShapeError should be used), just that some more information needs to be provided in order to build the Graph.","title":"class UnderspecifiedError"},{"location":"sonnet/#class-vanillarnn","text":"Basic fully connected vanilla RNN core.","title":"class VanillaRNN"},{"location":"sonnet/#vanillarnn__init__hidden_size-activationltfunction-tanh-at-0x7fcc5893e048gt-initializersnone-partitionersnone-regularizersnone-namevanilla_rnn","text":"Construct a Basic RNN core.","title":"VanillaRNN.__init__(hidden_size, activation=&lt;function tanh at 0x7fcc5893e048&gt;, initializers=None, partitioners=None, regularizers=None, name='vanilla_rnn')"},{"location":"sonnet/#args_260","text":"hidden_size : hidden size dimensionality. activation : activation function to use. initializers : optional dict containing ops to initialize the weights. This dictionary may contain the keys 'in_to_hidden' and/or 'hidden_to_hidden'. partitioners : optional dict containing ops to partition the weights. This dictionary may contain the keys 'in_to_hidden' and/or 'hidden_to_hidden'. regularizers : optional dict containing ops to regularize the weights. This dictionary may contain the keys 'in_to_hidden' and/or 'hidden_to_hidden'. name : name of the module.","title":"Args:"},{"location":"sonnet/#raises_481","text":"KeyError : if initializers contains any keys other than 'in_to_hidden' or 'hidden_to_hidden'. KeyError : if partitioners contains any keys other than 'in_to_hidden' or 'hidden_to_hidden'. KeyError : if regularizers contains any keys other than 'in_to_hidden' or 'hidden_to_hidden'. TypeError : If any of the given initializers are not callable. TypeError : If any of the given partitioners are not callable. TypeError : If any of the given regularizers are not callable.","title":"Raises:"},{"location":"sonnet/#vanillarnn__call__input_-prev_state","text":"Connects the VanillaRNN module into the graph. If this is not the first time the module has been connected to the graph, the Tensors provided as input_ and state must have the same final dimension, in order for the existing variables to be the correct size for their corresponding multiplications. The batch size may differ for each connection.","title":"VanillaRNN.__call__(input_, prev_state)"},{"location":"sonnet/#args_261","text":"input_ : a 2D Tensor of size [batch_size, input_size]. prev_state : a 2D Tensor of size [batch_size, hidden_size].","title":"Args:"},{"location":"sonnet/#returns_543","text":"output : a 2D Tensor of size [batch_size, hidden_size]. next_state : a Tensor of size [batch_size, hidden_size].","title":"Returns:"},{"location":"sonnet/#raises_482","text":"ValueError : if connecting the module into the graph any time after the first time, and the inferred size of the inputs does not match previous invocations.","title":"Raises:"},{"location":"sonnet/#vanillarnnconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"VanillaRNN.connected_subgraphs"},{"location":"sonnet/#vanillarnndefun","text":"Wraps this modules call method in a callable graph function.","title":"VanillaRNN.defun()"},{"location":"sonnet/#vanillarnndefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"VanillaRNN.defun_wrapped"},{"location":"sonnet/#vanillarnnget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"VanillaRNN.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_262","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_544","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_483","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#vanillarnnget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"VanillaRNN.get_possible_initializer_keys()"},{"location":"sonnet/#returns_545","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#vanillarnnget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"VanillaRNN.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_263","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_546","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_484","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#vanillarnngraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"VanillaRNN.graph"},{"location":"sonnet/#vanillarnnhidden_to_hidden_linear","text":"","title":"VanillaRNN.hidden_to_hidden_linear"},{"location":"sonnet/#vanillarnnhidden_to_hidden_variables","text":"","title":"VanillaRNN.hidden_to_hidden_variables"},{"location":"sonnet/#vanillarnnin_to_hidden_linear","text":"","title":"VanillaRNN.in_to_hidden_linear"},{"location":"sonnet/#vanillarnnin_to_hidden_variables","text":"","title":"VanillaRNN.in_to_hidden_variables"},{"location":"sonnet/#vanillarnninitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs","text":"Builds the default start state for an RNNCore.","title":"VanillaRNN.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)"},{"location":"sonnet/#args_264","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_547","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_485","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#vanillarnnis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"VanillaRNN.is_connected"},{"location":"sonnet/#vanillarnnlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"VanillaRNN.last_connected_subgraph"},{"location":"sonnet/#returns_548","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_486","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#vanillarnnmodule_name","text":"Returns the name of the Module.","title":"VanillaRNN.module_name"},{"location":"sonnet/#vanillarnnname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"VanillaRNN.name_scopes"},{"location":"sonnet/#vanillarnnnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"VanillaRNN.non_trainable_variables"},{"location":"sonnet/#returns_549","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_487","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#vanillarnnoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"VanillaRNN.output_size"},{"location":"sonnet/#vanillarnnscope_name","text":"Returns the full name of the Module's variable scope.","title":"VanillaRNN.scope_name"},{"location":"sonnet/#vanillarnnstate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"VanillaRNN.state_size"},{"location":"sonnet/#vanillarnntrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"VanillaRNN.trainable_variables"},{"location":"sonnet/#returns_550","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_488","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#vanillarnnvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"VanillaRNN.variable_scope"},{"location":"sonnet/#returns_551","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_489","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#vanillarnnvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"VanillaRNN.variables"},{"location":"sonnet/#returns_552","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_490","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#vanillarnnzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"VanillaRNN.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_265","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_553","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#check_initializersinitializers-keys","text":"Checks the given initializers. This checks that initializers is a dictionary that only contains keys in keys , and furthermore the entries in initializers are functions or further dictionaries (the latter used, for example, in passing initializers to modules inside modules) that must satisfy the same constraints.","title":"check_initializers(initializers, keys)"},{"location":"sonnet/#args_266","text":"initializers : Dictionary of initializers (allowing nested dictionaries) or None. keys : Iterable of valid keys for initializers .","title":"Args:"},{"location":"sonnet/#returns_554","text":"Copy of checked dictionary of initializers. If initializers=None , an empty dictionary will be returned.","title":"Returns:"},{"location":"sonnet/#raises_491","text":"KeyError : If an initializer is provided for a key not in keys . TypeError : If a provided initializer is not a callable function, or initializers is not a Mapping.","title":"Raises:"},{"location":"sonnet/#check_partitionerspartitioners-keys","text":"Checks the given partitioners. This checks that partitioners is a dictionary that only contains keys in keys , and furthermore the entries in partitioners are functions or further dictionaries (the latter used, for example, in passing partitioners to modules inside modules) that must satisfy the same constraints.","title":"check_partitioners(partitioners, keys)"},{"location":"sonnet/#args_267","text":"partitioners : Dictionary of partitioners (allowing nested dictionaries) or None. keys : Iterable of valid keys for partitioners .","title":"Args:"},{"location":"sonnet/#returns_555","text":"Checked dictionary of partitioners. If partitioners=None , an empty dictionary will be returned.","title":"Returns:"},{"location":"sonnet/#raises_492","text":"KeyError : If an partitioner is provided for a key not in keys . TypeError : If a provided partitioner is not a callable function, or partitioners is not a Mapping.","title":"Raises:"},{"location":"sonnet/#check_regularizersregularizers-keys","text":"Checks the given regularizers. This checks that regularizers is a dictionary that only contains keys in keys , and furthermore the entries in regularizers are functions or further dictionaries (the latter used, for example, in passing regularizers to modules inside modules) that must satisfy the same constraints.","title":"check_regularizers(regularizers, keys)"},{"location":"sonnet/#args_268","text":"regularizers : Dictionary of regularizers (allowing nested dictionaries) or None. keys : Iterable of valid keys for regularizers .","title":"Args:"},{"location":"sonnet/#returns_556","text":"Copy of checked dictionary of regularizers. If regularizers=None , an empty dictionary will be returned.","title":"Returns:"},{"location":"sonnet/#raises_493","text":"KeyError : If an regularizers is provided for a key not in keys . TypeError : If a provided regularizer is not a callable function, or regularizers is not a Mapping.","title":"Raises:"},{"location":"sonnet/#clip_gradientnet-clip_value_min-clip_value_max-namenone","text":"Clips respective gradients of a given tensor. Acts as identity for the forward pass, but clips gradient tensor element-wise by value during the backward pass. Any gradient values less than clip_value_min or greater than clip_values_max are set to the respective limit values.","title":"clip_gradient(net, clip_value_min, clip_value_max, name=None)"},{"location":"sonnet/#args_269","text":"net : A tf.Tensor . clip_value_min : A 0-D Tensor or scalar. The minimum value to clip by. clip_value_max : A 0-D Tensor or scalar. The maximum value to clip by. name : A name for the operation (optional, default 'clip_gradient').","title":"Args:"},{"location":"sonnet/#returns_557","text":"A tf.Tensor with the same type as the input tensor.","title":"Returns:"},{"location":"sonnet/#raises_494","text":"ValueError : If net dtype is non-float.","title":"Raises:"},{"location":"sonnet/#count_variables_by_typevariablesnone","text":"Returns a dict mapping dtypes to number of variables and scalars.","title":"count_variables_by_type(variables=None)"},{"location":"sonnet/#args_270","text":"variables : iterable of tf.Variable s, or None. If None is passed, then all global and local variables in the current graph are used.","title":"Args:"},{"location":"sonnet/#returns_558","text":"A dict mapping tf.dtype keys to a dict containing the keys 'num_scalars' and 'num_variables'.","title":"Returns:"},{"location":"sonnet/#custom_getter_routercustom_getter_map-name_fn","text":"Creates a custom getter than matches requests to dict of custom getters. Custom getters are callables which implement the [custom getter API] (https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/get_variable). The returned custom getter dispatches calls based on pattern matching the name of the requested variable to the keys of custom_getter_map. For example, { \".*/w\": snt.custom_getters.stop_gradient, } will match all variables named with the suffix \"/w\". The name_fn is provided to allow processing of the name, such as stripping off a scope prefix before matching.","title":"custom_getter_router(custom_getter_map, name_fn)"},{"location":"sonnet/#args_271","text":"custom_getter_map : Mapping of regular expressions to custom getter functions. name_fn : Callable to map variable name through before matching to regular expressions. This might, for example, strip off a scope prefix.","title":"Args:"},{"location":"sonnet/#returns_559","text":"A custom getter.","title":"Returns:"},{"location":"sonnet/#raises_495","text":"TypeError : If an entry in custom_getter_map is not a callable function.","title":"Raises:"},{"location":"sonnet/#deprecation_warningdeprecation_message","text":"Log a warning message the user is using deprecated functionality.","title":"deprecation_warning(deprecation_message)"},{"location":"sonnet/#format_variable_mapvariable_map-join_linestrue","text":"Takes a key-to-variable map and formats it as a table.","title":"format_variable_map(variable_map, join_lines=True)"},{"location":"sonnet/#format_variablesvariables-join_linestrue","text":"Takes a collection of variables and formats it as a table.","title":"format_variables(variables, join_lines=True)"},{"location":"sonnet/#get_lagrange_multipliershape-rate10-initializer10-maximizetrue-valid_rangenone-namelagrange_multiplier","text":"Lagrange multiplier factory. This factory returns ops that help setting up constrained optimization problems in Tensorflow. Given a constraint function op (either scalar or vectorial), use this function to instantiate a Lagrange multiplier op, then dot product the two and add them to the loss that is being optimized over. There is no need to instantiate a second optimizer to solve the minmax problem, as the Lagrange Multiplier op is setup to manipulate its own gradients so that a single optmizer can be used to update all the variables correctly.","title":"get_lagrange_multiplier(shape=(), rate=1.0, initializer=1.0, maximize=True, valid_range=None, name='lagrange_multiplier')"},{"location":"sonnet/#args_272","text":"shape : Lagrange multipliers can be used with both scalar and vector constraint functions; when using vector constraints use the shape kwarg to pass in shape information and instantiate variables of the correct shape. rate : Scalar used to scale the magnitude of gradients of the Lagrange multipliers, defaulting to 1e-2. Using the default value will make the Lagrange multipliers updates slower compared to the ones for the model's parameters. initializer : Initializer for the Lagrange multipliers. Note that when using inequality constraints the initial value of the multiplier will be transformed via the parametrization function. maximize : Boolean, True if we want to maximize the loss w.r.t. the Lagrange multipliers, False otherwise. valid_range : tuple, or list. of values used to clip the value of the (possibly reparametrized) Lagrange multipliers. name : Name of the Lagrange multiplier op.","title":"Args:"},{"location":"sonnet/#returns_560","text":"An op to be inserted in the graph, by multipling it with a constraint op and adding the resulting op to a loss. The Lagrange multiplier gradients are modified to that by calling minimize on the loss the optimizer will actually minimize w.r.t. to the model's parameters and maximize w.r.t. the Lagrande multipliers, hence enforcing the constraints.","title":"Returns:"},{"location":"sonnet/#raises_496","text":"ValueError : If the Lagrange multiplier is set to enforce an equality constraint and a parametrization function is also provided.","title":"Raises:"},{"location":"sonnet/#get_normalized_variable_mapscope_or_module-collectionvariables-contextnone-group_sliced_variablestrue","text":"Builds map of tf.Variable s in scope or module with normalized names. The names of the variables are normalized to remove the scope prefix.","title":"get_normalized_variable_map(scope_or_module, collection='variables', context=None, group_sliced_variables=True)"},{"location":"sonnet/#args_273","text":"scope_or_module : Scope or module to build map from. collection : Collection to restrict query to. By default this is tf.Graphkeys.GLOBAL_VARIABLES , which includes non-trainable variables such as moving averages. context : Scope or module, identical to or parent of scope . If given, this will be used as the stripped prefix. By default None , which means context=scope . group_sliced_variables : Boolean, if set to True, sliced variables are grouped together in the returned map; if set to False, each partition of a sliced variable is a separate (key, value) pair.","title":"Args:"},{"location":"sonnet/#returns_561","text":"Dictionary mapping normalized variable name to tf.Variable , or a list of tf.Variables if the variable is a sliced (partitioned) variable.","title":"Returns:"},{"location":"sonnet/#raises_497","text":"ValueError : If context is given but is not a proper prefix of scope .","title":"Raises:"},{"location":"sonnet/#get_saverscope-collectionsvariables-contextnone-kwargs","text":"Builds a tf.train.Saver for the scope or module, with normalized names. The names of the variables are normalized to remove the scope prefix. This allows the same variables to be restored into another similar scope or module using a complementary tf.train.Saver object.","title":"get_saver(scope, collections=('variables',), context=None, **kwargs)"},{"location":"sonnet/#args_274","text":"scope : Scope or module. Variables within will be saved or restored. collections : Sequence of collections of variables to restrict tf.train.Saver to. By default this is tf.GraphKeys.GLOBAL_VARIABLES which includes moving averages variables as well as trainable variables. context : Scope or module, identical to or parent of scope . If given, this will be used as the stripped prefix. **kwargs : Extra keyword arguments to pass to tf.train.Saver.","title":"Args:"},{"location":"sonnet/#returns_562","text":"A `tf.train.Saver` object for Variables in the scope or module.","title":"Returns:"},{"location":"sonnet/#get_variables_in_modulemodule-collectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside an snt.Module . Note that this operates by searching the variable scope a module contains, and so does not know about any modules which were constructed elsewhere but used inside this module.","title":"get_variables_in_module(module, collection='trainable_variables')"},{"location":"sonnet/#args_275","text":"module : snt.Module instance to query the scope of. collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_563","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_498","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#get_variables_in_scopescope-collectiontrainable_variables","text":"Returns a tuple tf.Variable s in a scope for a given collection.","title":"get_variables_in_scope(scope, collection='trainable_variables')"},{"location":"sonnet/#args_276","text":"scope : tf.VariableScope or string to retrieve variables from. collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_564","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#has_variable_scopeobj","text":"Determines whether the given object has a variable scope.","title":"has_variable_scope(obj)"},{"location":"sonnet/#highway_core_with_recurrent_dropouthidden_size-num_layers-keep_prob05-kwargs","text":"Highway core with recurrent dropout.","title":"highway_core_with_recurrent_dropout(hidden_size, num_layers, keep_prob=0.5, **kwargs)"},{"location":"sonnet/#args_277","text":"hidden_size : (int) Hidden size dimensionality. num_layers : (int) Number of highway layers. keep_prob : the probability to keep an entry when applying dropout. **kwargs : Extra keyword arguments to pass to the highway core.","title":"Args:"},{"location":"sonnet/#returns_565","text":"A tuple (train_core, test_core) where train_core is a higway core with recurrent dropout enabled to be used for training and test_core is the same highway core without recurrent dropout.","title":"Returns:"},{"location":"sonnet/#log_variablesvariablesnone","text":"Logs variable information. This function logs the name, shape, type, collections, and device for either all variables or a given iterable of variables. In the \"Device\" columns, the nature of the variable (legacy or resource (for ResourceVariables)) is also specified in parenthesis.","title":"log_variables(variables=None)"},{"location":"sonnet/#args_278","text":"variables : iterable of variables; if not provided, then all variables (in the default graph) are logged.","title":"Args:"},{"location":"sonnet/#lstm_with_recurrent_dropouthidden_size-keep_prob05-kwargs","text":"LSTM with recurrent dropout.","title":"lstm_with_recurrent_dropout(hidden_size, keep_prob=0.5, **kwargs)"},{"location":"sonnet/#args_279","text":"hidden_size : the LSTM hidden size. keep_prob : the probability to keep an entry when applying dropout. **kwargs : Extra keyword arguments to pass to the LSTM.","title":"Args:"},{"location":"sonnet/#returns_566","text":"A tuple (train_lstm, test_lstm) where train_lstm is an LSTM with recurrent dropout enabled to be used for training and test_lstm is the same LSTM without recurrent dropout.","title":"Returns:"},{"location":"sonnet/#lstm_with_zoneouthidden_size-keep_prob_c05-keep_prob_h095-kwargs","text":"LSTM with recurrent dropout.","title":"lstm_with_zoneout(hidden_size, keep_prob_c=0.5, keep_prob_h=0.95, **kwargs)"},{"location":"sonnet/#args_280","text":"hidden_size : the LSTM hidden size. keep_prob_c : the probability to use the new value of the cell state rather than freezing it. keep_prob_h : the probability to use the new value of the hidden state rather than freezing it. **kwargs : Extra keyword arguments to pass to the LSTM.","title":"Args:"},{"location":"sonnet/#returns_567","text":"A tuple (train_lstm, test_lstm) where train_lstm is an LSTM with recurrent dropout enabled to be used for training and test_lstm is the same LSTM without zoneout.","title":"Returns:"},{"location":"sonnet/#merge_leading_dimsarray_or_tensor-n_dims2","text":"Merge the first dimensions of a tensor.","title":"merge_leading_dims(array_or_tensor, n_dims=2)"},{"location":"sonnet/#args_281","text":"array_or_tensor : Tensor to have its first dimensions merged. Can also be an array or numerical value, which will be converted to a tensor for batch application, if needed. n_dims : Number of dimensions to merge.","title":"Args:"},{"location":"sonnet/#returns_568","text":"Either the input value converted to a Tensor, with the requested dimensions merged, or the unmodified input value if the input has less than n_dims dimensions.","title":"Returns:"},{"location":"sonnet/#raises_499","text":"ValueError : If the rank of array_or_tensor is not well-defined.","title":"Raises:"},{"location":"sonnet/#observe_connectionsobserver","text":"Notifies the observer whenever any Sonnet module is connected to the graph. If a module contains nested modules, the observer is notified once for each nested module, followed by the containing module. For example: def logging_observer(connected_subgraph): logging.info(connected_subgraph.module.module_name) with snt.observe_connections(logging_observer): output = imagenet_module(input_tensor)","title":"observe_connections(observer)"},{"location":"sonnet/#args_282","text":"observer : Callable accepting a single argument. Will be called with a ConnectedSubGraph each time a module is connected to the graph.","title":"Args:"},{"location":"sonnet/#yields","text":"None : just yields control to the inner context.","title":"Yields:"},{"location":"sonnet/#parse_string_to_constructorctor_string","text":"Returns a callable which corresponds to the constructor string. Various modules (eg, ConvNet2D) take constructor arguments which are callables, indicating a submodule to build. These can be passed as actual constructors, eg snt.LayerNorm , however that makes the config for that module not trivially serializable. This function tries to map a string representation to the underlying callable, allowing configs to remain serializable where necessary.","title":"parse_string_to_constructor(ctor_string)"},{"location":"sonnet/#args_283","text":"ctor_string : string representing some module in Sonnet. If the string is provided with no dots, we assume it is a member of Sonnet available at top level, i.e. \"LayerNorm\" maps to snt.LayerNorm .","title":"Args:"},{"location":"sonnet/#raises_500","text":"ValueError : if no matching constructor can be found.","title":"Raises:"},{"location":"sonnet/#returns_569","text":"Callable constructor which corresponds to ctor_string .","title":"Returns:"},{"location":"sonnet/#remove_unsupported_kwargsmodule_or_fn-all_kwargs_dict","text":"Removes any kwargs not supported by module_or_fn from all_kwargs_dict . A new dict is return with shallow copies of keys values from all_kwargs_dict , as long as the key is accepted by module_or_fn. The returned dict can then be used to connect module_or_fn (along with some other inputs, ie non-keyword arguments, in general). snt.supports_kwargs is used to tell whether a given kwarg is supported. Note that this method may give false negatives, which would lead to extraneous removals in the result of this function. Please read the docstring for snt.supports_kwargs for details, and manually inspect the results from this function if in doubt.","title":"remove_unsupported_kwargs(module_or_fn, all_kwargs_dict)"},{"location":"sonnet/#args_284","text":"module_or_fn : some callable which can be interrogated by snt.supports_kwargs . Generally a Sonnet module or a method (wrapped in @reuse_variables ) of a Sonnet module. all_kwargs_dict : a dict containing strings as keys, or None.","title":"Args:"},{"location":"sonnet/#raises_501","text":"ValueError : if all_kwargs_dict is not a dict.","title":"Raises:"},{"location":"sonnet/#returns_570","text":"A dict containing some subset of the keys and values in all_kwargs_dict . This subset may be empty. If all_kwargs_dict is None, this will be an empty dict.","title":"Returns:"},{"location":"sonnet/#reuse_variablesmethod","text":"Wraps an arbitrary method so it does variable sharing. This decorator creates variables the first time it calls method , and reuses them for subsequent calls. The object that calls method provides a tf.VariableScope , either as a variable_scope attribute or as the return value of an _enter_variable_scope() method. The first time the wrapped method is invoked, it enters the caller's tf.VariableScope with reuse=False . On all subsequent calls it enters the same variable scope with reuse=True . Variables are created in the context of the tf.VariableScope provided by the caller object. Ops are created with an additional tf.name_scope() , which adds a scope for the wrapped method name. For example: class MyClass(object): def __init__(self, name): with tf.variable_scope(None, default_name=name) as variable_scope: self.variable_scope = variable_scope @snt.reuse_variables def add_x(self, tensor): x = tf.get_variable( x , shape=tensor.get_shape()) return tensor + x module = MyClass( my_module_name ) input_tensor = tf.zeros(shape=(5,)) # This creates the variable my_module_name/x # and op my_module_name/add_x/add output = module.add_x(input_tensor) For performance when executing eagerly it may be desirable to additionally annotate these methods using defun , such that they are encapsulated as graph functions. This is not recommended if your method returns a variable since the output of defun would be an op that returned the variable's value when evaluated (rather than the variable instance). class FooModule(snt.AbstractModule): def _build(self, inputs): return complex_math(inputs) @tfe.defun @snt.reuse_variables def more_complex_stuff(self, inputs): return more_complex_math(inputs)","title":"reuse_variables(method)"},{"location":"sonnet/#args_285","text":"method : The method to wrap.","title":"Args:"},{"location":"sonnet/#returns_571","text":"The wrapped method.","title":"Returns:"},{"location":"sonnet/#scale_gradientnet-scale-namescale_gradient","text":"Scales gradients for the backwards pass. This might be used to, for example, allow one part of a model to learn at a lower rate than the rest. WARNING: Think carefully about how your optimizer works. If, for example, you use rmsprop, the gradient is always rescaled (with some additional epsilon) towards unity. This means scale_gradient won't have the effect of lowering the learning rate. If scale is 0.0 , this op reduces to tf.stop_gradient . If scale is 1.0 , this op reduces to tf.identity .","title":"scale_gradient(net, scale, name='scale_gradient')"},{"location":"sonnet/#args_286","text":"net : A tf.Tensor or in eager mode a callable that produces a tf.Tensor . scale : The scale factor for the gradient on the backwards pass. name : A name for the operation (optional).","title":"Args:"},{"location":"sonnet/#returns_572","text":"In graph mode returns a tf.Tensor with the same type as the input tensor. In eager mode returns a callable wrapping net whose gradients are scaled.","title":"Returns:"},{"location":"sonnet/#raises_502","text":"ValueError : If net dtype is non-float and scale is not zero or one.","title":"Raises:"},{"location":"sonnet/#split_leading_dimtensor-inputs-n_dims2","text":"Split the first dimension of a tensor.","title":"split_leading_dim(tensor, inputs, n_dims=2)"},{"location":"sonnet/#args_287","text":"tensor : Tensor to have its first dimension split. inputs : Original reference input to look the dimensions of. n_dims : Number of dimensions to split.","title":"Args:"},{"location":"sonnet/#returns_573","text":"The input tensor, with its first dimension split.","title":"Returns:"},{"location":"sonnet/#summarize_variablesvariablesnone","text":"Logs a summary of variable information. This function groups Variables by dtype and prints out the number of Variables and the total number of scalar values for each datatype, as well as the total memory consumed. For Variables of type tf.string, the memory usage cannot be accurately calculated from the Graph as the memory requirements change based on what strings are actually stored, which can only be determined inside a session. In this case, the amount of memory used to stored the pointers to the strings is logged, along with a warning.","title":"summarize_variables(variables=None)"},{"location":"sonnet/#args_288","text":"variables : iterable of variables; if not provided, then all variables (in the default graph) are summarized.","title":"Args:"},{"location":"sonnet/#supports_kwargsmodule_or_fn-kwargs_list","text":"Determines whether the provided callable supports all the kwargs. This is useful when you have a module that might or might not support a kwarg such as is_training . Rather than calling the module and catching the error, risking the potential modification of underlying state, this function introspects the module to see what kwargs are actually supported, using the python inspect module. Note that many TF functions do not export a valid argspec object, rather they have a generic args, *kwargs signature due to various layers of wrapping (deprecation decorators, etc). In those circumstances we return MAYBE_SUPPORTED, and users will have to use another method to tell whether the kwargs are supported (e.g. by just calling the function).","title":"supports_kwargs(module_or_fn, kwargs_list)"},{"location":"sonnet/#args_289","text":"module_or_fn : some callable, generally an object or a method of some object. If an object is provided, we check wither module_or_fn.__call__ supports the provided kwargs, which for a Sonnet module will automatically check the signature of _build. If module_or_fn is a function/method, then we check its signature directly, so non-Sonnet functions can be used. kwargs_list : string or iterable of strings of keyword arg names to test for. If an empty iterable is provided this function will always return True.","title":"Args:"},{"location":"sonnet/#raises_503","text":"ValueError : if a non-string is provided in kwargs_list .","title":"Raises:"},{"location":"sonnet/#returns_574","text":"a string, one of 'supported', 'not_supported' or 'maybe_supported'.","title":"Returns:"},{"location":"sonnet/#trainable_initial_statebatch_size-state_size-dtype-initializersnone-regularizersnone-namenone","text":"Creates an initial state consisting of trainable variables. The trainable variables are created with the same shapes as the elements of state_size and are tiled to produce an initial state.","title":"trainable_initial_state(batch_size, state_size, dtype, initializers=None, regularizers=None, name=None)"},{"location":"sonnet/#args_290","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. state_size : A TensorShape or nested tuple of TensorShape s to use for the shape of the trainable variables. dtype : The data type used to create the variables and thus initial state. initializers : An optional container of the same structure as state_size containing initializers for the variables. regularizers : An optional container of the same structure as state_size containing regularizers for the variables. name : optional string used to prefix the initial state variable names.","title":"Args:"},{"location":"sonnet/#returns_575","text":"A Tensor or nested tuple of Tensor s with the same size and structure as state_size , where each Tensor is a tiled trainable Variable .","title":"Returns:"},{"location":"sonnet/#raises_504","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#variable_map_itemsvariable_map","text":"Yields an iterator over (string, variable) pairs in the variable map. In general, variable maps map variable names to either a tf.Variable , or list of tf.Variable s (in case of sliced variables).","title":"variable_map_items(variable_map)"},{"location":"sonnet/#args_291","text":"variable_map : dict, variable map over which to iterate.","title":"Args:"},{"location":"sonnet/#yields_1","text":"(string, tf.Variable) pairs.","title":"Yields:"},{"location":"sonnet/#wrap_with_spectral_normmodule_class-sn_kwargsnone-pow_iter_collectionnone","text":"Returns a constructor for the inner class with spectral normalization. This function accepts a Sonnet AbstractModule class as argument (the class, not an instance of that class) alongside an optional dictionary of keyword arguments for the spectral_norm function, and returns a constructor which can be treated identically to the constructor of the input class, but with spectral normalization applied to the weights created by the class. Internally, this is just a partially evaluated SpectralNormWrapper module. pow_iter_collection , if not None, is treated as the name of a TensorFlow global collection. Each time the module's weight matrix is accessed ops are built for performing one step of power iteration to approximate that weight's first singular follow and ops are created for saving this new approximation in an internal variable. At build-time the resulting object takes a special boolean 'enable_power_iteration' keyword argument. If this is True (the default), a control dependency on the operation for updating this internal variable is attached to the returned weight. Otherwise, the update is not attached as a control dependency, but an op is placed into the pow_iter_collection global collection which causes the internal variable to be updated. It is then up to the user to choose whether to run this update.","title":"wrap_with_spectral_norm(module_class, sn_kwargs=None, pow_iter_collection=None)"},{"location":"sonnet/#args_292","text":"module_class : A constructor/class reference for a Sonnet module you would like to wrap and automatically apply spectral normalization. sn_kwargs : Keyword arguments to be passed to the spectral_norm function in addition to the weight tensor. pow_iter_collection : The name of a global collection for potentially storing ops for updating internal variables.","title":"Args:"},{"location":"sonnet/#returns_576","text":"An snt.AbstractModule class representing the original with spectral norm.","title":"Returns:"},{"location":"sonnet/#class-custom_getterscontext","text":"Contextually switching a custom getter on. Example usage, using snt.custom_getters.stop_gradient with Context to selectively disable gradients flowing to variables for particular connections of the module: custom_getter = snt.custom_getters.Context(snt.custom_getters.stop_gradient) lin = snt.Linear(10, custom_getter=custom_getter) lin(net1) # custom getter not used, gradients on with custom_getter: lin(net2) # custom getter used, gradients off Warning: If the custom getter affects the way the variable is created, then switching it on or off after the variable has been created will have no effect. For example, it is not possible to selectively switch off trainability using custom_getters.non_trainable , since this is a creation-time attribute. It is however possible to selectively switch off gradients using custom_getters.stop_gradient , since this applies an operation to the variable.","title":"class custom_getters.Context"},{"location":"sonnet/#custom_getterscontext__init__getter-verbosefalse-default_getternone","text":"Initializes a contextual switch for a custom getter.","title":"custom_getters.Context.__init__(getter, verbose=False, default_getter=None)"},{"location":"sonnet/#args_293","text":"getter : The custom getter which we may want to switch on. verbose : Log out every time a variable is fetched, and whether or not getter is used. default_getter : The custom getter to use when this context is not active. If None, the default custom getter is used.","title":"Args:"},{"location":"sonnet/#returns_577","text":"A custom getter which can also be used as a context manager. Entering the context enables the custom getter.","title":"Returns:"},{"location":"sonnet/#custom_gettersnon_trainablegetter-args-kwargs","text":"Custom getter which makes a variable non-trainable. Usage like: with tf.variable_scope(\"\", custom_getter=snt.custom_getters.non_trainable): net = snt.Linear(num_hidden)(net) or, using the custom_getter constructor argument, linear = snt.Linear(num_hidden, custom_getter=snt.custom_getters.non_trainable) net = linear(net) will result in the variables inside the linear having trainable=False , i.e. won't be added to tf.trainable_variables() and thus won't be optimized. Warning: If reuse=True and the variable has previously been created in the same graph with trainable=True , this custom getter will do nothing. Similarly if the variable is reused after being created by this custom getter it will still be non-trainable, even if trainable=True . When used with a Sonnet module, the module must be constructed inside the variable scope with the custom getter. Just building the module inside said variable scope will not use the custom getter.","title":"custom_getters.non_trainable(getter, *args, **kwargs)"},{"location":"sonnet/#args_294","text":"getter : The true getter to call. *args : Arguments, in the same format as tf.get_variable. **kwargs : Keyword arguments, in the same format as tf.get_variable.","title":"Args:"},{"location":"sonnet/#returns_578","text":"The return value of getter(*args, **kwargs) except with trainable=False enforced.","title":"Returns:"},{"location":"sonnet/#custom_gettersoverride_argskwargs","text":"Creates a custom getter that applies specified named arguments.","title":"custom_getters.override_args(**kwargs)"},{"location":"sonnet/#args_295","text":"**kwargs : Overriding arguments for the custom getter to use in preference the named arguments it's called with.","title":"Args:"},{"location":"sonnet/#returns_579","text":"Custom getter.","title":"Returns:"},{"location":"sonnet/#custom_gettersoverride_default_argskwargs","text":"Creates a custom getter that applies specified named arguments. The returned custom getter treats the specified named arguments as revised defaults, and does not override any non- None argument values supplied by the original get_variable call (or by a nested scope's custom getter).","title":"custom_getters.override_default_args(**kwargs)"},{"location":"sonnet/#args_296","text":"**kwargs : Overriding arguments for the custom getter to use in preference the named arguments it's called with.","title":"Args:"},{"location":"sonnet/#returns_580","text":"Custom getter.","title":"Returns:"},{"location":"sonnet/#custom_gettersrestore_initializerfilename-name_fnnone-collectionvariables","text":"Custom getter to restore all variables with snt.restore_initializer .","title":"custom_getters.restore_initializer(filename, name_fn=None, collection='variables')"},{"location":"sonnet/#args_297","text":"filename : The filename of the checkpoint. name_fn : A function which can map the name of the variable requested. This allows restoring variables with values having different names in the checkpoint. collection : Only set the restore initializer for variables in this collection. If None , it will attempt to restore all variables. By default tf.compat.v1.GraphKeys.GLOBAL_VARIABLES .","title":"Args:"},{"location":"sonnet/#returns_581","text":"A restore_initializer custom getter, which is a function taking arguments (getter, name, args, *kwargs).","title":"Returns:"},{"location":"sonnet/#custom_gettersstop_gradientgetter-args-kwargs","text":"Custom getter which prevents variables being optimized. Usage like: with tf.variable_scope(\"\", custom_getter=snt.custom_getters.stop_gradient): net = snt.Linear(num_hidden)(net) or, using the custom_getter constructor argument, linear = snt.Linear(num_hidden, custom_getter=snt.custom_getters.stop_gradient) net = linear(net) will result in the gradient with respect to the variables in the linear module being None . By default, the variables will still be in the trainable variables collection. When used with a Sonnet module, the module must be constructed inside the variable scope with the custom getter. Just building the module inside said variable scope will not use the custom getter.","title":"custom_getters.stop_gradient(getter, *args, **kwargs)"},{"location":"sonnet/#args_298","text":"getter : The true getter to call. *args : Arguments, in the same format as tf.get_variable. **kwargs : Keyword arguments, in the same format as tf.get_variable.","title":"Args:"},{"location":"sonnet/#returns_582","text":"The return value of getter(*args, **kwargs) with a tf.stop_gradient.","title":"Returns:"},{"location":"sonnet/#class-custom_gettersbayes_by_backpropestimatormodes","text":"","title":"class custom_getters.bayes_by_backprop.EstimatorModes"},{"location":"sonnet/#class-custom_gettersbayes_by_backprop_variablemetadata","text":"VariableMetadata(raw_variable_name, raw_variable_shape, scope_name, posterior, posterior_estimate, prior, kl_cost, prior_vars, posterior_vars)","title":"class custom_getters.bayes_by_backprop._VariableMetadata"},{"location":"sonnet/#custom_gettersbayes_by_backprop_variablemetadatakl_cost","text":"Alias for field number 6","title":"custom_getters.bayes_by_backprop._VariableMetadata.kl_cost"},{"location":"sonnet/#custom_gettersbayes_by_backprop_variablemetadataposterior","text":"Alias for field number 3","title":"custom_getters.bayes_by_backprop._VariableMetadata.posterior"},{"location":"sonnet/#custom_gettersbayes_by_backprop_variablemetadataposterior_estimate","text":"Alias for field number 4","title":"custom_getters.bayes_by_backprop._VariableMetadata.posterior_estimate"},{"location":"sonnet/#custom_gettersbayes_by_backprop_variablemetadataposterior_vars","text":"Alias for field number 8","title":"custom_getters.bayes_by_backprop._VariableMetadata.posterior_vars"},{"location":"sonnet/#custom_gettersbayes_by_backprop_variablemetadataprior","text":"Alias for field number 5","title":"custom_getters.bayes_by_backprop._VariableMetadata.prior"},{"location":"sonnet/#custom_gettersbayes_by_backprop_variablemetadataprior_vars","text":"Alias for field number 7","title":"custom_getters.bayes_by_backprop._VariableMetadata.prior_vars"},{"location":"sonnet/#custom_gettersbayes_by_backprop_variablemetadataraw_variable_name","text":"Alias for field number 0","title":"custom_getters.bayes_by_backprop._VariableMetadata.raw_variable_name"},{"location":"sonnet/#custom_gettersbayes_by_backprop_variablemetadataraw_variable_shape","text":"Alias for field number 1","title":"custom_getters.bayes_by_backprop._VariableMetadata.raw_variable_shape"},{"location":"sonnet/#custom_gettersbayes_by_backprop_variablemetadatascope_name","text":"Alias for field number 2","title":"custom_getters.bayes_by_backprop._VariableMetadata.scope_name"},{"location":"sonnet/#custom_gettersbayes_by_backpropadaptive_gaussian_prior_buildergetter-name-args-kwargs","text":"A pre-canned builder for adaptive scalar gaussian prior distributions. Given a true getter function and arguments forwarded from tf.get_variable , return a distribution object for a scalar-valued adaptive gaussian prior which will be broadcast over a variable of the requisite shape. This prior's parameters (e.g loc and scale for a gaussian) will consist of a single learned scalar for the entire tf.Variable for which it serves as the prior, regardless of that tf.Variable 's shape.","title":"custom_getters.bayes_by_backprop.adaptive_gaussian_prior_builder(getter, name, *args, **kwargs)"},{"location":"sonnet/#args_299","text":"getter : The getter passed to a custom_getter . Please see the documentation for tf.get_variable . name : The name argument passed to tf.get_variable . *args : See positional arguments passed to tf.get_variable . **kwargs : See keyword arguments passed to tf.get_variable .","title":"Args:"},{"location":"sonnet/#returns_583","text":"An instance of tfp.distributions.Normal representing the prior distribution over the variable in question.","title":"Returns:"},{"location":"sonnet/#custom_gettersbayes_by_backpropanalytic_kl_builderposterior-prior-sample","text":"A pre-canned builder for the analytic kl divergence.","title":"custom_getters.bayes_by_backprop.analytic_kl_builder(posterior, prior, sample)"},{"location":"sonnet/#custom_gettersbayes_by_backpropbayes_by_backprop_getterposterior_builderltfunction-diagonal_gaussian_posterior_builder-at-0x7fcc549372f0gt-prior_builderltfunction-fixed_gaussian_prior_builder-at-0x7fcc54937620gt-kl_builderltfunction-stochastic_kl_builder-at-0x7fcc54937730gt-sampling_mode_tensornone-fresh_noise_per_connectiontrue-keep_control_dependenciesfalse","text":"Creates a custom getter which does Bayes by Backprop. Please see tf.get_variable for general documentation on custom getters. All arguments are optional. If nothing is configued, then a diagonal gaussian posterior will be used, and a fixed N(0, 0.01) prior will be used. Please see the default posterior_builder and prior_builder for a more detailed understanding of the default settings.","title":"custom_getters.bayes_by_backprop.bayes_by_backprop_getter(posterior_builder=&lt;function diagonal_gaussian_posterior_builder at 0x7fcc549372f0&gt;, prior_builder=&lt;function fixed_gaussian_prior_builder at 0x7fcc54937620&gt;, kl_builder=&lt;function stochastic_kl_builder at 0x7fcc54937730&gt;, sampling_mode_tensor=None, fresh_noise_per_connection=True, keep_control_dependencies=False)"},{"location":"sonnet/#args_300","text":"posterior_builder : A builder function which constructs an instance of tfp.distributions.Distribution which shall serve as the posterior over the tf.Variable of interest. The builder receives the getter and the arguments forwarded from tf.get_variable . Suppose one wrote tf.get_variable( 'weights', shape=(3,), initializer=tf.zeros_initializer, dtype=tf.float32) then the posterior_builder argument would receive the name , shape , initializer , and dtype arguments passed above. The builder must return a tfp.distributions.Distribution object. Please see the tf.get_variable for documentation on custom_getter and getter , and see bbb.diagonal_gaussian_posterior_builder (the default) for an example of using this builder API. prior_builder : A builder function which constructs an instance of tfp.distributions.Distribution which shall serve as the prior over the tf.Variable of interest. Identical API to posterior_builder . See bbb.fixed_gaussian_prior_builder (the default) for an example. kl_builder : A builder function which receives the posterior distribution, prior distribution, and a sample from the posterior. It returns a scalar-shaped tf.Tensor representing the total KL cost for the tf.Variable in question. See bbb.stochastic_kl_builder (default) and bbb.analytic_kl_builder for examples. sampling_mode_tensor : A tf.Tensor which determines how an estimate from the posterior is produced. It must be scalar-shaped and have a dtype of tf.string . Valid values for this tensor are bbb.EstimatorModes.sample (which is the default), bbb.EstimatorModes.mean , and bbb.EstimatorModes.last_sample . bbb.EstimatorModes.sample is appropriate for training, and bbb.EstimatorModes.mean can be used at test time. fresh_noise_per_connection : A boolean. Indicates that each time a stochastic variable is retrieved with this custom getter, new sampling noise should be used. This is True by default. If this argument is set to False , then the same noise is used for each connection. Note that this does not apply to connections within a tf.while_loop ; the same sampling noise is always used in different iterations of a tf.while_loop within one session.run() call. See the unit tests for details. keep_control_dependencies : A boolean. This argument should only be used by advanced users. Indicates that each time a stochastic variable is retrieved in the loop body of a tf.while_loop construct, new sampling noise should be used. The default behavior is False , so that RNNs use the same weights at each recurrent time step. This is done by removing the creation of the Variable from any existing control flow contexts. Notably, the Variables will be created outside the context of any tf.while_loop, making them fetchable. When this argument is True , any Variables used in the loop body of a tf.while_loop will be non-fetchable. If the KL cost needs to be evaluated, the Variable must first be used outside the loop body. This op using the Variable simply needs to be placed on the graph to get a stochastic estimate of the KL; it doesn't need to ever be used. Example: ``` def loop_body(i): logits = sonnet_module(queue) i = i + 1 with tf.variable_scope('bbb', custom_getter=bbb.bayes_by_backprop_getter( fresh_noise_per_connection=True, keep_control_dependencies=True)): unused_op = sonnet_module(queue) # Adds KL estimate to bbb Collection final_i = tf.while_loop(lambda i: i 5, loop_body, tf.constant(0.)) ``` Here when we add unused_op to the graph, we also add a number of tensors associated with the particular stochastic variable, including its contribution to the KL cost, to a graph-level registry. These are organized in a per-stochastic-variable data structure and be accessed with bbb.get_variable_metadata() . Without this line, these Tensors would instead be added the first time the Variable is used in the while_loop, which would make them non-fetchable. In all cases, the KL cost is only added once per Variable, which is the correct behavior, since if a variable is used multiple times in a model, the KL cost should remain unaffected.","title":"Args:"},{"location":"sonnet/#returns_584","text":"A custom_getter function which implements Bayes by Backprop.","title":"Returns:"},{"location":"sonnet/#custom_gettersbayes_by_backpropdiagonal_gaussian_posterior_buildergetter-name-shapenone-args-kwargs","text":"A pre-canned builder for diagonal gaussian posterior distributions. Given a true getter function and arguments forwarded from tf.get_variable , return a distribution object for a diagonal posterior over a variable of the requisite shape.","title":"custom_getters.bayes_by_backprop.diagonal_gaussian_posterior_builder(getter, name, shape=None, *args, **kwargs)"},{"location":"sonnet/#args_301","text":"getter : The getter passed to a custom_getter . Please see the documentation for tf.get_variable . name : The name argument passed to tf.get_variable . shape : The shape argument passed to tf.get_variable . *args : See positional arguments passed to tf.get_variable . **kwargs : See keyword arguments passed to tf.get_variable .","title":"Args:"},{"location":"sonnet/#returns_585","text":"An instance of tfp.distributions.Normal representing the posterior distribution over the variable in question.","title":"Returns:"},{"location":"sonnet/#custom_gettersbayes_by_backpropfixed_gaussian_prior_buildergetter-name-dtypenone-args-kwargs","text":"A pre-canned builder for fixed gaussian prior distributions. Given a true getter function and arguments forwarded from tf.get_variable , return a distribution object for a scalar-valued fixed gaussian prior which will be broadcast over a variable of the requisite shape.","title":"custom_getters.bayes_by_backprop.fixed_gaussian_prior_builder(getter, name, dtype=None, *args, **kwargs)"},{"location":"sonnet/#args_302","text":"getter : The getter passed to a custom_getter . Please see the documentation for tf.get_variable . name : The name argument passed to tf.get_variable . dtype : The dtype argument passed to tf.get_variable . *args : See positional arguments passed to tf.get_variable . **kwargs : See keyword arguments passed to tf.get_variable .","title":"Args:"},{"location":"sonnet/#returns_586","text":"An instance of tfp.distributions.Normal representing the prior distribution over the variable in question.","title":"Returns:"},{"location":"sonnet/#custom_gettersbayes_by_backpropget_total_kl_costnametotal_kl_cost-filter_by_name_substringnone","text":"Get the total cost for all (or a subset of) the stochastic variables.","title":"custom_getters.bayes_by_backprop.get_total_kl_cost(name='total_kl_cost', filter_by_name_substring=None)"},{"location":"sonnet/#args_303","text":"name : A name for the tensor representing the total kl cost. filter_by_name_substring : A string used to filter which variables count toward the total KL cost. By default, this argument is None , and all variables trained using Bayes by Backprop are included. If this argument is provided, the variables whose KL costs are summed will be all those whose name contains filter_by_name_substring . An example use of this would be to select all variables within a particular scope.","title":"Args:"},{"location":"sonnet/#returns_587","text":"A tensor representing the total KL cost in the ELBO loss.","title":"Returns:"},{"location":"sonnet/#custom_gettersbayes_by_backpropget_variable_metadatascope_name_substringnone","text":"","title":"custom_getters.bayes_by_backprop.get_variable_metadata(scope_name_substring=None)"},{"location":"sonnet/#custom_gettersbayes_by_backpropinverse_softplusy","text":"The inverse of the softplus function. Computes the inverse of softplus, a function which maps an unconstrained real number to the positive reals, e.g. to squash an unconstrained neural network activation to parameterize a variance.","title":"custom_getters.bayes_by_backprop.inverse_softplus(y)"},{"location":"sonnet/#args_304","text":"y : A positive number.","title":"Args:"},{"location":"sonnet/#returns_588","text":"The number x such that softplus(x) = y.","title":"Returns:"},{"location":"sonnet/#custom_gettersbayes_by_backpropscale_variable_initializerdesired_scale","text":"","title":"custom_getters.bayes_by_backprop.scale_variable_initializer(desired_scale)"},{"location":"sonnet/#custom_gettersbayes_by_backpropstochastic_kl_builderposterior-prior-sample","text":"A pre-canned builder for a ubiquitous stochastic KL estimator.","title":"custom_getters.bayes_by_backprop.stochastic_kl_builder(posterior, prior, sample)"},{"location":"sonnet/#nestassert_same_structureargs-kwargs","text":"","title":"nest.assert_same_structure(*args, **kwargs)"},{"location":"sonnet/#nestassert_shallow_structureargs-kwargs","text":"","title":"nest.assert_shallow_structure(*args, **kwargs)"},{"location":"sonnet/#nestflattenargs-kwargs","text":"","title":"nest.flatten(*args, **kwargs)"},{"location":"sonnet/#nestflatten_dict_itemsargs-kwargs","text":"","title":"nest.flatten_dict_items(*args, **kwargs)"},{"location":"sonnet/#nestflatten_iterableargs-kwargs","text":"","title":"nest.flatten_iterable(*args, **kwargs)"},{"location":"sonnet/#nestflatten_up_toargs-kwargs","text":"","title":"nest.flatten_up_to(*args, **kwargs)"},{"location":"sonnet/#nestis_iterableargs-kwargs","text":"","title":"nest.is_iterable(*args, **kwargs)"},{"location":"sonnet/#nestis_sequenceargs-kwargs","text":"","title":"nest.is_sequence(*args, **kwargs)"},{"location":"sonnet/#nestmapargs-kwargs","text":"","title":"nest.map(*args, **kwargs)"},{"location":"sonnet/#nestmap_up_toargs-kwargs","text":"","title":"nest.map_up_to(*args, **kwargs)"},{"location":"sonnet/#nestpack_iterable_asargs-kwargs","text":"","title":"nest.pack_iterable_as(*args, **kwargs)"},{"location":"sonnet/#nestpack_sequence_asargs-kwargs","text":"","title":"nest.pack_sequence_as(*args, **kwargs)"},{"location":"sonnet/#nestwith_deprecation_warningfn-extra_message","text":"Wraps the function and prints a warn-once (per extra_message ) warning.","title":"nest.with_deprecation_warning(fn, extra_message='')"},{"location":"sonnet/#class-netsalexnet","text":"Implementation of AlexNet with full and mini versions. Based on: 'ImageNet Classification with Deep Convolutional Neural Networks' Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, NIPS 2012 http://papers.nips.cc/paper/4824-imagenet-classification-w","title":"class nets.AlexNet"},{"location":"sonnet/#netsalexnet__init__mode-use_batch_normfalse-batch_norm_confignone-initializersnone-partitionersnone-regularizersnone-bn_on_fc_layerstrue-custom_getternone-namealex_net","text":"Constructs AlexNet.","title":"nets.AlexNet.__init__(mode, use_batch_norm=False, batch_norm_config=None, initializers=None, partitioners=None, regularizers=None, bn_on_fc_layers=True, custom_getter=None, name='alex_net')"},{"location":"sonnet/#args_305","text":"mode : Construction mode of network: AlexNet.FULL or AlexNet.MINI . use_batch_norm : Whether to use batch normalization between the output of a layer and the activation function. batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializers are truncated normal initializers, which are commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). partitioners : Optional dict containing partitioners for the filters (with key 'w') and the biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . bn_on_fc_layers : If use_batch_norm is True, add batch normalization to the fully-connected layers. This is deprecated. custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_505","text":"base.Error: If the given mode is not one of AlexNet.FULL , or AlexNet.MINI . KeyError : If initializers , partitioners or regularizers contains any keys other than 'w' or 'b'.","title":"Raises:"},{"location":"sonnet/#netsalexnet__call__inputs-keep_probnone-is_trainingnone-test_local_statstrue","text":"Connects the AlexNet module into the graph. The is_training flag only controls the batch norm settings, if False it does not force no dropout by overriding any input keep_prob . To avoid any confusion this may cause, if is_training=False and keep_prob would cause dropout to be applied, an error is thrown.","title":"nets.AlexNet.__call__(inputs, keep_prob=None, is_training=None, test_local_stats=True)"},{"location":"sonnet/#args_306","text":"inputs : A Tensor of size [batch_size, input_height, input_width, input_channels], representing a batch of input images. keep_prob : A scalar Tensor representing the dropout keep probability. When is_training=False this must be None or 1 to give no dropout. is_training : Boolean to indicate if we are currently training. Must be specified if batch normalization or dropout is used. test_local_stats : Boolean to indicate to snt.BatchNorm if batch normalization should use local batch statistics at test time. By default True .","title":"Args:"},{"location":"sonnet/#returns_589","text":"A Tensor of size [batch_size, output_size], where output_size depends on the mode the network was constructed in.","title":"Returns:"},{"location":"sonnet/#raises_506","text":"base.IncompatibleShapeError: If any of the input image dimensions (input_height, input_width) are too small for the given network mode. ValueError : If keep_prob is not None or 1 when is_training=False . ValueError : If is_training is not explicitly specified when using batch normalization.","title":"Raises:"},{"location":"sonnet/#netsalexnetconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"nets.AlexNet.connected_subgraphs"},{"location":"sonnet/#netsalexnetconv_modules","text":"Returns list containing convolutional modules of network.","title":"nets.AlexNet.conv_modules"},{"location":"sonnet/#returns_590","text":"A list containing the Conv2D modules.","title":"Returns:"},{"location":"sonnet/#netsalexnetdefun","text":"Wraps this modules call method in a callable graph function.","title":"nets.AlexNet.defun()"},{"location":"sonnet/#netsalexnetdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"nets.AlexNet.defun_wrapped"},{"location":"sonnet/#netsalexnetget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"nets.AlexNet.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_307","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_591","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_507","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"nets.AlexNet.get_possible_initializer_keys()"},{"location":"sonnet/#returns_592","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#netsalexnetget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"nets.AlexNet.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_308","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_593","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_508","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"nets.AlexNet.graph"},{"location":"sonnet/#netsalexnetinitializers","text":"","title":"nets.AlexNet.initializers"},{"location":"sonnet/#netsalexnetis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"nets.AlexNet.is_connected"},{"location":"sonnet/#netsalexnetlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"nets.AlexNet.last_connected_subgraph"},{"location":"sonnet/#returns_594","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_509","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetlinear_modules","text":"Returns list containing linear modules of network.","title":"nets.AlexNet.linear_modules"},{"location":"sonnet/#returns_595","text":"A list containing the Linear modules.","title":"Returns:"},{"location":"sonnet/#netsalexnetmin_input_size","text":"Returns integer specifying the minimum width and height for the input. Note that the input can be non-square, but both the width and height must be = this number in size.","title":"nets.AlexNet.min_input_size"},{"location":"sonnet/#returns_596","text":"The minimum size as an integer.","title":"Returns:"},{"location":"sonnet/#netsalexnetmodule_name","text":"Returns the name of the Module.","title":"nets.AlexNet.module_name"},{"location":"sonnet/#netsalexnetname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"nets.AlexNet.name_scopes"},{"location":"sonnet/#netsalexnetnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.AlexNet.non_trainable_variables"},{"location":"sonnet/#returns_597","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_510","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetpartitioners","text":"","title":"nets.AlexNet.partitioners"},{"location":"sonnet/#netsalexnetregularizers","text":"","title":"nets.AlexNet.regularizers"},{"location":"sonnet/#netsalexnetscope_name","text":"Returns the full name of the Module's variable scope.","title":"nets.AlexNet.scope_name"},{"location":"sonnet/#netsalexnettrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.AlexNet.trainable_variables"},{"location":"sonnet/#returns_598","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_511","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"nets.AlexNet.variable_scope"},{"location":"sonnet/#returns_599","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_512","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.AlexNet.variables"},{"location":"sonnet/#returns_600","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_513","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-netsalexnetfull","text":"AlexNet constructed in the 'FULL' mode.","title":"class nets.AlexNetFull"},{"location":"sonnet/#netsalexnetfull__init__use_batch_normfalse-batch_norm_confignone-initializersnone-partitionersnone-regularizersnone-custom_getternone-namealex_net_full","text":"Constructs AlexNet.","title":"nets.AlexNetFull.__init__(use_batch_norm=False, batch_norm_config=None, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='alex_net_full')"},{"location":"sonnet/#args_309","text":"use_batch_norm : Whether to use batch normalization between the output of a layer and the activation function. batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializers are truncated normal initializers, which are commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). partitioners : Optional dict containing partitioners for the filters (with key 'w') and the biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_514","text":"KeyError : If initializers , partitioners or regularizers contains any keys other than 'w' or 'b'.","title":"Raises:"},{"location":"sonnet/#netsalexnetfull__call__inputs-keep_probnone-is_trainingnone-test_local_statstrue","text":"Connects the AlexNet module into the graph. The is_training flag only controls the batch norm settings, if False it does not force no dropout by overriding any input keep_prob . To avoid any confusion this may cause, if is_training=False and keep_prob would cause dropout to be applied, an error is thrown.","title":"nets.AlexNetFull.__call__(inputs, keep_prob=None, is_training=None, test_local_stats=True)"},{"location":"sonnet/#args_310","text":"inputs : A Tensor of size [batch_size, input_height, input_width, input_channels], representing a batch of input images. keep_prob : A scalar Tensor representing the dropout keep probability. When is_training=False this must be None or 1 to give no dropout. is_training : Boolean to indicate if we are currently training. Must be specified if batch normalization or dropout is used. test_local_stats : Boolean to indicate to snt.BatchNorm if batch normalization should use local batch statistics at test time. By default True .","title":"Args:"},{"location":"sonnet/#returns_601","text":"A Tensor of size [batch_size, output_size], where output_size depends on the mode the network was constructed in.","title":"Returns:"},{"location":"sonnet/#raises_515","text":"base.IncompatibleShapeError: If any of the input image dimensions (input_height, input_width) are too small for the given network mode. ValueError : If keep_prob is not None or 1 when is_training=False . ValueError : If is_training is not explicitly specified when using batch normalization.","title":"Raises:"},{"location":"sonnet/#netsalexnetfullconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"nets.AlexNetFull.connected_subgraphs"},{"location":"sonnet/#netsalexnetfullconv_modules","text":"Returns list containing convolutional modules of network.","title":"nets.AlexNetFull.conv_modules"},{"location":"sonnet/#returns_602","text":"A list containing the Conv2D modules.","title":"Returns:"},{"location":"sonnet/#netsalexnetfulldefun","text":"Wraps this modules call method in a callable graph function.","title":"nets.AlexNetFull.defun()"},{"location":"sonnet/#netsalexnetfulldefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"nets.AlexNetFull.defun_wrapped"},{"location":"sonnet/#netsalexnetfullget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"nets.AlexNetFull.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_311","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_603","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_516","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetfullget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"nets.AlexNetFull.get_possible_initializer_keys()"},{"location":"sonnet/#returns_604","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#netsalexnetfullget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"nets.AlexNetFull.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_312","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_605","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_517","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetfullgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"nets.AlexNetFull.graph"},{"location":"sonnet/#netsalexnetfullinitializers","text":"","title":"nets.AlexNetFull.initializers"},{"location":"sonnet/#netsalexnetfullis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"nets.AlexNetFull.is_connected"},{"location":"sonnet/#netsalexnetfulllast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"nets.AlexNetFull.last_connected_subgraph"},{"location":"sonnet/#returns_606","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_518","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetfulllinear_modules","text":"Returns list containing linear modules of network.","title":"nets.AlexNetFull.linear_modules"},{"location":"sonnet/#returns_607","text":"A list containing the Linear modules.","title":"Returns:"},{"location":"sonnet/#netsalexnetfullmin_input_size","text":"Returns integer specifying the minimum width and height for the input. Note that the input can be non-square, but both the width and height must be = this number in size.","title":"nets.AlexNetFull.min_input_size"},{"location":"sonnet/#returns_608","text":"The minimum size as an integer.","title":"Returns:"},{"location":"sonnet/#netsalexnetfullmodule_name","text":"Returns the name of the Module.","title":"nets.AlexNetFull.module_name"},{"location":"sonnet/#netsalexnetfullname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"nets.AlexNetFull.name_scopes"},{"location":"sonnet/#netsalexnetfullnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.AlexNetFull.non_trainable_variables"},{"location":"sonnet/#returns_609","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_519","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetfullpartitioners","text":"","title":"nets.AlexNetFull.partitioners"},{"location":"sonnet/#netsalexnetfullregularizers","text":"","title":"nets.AlexNetFull.regularizers"},{"location":"sonnet/#netsalexnetfullscope_name","text":"Returns the full name of the Module's variable scope.","title":"nets.AlexNetFull.scope_name"},{"location":"sonnet/#netsalexnetfulltrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.AlexNetFull.trainable_variables"},{"location":"sonnet/#returns_610","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_520","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetfullvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"nets.AlexNetFull.variable_scope"},{"location":"sonnet/#returns_611","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_521","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetfullvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.AlexNetFull.variables"},{"location":"sonnet/#returns_612","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_522","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-netsalexnetmini","text":"AlexNet constructed in the 'MINI' mode.","title":"class nets.AlexNetMini"},{"location":"sonnet/#netsalexnetmini__init__use_batch_normfalse-batch_norm_confignone-initializersnone-partitionersnone-regularizersnone-custom_getternone-namealex_net_mini","text":"Constructs AlexNet.","title":"nets.AlexNetMini.__init__(use_batch_norm=False, batch_norm_config=None, initializers=None, partitioners=None, regularizers=None, custom_getter=None, name='alex_net_mini')"},{"location":"sonnet/#args_313","text":"use_batch_norm : Whether to use batch normalization between the output of a layer and the activation function. batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. initializers : Optional dict containing ops to initialize the filters (with key 'w') or biases (with key 'b'). The default initializers are truncated normal initializers, which are commonly used when the inputs are zero centered (see https://arxiv.org/pdf/1502.03167v3.pdf). partitioners : Optional dict containing partitioners for the filters (with key 'w') and the biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_523","text":"KeyError : If initializers , partitioners or regularizers contains any keys other than 'w' or 'b'.","title":"Raises:"},{"location":"sonnet/#netsalexnetmini__call__inputs-keep_probnone-is_trainingnone-test_local_statstrue","text":"Connects the AlexNet module into the graph. The is_training flag only controls the batch norm settings, if False it does not force no dropout by overriding any input keep_prob . To avoid any confusion this may cause, if is_training=False and keep_prob would cause dropout to be applied, an error is thrown.","title":"nets.AlexNetMini.__call__(inputs, keep_prob=None, is_training=None, test_local_stats=True)"},{"location":"sonnet/#args_314","text":"inputs : A Tensor of size [batch_size, input_height, input_width, input_channels], representing a batch of input images. keep_prob : A scalar Tensor representing the dropout keep probability. When is_training=False this must be None or 1 to give no dropout. is_training : Boolean to indicate if we are currently training. Must be specified if batch normalization or dropout is used. test_local_stats : Boolean to indicate to snt.BatchNorm if batch normalization should use local batch statistics at test time. By default True .","title":"Args:"},{"location":"sonnet/#returns_613","text":"A Tensor of size [batch_size, output_size], where output_size depends on the mode the network was constructed in.","title":"Returns:"},{"location":"sonnet/#raises_524","text":"base.IncompatibleShapeError: If any of the input image dimensions (input_height, input_width) are too small for the given network mode. ValueError : If keep_prob is not None or 1 when is_training=False . ValueError : If is_training is not explicitly specified when using batch normalization.","title":"Raises:"},{"location":"sonnet/#netsalexnetminiconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"nets.AlexNetMini.connected_subgraphs"},{"location":"sonnet/#netsalexnetminiconv_modules","text":"Returns list containing convolutional modules of network.","title":"nets.AlexNetMini.conv_modules"},{"location":"sonnet/#returns_614","text":"A list containing the Conv2D modules.","title":"Returns:"},{"location":"sonnet/#netsalexnetminidefun","text":"Wraps this modules call method in a callable graph function.","title":"nets.AlexNetMini.defun()"},{"location":"sonnet/#netsalexnetminidefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"nets.AlexNetMini.defun_wrapped"},{"location":"sonnet/#netsalexnetminiget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"nets.AlexNetMini.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_315","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_615","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_525","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetminiget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"nets.AlexNetMini.get_possible_initializer_keys()"},{"location":"sonnet/#returns_616","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#netsalexnetminiget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"nets.AlexNetMini.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_316","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_617","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_526","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetminigraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"nets.AlexNetMini.graph"},{"location":"sonnet/#netsalexnetminiinitializers","text":"","title":"nets.AlexNetMini.initializers"},{"location":"sonnet/#netsalexnetminiis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"nets.AlexNetMini.is_connected"},{"location":"sonnet/#netsalexnetminilast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"nets.AlexNetMini.last_connected_subgraph"},{"location":"sonnet/#returns_618","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_527","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetminilinear_modules","text":"Returns list containing linear modules of network.","title":"nets.AlexNetMini.linear_modules"},{"location":"sonnet/#returns_619","text":"A list containing the Linear modules.","title":"Returns:"},{"location":"sonnet/#netsalexnetminimin_input_size","text":"Returns integer specifying the minimum width and height for the input. Note that the input can be non-square, but both the width and height must be = this number in size.","title":"nets.AlexNetMini.min_input_size"},{"location":"sonnet/#returns_620","text":"The minimum size as an integer.","title":"Returns:"},{"location":"sonnet/#netsalexnetminimodule_name","text":"Returns the name of the Module.","title":"nets.AlexNetMini.module_name"},{"location":"sonnet/#netsalexnetmininame_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"nets.AlexNetMini.name_scopes"},{"location":"sonnet/#netsalexnetmininon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.AlexNetMini.non_trainable_variables"},{"location":"sonnet/#returns_621","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_528","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetminipartitioners","text":"","title":"nets.AlexNetMini.partitioners"},{"location":"sonnet/#netsalexnetminiregularizers","text":"","title":"nets.AlexNetMini.regularizers"},{"location":"sonnet/#netsalexnetminiscope_name","text":"Returns the full name of the Module's variable scope.","title":"nets.AlexNetMini.scope_name"},{"location":"sonnet/#netsalexnetminitrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.AlexNetMini.trainable_variables"},{"location":"sonnet/#returns_622","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_529","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetminivariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"nets.AlexNetMini.variable_scope"},{"location":"sonnet/#returns_623","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_530","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsalexnetminivariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.AlexNetMini.variables"},{"location":"sonnet/#returns_624","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_531","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-netsconvnet2d","text":"A 2D Convolutional Network module.","title":"class nets.ConvNet2D"},{"location":"sonnet/#netsconvnet2d__init__output_channels-kernel_shapes-strides-paddings-rates1-activationltfunction-relu-at-0x7fcc58553c80gt-activate_finalfalse-normalization_ctornone-normalization_kwargsnone-normalize_finalnone-initializersnone-partitionersnone-regularizersnone-use_batch_normnone-use_biastrue-batch_norm_confignone-data_formatnhwc-custom_getternone-nameconv_net_2d","text":"Constructs a ConvNet2D module. By default, neither batch normalization nor activation are applied to the output of the final layer.","title":"nets.ConvNet2D.__init__(output_channels, kernel_shapes, strides, paddings, rates=(1,), activation=&lt;function relu at 0x7fcc58553c80&gt;, activate_final=False, normalization_ctor=None, normalization_kwargs=None, normalize_final=None, initializers=None, partitioners=None, regularizers=None, use_batch_norm=None, use_bias=True, batch_norm_config=None, data_format='NHWC', custom_getter=None, name='conv_net_2d')"},{"location":"sonnet/#args_317","text":"output_channels : Iterable of output channels, as defined in conv.Conv2D . Output channels can be defined either as number or via a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that entries can be called when build is called. Each entry in the iterable defines properties in the corresponding convolutional layer. kernel_shapes : Iterable of kernel sizes as defined in conv.Conv2D ; if the list contains one element only, the same kernel shape is used in each layer of the network. strides : Iterable of kernel strides as defined in conv.Conv2D ; if the list contains one element only, the same stride is used in each layer of the network. paddings : Iterable of padding options as defined in conv.Conv2D . Each can be snt.SAME , snt.VALID , snt.FULL , snt.CAUSAL , snt.REVERSE_CAUSAL or a pair of these to use for height and width. If the Iterable contains one element only, the same padding is used in each layer of the network. rates : Iterable of dilation rates as defined in conv.Conv2D ; if the list contains one element only, the same rate is used in each layer of the network. activation : An activation op. activate_final : Boolean determining if the activation and batch normalization, if turned on, are applied to the final layer. normalization_ctor : Constructor to return a callable which will perform normalization at each layer. Defaults to None / no normalization. Examples of what could go here: snt.BatchNormV2 , snt.LayerNorm . If a string is provided, importlib is used to convert the string to a callable, so either snt.LayerNorm or \"snt.LayerNorm\" can be provided. normalization_kwargs : kwargs to be provided to normalization_ctor when it is called. normalize_final : Whether to apply normalization after the final conv layer. Default is to take the value of activate_final. initializers : Optional dict containing ops to initialize the filters of the whole network (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters of the whole network (with key 'w') or biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . use_batch_norm : Boolean determining if batch normalization is applied after convolution. Deprecated, use normalization_ctor instead. use_bias : Boolean or iterable of booleans determining whether to include bias parameters in the convolutional layers. Default True . batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. Deprecated, use normalization_kwargs instead. data_format : A string, one of \"NCHW\" or \"NHWC\". Specifies whether the channel dimension of the input and output is the last dimension (default, \"NHWC\"), or the second dimension (\"NCHW\"). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. Note that this custom_getter will not be passed to the transpose method. If you want to use a custom getter with the transposed of this convolutional network, you should provide one to the transpose method instead. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_532","text":"TypeError : If output_channels is not iterable; or if kernel_shapes is not iterable; or strides is not iterable; or paddings is not iterable; or if activation is not callable. ValueError : If output_channels is empty; or if kernel_shapes has not length 1 or len(output_channels) ; or if strides has not length 1 or len(output_channels) ; or if paddings has not length 1 or len(output_channels) ; or if rates has not length 1 or len(output_channels) ; or if the given data_format is not a supported format (\"NHWC\" or \"NCHW\"); or if normalization_ctor is provided but cannot be mapped to a callable. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable.","title":"Raises:"},{"location":"sonnet/#netsconvnet2d__call__inputs-normalization_build_kwargs","text":"Assembles the ConvNet2D and connects it to the graph.","title":"nets.ConvNet2D.__call__(inputs, **normalization_build_kwargs)"},{"location":"sonnet/#args_318","text":"inputs : A 4D Tensor of shape [batch_size, input_height, input_width, input_channels] . **normalization_build_kwargs : kwargs passed to the normalization module at _build time.","title":"Args:"},{"location":"sonnet/#returns_625","text":"A 4D Tensor of shape [batch_size, output_height, output_width, output_channels[-1]] .","title":"Returns:"},{"location":"sonnet/#raises_533","text":"ValueError : If is_training is not explicitly specified when using batch normalization.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dactivate_final","text":"","title":"nets.ConvNet2D.activate_final"},{"location":"sonnet/#netsconvnet2dactivation","text":"","title":"nets.ConvNet2D.activation"},{"location":"sonnet/#netsconvnet2dbatch_norm_config","text":"","title":"nets.ConvNet2D.batch_norm_config"},{"location":"sonnet/#netsconvnet2dconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"nets.ConvNet2D.connected_subgraphs"},{"location":"sonnet/#netsconvnet2ddefun","text":"Wraps this modules call method in a callable graph function.","title":"nets.ConvNet2D.defun()"},{"location":"sonnet/#netsconvnet2ddefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"nets.ConvNet2D.defun_wrapped"},{"location":"sonnet/#netsconvnet2dget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"nets.ConvNet2D.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_319","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_626","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_534","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"nets.ConvNet2D.get_possible_initializer_keys()"},{"location":"sonnet/#returns_627","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#netsconvnet2dget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"nets.ConvNet2D.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_320","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_628","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_535","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"nets.ConvNet2D.graph"},{"location":"sonnet/#netsconvnet2dinitializers","text":"","title":"nets.ConvNet2D.initializers"},{"location":"sonnet/#netsconvnet2dinput_shape","text":"Returns shape of input Tensor passed at last call to build .","title":"nets.ConvNet2D.input_shape"},{"location":"sonnet/#netsconvnet2dis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"nets.ConvNet2D.is_connected"},{"location":"sonnet/#netsconvnet2dkernel_shapes","text":"","title":"nets.ConvNet2D.kernel_shapes"},{"location":"sonnet/#netsconvnet2dlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"nets.ConvNet2D.last_connected_subgraph"},{"location":"sonnet/#returns_629","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_536","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dlayers","text":"Returns a tuple containing the convolutional layers of the network.","title":"nets.ConvNet2D.layers"},{"location":"sonnet/#netsconvnet2dmodule_name","text":"Returns the name of the Module.","title":"nets.ConvNet2D.module_name"},{"location":"sonnet/#netsconvnet2dname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"nets.ConvNet2D.name_scopes"},{"location":"sonnet/#netsconvnet2dnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.ConvNet2D.non_trainable_variables"},{"location":"sonnet/#returns_630","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_537","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dnormalization_ctor","text":"","title":"nets.ConvNet2D.normalization_ctor"},{"location":"sonnet/#netsconvnet2dnormalization_kwargs","text":"","title":"nets.ConvNet2D.normalization_kwargs"},{"location":"sonnet/#netsconvnet2dnormalize_final","text":"","title":"nets.ConvNet2D.normalize_final"},{"location":"sonnet/#netsconvnet2doutput_channels","text":"","title":"nets.ConvNet2D.output_channels"},{"location":"sonnet/#netsconvnet2dpaddings","text":"","title":"nets.ConvNet2D.paddings"},{"location":"sonnet/#netsconvnet2dpartitioners","text":"","title":"nets.ConvNet2D.partitioners"},{"location":"sonnet/#netsconvnet2drates","text":"","title":"nets.ConvNet2D.rates"},{"location":"sonnet/#netsconvnet2dregularizers","text":"","title":"nets.ConvNet2D.regularizers"},{"location":"sonnet/#netsconvnet2dscope_name","text":"Returns the full name of the Module's variable scope.","title":"nets.ConvNet2D.scope_name"},{"location":"sonnet/#netsconvnet2dstrides","text":"","title":"nets.ConvNet2D.strides"},{"location":"sonnet/#netsconvnet2dtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.ConvNet2D.trainable_variables"},{"location":"sonnet/#returns_631","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_538","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dtransposenamenone-output_channelsnone-kernel_shapesnone-stridesnone-paddingsnone-activationnone-activate_finalnone-normalization_ctornone-normalization_kwargsnone-normalize_finalnone-initializersnone-partitionersnone-regularizersnone-use_batch_normnone-use_biasnone-batch_norm_confignone-data_formatnone-custom_getternone","text":"Returns transposed version of this network.","title":"nets.ConvNet2D.transpose(name=None, output_channels=None, kernel_shapes=None, strides=None, paddings=None, activation=None, activate_final=None, normalization_ctor=None, normalization_kwargs=None, normalize_final=None, initializers=None, partitioners=None, regularizers=None, use_batch_norm=None, use_bias=None, batch_norm_config=None, data_format=None, custom_getter=None)"},{"location":"sonnet/#args_321","text":"name : Optional string specifying the name of the transposed module. The default name is constructed by appending \"_transpose\" to self.module_name . output_channels : Optional iterable of numbers of output channels. kernel_shapes : Optional iterable of kernel sizes. The default value is constructed by reversing self.kernel_shapes . strides : Optional iterable of kernel strides. The default value is constructed by reversing self.strides . paddings : Optional iterable of padding options, either snt.SAME or snt.VALID ; The default value is constructed by reversing self.paddings . activation : Optional activation op. Default value is self.activation . activate_final : Optional boolean determining if the activation and batch normalization, if turned on, are applied to the final layer. normalization_ctor : Constructor to return a callable which will perform normalization at each layer. Defaults to None / no normalization. Examples of what could go here: snt.BatchNormV2 , snt.LayerNorm . If a string is provided, importlib is used to convert the string to a callable, so either snt.LayerNorm or \"snt.LayerNorm\" can be provided. normalization_kwargs : kwargs to be provided to normalization_ctor when it is called. normalize_final : Whether to apply normalization after the final conv layer. Default is to take the value of activate_final. initializers : Optional dict containing ops to initialize the filters of the whole network (with key 'w') or biases (with key 'b'). The default value is self.initializers . partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). The default value is self.partitioners . regularizers : Optional dict containing regularizers for the filters of the whole network (with key 'w') or biases (with key 'b'). The default is self.regularizers . use_batch_norm : Optional boolean determining if batch normalization is applied after convolution. The default value is self.use_batch_norm . use_bias : Optional boolean or iterable of booleans determining whether to include bias parameters in the convolutional layers. Default is constructed by reversing self.use_bias . batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. Default is self.batch_norm_config . data_format : Optional string, one of \"NCHW\" or \"NHWC\". Specifies whether the channel dimension of the input and output is the last dimension. Default is self._data_format . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API.","title":"Args:"},{"location":"sonnet/#returns_632","text":"Matching ConvNet2DTranspose module.","title":"Returns:"},{"location":"sonnet/#raises_539","text":"ValueError : If output_channels is specified and its length does not match the number of layers. ValueError : If the given data_format is not a supported format (\"NHWC\" or \"NCHW\"). NotImplementedError : If the convolutions are dilated.","title":"Raises:"},{"location":"sonnet/#netsconvnet2duse_batch_norm","text":"","title":"nets.ConvNet2D.use_batch_norm"},{"location":"sonnet/#netsconvnet2duse_bias","text":"","title":"nets.ConvNet2D.use_bias"},{"location":"sonnet/#netsconvnet2dvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"nets.ConvNet2D.variable_scope"},{"location":"sonnet/#returns_633","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_540","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.ConvNet2D.variables"},{"location":"sonnet/#returns_634","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_541","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-netsconvnet2dtranspose","text":"A 2D Transpose-Convolutional Network module.","title":"class nets.ConvNet2DTranspose"},{"location":"sonnet/#netsconvnet2dtranspose__init__output_channels-output_shapes-kernel_shapes-strides-paddings-activationltfunction-relu-at-0x7fcc58553c80gt-activate_finalfalse-normalization_ctornone-normalization_kwargsnone-normalize_finalnone-initializersnone-partitionersnone-regularizersnone-use_batch_normfalse-use_biastrue-batch_norm_confignone-data_formatnhwc-custom_getternone-nameconv_net_2d_transpose","text":"Constructs a ConvNetTranspose2D module. output_{shapes,channels} can be defined either as iterable of {iterables,integers} or via a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that entries can be called returning meaningful values when build is called. Each entry in the iterable defines properties in the corresponding convolutional layer. By default, neither batch normalization nor activation are applied to the output of the final layer.","title":"nets.ConvNet2DTranspose.__init__(output_channels, output_shapes, kernel_shapes, strides, paddings, activation=&lt;function relu at 0x7fcc58553c80&gt;, activate_final=False, normalization_ctor=None, normalization_kwargs=None, normalize_final=None, initializers=None, partitioners=None, regularizers=None, use_batch_norm=False, use_bias=True, batch_norm_config=None, data_format='NHWC', custom_getter=None, name='conv_net_2d_transpose')"},{"location":"sonnet/#args_322","text":"output_channels : Iterable of numbers of output channels. output_shapes : Iterable of output shapes as defined in conv.conv2DTranpose ; if the iterable contains one element only, the same shape is used in each layer of the network. kernel_shapes : Iterable of kernel sizes as defined in conv.Conv2D ; if the list contains one element only, the same kernel shape is used in each layer of the network. strides : Iterable of kernel strides as defined in conv.Conv2D ; if the list contains one element only, the same stride is used in each layer of the network. paddings : Iterable of padding options, either snt.SAME or snt.VALID ; if the Iterable contains one element only, the same padding is used in each layer of the network. activation : An activation op. activate_final : Boolean determining if the activation and batch normalization, if turned on, are applied to the final layer. normalization_ctor : Constructor to return a callable which will perform normalization at each layer. Defaults to None / no normalization. Examples of what could go here: snt.BatchNormV2 , snt.LayerNorm . If a string is provided, importlib is used to convert the string to a callable, so either snt.LayerNorm or \"snt.LayerNorm\" can be provided. normalization_kwargs : kwargs to be provided to normalization_ctor when it is called. normalize_final : Whether to apply normalization after the final conv layer. Default is to take the value of activate_final. initializers : Optional dict containing ops to initialize the filters of the whole network (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the filters of the whole network (with key 'w') or biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . use_batch_norm : Boolean determining if batch normalization is applied after convolution. use_bias : Boolean or iterable of booleans determining whether to include bias parameters in the convolutional layers. Default True . batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. data_format : A string, one of \"NCHW\" or \"NHWC\". Specifies whether the channel dimension of the input and output is the last dimension (default, \"NHWC\"), or the second dimension (\"NCHW\"). custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_542","text":"TypeError : If output_channels is not iterable; or if output_shapes is not iterable; or if kernel_shapes is not iterable; or if strides is not iterable; or if paddings is not iterable; or if activation is not callable. ValueError : If output_channels is empty; or if kernel_shapes has not length 1 or len(output_channels) ; or if strides has not length 1 or len(output_channels) ; or if paddings has not length 1 or len(output_channels) . ValueError : If the given data_format is not a supported format (\"NHWC\" or \"NCHW\"). ValueError : If normalization_ctor is provided but cannot be converted to a callable. KeyError : If initializers , partitioners or regularizers contain any keys other than 'w' or 'b'. TypeError : If any of the given initializers, partitioners or regularizers are not callable.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dtranspose__call__inputs-normalization_build_kwargs","text":"Assembles the ConvNet2D and connects it to the graph.","title":"nets.ConvNet2DTranspose.__call__(inputs, **normalization_build_kwargs)"},{"location":"sonnet/#args_323","text":"inputs : A 4D Tensor of shape [batch_size, input_height, input_width, input_channels] . **normalization_build_kwargs : kwargs passed to the normalization module at _build time.","title":"Args:"},{"location":"sonnet/#returns_635","text":"A 4D Tensor of shape [batch_size, output_height, output_width, output_channels[-1]] .","title":"Returns:"},{"location":"sonnet/#raises_543","text":"ValueError : If is_training is not explicitly specified when using batch normalization.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dtransposeactivate_final","text":"","title":"nets.ConvNet2DTranspose.activate_final"},{"location":"sonnet/#netsconvnet2dtransposeactivation","text":"","title":"nets.ConvNet2DTranspose.activation"},{"location":"sonnet/#netsconvnet2dtransposebatch_norm_config","text":"","title":"nets.ConvNet2DTranspose.batch_norm_config"},{"location":"sonnet/#netsconvnet2dtransposeconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"nets.ConvNet2DTranspose.connected_subgraphs"},{"location":"sonnet/#netsconvnet2dtransposedefun","text":"Wraps this modules call method in a callable graph function.","title":"nets.ConvNet2DTranspose.defun()"},{"location":"sonnet/#netsconvnet2dtransposedefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"nets.ConvNet2DTranspose.defun_wrapped"},{"location":"sonnet/#netsconvnet2dtransposeget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"nets.ConvNet2DTranspose.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_324","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_636","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_544","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dtransposeget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"nets.ConvNet2DTranspose.get_possible_initializer_keys()"},{"location":"sonnet/#returns_637","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#netsconvnet2dtransposeget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"nets.ConvNet2DTranspose.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_325","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_638","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_545","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dtransposegraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"nets.ConvNet2DTranspose.graph"},{"location":"sonnet/#netsconvnet2dtransposeinitializers","text":"","title":"nets.ConvNet2DTranspose.initializers"},{"location":"sonnet/#netsconvnet2dtransposeinput_shape","text":"Returns shape of input Tensor passed at last call to build .","title":"nets.ConvNet2DTranspose.input_shape"},{"location":"sonnet/#netsconvnet2dtransposeis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"nets.ConvNet2DTranspose.is_connected"},{"location":"sonnet/#netsconvnet2dtransposekernel_shapes","text":"","title":"nets.ConvNet2DTranspose.kernel_shapes"},{"location":"sonnet/#netsconvnet2dtransposelast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"nets.ConvNet2DTranspose.last_connected_subgraph"},{"location":"sonnet/#returns_639","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_546","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dtransposelayers","text":"Returns a tuple containing the convolutional layers of the network.","title":"nets.ConvNet2DTranspose.layers"},{"location":"sonnet/#netsconvnet2dtransposemodule_name","text":"Returns the name of the Module.","title":"nets.ConvNet2DTranspose.module_name"},{"location":"sonnet/#netsconvnet2dtransposename_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"nets.ConvNet2DTranspose.name_scopes"},{"location":"sonnet/#netsconvnet2dtransposenon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.ConvNet2DTranspose.non_trainable_variables"},{"location":"sonnet/#returns_640","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_547","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dtransposenormalization_ctor","text":"","title":"nets.ConvNet2DTranspose.normalization_ctor"},{"location":"sonnet/#netsconvnet2dtransposenormalization_kwargs","text":"","title":"nets.ConvNet2DTranspose.normalization_kwargs"},{"location":"sonnet/#netsconvnet2dtransposenormalize_final","text":"","title":"nets.ConvNet2DTranspose.normalize_final"},{"location":"sonnet/#netsconvnet2dtransposeoutput_channels","text":"","title":"nets.ConvNet2DTranspose.output_channels"},{"location":"sonnet/#netsconvnet2dtransposeoutput_shapes","text":"","title":"nets.ConvNet2DTranspose.output_shapes"},{"location":"sonnet/#netsconvnet2dtransposepaddings","text":"","title":"nets.ConvNet2DTranspose.paddings"},{"location":"sonnet/#netsconvnet2dtransposepartitioners","text":"","title":"nets.ConvNet2DTranspose.partitioners"},{"location":"sonnet/#netsconvnet2dtransposerates","text":"","title":"nets.ConvNet2DTranspose.rates"},{"location":"sonnet/#netsconvnet2dtransposeregularizers","text":"","title":"nets.ConvNet2DTranspose.regularizers"},{"location":"sonnet/#netsconvnet2dtransposescope_name","text":"Returns the full name of the Module's variable scope.","title":"nets.ConvNet2DTranspose.scope_name"},{"location":"sonnet/#netsconvnet2dtransposestrides","text":"","title":"nets.ConvNet2DTranspose.strides"},{"location":"sonnet/#netsconvnet2dtransposetrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.ConvNet2DTranspose.trainable_variables"},{"location":"sonnet/#returns_641","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_548","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dtransposetransposenamenone-output_channelsnone-kernel_shapesnone-stridesnone-paddingsnone-activationnone-activate_finalnone-normalization_ctornone-normalization_kwargsnone-normalize_finalnone-initializersnone-partitionersnone-regularizersnone-use_batch_normnone-use_biasnone-batch_norm_confignone-data_formatnone-custom_getternone","text":"Returns transposed version of this network.","title":"nets.ConvNet2DTranspose.transpose(name=None, output_channels=None, kernel_shapes=None, strides=None, paddings=None, activation=None, activate_final=None, normalization_ctor=None, normalization_kwargs=None, normalize_final=None, initializers=None, partitioners=None, regularizers=None, use_batch_norm=None, use_bias=None, batch_norm_config=None, data_format=None, custom_getter=None)"},{"location":"sonnet/#args_326","text":"name : Optional string specifying the name of the transposed module. The default name is constructed by appending \"_transpose\" to self.module_name . output_channels : Optional iterable of numbers of output channels. kernel_shapes : Optional iterable of kernel sizes. The default value is constructed by reversing self.kernel_shapes . strides : Optional iterable of kernel strides. The default value is constructed by reversing self.strides . paddings : Optional iterable of padding options, either snt.SAME or snt.VALID ; The default value is constructed by reversing self.paddings . activation : Optional activation op. Default value is self.activation . activate_final : Optional boolean determining if the activation and batch normalization, if turned on, are applied to the final layer. normalization_ctor : Constructor to return a callable which will perform normalization at each layer. Defaults to None / no normalization. Examples of what could go here: snt.BatchNormV2 , snt.LayerNorm . If a string is provided, importlib is used to convert the string to a callable, so either snt.LayerNorm or \"snt.LayerNorm\" can be provided. normalization_kwargs : kwargs to be provided to normalization_ctor when it is called. normalize_final : Whether to apply normalization after the final conv layer. Default is to take the value of activate_final. initializers : Optional dict containing ops to initialize the filters of the whole network (with key 'w') or biases (with key 'b'). The default value is self.initializers . partitioners : Optional dict containing partitioners to partition weights (with key 'w') or biases (with key 'b'). The default value is self.partitioners . regularizers : Optional dict containing regularizers for the filters of the whole network (with key 'w') or biases (with key 'b'). The default is self.regularizers . use_batch_norm : Optional boolean determining if batch normalization is applied after convolution. The default value is self.use_batch_norm . use_bias : Optional boolean or iterable of booleans determining whether to include bias parameters in the convolutional layers. Default is constructed by reversing self.use_bias . batch_norm_config : Optional mapping of additional configuration for the snt.BatchNorm modules. Default is self.batch_norm_config . data_format : Optional string, one of \"NCHW\" or \"NHWC\". Specifies whether the channel dimension of the input and output is the last dimension. Default is self._data_format . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API.","title":"Args:"},{"location":"sonnet/#returns_642","text":"Matching ConvNet2D module.","title":"Returns:"},{"location":"sonnet/#raises_549","text":"ValueError : If output_channels is specified and its length does not match the number of layers.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dtransposeuse_batch_norm","text":"","title":"nets.ConvNet2DTranspose.use_batch_norm"},{"location":"sonnet/#netsconvnet2dtransposeuse_bias","text":"","title":"nets.ConvNet2DTranspose.use_bias"},{"location":"sonnet/#netsconvnet2dtransposevariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"nets.ConvNet2DTranspose.variable_scope"},{"location":"sonnet/#returns_643","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_550","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsconvnet2dtransposevariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.ConvNet2DTranspose.variables"},{"location":"sonnet/#returns_644","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_551","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-netsdilation","text":"A convolutional module for per-pixel classification. Consists of 8 convolutional layers, 4 of which are dilated. When applied to the output of a model like VGG-16 (before fully connected layers), can be used to make predictions on a per-pixel basis. Note that the default initializers for the 'basic' model size require that the number of input channels be equal to the number of output classes, and the initializers for the 'large' model require it be a multiple. Based on: 'Multi-Scale Context Aggregation by Dilated Convolutions' Fisher Yu, Vladlen Koltun, ICLR 2016 https://arxiv.org/abs/1511.07122 Properties: conv_modules: list of sonnet modules. The 8 convolution layers used in the Dilation module.","title":"class nets.Dilation"},{"location":"sonnet/#netsdilation__init__num_output_classes-initializersnone-regularizersnone-model_sizebasic-namedilation","text":"Creates a dilation module.","title":"nets.Dilation.__init__(num_output_classes, initializers=None, regularizers=None, model_size='basic', name='dilation')"},{"location":"sonnet/#args_327","text":"num_output_classes : Int. Number of output classes to predict for each pixel in an image. initializers : Optional dict containing ops to initialize filters (with key 'w') or biases (with key 'b'). The default initializer makes this module equivalent to the identity. regularizers : Optional dict containing regularizers for the weights (with key 'w') or biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . model_size : string. One of 'basic' or 'large'. name : string. Name of module.","title":"Args:"},{"location":"sonnet/#netsdilation__call__images","text":"Build dilation module.","title":"nets.Dilation.__call__(images)"},{"location":"sonnet/#args_328","text":"images : Tensor of shape [batch_size, height, width, depth] and dtype float32. Represents a set of images with an arbitrary depth. Note that when using the default initializer, depth must equal num_output_classes.","title":"Args:"},{"location":"sonnet/#returns_645","text":"Tensor of shape [batch_size, height, width, num_output_classes] and dtype float32. Represents, for each image and pixel, logits for per-class predictions.","title":"Returns:"},{"location":"sonnet/#raises_552","text":"IncompatibleShapeError : If images is not rank 4. ValueError : If model_size is not one of 'basic' or 'large'.","title":"Raises:"},{"location":"sonnet/#netsdilationconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"nets.Dilation.connected_subgraphs"},{"location":"sonnet/#netsdilationconv_modules","text":"","title":"nets.Dilation.conv_modules"},{"location":"sonnet/#netsdilationdefun","text":"Wraps this modules call method in a callable graph function.","title":"nets.Dilation.defun()"},{"location":"sonnet/#netsdilationdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"nets.Dilation.defun_wrapped"},{"location":"sonnet/#netsdilationget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"nets.Dilation.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_329","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_646","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_553","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsdilationget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"nets.Dilation.get_possible_initializer_keys()"},{"location":"sonnet/#returns_647","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#netsdilationget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"nets.Dilation.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_330","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_648","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_554","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsdilationgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"nets.Dilation.graph"},{"location":"sonnet/#netsdilationis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"nets.Dilation.is_connected"},{"location":"sonnet/#netsdilationlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"nets.Dilation.last_connected_subgraph"},{"location":"sonnet/#returns_649","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_555","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsdilationmodule_name","text":"Returns the name of the Module.","title":"nets.Dilation.module_name"},{"location":"sonnet/#netsdilationname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"nets.Dilation.name_scopes"},{"location":"sonnet/#netsdilationnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.Dilation.non_trainable_variables"},{"location":"sonnet/#returns_650","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_556","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsdilationscope_name","text":"Returns the full name of the Module's variable scope.","title":"nets.Dilation.scope_name"},{"location":"sonnet/#netsdilationtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.Dilation.trainable_variables"},{"location":"sonnet/#returns_651","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_557","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsdilationvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"nets.Dilation.variable_scope"},{"location":"sonnet/#returns_652","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_558","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsdilationvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.Dilation.variables"},{"location":"sonnet/#returns_653","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_559","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-netsmlp","text":"A Multi-Layer perceptron module.","title":"class nets.MLP"},{"location":"sonnet/#netsmlp__init__output_sizes-activationltfunction-relu-at-0x7fcc58553c80gt-activate_finalfalse-initializersnone-partitionersnone-regularizersnone-use_biastrue-use_dropoutfalse-custom_getternone-namemlp","text":"Constructs an MLP module.","title":"nets.MLP.__init__(output_sizes, activation=&lt;function relu at 0x7fcc58553c80&gt;, activate_final=False, initializers=None, partitioners=None, regularizers=None, use_bias=True, use_dropout=False, custom_getter=None, name='mlp')"},{"location":"sonnet/#args_331","text":"output_sizes : An iterable of output dimensionalities as defined in basic.Linear . Output size can be defined either as number or via a callable. In the latter case, since the function invocation is deferred to graph construction time, the user must only ensure that entries can be called when build is called. Each entry in the iterable defines properties in the corresponding linear layer. activation : An activation op. The activation is applied to intermediate layers, and optionally to the output of the final layer. activate_final : Boolean determining if the activation is applied to the output of the final layer. Default False . initializers : Optional dict containing ops to initialize the linear layers' weights (with key 'w') or biases (with key 'b'). partitioners : Optional dict containing partitioners to partition the linear layers' weights (with key 'w') or biases (with key 'b'). regularizers : Optional dict containing regularizers for the linear layers' weights (with key 'w') and the biases (with key 'b'). As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . use_bias : Whether to include bias parameters in the linear layers. Default True . use_dropout : Whether to perform dropout on the linear layers. Default False . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_560","text":"KeyError : If initializers contains any keys other than 'w' or 'b'. KeyError : If regularizers contains any keys other than 'w' or 'b'. ValueError : If output_sizes is empty. TypeError : If activation is not callable; or if output_sizes is not iterable.","title":"Raises:"},{"location":"sonnet/#netsmlp__call__inputs-is_trainingtrue-dropout_keep_prob05","text":"Assembles the MLP and connects it to the graph.","title":"nets.MLP.__call__(inputs, is_training=True, dropout_keep_prob=0.5)"},{"location":"sonnet/#args_332","text":"inputs : A 2D Tensor of size [batch_size, input_size] . is_training : A bool or tf.Bool Tensor. Indicates whether we are currently training. Defaults to True . dropout_keep_prob : The probability that each element is kept when both use_dropout and is_training are True. Defaults to 0.5.","title":"Args:"},{"location":"sonnet/#returns_654","text":"A 2D Tensor of size [batch_size, output_sizes[-1]] .","title":"Returns:"},{"location":"sonnet/#netsmlpactivate_final","text":"","title":"nets.MLP.activate_final"},{"location":"sonnet/#netsmlpactivation","text":"","title":"nets.MLP.activation"},{"location":"sonnet/#netsmlpclonenamenone","text":"Creates a new MLP with the same structure.","title":"nets.MLP.clone(name=None)"},{"location":"sonnet/#args_333","text":"name : Optional string specifying the name of the new module. The default name is constructed by appending \"_clone\" to the original name.","title":"Args:"},{"location":"sonnet/#returns_655","text":"A cloned MLP module.","title":"Returns:"},{"location":"sonnet/#netsmlpconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"nets.MLP.connected_subgraphs"},{"location":"sonnet/#netsmlpdefun","text":"Wraps this modules call method in a callable graph function.","title":"nets.MLP.defun()"},{"location":"sonnet/#netsmlpdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"nets.MLP.defun_wrapped"},{"location":"sonnet/#netsmlpget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"nets.MLP.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_334","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_656","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_561","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsmlpget_possible_initializer_keysuse_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"nets.MLP.get_possible_initializer_keys(use_bias=True)"},{"location":"sonnet/#returns_657","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#netsmlpget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"nets.MLP.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_335","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_658","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_562","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsmlpgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"nets.MLP.graph"},{"location":"sonnet/#netsmlpinitializers","text":"Returns the intializers dictionary.","title":"nets.MLP.initializers"},{"location":"sonnet/#netsmlpinput_shape","text":"Returns shape of input Tensor passed at last call to build .","title":"nets.MLP.input_shape"},{"location":"sonnet/#netsmlpis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"nets.MLP.is_connected"},{"location":"sonnet/#netsmlplast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"nets.MLP.last_connected_subgraph"},{"location":"sonnet/#returns_659","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_563","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsmlplayers","text":"Returns a tuple containing the linear layers of the MLP .","title":"nets.MLP.layers"},{"location":"sonnet/#netsmlpmodule_name","text":"Returns the name of the Module.","title":"nets.MLP.module_name"},{"location":"sonnet/#netsmlpname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"nets.MLP.name_scopes"},{"location":"sonnet/#netsmlpnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.MLP.non_trainable_variables"},{"location":"sonnet/#returns_660","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_564","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsmlpoutput_size","text":"Returns the size of the module output, not including the batch dimension. This allows the MLP to be used inside a DeepRNN.","title":"nets.MLP.output_size"},{"location":"sonnet/#returns_661","text":"The scalar size of the module output.","title":"Returns:"},{"location":"sonnet/#netsmlpoutput_sizes","text":"Returns a tuple of all output sizes of all the layers.","title":"nets.MLP.output_sizes"},{"location":"sonnet/#netsmlppartitioners","text":"Returns the partitioners dictionary.","title":"nets.MLP.partitioners"},{"location":"sonnet/#netsmlpregularizers","text":"Returns the regularizers dictionary.","title":"nets.MLP.regularizers"},{"location":"sonnet/#netsmlpscope_name","text":"Returns the full name of the Module's variable scope.","title":"nets.MLP.scope_name"},{"location":"sonnet/#netsmlptrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.MLP.trainable_variables"},{"location":"sonnet/#returns_662","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_565","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsmlptransposenamenone-activate_finalnone","text":"Returns transposed MLP .","title":"nets.MLP.transpose(name=None, activate_final=None)"},{"location":"sonnet/#args_336","text":"name : Optional string specifying the name of the transposed module. The default name is constructed by appending \"_transpose\" to self.module_name . activate_final : Optional boolean determining if the activation and batch normalization, if turned on, are applied to the final layer.","title":"Args:"},{"location":"sonnet/#returns_663","text":"Matching transposed MLP module.","title":"Returns:"},{"location":"sonnet/#netsmlpuse_bias","text":"","title":"nets.MLP.use_bias"},{"location":"sonnet/#netsmlpuse_dropout","text":"","title":"nets.MLP.use_dropout"},{"location":"sonnet/#netsmlpvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"nets.MLP.variable_scope"},{"location":"sonnet/#returns_664","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_566","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsmlpvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.MLP.variables"},{"location":"sonnet/#returns_665","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_567","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-netsvectorquantizer","text":"Sonnet module representing the VQ-VAE layer. Implements the algorithm presented in 'Neural Discrete Representation Learning' by van den Oord et al. https://arxiv.org/abs/1711.00937 Input any tensor to be quantized. Last dimension will be used as space in which to quantize. All other dimensions will be flattened and will be seen as different examples to quantize. The output tensor will have the same shape as the input. For example a tensor with shape [16, 32, 32, 64] will be reshaped into [16384, 64] and all 16384 vectors (each of 64 dimensions) will be quantized independently. Args: embedding_dim: integer representing the dimensionality of the tensors in the quantized space. Inputs to the modules must be in this format as well. num_embeddings: integer, the number of vectors in the quantized space. commitment_cost: scalar which controls the weighting of the loss terms (see equation 4 in the paper - this variable is Beta).","title":"class nets.VectorQuantizer"},{"location":"sonnet/#netsvectorquantizer__init__embedding_dim-num_embeddings-commitment_cost-namevq_layer","text":"Performs the initialisation necessary for all AbstractModule instances. Every subclass of AbstractModule must begin their constructor with a call to this constructor, i.e. super(MySubModule, self).__init__(custom_getter=custom_getter, name=name) . If you instantiate sub-modules in init you must create them within the _enter_variable_scope context manager to ensure they are in the module's variable scope. Alternatively, instantiate sub-modules in _build .","title":"nets.VectorQuantizer.__init__(embedding_dim, num_embeddings, commitment_cost, name='vq_layer')"},{"location":"sonnet/#args_337","text":"_sentinel: Variable that only carries a non-None value if __init__ was called without named parameters. If this is the case, a deprecation warning is issued in form of a ValueError . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of this module. Used to construct the Templated build function. If None the module's class name is used (converted to snake case).","title":"Args:"},{"location":"sonnet/#raises_568","text":"TypeError : If name is not a string. TypeError : If a given custom_getter is not callable. ValueError : If __init__ was called without named arguments.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizer__call__inputs-is_training","text":"Connects the module to some inputs.","title":"nets.VectorQuantizer.__call__(inputs, is_training)"},{"location":"sonnet/#args_338","text":"inputs : Tensor, final dimension must be equal to embedding_dim. All other leading dimensions will be flattened and treated as a large batch. is_training : boolean, whether this connection is to training data.","title":"Args:"},{"location":"sonnet/#returns_666","text":"dict containing the following keys and values: quantize : Tensor containing the quantized version of the input. loss : Tensor containing the loss to optimize. perplexity : Tensor containing the perplexity of the encodings. encodings : Tensor containing the discrete encodings, ie which element of the quantized space each input element was mapped to. encoding_indices : Tensor containing the discrete encoding indices, ie which element of the quantized space each input element was mapped to.","title":"Returns:"},{"location":"sonnet/#netsvectorquantizerconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"nets.VectorQuantizer.connected_subgraphs"},{"location":"sonnet/#netsvectorquantizerdefun","text":"Wraps this modules call method in a callable graph function.","title":"nets.VectorQuantizer.defun()"},{"location":"sonnet/#netsvectorquantizerdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"nets.VectorQuantizer.defun_wrapped"},{"location":"sonnet/#netsvectorquantizerembeddings","text":"","title":"nets.VectorQuantizer.embeddings"},{"location":"sonnet/#netsvectorquantizerget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"nets.VectorQuantizer.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_339","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_667","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_569","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizerget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"nets.VectorQuantizer.get_possible_initializer_keys()"},{"location":"sonnet/#returns_668","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#netsvectorquantizerget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"nets.VectorQuantizer.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_340","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_669","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_570","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizergraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"nets.VectorQuantizer.graph"},{"location":"sonnet/#netsvectorquantizeris_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"nets.VectorQuantizer.is_connected"},{"location":"sonnet/#netsvectorquantizerlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"nets.VectorQuantizer.last_connected_subgraph"},{"location":"sonnet/#returns_670","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_571","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizermodule_name","text":"Returns the name of the Module.","title":"nets.VectorQuantizer.module_name"},{"location":"sonnet/#netsvectorquantizername_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"nets.VectorQuantizer.name_scopes"},{"location":"sonnet/#netsvectorquantizernon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.VectorQuantizer.non_trainable_variables"},{"location":"sonnet/#returns_671","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_572","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizerquantizeencoding_indices","text":"","title":"nets.VectorQuantizer.quantize(encoding_indices)"},{"location":"sonnet/#netsvectorquantizerscope_name","text":"Returns the full name of the Module's variable scope.","title":"nets.VectorQuantizer.scope_name"},{"location":"sonnet/#netsvectorquantizertrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.VectorQuantizer.trainable_variables"},{"location":"sonnet/#returns_672","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_573","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizervariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"nets.VectorQuantizer.variable_scope"},{"location":"sonnet/#returns_673","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_574","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizervariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.VectorQuantizer.variables"},{"location":"sonnet/#returns_674","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_575","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#class-netsvectorquantizerema","text":"Sonnet module representing the VQ-VAE layer. Implements a slightly modified version of the algorithm presented in 'Neural Discrete Representation Learning' by van den Oord et al. https://arxiv.org/abs/1711.00937 The difference between VectorQuantizerEMA and VectorQuantizer is that this module uses exponential moving averages to update the embedding vectors instead of an auxiliary loss. This has the advantage that the embedding updates are independent of the choice of optimizer (SGD, RMSProp, Adam, K-Fac, ...) used for the encoder, decoder and other parts of the architecture. For most experiments the EMA version trains faster than the non-EMA version. Input any tensor to be quantized. Last dimension will be used as space in which to quantize. All other dimensions will be flattened and will be seen as different examples to quantize. The output tensor will have the same shape as the input. For example a tensor with shape [16, 32, 32, 64] will be reshaped into [16384, 64] and all 16384 vectors (each of 64 dimensions) will be quantized independently. Args: embedding_dim: integer representing the dimensionality of the tensors in the quantized space. Inputs to the modules must be in this format as well. num_embeddings: integer, the number of vectors in the quantized space. commitment_cost: scalar which controls the weighting of the loss terms (see equation 4 in the paper). decay: float, decay for the moving averages. epsilon: small float constant to avoid numerical instability.","title":"class nets.VectorQuantizerEMA"},{"location":"sonnet/#netsvectorquantizerema__init__embedding_dim-num_embeddings-commitment_cost-decay-epsilon1e-05-namevectorquantizerema","text":"Performs the initialisation necessary for all AbstractModule instances. Every subclass of AbstractModule must begin their constructor with a call to this constructor, i.e. super(MySubModule, self).__init__(custom_getter=custom_getter, name=name) . If you instantiate sub-modules in init you must create them within the _enter_variable_scope context manager to ensure they are in the module's variable scope. Alternatively, instantiate sub-modules in _build .","title":"nets.VectorQuantizerEMA.__init__(embedding_dim, num_embeddings, commitment_cost, decay, epsilon=1e-05, name='VectorQuantizerEMA')"},{"location":"sonnet/#args_341","text":"_sentinel: Variable that only carries a non-None value if __init__ was called without named parameters. If this is the case, a deprecation warning is issued in form of a ValueError . custom_getter : Callable or dictionary of callables to use as custom getters inside the module. If a dictionary, the keys correspond to regexes to match variable names. See the tf.get_variable documentation for information about the custom_getter API. name : Name of this module. Used to construct the Templated build function. If None the module's class name is used (converted to snake case).","title":"Args:"},{"location":"sonnet/#raises_576","text":"TypeError : If name is not a string. TypeError : If a given custom_getter is not callable. ValueError : If __init__ was called without named arguments.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizerema__call__inputs-is_training","text":"Connects the module to some inputs.","title":"nets.VectorQuantizerEMA.__call__(inputs, is_training)"},{"location":"sonnet/#args_342","text":"inputs : Tensor, final dimension must be equal to embedding_dim. All other leading dimensions will be flattened and treated as a large batch. is_training : boolean, whether this connection is to training data. When this is set to False, the internal moving average statistics will not be updated.","title":"Args:"},{"location":"sonnet/#returns_675","text":"dict containing the following keys and values: quantize : Tensor containing the quantized version of the input. loss : Tensor containing the loss to optimize. perplexity : Tensor containing the perplexity of the encodings. encodings : Tensor containing the discrete encodings, ie which element of the quantized space each input element was mapped to. encoding_indices : Tensor containing the discrete encoding indices, ie which element of the quantized space each input element was mapped to.","title":"Returns:"},{"location":"sonnet/#netsvectorquantizeremaconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"nets.VectorQuantizerEMA.connected_subgraphs"},{"location":"sonnet/#netsvectorquantizeremadefun","text":"Wraps this modules call method in a callable graph function.","title":"nets.VectorQuantizerEMA.defun()"},{"location":"sonnet/#netsvectorquantizeremadefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"nets.VectorQuantizerEMA.defun_wrapped"},{"location":"sonnet/#netsvectorquantizeremaembeddings","text":"","title":"nets.VectorQuantizerEMA.embeddings"},{"location":"sonnet/#netsvectorquantizeremaget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"nets.VectorQuantizerEMA.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_343","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_676","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_577","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizeremaget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"nets.VectorQuantizerEMA.get_possible_initializer_keys()"},{"location":"sonnet/#returns_677","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#netsvectorquantizeremaget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"nets.VectorQuantizerEMA.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_344","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_678","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_578","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizeremagraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"nets.VectorQuantizerEMA.graph"},{"location":"sonnet/#netsvectorquantizeremais_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"nets.VectorQuantizerEMA.is_connected"},{"location":"sonnet/#netsvectorquantizeremalast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"nets.VectorQuantizerEMA.last_connected_subgraph"},{"location":"sonnet/#returns_679","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_579","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizeremamodule_name","text":"Returns the name of the Module.","title":"nets.VectorQuantizerEMA.module_name"},{"location":"sonnet/#netsvectorquantizeremaname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"nets.VectorQuantizerEMA.name_scopes"},{"location":"sonnet/#netsvectorquantizeremanon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.VectorQuantizerEMA.non_trainable_variables"},{"location":"sonnet/#returns_680","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_580","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizeremaquantizeencoding_indices","text":"","title":"nets.VectorQuantizerEMA.quantize(encoding_indices)"},{"location":"sonnet/#netsvectorquantizeremascope_name","text":"Returns the full name of the Module's variable scope.","title":"nets.VectorQuantizerEMA.scope_name"},{"location":"sonnet/#netsvectorquantizerematrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.VectorQuantizerEMA.trainable_variables"},{"location":"sonnet/#returns_681","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_581","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizeremavariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"nets.VectorQuantizerEMA.variable_scope"},{"location":"sonnet/#returns_682","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_582","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsvectorquantizeremavariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"nets.VectorQuantizerEMA.variables"},{"location":"sonnet/#returns_683","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_583","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#netsidentity_kernel_initializershape-dtypetffloat32-partition_infonone","text":"An initializer for constructing identity convolution kernels. Constructs a convolution kernel such that applying it is the same as an identity operation on the input. Formally, the kernel has entry [i, j, in, out] = 1 if in equals out and i and j are the middle of the kernel and 0 otherwise.","title":"nets.identity_kernel_initializer(shape, dtype=tf.float32, partition_info=None)"},{"location":"sonnet/#args_345","text":"shape : List of integers. Represents shape of result. dtype : data type for values in result. partition_info : Partition information for initializer functions. Ignored.","title":"Args:"},{"location":"sonnet/#returns_684","text":"Tensor of desired shape and dtype such that applying it as a convolution kernel results in the identity operation.","title":"Returns:"},{"location":"sonnet/#raises_584","text":"ValueError : If shape does not define a valid kernel. If filter width and height differ. If filter width and height are not odd numbers. If number of input and output channels differ.","title":"Raises:"},{"location":"sonnet/#netsnoisy_identity_kernel_initializerbase_num_channels-stddev1e-08","text":"Build an initializer for constructing near-identity convolution kernels. Construct a convolution kernel where in_channels and out_channels are multiples of base_num_channels, but need not be equal. This initializer is essentially the same as identity_kernel_initializer, except that magnitude is \"spread out\" across multiple copies of the input.","title":"nets.noisy_identity_kernel_initializer(base_num_channels, stddev=1e-08)"},{"location":"sonnet/#args_346","text":"base_num_channels : int. Number that divides both in_channels and out_channels. stddev : float. Standard deviation of truncated normal noise added to off-entries to break ties.","title":"Args:"},{"location":"sonnet/#returns_685","text":"Initializer function for building a noisy identity kernel.","title":"Returns:"},{"location":"sonnet/#class-protosmodule_pb2nesteddata","text":"A ProtocolMessage","title":"class protos.module_pb2.NestedData"},{"location":"sonnet/#class-protosmodule_pb2sonnetmodule","text":"A ProtocolMessage","title":"class protos.module_pb2.SonnetModule"},{"location":"sonnet/#protosmodule_pb2_bx","text":"","title":"protos.module_pb2._b(x)"},{"location":"sonnet/#class-pythonmodulesattentionattentionoutput","text":"AttentionOutput(read, weights, weight_logits)","title":"class python.modules.attention.AttentionOutput"},{"location":"sonnet/#pythonmodulesattentionattentionoutputread","text":"Alias for field number 0","title":"python.modules.attention.AttentionOutput.read"},{"location":"sonnet/#pythonmodulesattentionattentionoutputweight_logits","text":"Alias for field number 2","title":"python.modules.attention.AttentionOutput.weight_logits"},{"location":"sonnet/#pythonmodulesattentionattentionoutputweights","text":"Alias for field number 1","title":"python.modules.attention.AttentionOutput.weights"},{"location":"sonnet/#pythonmodulesbaseget_connection_stack","text":"","title":"python.modules.base.get_connection_stack()"},{"location":"sonnet/#pythonmodulesbaseget_module_stack","text":"","title":"python.modules.base.get_module_stack()"},{"location":"sonnet/#class-pythonmodulesbase_infoconnectedsubgraph","text":"ConnectedSubGraph(module, name_scope, inputs, outputs)","title":"class python.modules.base_info.ConnectedSubGraph"},{"location":"sonnet/#pythonmodulesbase_infoconnectedsubgraphinputs","text":"Alias for field number 2","title":"python.modules.base_info.ConnectedSubGraph.inputs"},{"location":"sonnet/#pythonmodulesbase_infoconnectedsubgraphmodule","text":"Alias for field number 0","title":"python.modules.base_info.ConnectedSubGraph.module"},{"location":"sonnet/#pythonmodulesbase_infoconnectedsubgraphname_scope","text":"Alias for field number 1","title":"python.modules.base_info.ConnectedSubGraph.name_scope"},{"location":"sonnet/#pythonmodulesbase_infoconnectedsubgraphoutputs","text":"Alias for field number 3","title":"python.modules.base_info.ConnectedSubGraph.outputs"},{"location":"sonnet/#class-pythonmodulesbase_infomoduleinfo","text":"ModuleInfo(module_name, scope_name, class_name, connected_subgraphs)","title":"class python.modules.base_info.ModuleInfo"},{"location":"sonnet/#pythonmodulesbase_infomoduleinfoclass_name","text":"Alias for field number 2","title":"python.modules.base_info.ModuleInfo.class_name"},{"location":"sonnet/#pythonmodulesbase_infomoduleinfoconnected_subgraphs","text":"Alias for field number 3","title":"python.modules.base_info.ModuleInfo.connected_subgraphs"},{"location":"sonnet/#pythonmodulesbase_infomoduleinfomodule_name","text":"Alias for field number 0","title":"python.modules.base_info.ModuleInfo.module_name"},{"location":"sonnet/#pythonmodulesbase_infomoduleinfoscope_name","text":"Alias for field number 1","title":"python.modules.base_info.ModuleInfo.scope_name"},{"location":"sonnet/#pythonmodulesbasiccalculate_bias_shapeinput_shape-bias_dims","text":"Calculate bias_shape based on the input_shape and bias_dims .","title":"python.modules.basic.calculate_bias_shape(input_shape, bias_dims)"},{"location":"sonnet/#args_347","text":"input_shape : Shape of the input being passed into the module. The leading dimension is the minibatch size. bias_dims : The dimensions that bias should be applied over. The remaining dimensions will get broadcasted over.","title":"Args:"},{"location":"sonnet/#returns_686","text":"bias_shape : Tuple corresponding to the shape of bias Variable to create.","title":"Returns:"},{"location":"sonnet/#raises_585","text":"ValueError : If the user attempts to add bias over the minibatch dimension, e.g. bias_dims=[0] .","title":"Raises:"},{"location":"sonnet/#pythonmodulesbasiccreate_bias_initializerunused_bias_shape-dtypetffloat32","text":"Returns a default initializer for the biases of a linear/AddBias module.","title":"python.modules.basic.create_bias_initializer(unused_bias_shape, dtype=tf.float32)"},{"location":"sonnet/#pythonmodulesbasiccreate_linear_initializerinput_size-dtypetffloat32","text":"Returns a default initializer for weights of a linear module.","title":"python.modules.basic.create_linear_initializer(input_size, dtype=tf.float32)"},{"location":"sonnet/#pythonmodulesbatch_normcreate_beta_initializer","text":"Returns a default initializer for the beta in batch norm.","title":"python.modules.batch_norm.create_beta_initializer()"},{"location":"sonnet/#pythonmodulesbatch_normcreate_gamma_initializer","text":"Returns a default initializer for the gamma in batch norm.","title":"python.modules.batch_norm.create_gamma_initializer()"},{"location":"sonnet/#pythonmodulesbatch_normcreate_mean_initializer","text":"Returns a default initializer for the moving_mean in batch norm.","title":"python.modules.batch_norm.create_mean_initializer()"},{"location":"sonnet/#pythonmodulesbatch_normcreate_variance_initializer","text":"Returns a default initializer for the moving_variance in batch norm.","title":"python.modules.batch_norm.create_variance_initializer()"},{"location":"sonnet/#pythonmodulesbatch_norm_v2create_beta_initializer","text":"Returns a default initializer for the beta in batch norm.","title":"python.modules.batch_norm_v2.create_beta_initializer()"},{"location":"sonnet/#pythonmodulesbatch_norm_v2create_gamma_initializer","text":"Returns a default initializer for the gamma in batch norm.","title":"python.modules.batch_norm_v2.create_gamma_initializer()"},{"location":"sonnet/#pythonmodulesbatch_norm_v2create_mean_initializer","text":"Returns a default initializer for the moving_mean in batch norm.","title":"python.modules.batch_norm_v2.create_mean_initializer()"},{"location":"sonnet/#pythonmodulesbatch_norm_v2create_variance_initializer","text":"Returns a default initializer for the moving_variance in batch norm.","title":"python.modules.batch_norm_v2.create_variance_initializer()"},{"location":"sonnet/#pythonmodulesconvcreate_bias_initializerunused_bias_shape-dtypetffloat32","text":"Returns a default initializer for the biases of a convolutional module.","title":"python.modules.conv.create_bias_initializer(unused_bias_shape, dtype=tf.float32)"},{"location":"sonnet/#pythonmodulesconvcreate_weight_initializerfan_in_shape-dtypetffloat32","text":"Returns a default initializer for the weights of a convolutional module.","title":"python.modules.conv.create_weight_initializer(fan_in_shape, dtype=tf.float32)"},{"location":"sonnet/#class-pythonmodulesgated_rnnconvlstm","text":"Convolutional LSTM.","title":"class python.modules.gated_rnn.ConvLSTM"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstm__init__conv_ndims-input_shape-output_channels-kernel_shape-stride1-rate1-paddingsame-use_biastrue-legacy_bias_behaviourtrue-forget_bias10-initializersnone-partitionersnone-regularizersnone-use_layer_normfalse-custom_getternone-nameconv_lstm","text":"Construct ConvLSTM.","title":"python.modules.gated_rnn.ConvLSTM.__init__(conv_ndims, input_shape, output_channels, kernel_shape, stride=1, rate=1, padding='SAME', use_bias=True, legacy_bias_behaviour=True, forget_bias=1.0, initializers=None, partitioners=None, regularizers=None, use_layer_norm=False, custom_getter=None, name='conv_lstm')"},{"location":"sonnet/#args_348","text":"conv_ndims : Convolution dimensionality (1, 2 or 3). input_shape : Shape of the input as an iterable, excluding the batch size. output_channels : Number of output channels of the conv LSTM. kernel_shape : Sequence of kernel sizes (of size conv_ndims), or integer that is used to define kernel size in all dimensions. stride : Sequence of kernel strides (of size conv_ndims), or integer that is used to define stride in all dimensions. rate : Sequence of dilation rates (of size conv_ndims), or integer that is used to define dilation rate in all dimensions. 1 corresponds to a standard convolution, while rate 1 corresponds to a dilated convolution. Cannot be 1 if any of stride is also 1. padding : Padding algorithm, either snt.SAME or snt.VALID . use_bias : Use bias in convolutions. legacy_bias_behaviour : If True, bias is applied to both input and hidden convolutions, creating a redundant bias variable. If False, bias is only applied to input convolution, removing the redundancy. forget_bias : Forget bias. initializers : Dict containing ops to initialize the convolutional weights. partitioners : Optional dict containing partitioners to partition the convolutional weights and biases. As a default, no partitioners are used. regularizers : Optional dict containing regularizers for the convolutional weights and biases. As a default, no regularizers are used. use_layer_norm : Boolean that indicates whether to apply layer normalization. This is applied across the entire layer, normalizing over all non-batch dimensions. custom_getter : Callable that takes as a first argument the true getter, and allows overwriting the internal get_variable method. See the tf.get_variable documentation for more details. name : Name of the module.","title":"Args:"},{"location":"sonnet/#raises_586","text":"ValueError : If skip_connection is True and stride is different from 1 or if input_shape is incompatible with conv_ndims .","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstm__call__inputs-state","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"python.modules.gated_rnn.ConvLSTM.__call__(inputs, state)"},{"location":"sonnet/#args_349","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_687","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"python.modules.gated_rnn.ConvLSTM.connected_subgraphs"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmconvolutions","text":"","title":"python.modules.gated_rnn.ConvLSTM.convolutions"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmdefun","text":"Wraps this modules call method in a callable graph function.","title":"python.modules.gated_rnn.ConvLSTM.defun()"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"python.modules.gated_rnn.ConvLSTM.defun_wrapped"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"python.modules.gated_rnn.ConvLSTM.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_350","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_688","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_587","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmget_possible_initializer_keysconv_ndims-use_biastrue","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"python.modules.gated_rnn.ConvLSTM.get_possible_initializer_keys(conv_ndims, use_bias=True)"},{"location":"sonnet/#returns_689","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"python.modules.gated_rnn.ConvLSTM.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_351","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_690","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_588","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmgraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"python.modules.gated_rnn.ConvLSTM.graph"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstminitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone-unused_kwargs","text":"Builds the default start state for an RNNCore.","title":"python.modules.gated_rnn.ConvLSTM.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None, **unused_kwargs)"},{"location":"sonnet/#args_352","text":"batch_size : An int, or scalar int32 Tensor representing the batch size. dtype : The data type to use for the state. trainable : Boolean that indicates whether to learn the initial state. Note that intializers and regularizers will be ignored if trainable=False . trainable_initializers : An initializer function or nested structure of functions with same structure as the state_size property of the core, to be used as initializers of the initial state variable. trainable_regularizers : Optional regularizer function or nested structure of functions with the same structure as the state_size property of the core, to be used as regularizers of the initial state variable. As a default, no regularizers are used. A regularizer should be a function that takes a single Tensor as an input and returns a scalar Tensor output, e.g. the L1 and L2 regularizers in tf.contrib.layers . name : Optional string used to prefix the initial state variable names, in the case of a trainable initial state. If not provided, defaults to the name of the module.","title":"Args:"},{"location":"sonnet/#returns_691","text":"A tensor or nested tuple of tensors with same structure and shape as the state_size property of the core.","title":"Returns:"},{"location":"sonnet/#raises_589","text":"ValueError : if the user passes initializers that are not functions. ValueError : if the user passes regularizers that are not functions.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"python.modules.gated_rnn.ConvLSTM.is_connected"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"python.modules.gated_rnn.ConvLSTM.last_connected_subgraph"},{"location":"sonnet/#returns_692","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_590","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmmodule_name","text":"Returns the name of the Module.","title":"python.modules.gated_rnn.ConvLSTM.module_name"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmname_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"python.modules.gated_rnn.ConvLSTM.name_scopes"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmnon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"python.modules.gated_rnn.ConvLSTM.non_trainable_variables"},{"location":"sonnet/#returns_693","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_591","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmoutput_size","text":"tf.TensorShape indicating the size of the core output.","title":"python.modules.gated_rnn.ConvLSTM.output_size"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmscope_name","text":"Returns the full name of the Module's variable scope.","title":"python.modules.gated_rnn.ConvLSTM.scope_name"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmstate_size","text":"Tuple of tf.TensorShape s indicating the size of state tensors.","title":"python.modules.gated_rnn.ConvLSTM.state_size"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmtrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"python.modules.gated_rnn.ConvLSTM.trainable_variables"},{"location":"sonnet/#returns_694","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_592","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmuse_layer_norm","text":"Boolean indicating whether layer norm is enabled.","title":"python.modules.gated_rnn.ConvLSTM.use_layer_norm"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmvariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"python.modules.gated_rnn.ConvLSTM.variable_scope"},{"location":"sonnet/#returns_695","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_593","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmvariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"python.modules.gated_rnn.ConvLSTM.variables"},{"location":"sonnet/#returns_696","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_594","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnconvlstmzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"python.modules.gated_rnn.ConvLSTM.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_353","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_697","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-pythonmodulesgated_rnnrecurrentdropoutwrapper","text":"Wraps an RNNCore so that recurrent dropout can be applied.","title":"class python.modules.gated_rnn.RecurrentDropoutWrapper"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapper__init__core-keep_probs","text":"Builds a new wrapper around a given core.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.__init__(core, keep_probs)"},{"location":"sonnet/#args_354","text":"core : the RNN core to be wrapped. keep_probs : the recurrent dropout keep probabilities to apply. This should have the same structure has core.init_state. No dropout is applied for leafs set to None.","title":"Args:"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapper__call__inputs-prev_state","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.__call__(inputs, prev_state)"},{"location":"sonnet/#args_355","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_698","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.connected_subgraphs"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperdefun","text":"Wraps this modules call method in a callable graph function.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.defun()"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.defun_wrapped"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_356","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_699","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_595","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.get_possible_initializer_keys()"},{"location":"sonnet/#returns_700","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_357","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_701","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_596","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrappergraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.graph"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone","text":"Builds the default start state tensor of zeros.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None)"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.is_connected"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.last_connected_subgraph"},{"location":"sonnet/#returns_702","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_597","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrappermodule_name","text":"Returns the name of the Module.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.module_name"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrappername_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.name_scopes"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrappernon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.non_trainable_variables"},{"location":"sonnet/#returns_703","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_598","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.output_size"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperscope_name","text":"Returns the full name of the Module's variable scope.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.scope_name"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperstate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.state_size"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrappertrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.trainable_variables"},{"location":"sonnet/#returns_704","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_599","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrappervariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.variable_scope"},{"location":"sonnet/#returns_705","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_600","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrappervariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.variables"},{"location":"sonnet/#returns_706","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_601","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnrecurrentdropoutwrapperzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"python.modules.gated_rnn.RecurrentDropoutWrapper.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_358","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_707","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#class-pythonmodulesgated_rnnzoneoutwrapper","text":"Wraps an RNNCore so that zoneout can be applied. Zoneout was introduced in https://arxiv.org/abs/1606.01305 It consists of randomly freezing some RNN state in the same way recurrent dropout would replace this state with zero.","title":"class python.modules.gated_rnn.ZoneoutWrapper"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapper__init__core-keep_probs-is_training","text":"Builds a new wrapper around a given core.","title":"python.modules.gated_rnn.ZoneoutWrapper.__init__(core, keep_probs, is_training)"},{"location":"sonnet/#args_359","text":"core : the RNN core to be wrapped. keep_probs : the probabilities to use the updated states rather than keeping the old state values. This is one minus the probability that zoneout gets applied. This should have the same structure has core.init_state. No zoneout is applied for leafs set to None. is_training : when set, apply some stochastic zoneout. Otherwise perform a linear combination of the previous state and the current state based on the zoneout probability.","title":"Args:"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapper__call__inputs-prev_state","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"python.modules.gated_rnn.ZoneoutWrapper.__call__(inputs, prev_state)"},{"location":"sonnet/#args_360","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_708","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"python.modules.gated_rnn.ZoneoutWrapper.connected_subgraphs"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperdefun","text":"Wraps this modules call method in a callable graph function.","title":"python.modules.gated_rnn.ZoneoutWrapper.defun()"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"python.modules.gated_rnn.ZoneoutWrapper.defun_wrapped"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"python.modules.gated_rnn.ZoneoutWrapper.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_361","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_709","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_602","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"python.modules.gated_rnn.ZoneoutWrapper.get_possible_initializer_keys()"},{"location":"sonnet/#returns_710","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"python.modules.gated_rnn.ZoneoutWrapper.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_362","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_711","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_603","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrappergraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"python.modules.gated_rnn.ZoneoutWrapper.graph"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperinitial_statebatch_size-dtypetffloat32-trainablefalse-trainable_initializersnone-trainable_regularizersnone-namenone","text":"Builds the default start state tensor of zeros.","title":"python.modules.gated_rnn.ZoneoutWrapper.initial_state(batch_size, dtype=tf.float32, trainable=False, trainable_initializers=None, trainable_regularizers=None, name=None)"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"python.modules.gated_rnn.ZoneoutWrapper.is_connected"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"python.modules.gated_rnn.ZoneoutWrapper.last_connected_subgraph"},{"location":"sonnet/#returns_712","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_604","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrappermodule_name","text":"Returns the name of the Module.","title":"python.modules.gated_rnn.ZoneoutWrapper.module_name"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrappername_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"python.modules.gated_rnn.ZoneoutWrapper.name_scopes"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrappernon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"python.modules.gated_rnn.ZoneoutWrapper.non_trainable_variables"},{"location":"sonnet/#returns_713","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_605","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperoutput_size","text":"Integer or TensorShape: size of outputs produced by this cell.","title":"python.modules.gated_rnn.ZoneoutWrapper.output_size"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperscope_name","text":"Returns the full name of the Module's variable scope.","title":"python.modules.gated_rnn.ZoneoutWrapper.scope_name"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperstate_size","text":"size(s) of state(s) used by this cell. It can be represented by an Integer, a TensorShape or a tuple of Integers or TensorShapes.","title":"python.modules.gated_rnn.ZoneoutWrapper.state_size"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrappertrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"python.modules.gated_rnn.ZoneoutWrapper.trainable_variables"},{"location":"sonnet/#returns_714","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_606","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrappervariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"python.modules.gated_rnn.ZoneoutWrapper.variable_scope"},{"location":"sonnet/#returns_715","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_607","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrappervariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"python.modules.gated_rnn.ZoneoutWrapper.variables"},{"location":"sonnet/#returns_716","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_608","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesgated_rnnzoneoutwrapperzero_statebatch_size-dtype","text":"Return zero-filled state tensor(s).","title":"python.modules.gated_rnn.ZoneoutWrapper.zero_state(batch_size, dtype)"},{"location":"sonnet/#args_363","text":"batch_size : int, float, or unit Tensor representing the batch size. dtype : the data type to use for the state.","title":"Args:"},{"location":"sonnet/#returns_717","text":"If state_size is an int or TensorShape, then the return value is a N-D tensor of shape [batch_size x state_size] filled with zeros. If state_size is a nested list or tuple, then the return value is a nested list or tuple (of the same structure) of 2-D tensors with the shapes [batch_size x s] for each s in state_size .","title":"Returns:"},{"location":"sonnet/#pythonmoduleslayer_normcreate_beta_initializer","text":"Returns a default initializer for the beta in layer norm.","title":"python.modules.layer_norm.create_beta_initializer()"},{"location":"sonnet/#pythonmoduleslayer_normcreate_gamma_initializer","text":"Returns a default initializer for the gamma in layer norm.","title":"python.modules.layer_norm.create_gamma_initializer()"},{"location":"sonnet/#pythonmodulesrnn_corewith_docfn_with_doc_to_copy","text":"Returns a decorator to copy documentation from the given function. Docstring is copied, including args and *kwargs documentation.","title":"python.modules.rnn_core.with_doc(fn_with_doc_to_copy)"},{"location":"sonnet/#args_364","text":"fn_with_doc_to_copy : Function whose docstring, including args and *kwargs documentation, is to be copied.","title":"Args:"},{"location":"sonnet/#returns_718","text":"Decorated version of wrapper_init with documentation copied from fn_with_doc_to_copy .","title":"Returns:"},{"location":"sonnet/#class-pythonmodulesspectral_normalizationspectralnormwrapper","text":"Wraps a Sonnet Module to selectively apply Spectral Normalization.","title":"class python.modules.spectral_normalization.SpectralNormWrapper"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrapper__init__module-sn_kwargs-pow_iter_collection-args-kwargs","text":"Constructs a wrapped Sonnet module with Spectral Normalization. The module expects a first argument which should be a Sonnet AbstractModule and a second argument which is a dictionary which is passed to the inner spectral_norm function as kwargs. When connecting this module to the graph,the argument 'pow_iter_collection' is treated specially for this wrapper (rather than for the _build method of the inner module). If pow_iter_collection is None (the default), the approximate first singular value for weights will not be updated based on the inputs passed at the given _build call. However an op for updating the singular value will be placed into the pow_iter_collection global collection. If pow_iter_collection is None or not passed, a control dependency on the update op will be applied to the output of the _build function. Regardless, the kwarg is deleted from the list of keywords passed to the inner module.","title":"python.modules.spectral_normalization.SpectralNormWrapper.__init__(module, sn_kwargs, pow_iter_collection, *args, **kwargs)"},{"location":"sonnet/#args_365","text":"module : A constructor/class reference for a Sonnet module you would like to construct. sn_kwargs : Keyword arguments to be passed to the spectral_norm function in addition to the weight tensor. pow_iter_collection : The name of a global collection for potentially storing ops for updating internal variables. *args : Construction-time arguments to the module. **kwargs : Construction-time keyword arguments to the module.","title":"Args:"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrapper__call__args-kwargs","text":"Add elements to the Graph, computing output Tensors from input Tensors. Subclasses must implement this method, which will be wrapped in a Template.","title":"python.modules.spectral_normalization.SpectralNormWrapper.__call__(*args, **kwargs)"},{"location":"sonnet/#args_366","text":"*args : Input Tensors. **kwargs : Additional Python flags controlling connection.","title":"Args:"},{"location":"sonnet/#returns_719","text":"output Tensor(s).","title":"Returns:"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrapperconnected_subgraphs","text":"Returns the subgraphs created by this module so far.","title":"python.modules.spectral_normalization.SpectralNormWrapper.connected_subgraphs"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrapperdefun","text":"Wraps this modules call method in a callable graph function.","title":"python.modules.spectral_normalization.SpectralNormWrapper.defun()"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrapperdefun_wrapped","text":"Returns boolean indicating whether this module is defun wrapped.","title":"python.modules.spectral_normalization.SpectralNormWrapper.defun_wrapped"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrapperget_all_variablescollectiontrainable_variables","text":"Returns all tf.Variable s used when the module is connected. See the documentation for AbstractModule._capture_variables() for more information.","title":"python.modules.spectral_normalization.SpectralNormWrapper.get_all_variables(collection='trainable_variables')"},{"location":"sonnet/#args_367","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_720","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_609","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrapperget_possible_initializer_keys","text":"Returns the keys the dictionary of variable initializers may contain. This provides the user with a way of knowing the initializer keys that are available without having to instantiate a sonnet module. Subclasses may override this class method if they need additional arguments to determine what initializer keys may be provided.","title":"python.modules.spectral_normalization.SpectralNormWrapper.get_possible_initializer_keys()"},{"location":"sonnet/#returns_721","text":"Set with strings corresponding to the strings that may be passed to the constructor.","title":"Returns:"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrapperget_variablescollectiontrainable_variables","text":"Returns tuple of tf.Variable s declared inside this module. Note that this operates by searching this module's variable scope, and so does not know about any modules that were constructed elsewhere but used inside this module. This method explicitly re-enters the Graph which this module has been connected to.","title":"python.modules.spectral_normalization.SpectralNormWrapper.get_variables(collection='trainable_variables')"},{"location":"sonnet/#args_368","text":"collection : Collection to restrict query to. By default this is tf.Graphkeys.TRAINABLE_VARIABLES , which doesn't include non-trainable variables such as moving averages.","title":"Args:"},{"location":"sonnet/#returns_722","text":"A tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_610","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrappergraph","text":"Returns the Graph instance which the module is connected to, or None.","title":"python.modules.spectral_normalization.SpectralNormWrapper.graph"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrapperis_connected","text":"Returns true iff the Module been connected to the Graph at least once.","title":"python.modules.spectral_normalization.SpectralNormWrapper.is_connected"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrapperlast_connected_subgraph","text":"Returns the last subgraph created by this module.","title":"python.modules.spectral_normalization.SpectralNormWrapper.last_connected_subgraph"},{"location":"sonnet/#returns_723","text":"The last connected subgraph.","title":"Returns:"},{"location":"sonnet/#raises_611","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrappermodule_name","text":"Returns the name of the Module.","title":"python.modules.spectral_normalization.SpectralNormWrapper.module_name"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrappername_scopes","text":"Returns a tuple of all name_scopes generated by this module.","title":"python.modules.spectral_normalization.SpectralNormWrapper.name_scopes"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrappernon_trainable_variables","text":"All non-trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"python.modules.spectral_normalization.SpectralNormWrapper.non_trainable_variables"},{"location":"sonnet/#returns_724","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_612","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrapperscope_name","text":"Returns the full name of the Module's variable scope.","title":"python.modules.spectral_normalization.SpectralNormWrapper.scope_name"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrappersn_getterspectral_norm_kwargs","text":"Returns a curried spectral normalization Custom Getter.","title":"python.modules.spectral_normalization.SpectralNormWrapper.sn_getter(spectral_norm_kwargs)"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrappertrainable_variables","text":"All trainable tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"python.modules.spectral_normalization.SpectralNormWrapper.trainable_variables"},{"location":"sonnet/#returns_725","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_613","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrappervariable_scope","text":"Returns the variable_scope declared by the module. It is valid for library users to access the internal templated variable_scope, but only makes sense to do so after connection. Therefore we raise an error here if the variable_scope is requested before connection. The only case where it does make sense to access the variable_scope before connection is to get the post-uniquification name, which we support using the separate .scope_name property.","title":"python.modules.spectral_normalization.SpectralNormWrapper.variable_scope"},{"location":"sonnet/#returns_726","text":"variable_scope : tf.VariableScope instance of the internal tf.Template .","title":"Returns:"},{"location":"sonnet/#raises_614","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectralnormwrappervariables","text":"All tf.Variable s used when the module is connected. This property does not rely on global collections and should generally be preferred vs. get_variables and get_all_variables . See the documentation for AbstractModule._capture_variables() for more information about what variables are captured.","title":"python.modules.spectral_normalization.SpectralNormWrapper.variables"},{"location":"sonnet/#returns_727","text":"A sorted (by variable name) tuple of tf.Variable objects.","title":"Returns:"},{"location":"sonnet/#raises_615","text":"NotConnectedError : If the module is not connected to the Graph.","title":"Raises:"},{"location":"sonnet/#pythonmodulesspectral_normalizationspectral_normweight-num_iters1-update_collectionnone-eps00001","text":"Spectral Weight Normalization. Applies first-singular-value spectral normalization to weight and returns a tensor equivalent to weight with spectral normalization applies. By default, it also updates an inner variable for keeping track of the spectral values of this weight matrix. If update_collection is not None, however, this function does not update the variable automatically, instead placing an op for this update in the 'update_collection' global collection.","title":"python.modules.spectral_normalization.spectral_norm(weight, num_iters=1, update_collection=None, eps=0.0001)"},{"location":"sonnet/#args_369","text":"weight : The weight tensor which requires spectral normalization num_iters : Number of SN iterations. update_collection : The update collection for assigning persisted variable u. If None, the function will update u0 during the forward pass. Otherwise if the update_collection equals 'update_collection', it will put the assignment in a collection defined by the user. Then the user will need to run the assignment explicitly. eps : numerical stability constant 0.","title":"Args:"},{"location":"sonnet/#returns_728","text":"A dictionary of: w_bar : The normalized weight tensor sigma : The estimated singular value for the weight tensor. u0 : The internal persisted variable.","title":"Returns:"},{"location":"sonnet/#pythonmodulesutilget_variable_scope_namevalue","text":"Returns the name of the variable scope indicated by the given value.","title":"python.modules.util.get_variable_scope_name(value)"},{"location":"sonnet/#args_370","text":"value : String, variable scope, or object with variable_scope attribute (e.g., Sonnet module).","title":"Args:"},{"location":"sonnet/#returns_729","text":"The name (a string) of the corresponding variable scope.","title":"Returns:"},{"location":"sonnet/#raises_616","text":"ValueError : If value does not identify a variable scope.","title":"Raises:"},{"location":"sonnet/#pythonmodulesutilname_for_callablefunc","text":"Returns a module name for a callable or None if no name can be found.","title":"python.modules.util.name_for_callable(func)"},{"location":"sonnet/#pythonmodulesutilnotify_about_new_variablescallback","text":"Calls callback(var) for all newly created variables. Callback should not modify the variable passed in. Use cases that require variables to be modified should use variable_creator_scope directly and sit within the variable creator stack. variables = [] with notify_about_variables(variables.append): ... v = tf.Variable(1.0, name='v') ... w = tf.get_variable('w', []) assert variables == [v, w]","title":"python.modules.util.notify_about_new_variables(callback)"},{"location":"sonnet/#args_371","text":"callback : a callable taking a single argument which is a tf.Variable.","title":"Args:"},{"location":"sonnet/#yields_2","text":"None - used for contextmanager API.","title":"Yields:"},{"location":"sonnet/#pythonmodulesutilsort_by_namevariables","text":"Returns a tuple of variables sorted ascending by name.","title":"python.modules.util.sort_by_name(variables)"},{"location":"sonnet/#pythonmodulesutilto_snake_casecamel_case","text":"Returns a CamelCase string as a snake_case string.","title":"python.modules.util.to_snake_case(camel_case)"}]}